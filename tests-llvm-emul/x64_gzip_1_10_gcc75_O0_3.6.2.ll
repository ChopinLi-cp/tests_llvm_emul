; ModuleID = 'pp-dasm output'
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"

%_TYPEDEF_sigset_t = type { [1 x i32] }
%timespec = type { i64, i32 }
%tm = type { i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i8* }
%sigaction = type { void (i32)*, i32, void ()*, %_TYPEDEF_sigset_t }
%_IO_FILE = type { i32 }
%__dirstream = type { i32 }
%stat = type { i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32 }
%option = type { i8*, i32, i32*, i32 }
%timeval = type { i64, i64 }
%dirent = type { i32, i32, i16, i8, [256 x i8] }
%z_stream_s = type { i64*, i32, i32, i8*, i32, i32, i8*, %internal_state*, i64* (i64*, i32, i32)*, void (i64*, i64*)*, i64*, i32, i32, i32 }
%internal_state = type { i32 }

@assembly_address = internal global i64 0
@cf = internal global i1 false
@pf = internal global i1 false
@az = internal global i1 false
@zf = internal global i1 false
@sf = internal global i1 false
@tf = internal global i1 false
@if = internal global i1 false
@df = internal global i1 false
@of = internal global i1 false
@iopl = internal global i2 0
@nt = internal global i1 false
@rf = internal global i1 false
@vm = internal global i1 false
@ac = internal global i1 false
@vif = internal global i1 false
@vip = internal global i1 false
@id = internal global i1 false
@rflags = internal global i64 0
@ss = internal global i16 0
@cs = internal global i16 0
@ds = internal global i16 0
@es = internal global i16 0
@fs = internal global i16 0
@gs = internal global i16 0
@st0 = internal global x86_fp80 0xK00000000000000000000
@st1 = internal global x86_fp80 0xK00000000000000000000
@st2 = internal global x86_fp80 0xK00000000000000000000
@st3 = internal global x86_fp80 0xK00000000000000000000
@st4 = internal global x86_fp80 0xK00000000000000000000
@st5 = internal global x86_fp80 0xK00000000000000000000
@st6 = internal global x86_fp80 0xK00000000000000000000
@st7 = internal global x86_fp80 0xK00000000000000000000
@fpu_stat_IE = internal global i1 false
@fpu_stat_DE = internal global i1 false
@fpu_stat_ZE = internal global i1 false
@fpu_stat_OE = internal global i1 false
@fpu_stat_UE = internal global i1 false
@fpu_stat_PE = internal global i1 false
@fpu_stat_SF = internal global i1 false
@fpu_stat_ES = internal global i1 false
@fpu_stat_C0 = internal global i1 false
@fpu_stat_C1 = internal global i1 false
@fpu_stat_C2 = internal global i1 false
@fpu_stat_C3 = internal global i1 false
@fpu_stat_TOP = internal global i3 0
@fpu_stat_B = internal global i1 false
@fpu_control_IM = internal global i1 false
@fpu_control_DM = internal global i1 false
@fpu_control_ZM = internal global i1 false
@fpu_control_OM = internal global i1 false
@fpu_control_UM = internal global i1 false
@fpu_control_PM = internal global i1 false
@fpu_control_PC = internal global i2 0
@fpu_control_RC = internal global i2 0
@fpu_control_X = internal global i1 false
@fpu_tag_0 = internal global i2 0
@fpu_tag_1 = internal global i2 0
@fpu_tag_2 = internal global i2 0
@fpu_tag_3 = internal global i2 0
@fpu_tag_4 = internal global i2 0
@fpu_tag_5 = internal global i2 0
@fpu_tag_6 = internal global i2 0
@fpu_tag_7 = internal global i2 0
@fp0 = internal global double 0.000000e+00
@fp1 = internal global double 0.000000e+00
@fp2 = internal global double 0.000000e+00
@fp3 = internal global double 0.000000e+00
@fp4 = internal global double 0.000000e+00
@fp5 = internal global double 0.000000e+00
@fp6 = internal global double 0.000000e+00
@fp7 = internal global double 0.000000e+00
@k0 = internal global i64 0
@k1 = internal global i64 0
@k2 = internal global i64 0
@k3 = internal global i64 0
@k4 = internal global i64 0
@k5 = internal global i64 0
@k6 = internal global i64 0
@k7 = internal global i64 0
@mm0 = internal global i64 0
@mm1 = internal global i64 0
@mm2 = internal global i64 0
@mm3 = internal global i64 0
@mm4 = internal global i64 0
@mm5 = internal global i64 0
@mm6 = internal global i64 0
@mm7 = internal global i64 0
@xmm0 = internal global i128 0
@xmm1 = internal global i128 0
@xmm2 = internal global i128 0
@xmm3 = internal global i128 0
@xmm4 = internal global i128 0
@xmm5 = internal global i128 0
@xmm6 = internal global i128 0
@xmm7 = internal global i128 0
@xmm8 = internal global i128 0
@xmm9 = internal global i128 0
@xmm10 = internal global i128 0
@xmm11 = internal global i128 0
@xmm12 = internal global i128 0
@xmm13 = internal global i128 0
@xmm14 = internal global i128 0
@xmm15 = internal global i128 0
@xmm16 = internal global i128 0
@xmm17 = internal global i128 0
@xmm18 = internal global i128 0
@xmm19 = internal global i128 0
@xmm20 = internal global i128 0
@xmm21 = internal global i128 0
@xmm22 = internal global i128 0
@xmm23 = internal global i128 0
@xmm24 = internal global i128 0
@xmm25 = internal global i128 0
@xmm26 = internal global i128 0
@xmm27 = internal global i128 0
@xmm28 = internal global i128 0
@xmm29 = internal global i128 0
@xmm30 = internal global i128 0
@xmm31 = internal global i128 0
@ymm0 = internal global i256 0
@ymm1 = internal global i256 0
@ymm2 = internal global i256 0
@ymm3 = internal global i256 0
@ymm4 = internal global i256 0
@ymm5 = internal global i256 0
@ymm6 = internal global i256 0
@ymm7 = internal global i256 0
@ymm8 = internal global i256 0
@ymm9 = internal global i256 0
@ymm10 = internal global i256 0
@ymm11 = internal global i256 0
@ymm12 = internal global i256 0
@ymm13 = internal global i256 0
@ymm14 = internal global i256 0
@ymm15 = internal global i256 0
@ymm16 = internal global i256 0
@ymm17 = internal global i256 0
@ymm18 = internal global i256 0
@ymm19 = internal global i256 0
@ymm20 = internal global i256 0
@ymm21 = internal global i256 0
@ymm22 = internal global i256 0
@ymm23 = internal global i256 0
@ymm24 = internal global i256 0
@ymm25 = internal global i256 0
@ymm26 = internal global i256 0
@ymm27 = internal global i256 0
@ymm28 = internal global i256 0
@ymm29 = internal global i256 0
@ymm30 = internal global i256 0
@ymm31 = internal global i256 0
@zmm0 = internal global i512 0
@zmm1 = internal global i512 0
@zmm2 = internal global i512 0
@zmm3 = internal global i512 0
@zmm4 = internal global i512 0
@zmm5 = internal global i512 0
@zmm6 = internal global i512 0
@zmm7 = internal global i512 0
@zmm8 = internal global i512 0
@zmm9 = internal global i512 0
@zmm10 = internal global i512 0
@zmm11 = internal global i512 0
@zmm12 = internal global i512 0
@zmm13 = internal global i512 0
@zmm14 = internal global i512 0
@zmm15 = internal global i512 0
@zmm16 = internal global i512 0
@zmm17 = internal global i512 0
@zmm18 = internal global i512 0
@zmm19 = internal global i512 0
@zmm20 = internal global i512 0
@zmm21 = internal global i512 0
@zmm22 = internal global i512 0
@zmm23 = internal global i512 0
@zmm24 = internal global i512 0
@zmm25 = internal global i512 0
@zmm26 = internal global i512 0
@zmm27 = internal global i512 0
@zmm28 = internal global i512 0
@zmm29 = internal global i512 0
@zmm30 = internal global i512 0
@zmm31 = internal global i512 0
@dr0 = internal global i64 0
@dr1 = internal global i64 0
@dr2 = internal global i64 0
@dr3 = internal global i64 0
@dr4 = internal global i64 0
@dr5 = internal global i64 0
@dr6 = internal global i64 0
@dr7 = internal global i64 0
@dr8 = internal global i64 0
@dr9 = internal global i64 0
@dr10 = internal global i64 0
@dr11 = internal global i64 0
@dr12 = internal global i64 0
@dr13 = internal global i64 0
@dr14 = internal global i64 0
@dr15 = internal global i64 0
@cr0 = internal global i64 0
@cr1 = internal global i64 0
@cr2 = internal global i64 0
@cr3 = internal global i64 0
@cr4 = internal global i64 0
@cr5 = internal global i64 0
@cr6 = internal global i64 0
@cr7 = internal global i64 0
@cr8 = internal global i64 0
@cr9 = internal global i64 0
@cr10 = internal global i64 0
@cr11 = internal global i64 0
@cr12 = internal global i64 0
@cr13 = internal global i64 0
@cr14 = internal global i64 0
@cr15 = internal global i64 0
@fpsw = internal global i64 0
@rax = internal global i64 0
@rcx = internal global i64 0
@rdx = internal global i64 0
@rbx = internal global i64 0
@rsp = internal global i64 0
@rbp = internal global i64 0
@rsi = internal global i64 0
@rdi = internal global i64 0
@r8 = internal global i64 0
@r9 = internal global i64 0
@r10 = internal global i64 0
@r11 = internal global i64 0
@r12 = internal global i64 0
@r13 = internal global i64 0
@r14 = internal global i64 0
@r15 = internal global i64 0
@rip = internal global i64 0
@riz = internal global i64 0
@global_var_215fd0 = global i64 0
@global_var_215d58 = global i64 0
@global_var_215d60 = global i64 0
@global_var_216560 = global i64 0
@global_var_215fc0 = global i64 0
@global_var_215fd8 = global i64 0
@0 = global i64 0
@global_var_215fe0 = global i64 0
@global_var_216008 = external global i64
@1 = global i64 0
@2 = global i64 0
@3 = global i64 0
@global_var_21a420 = global i64 0
@global_var_25f4e0 = global i64 0
@4 = global i64 0
@global_var_24a8a0 = global i64 0
@global_var_10d84 = constant [15 x i8] c"bad pack level\00"
@5 = global i64 0
@global_var_22a880 = global i64 0
@global_var_2165b8 = global i64 0
@6 = global i64 0
@global_var_216042 = global i64 1125899906842624
@7 = global i64 0
@global_var_216040 = global i64 0
@8 = global i64 0
@global_var_216044 = global i64 1125917086711808
@9 = global i64 0
@global_var_216046 = global i64 2251816993816576
@10 = global i64 0
@global_var_21a428 = global i64 0
@global_var_21a430 = global i64 0
@global_var_24f4c0 = global i64 0
@11 = global i64 0
@12 = global i64 0
@13 = global i64 0
@14 = global i64 0
@global_var_21a52a = global i64 0
@global_var_21a880 = global i64 0
@15 = global i64 0
@16 = global i64 65536
@global_var_2574c0 = global i64 0
@global_var_1000 = global [2 x i8] c"\08\00"
@17 = global i64 0
@global_var_21e598 = global i64 0
@18 = global i64 0
@19 = global i64 0
@20 = global i64 0
@21 = global i64 0
@22 = global i64 0
@global_var_10fc8 = constant [39 x i8] c"Try `%s --help' for more information.\0A\00"
@global_var_11b08 = constant [68 x i8] c"Compress or uncompress FILEs (by default, compress FILES in-place).\00"
@23 = constant i64 8243311783451128320
@global_var_11b50 = constant [73 x i8] c"Mandatory arguments to long options are mandatory for short options too.\00"
@global_var_11ba0 = constant [76 x i8] c"  -c, --stdout      write on standard output, keep original files unchanged\00"
@global_var_11bf0 = constant [31 x i8] c"  -d, --decompress  decompress\00"
@global_var_11c10 = constant [70 x i8] c"  -f, --force       force overwrite of output file and compress links\00"
@global_var_11c58 = constant [35 x i8] c"  -h, --help        give this help\00"
@global_var_11c80 = constant [52 x i8] c"  -k, --keep        keep (don't delete) input files\00"
@global_var_11cb8 = constant [50 x i8] c"  -l, --list        list compressed file contents\00"
@global_var_11cf0 = constant [45 x i8] c"  -L, --license     display software license\00"
@global_var_11d20 = constant [75 x i8] c"  -n, --no-name     do not save or restore the original name and timestamp\00"
@global_var_11d70 = constant [68 x i8] c"  -N, --name        save or restore the original name and timestamp\00"
@global_var_11db8 = constant [42 x i8] c"  -q, --quiet       suppress all warnings\00"
@global_var_11de8 = constant [55 x i8] c"  -r, --recursive   operate recursively on directories\00"
@global_var_11e20 = constant [48 x i8] c"      --rsyncable   make rsync-friendly archive\00"
@global_var_11e50 = constant [55 x i8] c"  -S, --suffix=SUF  use suffix SUF on compressed files\00"
@global_var_11e88 = constant [77 x i8] c"      --synchronous synchronous output (safer if system crashes, but slower)\00"
@global_var_11ed8 = constant [51 x i8] c"  -t, --test        test compressed file integrity\00"
@global_var_11f10 = constant [33 x i8] c"  -v, --verbose     verbose mode\00"
@global_var_11f38 = constant [43 x i8] c"  -V, --version     display version number\00"
@global_var_11f68 = constant [36 x i8] c"  -1, --fast        compress faster\00"
@global_var_11f90 = constant [36 x i8] c"  -9, --best        compress better\00"
@global_var_11fb8 = constant [54 x i8] c"With no FILE, or when FILE is -, read standard input.\00"
@global_var_11ff0 = constant [35 x i8] c"Report bugs to <bug-gzip@gnu.org>.\00"
@global_var_215a20 = global [27 x i8*] [i8* getelementptr inbounds ([68 x i8]* @global_var_11b08, i32 0, i32 0), i8* bitcast (i64* @global_var_114ff to i8*), i8* getelementptr inbounds ([73 x i8]* @global_var_11b50, i32 0, i32 0), i8* bitcast (i64* @global_var_114ff to i8*), i8* getelementptr inbounds ([76 x i8]* @global_var_11ba0, i32 0, i32 0), i8* getelementptr inbounds ([31 x i8]* @global_var_11bf0, i32 0, i32 0), i8* getelementptr inbounds ([70 x i8]* @global_var_11c10, i32 0, i32 0), i8* getelementptr inbounds ([35 x i8]* @global_var_11c58, i32 0, i32 0), i8* getelementptr inbounds ([52 x i8]* @global_var_11c80, i32 0, i32 0), i8* getelementptr inbounds ([50 x i8]* @global_var_11cb8, i32 0, i32 0), i8* getelementptr inbounds ([45 x i8]* @global_var_11cf0, i32 0, i32 0), i8* getelementptr inbounds ([75 x i8]* @global_var_11d20, i32 0, i32 0), i8* getelementptr inbounds ([68 x i8]* @global_var_11d70, i32 0, i32 0), i8* getelementptr inbounds ([42 x i8]* @global_var_11db8, i32 0, i32 0), i8* getelementptr inbounds ([55 x i8]* @global_var_11de8, i32 0, i32 0), i8* getelementptr inbounds ([48 x i8]* @global_var_11e20, i32 0, i32 0), i8* getelementptr inbounds ([55 x i8]* @global_var_11e50, i32 0, i32 0), i8* getelementptr inbounds ([77 x i8]* @global_var_11e88, i32 0, i32 0), i8* getelementptr inbounds ([51 x i8]* @global_var_11ed8, i32 0, i32 0), i8* getelementptr inbounds ([33 x i8]* @global_var_11f10, i32 0, i32 0), i8* getelementptr inbounds ([43 x i8]* @global_var_11f38, i32 0, i32 0), i8* getelementptr inbounds ([36 x i8]* @global_var_11f68, i32 0, i32 0), i8* getelementptr inbounds ([36 x i8]* @global_var_11f90, i32 0, i32 0), i8* bitcast (i64* @global_var_114ff to i8*), i8* getelementptr inbounds ([54 x i8]* @global_var_11fb8, i32 0, i32 0), i8* bitcast (i64* @global_var_114ff to i8*), i8* getelementptr inbounds ([35 x i8]* @global_var_11ff0, i32 0, i32 0)]
@global_var_10ff0 = constant [33 x i8] c"Usage: %s [OPTION]... [FILE]...\0A\00"
@global_var_10da0 = constant [50 x i8] c"Copyright (C) 2018 Free Software Foundation, Inc.\00"
@global_var_10dd8 = constant [37 x i8] c"Copyright (C) 1993 Jean-loup Gailly.\00"
@global_var_10e00 = constant [77 x i8] c"This is free software.  You may redistribute copies of it under the terms of\00"
@global_var_10e50 = constant [72 x i8] c"the GNU General Public License <https://www.gnu.org/licenses/gpl.html>.\00"
@global_var_10e98 = constant [54 x i8] c"There is NO WARRANTY, to the extent permitted by law.\00"
@global_var_215680 = global [5 x i8*] [i8* getelementptr inbounds ([50 x i8]* @global_var_10da0, i32 0, i32 0), i8* getelementptr inbounds ([37 x i8]* @global_var_10dd8, i32 0, i32 0), i8* getelementptr inbounds ([77 x i8]* @global_var_10e00, i32 0, i32 0), i8* getelementptr inbounds ([72 x i8]* @global_var_10e50, i32 0, i32 0), i8* getelementptr inbounds ([54 x i8]* @global_var_10e98, i32 0, i32 0)]
@global_var_12d11 = constant [5 x i8] c"1.10\00"
@global_var_216558 = global [5 x i8]* @global_var_12d11
@global_var_11011 = constant [7 x i8] c"%s %s\0A\00"
@global_var_11018 = constant [29 x i8] c"Written by Jean-loup Gailly.\00"
@global_var_11035 = constant [5 x i8] c"%s: \00"
@24 = global i64 0
@global_var_1103a = constant [5 x i8] c".exe\00"
@global_var_1103f = constant [5 x i8] c"GZIP\00"
@global_var_216628 = global i64 0
@global_var_11044 = constant [4 x i8] c".gz\00"
@25 = global i64 0
@26 = global i64 0
@27 = global i64 0
@28 = constant i64 11565
@global_var_10f02 = constant [6 x i8] c"ascii\00"
@global_var_2156c0 = global [6 x i8]* @global_var_10f02
@global_var_10ee0 = constant [34 x i8] c"ab:cdfhH?klLmMnNqrS:tvVZ123456789\00"
@global_var_11050 = constant [49 x i8] c"%s: %s: non-option in GZIP environment variable\0A\00"
@29 = global i64 0
@global_var_11088 = constant [78 x i8] c"%s: warning: GZIP environment variable is deprecated; use an alias or script\0A\00"
@30 = global i64 73014444031
@global_var_111ac = constant i64 -227822245302069
@31 = global i64 0
@32 = global i64 0
@33 = global i64 34359738384
@global_var_110d8 = constant [34 x i8] c"%s: -b operand is not an integer\0A\00"
@34 = global i64 0
@35 = global i64 0
@36 = global i64 0
@37 = global i64 0
@38 = global i64 0
@39 = global i64 0
@40 = global i64 -1
@41 = global i64 0
@42 = global i64 0
@43 = global i64 0
@44 = global i64 0
@45 = global i64 0
@46 = global i64 0
@global_var_11100 = constant [38 x i8] c"%s: -Z not supported in this version\0A\00"
@47 = global i64 -4294967290
@global_var_11126 = constant [6 x i8] c"-%c: \00"
@global_var_1112c = constant [7 x i8] c"--%s: \00"
@global_var_11138 = constant [47 x i8] c"option not valid in GZIP environment variable\0A\00"
@global_var_11168 = constant [43 x i8] c"%s: option --ascii ignored on this system\0A\00"
@global_var_11193 = constant [25 x i8] c"%s: invalid suffix '%s'\0A\00"
@48 = global i64 0
@global_var_2160d0 = global i64 58548
@49 = global i64 0
@50 = global i64 0
@51 = global i64 0
@52 = global i64 0
@global_var_21a464 = global i64 0
@53 = global i64 0
@54 = global i64 0
@55 = global i64 0
@global_var_24a890 = global i64 0
@global_var_25f4d8 = global i64 0
@56 = global i64 0
@global_var_216f30 = global i64 0
@57 = global i64 0
@58 = global i64 0
@global_var_114fc = constant [3 x i8] c"de\00"
@global_var_11500 = constant [10 x i8] c"read from\00"
@global_var_1150a = constant [11 x i8] c"written to\00"
@global_var_11518 = constant [93 x i8] c"%s: compressed data not %s a terminal. Use -f to force %scompression.\0AFor help, type: %s -h\0A\00"
@59 = global i64 0
@global_var_24f0c4 = global i64 0
@global_var_24f0c6 = global i64 0
@global_var_11575 = constant [15 x i8] c"standard input\00"
@60 = global i64 0
@61 = global i64 0
@global_var_21609c = global [2 x i8] c"\08\00"
@62 = global i64 0
@global_var_11584 = constant [5 x i8] c" OK\0A\00"
@63 = global i64 0
@global_var_267540 = global i64 0
@global_var_11589 = constant [3 x i8] c".-\00"
@global_var_216b00 = global i64 0
@64 = global i64 4294967295
@global_var_1158a = constant [2 x i8] c"-\00"
@global_var_11590 = constant [34 x i8] c"%s: %s is a directory -- ignored\0A\00"
@global_var_115b8 = constant [55 x i8] c"%s: %s is not a directory or a regular file - ignored\0A\00"
@65 = global i64 0
@global_var_115f0 = constant [46 x i8] c"%s: %s is set-user-ID on execution - ignored\0A\00"
@66 = global i64 0
@global_var_11620 = constant [47 x i8] c"%s: %s is set-group-ID on execution - ignored\0A\00"
@global_var_11650 = constant [46 x i8] c"%s: %s has the sticky bit set - file ignored\0A\00"
@global_var_216f10 = global i64 0
@global_var_11680 = constant [42 x i8] c"%s: %s has %lu other link%c -- unchanged\0A\00"
@67 = global i64 0
@68 = global i64 0
@global_var_116aa = constant [25 x i8] c"%s: %s compressed to %s\0A\00"
@global_var_116c3 = constant [5 x i8] c"%s:\09\00"
@69 = global i64 0
@70 = global i64 -1
@global_var_116c8 = constant [4 x i8] c" OK\00"
@global_var_116cc = constant [8 x i8] c"created\00"
@global_var_116d4 = constant [14 x i8] c"replaced with\00"
@global_var_116e2 = constant [10 x i8] c" -- %s %s\00"
@global_var_2166e0 = global i64 0
@global_var_116f0 = constant [33 x i8] c"%s: %s: warning, name truncated\0A\00"
@global_var_12013 = constant [3 x i8] c".z\00"
@global_var_11758 = constant [5 x i8] c".taz\00"
@global_var_11753 = constant [5 x i8] c".tgz\00"
@global_var_12016 = constant [4 x i8] c"-gz\00"
@global_var_1201a = constant [3 x i8] c"-z\00"
@global_var_1201d = constant [3 x i8] c"_z\00"
@global_var_2160e8 = global [7 x i8*] [i8* getelementptr inbounds ([4 x i8]* @global_var_11044, i32 0, i32 0), i8* getelementptr inbounds ([3 x i8]* @global_var_12013, i32 0, i32 0), i8* getelementptr inbounds ([5 x i8]* @global_var_11758, i32 0, i32 0), i8* getelementptr inbounds ([5 x i8]* @global_var_11753, i32 0, i32 0), i8* getelementptr inbounds ([4 x i8]* @global_var_12016, i32 0, i32 0), i8* getelementptr inbounds ([3 x i8]* @global_var_1201a, i32 0, i32 0), i8* getelementptr inbounds ([3 x i8]* @global_var_1201d, i32 0, i32 0)]
@global_var_2160e0 = global i64 0
@global_var_216140 = global i64 0
@71 = global i64 77309411935
@global_var_12020 = constant [3 x i8] c".Z\00"
@global_var_216148 = global [4 x i8*] [i8* getelementptr inbounds ([4 x i8]* @global_var_11044, i32 0, i32 0), i8* getelementptr inbounds ([3 x i8]* @global_var_12013, i32 0, i32 0), i8* getelementptr inbounds ([3 x i8]* @global_var_1201a, i32 0, i32 0), i8* getelementptr inbounds ([3 x i8]* @global_var_12020, i32 0, i32 0)]
@global_var_11711 = constant [28 x i8] c"%s: %s: file name too long\0A\00"
@global_var_11730 = constant [35 x i8] c"%s: %s: unknown suffix -- ignored\0A\00"
@global_var_11760 = constant [43 x i8] c"%s: %s already has %s suffix -- unchanged\0A\00"
@72 = global i64 0
@global_var_25f500 = global i64 0
@73 = global i64 0
@global_var_1178b = constant i64 679124962079
@global_var_1178e = constant i64 40479
@global_var_11798 = constant [44 x i8] c"%s: %s: unknown method %d -- not supported\0A\00"
@global_var_117c8 = constant [38 x i8] c"%s: %s is encrypted -- not supported\0A\00"
@global_var_117f0 = constant [40 x i8] c"%s: %s has flags 0x%x -- not supported\0A\00"
@global_var_11818 = constant [50 x i8] c"%s: %s: MTIME %lu out of range for this platform\0A\00"
@global_var_11850 = constant [41 x i8] c"%s: %s: extra field of %u bytes ignored\0A\00"
@global_var_11880 = constant [39 x i8] c"corrupted input -- file name too large\00"
@global_var_118a8 = constant [60 x i8] c"%s: %s: header checksum 0x%04x != computed checksum 0x%04x\0A\00"
@74 = global i64 0
@global_var_118e4 = constant i64 8478334229105488
@global_var_118e9 = constant i64 -6908802628533084641
@global_var_118ec = constant i64 2668946416885341471
@global_var_118ef = constant i64 2322295160781709343
@global_var_118f2 = constant [29 x i8] c"\0A%s: %s: not in gzip format\0A\00"
@global_var_11910 = constant [56 x i8] c"\0A%s: %s: decompression OK, trailing zero bytes ignored\0A\00"
@global_var_11948 = constant [53 x i8] c"\0A%s: %s: decompression OK, trailing garbage ignored\0A\00"
@75 = global i64 1
@global_var_1197d = constant [29 x i8] c"method  crc     date  time  \00"
@global_var_119ce = constant [13 x i8] c"uncompressed\00"
@global_var_1199a = constant [11 x i8] c"compressed\00"
@global_var_119a8 = constant [38 x i8] c"%*.*s %*.*s  ratio uncompressed_name\0A\00"
@global_var_216ae8 = global i64 0
@global_var_216af0 = global i64 0
@global_var_119db = constant [29 x i8] c"                            \00"
@global_var_119f8 = constant [10 x i8] c" (totals)\00"
@global_var_12023 = constant [6 x i8] c"store\00"
@global_var_12029 = constant [6 x i8] c"compr\00"
@global_var_1202f = constant [6 x i8] c"pack \00"
@global_var_12035 = constant [6 x i8] c"lzh  \00"
@global_var_1203b = constant [6 x i8] c"defla\00"
@global_var_215b00 = global [9 x i8*] [i8* getelementptr inbounds ([6 x i8]* @global_var_12023, i32 0, i32 0), i8* getelementptr inbounds ([6 x i8]* @global_var_12029, i32 0, i32 0), i8* getelementptr inbounds ([6 x i8]* @global_var_1202f, i32 0, i32 0), i8* getelementptr inbounds ([6 x i8]* @global_var_12035, i32 0, i32 0), i8* bitcast (i64* @global_var_114ff to i8*), i8* bitcast (i64* @global_var_114ff to i8*), i8* bitcast (i64* @global_var_114ff to i8*), i8* bitcast (i64* @global_var_114ff to i8*), i8* getelementptr inbounds ([6 x i8]* @global_var_1203b, i32 0, i32 0)]
@global_var_11a02 = constant [11 x i8] c"%5s %08lx \00"
@global_var_12060 = constant [4 x i8] c"Jan\00"
@global_var_11a0d = constant [17 x i8] c"%s%3d %02d:%02d \00"
@global_var_11a1e = constant [14 x i8] c"??? ?? ??:?? \00"
@global_var_11a2c = constant [5 x i8] c" %s\0A\00"
@global_var_11a31 = constant [15 x i8] c"name too short\00"
@global_var_11a40 = constant [22 x i8] c"can't recover suffix\0A\00"
@global_var_11a56 = constant [5 x i8] c".tar\00"
@global_var_11a5b = constant [2 x i8] c".\00"
@global_var_11a60 = constant [31 x i8] c"internal error in shorten_name\00"
@global_var_11a7f = constant [23 x i8] c"%s: %s already exists;\00"
@76 = global i64 0
@global_var_11a98 = constant [37 x i8] c" do you wish to overwrite (y or n)? \00"
@global_var_11abd = constant [18 x i8] c"\09not overwritten\0A\00"
@global_var_11acf = constant [24 x i8] c"%s: timestamp restored\0A\00"
@global_var_11ae7 = constant [3 x i8] c"..\00"
@global_var_11aea = constant [30 x i8] c"%s: %s/%s: pathname too long\0A\00"
@global_var_2160b0 = global i64 4294967298
@global_var_216648 = global i64 0
@global_var_216650 = global i64 0
@global_var_216658 = global i64 0
@global_var_216660 = global i64 0
@global_var_216668 = global i64 0
@global_var_216670 = global i64 0
@global_var_216678 = global i64 0
@global_var_216680 = global i64 0
@global_var_216688 = global i64 0
@global_var_216690 = global i64 0
@global_var_216698 = global i64 0
@global_var_2166a0 = global i64 0
@global_var_2166a8 = global i64 0
@global_var_2166b0 = global i64 0
@global_var_2166b8 = global i64 0
@77 = global i64 0
@78 = global i64 0
@79 = global i64 0
@80 = global i64 0
@81 = global i64 0
@global_var_2162e0 = global i64 1970337721942016
@global_var_216220 = global i64 0
@global_var_2161e0 = global i64 1688871335362563
@global_var_2162a0 = global i64 0
@global_var_216260 = global i64 1125912791875585
@global_var_216180 = global i64 73014444048
@global_var_216304 = global [2 x i8] c"\09\00"
@82 = global i64 6
@83 = global i64 0
@global_var_12090 = constant [44 x i8] c"output in compress .Z format not supported\0A\00"
@84 = global i64 0
@global_var_219ef8 = global i64 0
@global_var_219ee8 = global i64 0
@85 = global i64 0
@86 = global i64 0
@global_var_218dc0 = global i64 0
@global_var_218ac0 = global i64 0
@global_var_216320 = global i64 0
@global_var_218e40 = global i64 0
@global_var_218bc0 = global i64 0
@global_var_2163a0 = global i64 0
@global_var_217f60 = global i64 0
@global_var_2179c2 = global i64 0
@87 = global i64 0
@88 = global i64 0
@89 = global i64 0
@90 = global i64 0
@91 = global i64 0
@92 = global i64 0
@global_var_2179c0 = global i64 0
@global_var_217e40 = global i64 0
@global_var_216fc0 = global i64 0
@global_var_2178c0 = global i64 0
@global_var_217ec0 = global i64 0
@global_var_2173c0 = global i64 0
@global_var_219ed8 = global i64 0
@global_var_219ed0 = global i64 0
@93 = global i64 0
@94 = global i64 0
@95 = global i64 0
@96 = global i64 0
@97 = global i64 0
@global_var_217f80 = global i64 0
@98 = global i64 0
@global_var_218880 = global i64 0
@99 = global i64 0
@100 = global i64 0
@global_var_23c = constant [24 x i8] c"64/ld-linux-x86-64.so.2\00"
@global_var_23d = constant [23 x i8] c"4/ld-linux-x86-64.so.2\00"
@101 = global i64 0
@102 = global i64 0
@103 = global i64 0
@104 = global i64 0
@105 = global i64 0
@106 = global i64 0
@107 = global i64 0
@108 = global i64 0
@109 = global i64 0
@global_var_217ec2 = global i64 0
@110 = global i64 0
@111 = global i64 0
@112 = global i64 0
@113 = global i64 0
@114 = global i64 0
@global_var_216520 = global i64* @global_var_217ec0
@global_var_216470 = global i64 434886569960280336
@global_var_218ec0 = global i64 0
@global_var_2164a0 = external global i64
@global_var_2164e0 = external global i64
@115 = global i64 0
@116 = global i64 0
@117 = global i64 0
@118 = global i64 0
@global_var_23a880 = global i64 0
@119 = global i64 0
@global_var_fff = global i64* @global_var_800
@120 = global i64 0
@121 = global i64 0
@122 = global i64 0
@global_var_120bc = constant [11 x i8] c"Bad table\0A\00"
@global_var_219f00 = global i64 0
@global_var_219f40 = global i64 0
@123 = global i64 0
@124 = global i64 0
@125 = global i64 0
@126 = global i64 0
@127 = global i64 0
@128 = global i64 128
@global_var_120c8 = constant [38 x i8] c"\0A%s: %s: warning, unknown flags 0x%x\0A\00"
@global_var_120f0 = constant [59 x i8] c"\0A%s: %s: compressed with %d bits, can only handle %d bits\0A\00"
@global_var_1212b = constant [15 x i8] c"corrupt input.\00"
@global_var_24a87e = global i64 0
@global_var_12140 = constant [46 x i8] c"corrupt input. Use zcat to recover some data.\00"
@global_var_12170 = constant [50 x i8] c"invalid compressed data -- unexpected end of file\00"
@global_var_21a160 = global i64 0
@129 = global i64 0
@global_var_121a8 = constant [64 x i8] c"invalid compressed data -- Huffman code bit length out of range\00"
@global_var_21a300 = global i64 0
@global_var_121e8 = constant [32 x i8] c"too many leaves in Huffman tree\00"
@global_var_21a280 = global i64 0
@global_var_21a180 = global i64 0
@global_var_21a380 = global i64 0
@global_var_12208 = constant [31 x i8] c"too few leaves in Huffman tree\00"
@130 = global i64 0
@131 = global i64 0
@132 = global i64 0
@133 = global i64 0
@global_var_12228 = constant [38 x i8] c"invalid compressed data--length error\00"
@global_var_12250 = constant [31 x i8] c"\0A%s: %s: not a valid zip file\0A\00"
@global_var_12270 = constant [58 x i8] c"\0A%s: %s: first entry not deflated or stored -- use unzip\0A\00"
@134 = global i64 0
@global_var_122b0 = constant [38 x i8] c"\0A%s: %s: encrypted file -- use unzip\0A\00"
@135 = global i64 0
@136 = global i64 0
@137 = global i64 0
@138 = global i64 0
@139 = global i64 0
@140 = global i64 0
@141 = global i64 0
@142 = global i64 0
@143 = global i64 0
@global_var_122d8 = constant [41 x i8] c"invalid compressed data--format violated\00"
@144 = global i64 0
@145 = global i64 0
@146 = global i64 0
@147 = global i64 0
@global_var_12301 = constant [18 x i8] c"len %lu, siz %lu\0A\00"
@global_var_12318 = constant [41 x i8] c"invalid compressed data--length mismatch\00"
@global_var_12348 = constant [31 x i8] c"internal error, invalid method\00"
@global_var_12368 = constant [45 x i8] c"\0A%s: %s: invalid compressed data--crc error\0A\00"
@global_var_12398 = constant [48 x i8] c"\0A%s: %s: invalid compressed data--length error\0A\00"
@global_var_24a886 = global i64 0
@global_var_24a887 = global i64 0
@global_var_123c8 = constant [46 x i8] c"%s: %s has more than one entry--rest ignored\0A\00"
@global_var_123f8 = constant [45 x i8] c"%s: %s has more than one entry -- unchanged\0A\00"
@148 = global i64 4294967295
@global_var_12440 = constant i64 0
@global_var_12c40 = constant [3 x i8] c" \09\00"
@global_var_12c43 = constant [13 x i8] c"\0A%s: %s: %s\0A\00"
@global_var_12c50 = constant [23 x i8] c"\0A%s: memory_exhausted\0A\00"
@global_var_12c67 = constant [21 x i8] c"%s: %s: warning: %s\0A\00"
@global_var_12c7c = constant [6 x i8] c"\0A%s: \00"
@global_var_12c82 = constant [28 x i8] c"%s: unexpected end of file\0A\00"
@global_var_12ca8 = constant i64 4636737291354636288
@global_var_12cb0 = constant i64 0
@global_var_12c9e = constant [8 x i8] c"%5.1f%%\00"
@global_var_12cb8 = constant [44 x i8] c"file timestamp out of range for gzip format\00"
@149 = global i64 0
@global_var_12ce8 = constant [41 x i8] c"%s: %s: file size changed while zipping\0A\00"
@global_var_215b50 = global i64 0
@150 = global i64 0
@151 = global i64 0
@152 = global i64 19791209304576
@153 = global i64 5066549581971456
@154 = global i64 301989888
@155 = global i64 0
@global_var_215670 = global i64 9568
@global_var_215678 = global i64 9504
@global_var_216588 = global i64 0
@global_var_21658c = global i64 0
@global_var_216590 = global i64 0
@global_var_216594 = global i64 0
@global_var_25f4e1 = global i64 0
@global_var_2165a8 = global i64 0
@global_var_2165b0 = global i64 0
@global_var_2165a4 = global i64 0
@global_var_21a438 = global i64 0
@global_var_2165c0 = global i64 0
@global_var_21a43c = global i64 0
@global_var_2165a0 = global i64 0
@global_var_21659c = global i64 0
@global_var_216598 = global i64 0
@global_var_21a440 = global i64 0
@global_var_21a444 = global i64 0
@global_var_216020 = global i64 65536
@global_var_21a42a = global i64 0
@global_var_2165f4 = global i64 0
@global_var_21a429 = global i64 0
@global_var_21a441 = global i64 0
@global_var_25f4c8 = global i64 0
@global_var_216580 = global i64 0
@global_var_114ff = constant i64 8243311783451128320
@global_var_2165f0 = global i64 0
@global_var_216630 = global i64 0
@global_var_216638 = global i64 0
@global_var_216568 = global i64 0
@global_var_11048 = constant i64 11565
@global_var_2165e8 = global i64 0
@global_var_216094 = global i64 73014444031
@global_var_2165fc = global i64 0
@global_var_216570 = global i64 0
@global_var_216098 = global i64 34359738384
@global_var_2165e0 = global i64 0
@global_var_216600 = global i64 0
@global_var_216604 = global i64 0
@global_var_216605 = global i64 0
@global_var_216608 = global i64 0
@global_var_216610 = global i64 0
@global_var_216090 = global i64 -1
@global_var_2165f8 = global i64 0
@global_var_2165e4 = global i64 0
@global_var_21660c = global i64 0
@global_var_2165f9 = global i64 0
@global_var_2165ec = global i64 0
@global_var_2165e5 = global i64 0
@global_var_2160a0 = global i64 -4294967290
@global_var_216614 = global i64 0
@global_var_2166c0 = global i64 0
@global_var_216569 = global i64 0
@global_var_216ae0 = global i64 0
@global_var_21a460 = global i64 0
@global_var_21661c = global i64 0
@global_var_24a884 = global i64 0
@global_var_25f4e4 = global i64 0
@global_var_216f18 = global i64 0
@global_var_216f00 = global i64 0
@global_var_25f4d0 = global i64 0
@global_var_24f0c0 = global i64 0
@global_var_216620 = global i64 0
@global_var_24f0a0 = global i64 0
@global_var_25f4c0 = global i64 0
@global_var_21a860 = global i64 0
@global_var_2160a8 = global i64 4294967295
@global_var_800 = global i64 0
@global_var_400 = global i64 0
@global_var_24a880 = global i64 0
@global_var_24a888 = global i64 0
@global_var_216640 = global i64 0
@global_var_2160a4 = global i64 -1
@global_var_900 = global i64 77309411935
@global_var_24a885 = global i64 0
@global_var_216621 = global i64 0
@global_var_24a88c = global i64 0
@global_var_2160c8 = global i64 1
@global_var_216618 = global i64 0
@global_var_216f90 = global i64 0
@global_var_216fa4 = global i64 0
@global_var_216fa7 = global i64 0
@global_var_216f98 = global i64 0
@global_var_216fa0 = global i64 0
@global_var_216308 = global i64 6
@global_var_216fa8 = global i64 0
@global_var_219ef0 = global i64 0
@global_var_219ee0 = global i64 0
@global_var_217e42 = global i64 0
@global_var_217f70 = global i64 0
@global_var_217f71 = global i64 0
@global_var_217f72 = global i64 0
@global_var_217f73 = global i64 0
@global_var_217f6e = global i64 0
@global_var_217f6f = global i64 0
@global_var_219ec8 = global i64 0
@global_var_219ec4 = global i64 0
@global_var_219ec0 = global i64 0
@global_var_219ecc = global i64 0
@global_var_219ecd = global i64 0
@global_var_218874 = global i64 0
@global_var_218878 = global i64 0
@global_var_218879 = global i64 0
@global_var_218875 = global i64 0
@global_var_217f84 = global i64 0
@global_var_218873 = global i64 0
@global_var_217f00 = global i64 0
@global_var_217f01 = global i64 0
@global_var_217f04 = global i64 0
@global_var_217f05 = global i64 0
@global_var_217f08 = global i64 0
@global_var_217f09 = global i64 0
@global_var_217f02 = global i64 0
@global_var_217f06 = global i64 0
@global_var_217f0a = global i64 0
@global_var_2164c4 = global i64 0
@global_var_216504 = global i64 0
@global_var_216505 = global i64 0
@global_var_2164c5 = global i64 0
@global_var_219ec1 = global i64 0
@global_var_219ec5 = global i64 0
@global_var_219ec9 = global i64 0
@global_var_21a140 = global i64 0
@global_var_21a144 = global i64 0
@global_var_21a148 = global i64 0
@global_var_219f20 = global i64 0
@global_var_21a14c = global i64 0
@global_var_21a150 = global i64 0
@global_var_21a154 = global i64 0
@global_var_21a155 = global i64 0
@global_var_216548 = global i64 128
@global_var_21a168 = global i64 0
@global_var_21a3e8 = global i64 0
@global_var_21a3f8 = global i64 0
@global_var_21a3f0 = global i64 0
@global_var_21a400 = global i64 0
@global_var_21a3fc = global i64 0
@global_var_21a404 = global i64 0
@global_var_25f50e = global i64 0
@global_var_25f50f = global i64 0
@global_var_25f510 = global i64 0
@global_var_25f511 = global i64 0
@global_var_25f516 = global i64 0
@global_var_25f517 = global i64 0
@global_var_25f518 = global i64 0
@global_var_25f519 = global i64 0
@global_var_25f512 = global i64 0
@global_var_25f513 = global i64 0
@global_var_25f514 = global i64 0
@global_var_25f515 = global i64 0
@global_var_216550 = global i64 4294967295
@global_var_21a408 = global i64 0
@global_var_21a410 = global i64 0
@global_var_21a414 = global i64 0
@global_var_407 = global i64 19791209304576
@global_var_406 = global i64 5066549581971456
@global_var_409 = global i64 301989888
@global_var_21a418 = global i64 0

define i64 @_init(i64 %arg1) {
block_1f70:
  %rdi = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8048, i64* @assembly_address
  %0 = load i64* %rsp
  %1 = sub i64 %0, 8
  %2 = and i64 %0, 15
  %3 = sub i64 %2, 8
  %4 = icmp ugt i64 %3, 15
  %5 = icmp ult i64 %0, 8
  %6 = xor i64 %0, 8
  %7 = xor i64 %0, %1
  %8 = and i64 %6, %7
  %9 = icmp slt i64 %8, 0
  store i1 %4, i1* %az
  store i1 %5, i1* %cf
  store i1 %9, i1* %of
  %10 = icmp eq i64 %1, 0
  store i1 %10, i1* %zf
  %11 = icmp slt i64 %1, 0
  store i1 %11, i1* %sf
  %12 = trunc i64 %1 to i8
  %13 = call i8 @llvm.ctpop.i8(i8 %12)
  %14 = and i8 %13, 1
  %15 = icmp eq i8 %14, 0
  store i1 %15, i1* %pf
  %16 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %16, i64* %rsp
  store volatile i64 8052, i64* @assembly_address
  %17 = load i64* @global_var_215fd0
  store i64 %17, i64* %rax
  store volatile i64 8059, i64* @assembly_address
  %18 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %19 = icmp eq i64 %18, 0
  store i1 %19, i1* %zf
  %20 = icmp slt i64 %18, 0
  store i1 %20, i1* %sf
  %21 = trunc i64 %18 to i8
  %22 = call i8 @llvm.ctpop.i8(i8 %21)
  %23 = and i8 %22, 1
  %24 = icmp eq i8 %23, 0
  store i1 %24, i1* %pf
  store volatile i64 8062, i64* @assembly_address
  %25 = load i1* %zf
  br i1 %25, label %block_1f82, label %block_1f80

block_1f80:                                       ; preds = %block_1f70
  store volatile i64 8064, i64* @assembly_address
  call void @__gmon_start__()
  br label %block_1f82

block_1f82:                                       ; preds = %block_1f80, %block_1f70
  store volatile i64 8066, i64* @assembly_address
  %26 = load i64* %rsp
  %27 = add i64 %26, 8
  %28 = and i64 %26, 15
  %29 = add i64 %28, 8
  %30 = icmp ugt i64 %29, 15
  %31 = icmp ult i64 %27, %26
  %32 = xor i64 %26, %27
  %33 = xor i64 8, %27
  %34 = and i64 %32, %33
  %35 = icmp slt i64 %34, 0
  store i1 %30, i1* %az
  store i1 %31, i1* %cf
  store i1 %35, i1* %of
  %36 = icmp eq i64 %27, 0
  store i1 %36, i1* %zf
  %37 = icmp slt i64 %27, 0
  store i1 %37, i1* %sf
  %38 = trunc i64 %27 to i8
  %39 = call i8 @llvm.ctpop.i8(i8 %38)
  %40 = and i8 %39, 1
  %41 = icmp eq i8 %40, 0
  store i1 %41, i1* %pf
  %42 = ptrtoint i64* %stack_var_0 to i64
  store i64 %42, i64* %rsp
  store volatile i64 8070, i64* @assembly_address
  %43 = load i64* %rax
  %44 = load i64* %rax
  ret i64 %44
}

define i64 @function_1f90() {
block_1f90:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8080, i64* @assembly_address
  %0 = load i64* @global_var_215d58
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 8086, i64* @assembly_address
  %2 = load i64* @global_var_215d60
  %3 = call i64 @__pseudo_call(i64 %2)
  %4 = load i64* %rax
  %5 = load i64* %rax
  ret i64 %5
}

define i8* @function_1fa0(i8* %name) {
block_1fa0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %name to i64
  store i64 %0, i64* %rdi
  store volatile i64 8096, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  %3 = call i8* @getenv(i8* %name)
  %4 = ptrtoint i8* %3 to i64
  store i64 %4, i64* %rax
  %5 = ptrtoint i8* %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to i8*
  ret i8* %7
}

define i64 @function_1fa6() {
block_1fa6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8102, i64* @assembly_address
  store i64 0, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8107, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_1fb0(i32 %how, %_TYPEDEF_sigset_t* %set, %_TYPEDEF_sigset_t* %oset) {
block_1fb0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_TYPEDEF_sigset_t* %oset to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint %_TYPEDEF_sigset_t* %set to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %how to i64
  store i64 %2, i64* %rdi
  store volatile i64 8112, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to %_TYPEDEF_sigset_t*
  %7 = load i64* %rdx
  %8 = inttoptr i64 %7 to %_TYPEDEF_sigset_t*
  %9 = call i32 @sigprocmask(i32 %how, %_TYPEDEF_sigset_t* %set, %_TYPEDEF_sigset_t* %oset)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_1fb6() {
block_1fb6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8118, i64* @assembly_address
  store i64 1, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8123, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_1fc0(i32 %sig) {
block_1fc0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %sig to i64
  store i64 %0, i64* %rdi
  store volatile i64 8128, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i32 @raise(i32 %sig)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_1fc6() {
block_1fc6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8134, i64* @assembly_address
  store i64 2, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8139, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void @function_1fd0(i64* %ptr) {
block_1fd0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i64* %ptr to i64
  store i64 %0, i64* %rdi
  store volatile i64 8144, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i64*
  call void @free(i64* %ptr)
  %3 = load i64* %rax
  ret void
}

define i64 @function_1fd6() {
block_1fd6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8150, i64* @assembly_address
  store i64 3, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8155, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_1fe0(i32 %fd, i8* %path, [2 x %timespec] %times, i32 %flags) {
block_1fe0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %flags to i64
  store i64 %0, i64* %rcx
  %1 = extractvalue [2 x %timespec] %times, 0
  %2 = extractvalue %timespec %1, 0
  store i64 %2, i64* %rdx
  %3 = ptrtoint i8* %path to i64
  store i64 %3, i64* %rsi
  %4 = sext i32 %fd to i64
  store i64 %4, i64* %rdi
  store volatile i64 8160, i64* @assembly_address
  %5 = load i64* %rdi
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rsi
  %8 = inttoptr i64 %7 to i8*
  %9 = load i64* %rdx
  %10 = insertvalue %timespec undef, i64 %9, 0
  %11 = insertvalue [2 x %timespec] undef, %timespec %10, 0
  %12 = load i64* %rcx
  %13 = trunc i64 %12 to i32
  %14 = call i32 @utimensat(i32 %fd, i8* %path, [2 x %timespec] %times, i32 %flags)
  %15 = sext i32 %14 to i64
  store i64 %15, i64* %rax
  %16 = sext i32 %14 to i64
  store i64 %16, i64* %rax
  %17 = load i64* %rax
  %18 = trunc i64 %17 to i32
  ret i32 %18
}

define i64 @function_1fe6() {
block_1fe6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8166, i64* @assembly_address
  store i64 4, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8171, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_1ff0(i32 %c) {
block_1ff0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %c to i64
  store i64 %0, i64* %rdi
  store volatile i64 8176, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i32 @putchar(i32 %c)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_1ff6() {
block_1ff6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8182, i64* @assembly_address
  store i64 5, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8187, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define %tm* @function_2000(i32* %timer) {
block_2000:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i32* %timer to i64
  store i64 %0, i64* %rdi
  store volatile i64 8192, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i32*
  %3 = call %tm* @localtime(i32* %timer)
  %4 = ptrtoint %tm* %3 to i64
  store i64 %4, i64* %rax
  %5 = ptrtoint %tm* %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to %tm*
  ret %tm* %7
}

define i64 @function_2006() {
block_2006:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8198, i64* @assembly_address
  store i64 6, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8203, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32* @function_2010() {
block_2010:
  %rax = alloca i64
  store volatile i64 8208, i64* @assembly_address
  %0 = call i32* @__errno_location()
  %1 = ptrtoint i32* %0 to i64
  store i64 %1, i64* %rax
  %2 = ptrtoint i32* %0 to i64
  store i64 %2, i64* %rax
  %3 = ptrtoint i32* %0 to i64
  store i64 %3, i64* %rax
  %4 = load i64* %rax
  %5 = inttoptr i64 %4 to i32*
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to i32*
  ret i32* %7
}

define i64 @function_2016() {
block_2016:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8214, i64* @assembly_address
  store i64 7, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8219, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2020(i32 %fildes) {
block_2020:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %fildes to i64
  store i64 %0, i64* %rdi
  store volatile i64 8224, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i32 @fdatasync(i32 %fildes)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2026() {
block_2026:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8230, i64* @assembly_address
  store i64 8, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8235, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2030(i8* %name) {
block_2030:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %name to i64
  store i64 %0, i64* %rdi
  store volatile i64 8240, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  %3 = call i32 @unlink(i8* %name)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2036() {
block_2036:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8246, i64* @assembly_address
  store i64 9, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8251, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void @function_2040(i32 %status) {
block_2040:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %status to i64
  store i64 %0, i64* %rdi
  store volatile i64 8256, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  call void @_exit(i32 %status)
  %3 = load i64* %rax
  ret void
}

define i64 @function_2046() {
block_2046:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8262, i64* @assembly_address
  store i64 10, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8267, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i8* @function_2050(i8* %dest, i8* %src) {
block_2050:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %src to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %dest to i64
  store i64 %1, i64* %rdi
  store volatile i64 8272, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to i8*
  %6 = call i8* @strcpy(i8* %dest, i8* %src)
  %7 = ptrtoint i8* %6 to i64
  store i64 %7, i64* %rax
  %8 = ptrtoint i8* %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = inttoptr i64 %9 to i8*
  ret i8* %10
}

define i64 @function_2056() {
block_2056:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8278, i64* @assembly_address
  store i64 11, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8283, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2060(i32 %fd, i8* %name, i32 %flag) {
block_2060:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %flag to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i8* %name to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %fd to i64
  store i64 %2, i64* %rdi
  store volatile i64 8288, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i8*
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 @unlinkat(i32 %fd, i8* %name, i32 %flag)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2066() {
block_2066:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8294, i64* @assembly_address
  store i64 12, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8299, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2070(i8* %s) {
block_2070:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %s to i64
  store i64 %0, i64* %rdi
  store volatile i64 8304, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  %3 = call i32 @puts(i8* %s)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2076() {
block_2076:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8310, i64* @assembly_address
  store i64 13, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8315, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void @function_2080(i64* %base, i32 %nmemb, i32 %size, i32 (i64*, i64*)* %compar) {
block_2080:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i32 (i64*, i64*)* %compar to i64
  store i64 %0, i64* %rcx
  %1 = sext i32 %size to i64
  store i64 %1, i64* %rdx
  %2 = sext i32 %nmemb to i64
  store i64 %2, i64* %rsi
  %3 = ptrtoint i64* %base to i64
  store i64 %3, i64* %rdi
  store volatile i64 8320, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = inttoptr i64 %4 to i64*
  %6 = load i64* %rsi
  %7 = trunc i64 %6 to i32
  %8 = load i64* %rdx
  %9 = trunc i64 %8 to i32
  %10 = load i64* %rcx
  %11 = inttoptr i64 %10 to i32 (i64*, i64*)*
  call void @qsort(i64* %base, i32 %nmemb, i32 %size, i32 (i64*, i64*)* %compar)
  %12 = load i64* %rax
  ret void
}

define i64 @function_2086() {
block_2086:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8326, i64* @assembly_address
  store i64 14, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8331, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2090(i32 %fd) {
block_2090:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %fd to i64
  store i64 %0, i64* %rdi
  store volatile i64 8336, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i32 @isatty(i32 %fd)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2096() {
block_2096:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8342, i64* @assembly_address
  store i64 15, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8347, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_20a0(i32 %sig, %sigaction* %act, %sigaction* %oact) {
block_20a0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %sigaction* %oact to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint %sigaction* %act to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %sig to i64
  store i64 %2, i64* %rdi
  store volatile i64 8352, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to %sigaction*
  %7 = load i64* %rdx
  %8 = inttoptr i64 %7 to %sigaction*
  %9 = call i32 @sigaction(i32 %sig, %sigaction* %act, %sigaction* %oact)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_20a6() {
block_20a6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8358, i64* @assembly_address
  store i64 16, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8363, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_20b0(i32 %fd, i32 %cmd, ...) {
block_20b0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %cmd to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %fd to i64
  store i64 %1, i64* %rdi
  store volatile i64 8368, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = trunc i64 %2 to i32
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i32 (i32, i32, ...)* @fcntl(i32 %fd, i32 %cmd)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_20b6() {
block_20b6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8374, i64* @assembly_address
  store i64 17, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8379, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_20c0(i32 %clock_id, %timespec* %tp) {
block_20c0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %timespec* %tp to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %clock_id to i64
  store i64 %1, i64* %rdi
  store volatile i64 8384, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = trunc i64 %2 to i32
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to %timespec*
  %6 = call i32 @clock_gettime(i32 %clock_id, %timespec* %tp)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_20c6() {
block_20c6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8390, i64* @assembly_address
  store i64 18, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8395, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_20d0(i32 %fd, i64* %buf, i32 %n) {
block_20d0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %n to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %buf to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %fd to i64
  store i64 %2, i64* %rdi
  store volatile i64 8400, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 @write(i32 %fd, i64* %buf, i32 %n)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_20d6() {
block_20d6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8406, i64* @assembly_address
  store i64 19, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8411, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_20e0(%_IO_FILE* %stream) {
block_20e0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_IO_FILE* %stream to i64
  store i64 %0, i64* %rdi
  store volatile i64 8416, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %_IO_FILE*
  %3 = call i32 @fclose(%_IO_FILE* %stream)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_20e6() {
block_20e6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8422, i64* @assembly_address
  store i64 20, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8427, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define %__dirstream* @function_20f0(i8* %name) {
block_20f0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %name to i64
  store i64 %0, i64* %rdi
  store volatile i64 8432, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  %3 = call %__dirstream* @opendir(i8* %name)
  %4 = ptrtoint %__dirstream* %3 to i64
  store i64 %4, i64* %rax
  %5 = ptrtoint %__dirstream* %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to %__dirstream*
  ret %__dirstream* %7
}

define i64 @function_20f6() {
block_20f6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8438, i64* @assembly_address
  store i64 21, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8443, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i8* @function_2100(i8* %dest, i8* %src) {
block_2100:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %src to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %dest to i64
  store i64 %1, i64* %rdi
  store volatile i64 8448, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to i8*
  %6 = call i8* @stpcpy(i8* %dest, i8* %src)
  %7 = ptrtoint i8* %6 to i64
  store i64 %7, i64* %rax
  %8 = ptrtoint i8* %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = inttoptr i64 %9 to i8*
  ret i8* %10
}

define i64 @function_2106() {
block_2106:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8454, i64* @assembly_address
  store i64 22, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8459, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2110(i8* %s) {
block_2110:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %s to i64
  store i64 %0, i64* %rdi
  store volatile i64 8464, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  %3 = call i32 @strlen(i8* %s)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2116() {
block_2116:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8470, i64* @assembly_address
  store i64 23, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8475, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2120(i32 %ver, i8* %filename, %stat* %stat_buf) {
block_2120:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %stat* %stat_buf to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i8* %filename to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %ver to i64
  store i64 %2, i64* %rdi
  store volatile i64 8480, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i8*
  %7 = load i64* %rdx
  %8 = inttoptr i64 %7 to %stat*
  %9 = call i32 @__lxstat(i32 %ver, i8* %filename, %stat* %stat_buf)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2126() {
block_2126:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8486, i64* @assembly_address
  store i64 24, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8491, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2130(i32 %fd, i8* %file, i32 %oflag, ...) {
block_2130:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %oflag to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i8* %file to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %fd to i64
  store i64 %2, i64* %rdi
  store volatile i64 8496, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i8*
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 (i32, i8*, i32, ...)* @openat(i32 %fd, i8* %file, i32 %oflag)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2136() {
block_2136:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8502, i64* @assembly_address
  store i64 25, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8507, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void @function_2140() {
block_2140:
  %rax = alloca i64
  store volatile i64 8512, i64* @assembly_address
  call void @__stack_chk_fail()
  %0 = load i64* %rax
  %1 = load i64* %rax
  ret void
}

define i64 @function_2146() {
block_2146:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8518, i64* @assembly_address
  store i64 26, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8523, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2150(i32 %argc, i8** %argv, i8* %shortopts, %option* %longopts, i32* %longind) {
block_2150:
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i32* %longind to i64
  store i64 %0, i64* %r8
  %1 = ptrtoint %option* %longopts to i64
  store i64 %1, i64* %rcx
  %2 = ptrtoint i8* %shortopts to i64
  store i64 %2, i64* %rdx
  %3 = ptrtoint i8** %argv to i64
  store i64 %3, i64* %rsi
  %4 = sext i32 %argc to i64
  store i64 %4, i64* %rdi
  store volatile i64 8528, i64* @assembly_address
  %5 = load i64* %rdi
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rsi
  %8 = inttoptr i64 %7 to i8**
  %9 = load i64* %rdx
  %10 = inttoptr i64 %9 to i8*
  %11 = load i64* %rcx
  %12 = inttoptr i64 %11 to %option*
  %13 = load i64* %r8
  %14 = inttoptr i64 %13 to i32*
  %15 = call i32 @getopt_long(i32 %argc, i8** %argv, i8* %shortopts, %option* %longopts, i32* %longind)
  %16 = sext i32 %15 to i64
  store i64 %16, i64* %rax
  %17 = sext i32 %15 to i64
  store i64 %17, i64* %rax
  %18 = load i64* %rax
  %19 = trunc i64 %18 to i32
  ret i32 %19
}

define i64 @function_2156() {
block_2156:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8534, i64* @assembly_address
  store i64 27, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8539, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2160(i8* %format, ...) {
block_2160:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %format to i64
  store i64 %0, i64* %rdi
  store volatile i64 8544, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  %3 = call i32 (i8*, ...)* @printf(i8* %format)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2166() {
block_2166:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8550, i64* @assembly_address
  store i64 28, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8555, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2170(i32 %c, %_IO_FILE* %fp) {
block_2170:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_IO_FILE* %fp to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %c to i64
  store i64 %1, i64* %rdi
  store volatile i64 8560, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = trunc i64 %2 to i32
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to %_IO_FILE*
  %6 = call i32 @_IO_putc(i32 %c, %_IO_FILE* %fp)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_2176() {
block_2176:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8566, i64* @assembly_address
  store i64 29, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8571, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i8* @function_2180(i8* %s, i32 %c) {
block_2180:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %c to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %s to i64
  store i64 %1, i64* %rdi
  store volatile i64 8576, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i8* @strrchr(i8* %s, i32 %c)
  %7 = ptrtoint i8* %6 to i64
  store i64 %7, i64* %rax
  %8 = ptrtoint i8* %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = inttoptr i64 %9 to i8*
  ret i8* %10
}

define i64 @function_2186() {
block_2186:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8582, i64* @assembly_address
  store i64 30, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8587, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2190(i32 %fd, i32 %offset, i32 %whence) {
block_2190:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %whence to i64
  store i64 %0, i64* %rdx
  %1 = sext i32 %offset to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %fd to i64
  store i64 %2, i64* %rdi
  store volatile i64 8592, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 @lseek(i32 %fd, i32 %offset, i32 %whence)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2196() {
block_2196:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8598, i64* @assembly_address
  store i64 31, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8603, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i64* @function_21a0(i64* %s, i32 %c, i32 %n) {
block_21a0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %n to i64
  store i64 %0, i64* %rdx
  %1 = sext i32 %c to i64
  store i64 %1, i64* %rsi
  %2 = ptrtoint i64* %s to i64
  store i64 %2, i64* %rdi
  store volatile i64 8608, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64* %rsi
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i64* @memset(i64* %s, i32 %c, i32 %n)
  %10 = ptrtoint i64* %9 to i64
  store i64 %10, i64* %rax
  %11 = ptrtoint i64* %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = inttoptr i64 %12 to i64*
  ret i64* %13
}

define i64 @function_21a6() {
block_21a6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8614, i64* @assembly_address
  store i64 32, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8619, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_21b0(i32 %fd) {
block_21b0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %fd to i64
  store i64 %0, i64* %rdi
  store volatile i64 8624, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i32 @close(i32 %fd)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_21b6() {
block_21b6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8630, i64* @assembly_address
  store i64 33, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8635, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_21c0(i8* %s, i8* %accept) {
block_21c0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %accept to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %s to i64
  store i64 %1, i64* %rdi
  store volatile i64 8640, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to i8*
  %6 = call i32 @strspn(i8* %s, i8* %accept)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_21c6() {
block_21c6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8646, i64* @assembly_address
  store i64 34, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8651, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_21d0(%__dirstream* %dirp) {
block_21d0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %__dirstream* %dirp to i64
  store i64 %0, i64* %rdi
  store volatile i64 8656, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %__dirstream*
  %3 = call i32 @closedir(%__dirstream* %dirp)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_21d6() {
block_21d6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8662, i64* @assembly_address
  store i64 35, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8667, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_21e0(i32 %c, %_IO_FILE* %stream) {
block_21e0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_IO_FILE* %stream to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %c to i64
  store i64 %1, i64* %rdi
  store volatile i64 8672, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = trunc i64 %2 to i32
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to %_IO_FILE*
  %6 = call i32 @fputc(i32 %c, %_IO_FILE* %stream)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_21e6() {
block_21e6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8678, i64* @assembly_address
  store i64 36, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8683, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_21f0(i8* %s, i8* %reject) {
block_21f0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %reject to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %s to i64
  store i64 %1, i64* %rdi
  store volatile i64 8688, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to i8*
  %6 = call i32 @strcspn(i8* %s, i8* %reject)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_21f6() {
block_21f6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8694, i64* @assembly_address
  store i64 37, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8699, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2200(i32 %fd, i64* %buf, i32 %nbytes) {
block_2200:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %nbytes to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %buf to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %fd to i64
  store i64 %2, i64* %rdi
  store volatile i64 8704, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 @read(i32 %fd, i64* %buf, i32 %nbytes)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2206() {
block_2206:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8710, i64* @assembly_address
  store i64 38, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8715, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2210(i64* %s1, i64* %s2, i32 %n) {
block_2210:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %n to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %s2 to i64
  store i64 %1, i64* %rsi
  %2 = ptrtoint i64* %s1 to i64
  store i64 %2, i64* %rdi
  store volatile i64 8720, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 @memcmp(i64* %s1, i64* %s2, i32 %n)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2216() {
block_2216:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8726, i64* @assembly_address
  store i64 39, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8731, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2220(i8* %file, [2 x %timeval] %tvp) {
block_2220:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = extractvalue [2 x %timeval] %tvp, 0
  %1 = extractvalue %timeval %0, 0
  store i64 %1, i64* %rsi
  %2 = ptrtoint i8* %file to i64
  store i64 %2, i64* %rdi
  store volatile i64 8736, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = inttoptr i64 %3 to i8*
  %5 = load i64* %rsi
  %6 = insertvalue %timeval undef, i64 %5, 0
  %7 = insertvalue [2 x %timeval] undef, %timeval %6, 0
  %8 = call i32 @utimes(i8* %file, [2 x %timeval] %tvp)
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %rax
  %10 = sext i32 %8 to i64
  store i64 %10, i64* %rax
  %11 = load i64* %rax
  %12 = trunc i64 %11 to i32
  ret i32 %12
}

define i64 @function_2226() {
block_2226:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8742, i64* @assembly_address
  store i64 40, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8747, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i64* @function_2230(i32 %nmemb, i32 %size) {
block_2230:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %size to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %nmemb to i64
  store i64 %1, i64* %rdi
  store volatile i64 8752, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = trunc i64 %2 to i32
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i64* @calloc(i32 %nmemb, i32 %size)
  %7 = ptrtoint i64* %6 to i64
  store i64 %7, i64* %rax
  %8 = ptrtoint i64* %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = inttoptr i64 %9 to i64*
  ret i64* %10
}

define i64 @function_2236() {
block_2236:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8758, i64* @assembly_address
  store i64 41, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8763, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2240(i8* %s1, i8* %s2) {
block_2240:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %s2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %s1 to i64
  store i64 %1, i64* %rdi
  store volatile i64 8768, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to i8*
  %6 = call i32 @strcmp(i8* %s1, i8* %s2)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_2246() {
block_2246:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8774, i64* @assembly_address
  store i64 42, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8779, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2250() {
block_2250:
  %rax = alloca i64
  store volatile i64 8784, i64* @assembly_address
  %0 = call i32 @getchar()
  %1 = sext i32 %0 to i64
  store i64 %1, i64* %rax
  %2 = sext i32 %0 to i64
  store i64 %2, i64* %rax
  %3 = sext i32 %0 to i64
  store i64 %3, i64* %rax
  %4 = load i64* %rax
  %5 = trunc i64 %4 to i32
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2256() {
block_2256:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8790, i64* @assembly_address
  store i64 43, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8795, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void (i32)* @function_2260(i32 %sig, void (i32)* %handler) {
block_2260:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint void (i32)* %handler to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %sig to i64
  store i64 %1, i64* %rdi
  store volatile i64 8800, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = trunc i64 %2 to i32
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to void (i32)*
  %6 = call void (i32)* (i32, void (i32)*)* @signal(i32 %sig, void (i32)* %handler)
  %7 = ptrtoint void (i32)* %6 to i64
  store i64 %7, i64* %rax
  %8 = ptrtoint void (i32)* %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = inttoptr i64 %9 to void (i32)*
  ret void (i32)* %10
}

define i64 @function_2266() {
block_2266:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8806, i64* @assembly_address
  store i64 44, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8811, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2270(%__dirstream* %dirp) {
block_2270:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %__dirstream* %dirp to i64
  store i64 %0, i64* %rdi
  store volatile i64 8816, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %__dirstream*
  %3 = call i32 @dirfd(%__dirstream* %dirp)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2276() {
block_2276:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8822, i64* @assembly_address
  store i64 45, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8827, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2280(%_IO_FILE* %stream, i8* %format, ...) {
block_2280:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %format to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint %_IO_FILE* %stream to i64
  store i64 %1, i64* %rdi
  store volatile i64 8832, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to %_IO_FILE*
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to i8*
  %6 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %stream, i8* %format)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_2286() {
block_2286:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8838, i64* @assembly_address
  store i64 46, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8843, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2290(%_TYPEDEF_sigset_t* %set) {
block_2290:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_TYPEDEF_sigset_t* %set to i64
  store i64 %0, i64* %rdi
  store volatile i64 8848, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %_TYPEDEF_sigset_t*
  %3 = call i32 @sigemptyset(%_TYPEDEF_sigset_t* %set)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2296() {
block_2296:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8854, i64* @assembly_address
  store i64 47, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8859, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i64* @function_22a0(i64* %dest, i64* %src, i32 %n) {
block_22a0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %n to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %src to i64
  store i64 %1, i64* %rsi
  %2 = ptrtoint i64* %dest to i64
  store i64 %2, i64* %rdi
  store volatile i64 8864, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i64* @memcpy(i64* %dest, i64* %src, i32 %n)
  %10 = ptrtoint i64* %9 to i64
  store i64 %10, i64* %rax
  %11 = ptrtoint i64* %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = inttoptr i64 %12 to i64*
  ret i64* %13
}

define i64 @function_22a6() {
block_22a6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8870, i64* @assembly_address
  store i64 48, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8875, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_22b0(%_IO_FILE* %stream) {
block_22b0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_IO_FILE* %stream to i64
  store i64 %0, i64* %rdi
  store volatile i64 8880, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %_IO_FILE*
  %3 = call i32 @fileno(%_IO_FILE* %stream)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_22b6() {
block_22b6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8886, i64* @assembly_address
  store i64 49, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8891, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_22c0(i32 %ver, i8* %filename, %stat* %stat_buf) {
block_22c0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %stat* %stat_buf to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i8* %filename to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %ver to i64
  store i64 %2, i64* %rdi
  store volatile i64 8896, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i8*
  %7 = load i64* %rdx
  %8 = inttoptr i64 %7 to %stat*
  %9 = call i32 @__xstat(i32 %ver, i8* %filename, %stat* %stat_buf)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_22c6() {
block_22c6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8902, i64* @assembly_address
  store i64 50, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8907, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define %dirent* @function_22d0(%__dirstream* %dirp) {
block_22d0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %__dirstream* %dirp to i64
  store i64 %0, i64* %rdi
  store volatile i64 8912, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %__dirstream*
  %3 = call %dirent* @readdir(%__dirstream* %dirp)
  %4 = ptrtoint %dirent* %3 to i64
  store i64 %4, i64* %rax
  %5 = ptrtoint %dirent* %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to %dirent*
  ret %dirent* %7
}

define i64 @function_22d6() {
block_22d6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8918, i64* @assembly_address
  store i64 51, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8923, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_22e0(i32 %c) {
block_22e0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %c to i64
  store i64 %0, i64* %rdi
  store volatile i64 8928, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i32 @tolower(i32 %c)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_22e6() {
block_22e6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8934, i64* @assembly_address
  store i64 52, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8939, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i64* @function_22f0(i32 %size) {
block_22f0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %size to i64
  store i64 %0, i64* %rdi
  store volatile i64 8944, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i64* @malloc(i32 %size)
  %4 = ptrtoint i64* %3 to i64
  store i64 %4, i64* %rax
  %5 = ptrtoint i64* %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to i64*
  ret i64* %7
}

define i64 @function_22f6() {
block_22f6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8950, i64* @assembly_address
  store i64 53, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8955, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2300(%_IO_FILE* %stream) {
block_2300:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_IO_FILE* %stream to i64
  store i64 %0, i64* %rdi
  store volatile i64 8960, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %_IO_FILE*
  %3 = call i32 @fflush(%_IO_FILE* %stream)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2306() {
block_2306:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8966, i64* @assembly_address
  store i64 54, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8971, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2310(i32 %ver, i32 %fildes, %stat* %stat_buf) {
block_2310:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %stat* %stat_buf to i64
  store i64 %0, i64* %rdx
  %1 = sext i32 %fildes to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %ver to i64
  store i64 %2, i64* %rdi
  store volatile i64 8976, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rdx
  %8 = inttoptr i64 %7 to %stat*
  %9 = call i32 @__fxstat(i32 %ver, i32 %fildes, %stat* %stat_buf)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2316() {
block_2316:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8982, i64* @assembly_address
  store i64 55, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 8987, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2320(%_IO_FILE* %fp) {
block_2320:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_IO_FILE* %fp to i64
  store i64 %0, i64* %rdi
  store volatile i64 8992, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to %_IO_FILE*
  %3 = call i32 @__freading(%_IO_FILE* %fp)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2326() {
block_2326:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 8998, i64* @assembly_address
  store i64 56, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9003, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i64* @function_2330(i64* %ptr, i32 %size) {
block_2330:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %size to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i64* %ptr to i64
  store i64 %1, i64* %rdi
  store volatile i64 9008, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i64* @realloc(i64* %ptr, i32 %size)
  %7 = ptrtoint i64* %6 to i64
  store i64 %7, i64* %rax
  %8 = ptrtoint i64* %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = inttoptr i64 %9 to i64*
  ret i64* %10
}

define i64 @function_2336() {
block_2336:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9014, i64* @assembly_address
  store i64 57, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9019, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2340(i32 %fd, i32 %mode) {
block_2340:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %mode to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %fd to i64
  store i64 %1, i64* %rdi
  store volatile i64 9024, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = trunc i64 %2 to i32
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i32 @fchmod(i32 %fd, i32 %mode)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_2346() {
block_2346:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9030, i64* @assembly_address
  store i64 58, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9035, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i64* @function_2350(i64* %dest, i64* %src, i32 %n) {
block_2350:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %n to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %src to i64
  store i64 %1, i64* %rsi
  %2 = ptrtoint i64* %dest to i64
  store i64 %2, i64* %rdi
  store volatile i64 9040, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64* %rsi
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i64* @memmove(i64* %dest, i64* %src, i32 %n)
  %10 = ptrtoint i64* %9 to i64
  store i64 %10, i64* %rax
  %11 = ptrtoint i64* %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = inttoptr i64 %12 to i64*
  ret i64* %13
}

define i64 @function_2356() {
block_2356:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9046, i64* @assembly_address
  store i64 59, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9051, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2360(i32 %fd) {
block_2360:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %fd to i64
  store i64 %0, i64* %rdi
  store volatile i64 9056, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call i32 @fsync(i32 %fd)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_2366() {
block_2366:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9062, i64* @assembly_address
  store i64 60, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9067, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2370(i8* %file, i32 %oflag, ...) {
block_2370:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %oflag to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %file to i64
  store i64 %1, i64* %rdi
  store volatile i64 9072, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i32 (i8*, i32, ...)* @open(i8* %file, i32 %oflag)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_2376() {
block_2376:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9078, i64* @assembly_address
  store i64 61, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9083, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2380(%_IO_FILE* %stream, i32 %off, i32 %whence) {
block_2380:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %whence to i64
  store i64 %0, i64* %rdx
  %1 = sext i32 %off to i64
  store i64 %1, i64* %rsi
  %2 = ptrtoint %_IO_FILE* %stream to i64
  store i64 %2, i64* %rdi
  store volatile i64 9088, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = inttoptr i64 %3 to %_IO_FILE*
  %5 = load i64* %rsi
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 @fseeko(%_IO_FILE* %stream, i32 %off, i32 %whence)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2386() {
block_2386:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9094, i64* @assembly_address
  store i64 62, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9099, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2390(i32 %fd, i32 %owner, i32 %group) {
block_2390:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = sext i32 %group to i64
  store i64 %0, i64* %rdx
  %1 = sext i32 %owner to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %fd to i64
  store i64 %2, i64* %rdi
  store volatile i64 9104, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rdx
  %8 = trunc i64 %7 to i32
  %9 = call i32 @fchown(i32 %fd, i32 %owner, i32 %group)
  %10 = sext i32 %9 to i64
  store i64 %10, i64* %rax
  %11 = sext i32 %9 to i64
  store i64 %11, i64* %rax
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  ret i32 %13
}

define i64 @function_2396() {
block_2396:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9110, i64* @assembly_address
  store i64 63, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9115, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void @function_23a0(i8* %s) {
block_23a0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %s to i64
  store i64 %0, i64* %rdi
  store volatile i64 9120, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  call void @perror(i8* %s)
  %3 = load i64* %rax
  ret void
}

define i64 @function_23a6() {
block_23a6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9126, i64* @assembly_address
  store i64 64, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9131, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define %__dirstream* @function_23b0(i32 %fd) {
block_23b0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %fd to i64
  store i64 %0, i64* %rdi
  store volatile i64 9136, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  %3 = call %__dirstream* @fdopendir(i32 %fd)
  %4 = ptrtoint %__dirstream* %3 to i64
  store i64 %4, i64* %rax
  %5 = ptrtoint %__dirstream* %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to %__dirstream*
  ret %__dirstream* %7
}

define i64 @function_23b6() {
block_23b6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9142, i64* @assembly_address
  store i64 65, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9147, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_23c0(i32 %fd, [2 x %timespec] %times) {
block_23c0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = extractvalue [2 x %timespec] %times, 0
  %1 = extractvalue %timespec %0, 0
  store i64 %1, i64* %rsi
  %2 = sext i32 %fd to i64
  store i64 %2, i64* %rdi
  store volatile i64 9152, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = load i64* %rsi
  %6 = insertvalue %timespec undef, i64 %5, 0
  %7 = insertvalue [2 x %timespec] undef, %timespec %6, 0
  %8 = call i32 @futimens(i32 %fd, [2 x %timespec] %times)
  %9 = sext i32 %8 to i64
  store i64 %9, i64* %rax
  %10 = sext i32 %8 to i64
  store i64 %10, i64* %rax
  %11 = load i64* %rax
  %12 = trunc i64 %11 to i32
  ret i32 %12
}

define i64 @function_23c6() {
block_23c6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9158, i64* @assembly_address
  store i64 66, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9163, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_23d0(i8* %nptr) {
block_23d0:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %nptr to i64
  store i64 %0, i64* %rdi
  store volatile i64 9168, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i8*
  %3 = call i32 @atoi(i8* %nptr)
  %4 = sext i32 %3 to i64
  store i64 %4, i64* %rax
  %5 = sext i32 %3 to i64
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  ret i32 %7
}

define i64 @function_23d6() {
block_23d6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9174, i64* @assembly_address
  store i64 67, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9179, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i8* @function_23e0(i8* %dest, i8* %src) {
block_23e0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i8* %src to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %dest to i64
  store i64 %1, i64* %rdi
  store volatile i64 9184, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to i8*
  %4 = load i64* %rsi
  %5 = inttoptr i64 %4 to i8*
  %6 = call i8* @strcat(i8* %dest, i8* %src)
  %7 = ptrtoint i8* %6 to i64
  store i64 %7, i64* %rax
  %8 = ptrtoint i8* %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = inttoptr i64 %9 to i8*
  ret i8* %10
}

define i64 @function_23e6() {
block_23e6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9190, i64* @assembly_address
  store i64 68, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9195, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_23f0(%_TYPEDEF_sigset_t* %set, i32 %signo) {
block_23f0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %signo to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint %_TYPEDEF_sigset_t* %set to i64
  store i64 %1, i64* %rdi
  store volatile i64 9200, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to %_TYPEDEF_sigset_t*
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i32 @sigismember(%_TYPEDEF_sigset_t* %set, i32 %signo)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_23f6() {
block_23f6:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9206, i64* @assembly_address
  store i64 69, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9211, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void @function_2400(i32 %status) {
block_2400:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %status to i64
  store i64 %0, i64* %rdi
  store volatile i64 9216, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = trunc i64 %1 to i32
  call void @exit(i32 %status)
  %3 = load i64* %rax
  ret void
}

define i64 @function_2406() {
block_2406:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9222, i64* @assembly_address
  store i64 70, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9227, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2410(i64* %ptr, i32 %size, i32 %n, %_IO_FILE* %s) {
block_2410:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %_IO_FILE* %s to i64
  store i64 %0, i64* %rcx
  %1 = sext i32 %n to i64
  store i64 %1, i64* %rdx
  %2 = sext i32 %size to i64
  store i64 %2, i64* %rsi
  %3 = ptrtoint i64* %ptr to i64
  store i64 %3, i64* %rdi
  store volatile i64 9232, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = inttoptr i64 %4 to i64*
  %6 = load i64* %rsi
  %7 = trunc i64 %6 to i32
  %8 = load i64* %rdx
  %9 = trunc i64 %8 to i32
  %10 = load i64* %rcx
  %11 = inttoptr i64 %10 to %_IO_FILE*
  %12 = call i32 @fwrite(i64* %ptr, i32 %size, i32 %n, %_IO_FILE* %s)
  %13 = sext i32 %12 to i64
  store i64 %13, i64* %rax
  %14 = sext i32 %12 to i64
  store i64 %14, i64* %rax
  %15 = load i64* %rax
  %16 = trunc i64 %15 to i32
  ret i32 %16
}

define i64 @function_2416() {
block_2416:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9238, i64* @assembly_address
  store i64 71, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9243, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2420(%_TYPEDEF_sigset_t* %set, i32 %signo) {
block_2420:
  %rdi = alloca i64
  %rsi = alloca i64
  %rax = alloca i64
  %0 = sext i32 %signo to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint %_TYPEDEF_sigset_t* %set to i64
  store i64 %1, i64* %rdi
  store volatile i64 9248, i64* @assembly_address
  %2 = load i64* %rdi
  %3 = inttoptr i64 %2 to %_TYPEDEF_sigset_t*
  %4 = load i64* %rsi
  %5 = trunc i64 %4 to i32
  %6 = call i32 @sigaddset(%_TYPEDEF_sigset_t* %set, i32 %signo)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = trunc i64 %9 to i32
  ret i32 %10
}

define i64 @function_2426() {
block_2426:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9254, i64* @assembly_address
  store i64 72, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9259, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i32 @function_2430(i32 %fd, i8* %file, [2 x %timeval] %tvp) {
block_2430:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = extractvalue [2 x %timeval] %tvp, 0
  %1 = extractvalue %timeval %0, 0
  store i64 %1, i64* %rdx
  %2 = ptrtoint i8* %file to i64
  store i64 %2, i64* %rsi
  %3 = sext i32 %fd to i64
  store i64 %3, i64* %rdi
  store volatile i64 9264, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = trunc i64 %4 to i32
  %6 = load i64* %rsi
  %7 = inttoptr i64 %6 to i8*
  %8 = load i64* %rdx
  %9 = insertvalue %timeval undef, i64 %8, 0
  %10 = insertvalue [2 x %timeval] undef, %timeval %9, 0
  %11 = call i32 @futimesat(i32 %fd, i8* %file, [2 x %timeval] %tvp)
  %12 = sext i32 %11 to i64
  store i64 %12, i64* %rax
  %13 = sext i32 %11 to i64
  store i64 %13, i64* %rax
  %14 = load i64* %rax
  %15 = trunc i64 %14 to i32
  ret i32 %15
}

define i64 @function_2436() {
block_2436:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9270, i64* @assembly_address
  store i64 73, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9275, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define i16** @function_2440() {
block_2440:
  %rax = alloca i64
  store volatile i64 9280, i64* @assembly_address
  %0 = call i16** @__ctype_b_loc()
  %1 = ptrtoint i16** %0 to i64
  store i64 %1, i64* %rax
  %2 = ptrtoint i16** %0 to i64
  store i64 %2, i64* %rax
  %3 = ptrtoint i16** %0 to i64
  store i64 %3, i64* %rax
  %4 = load i64* %rax
  %5 = inttoptr i64 %4 to i16**
  %6 = load i64* %rax
  %7 = inttoptr i64 %6 to i16**
  ret i16** %7
}

define i64 @function_2446() {
block_2446:
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9286, i64* @assembly_address
  store i64 74, i64* %stack_var_-8
  %0 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %0, i64* %rsp
  store volatile i64 9291, i64* @assembly_address
  %1 = call i64 @function_1f90()
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  store i64 %1, i64* %rax
  %2 = load i64* %rax
  %3 = load i64* %rax
  ret i64 %3
}

define void @function_2450(i64* %d) {
block_2450:
  %rdi = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i64* %d to i64
  store i64 %0, i64* %rdi
  store volatile i64 9296, i64* @assembly_address
  %1 = load i64* %rdi
  %2 = inttoptr i64 %1 to i64*
  call void @__cxa_finalize(i64* %d)
  %3 = load i64* %rax
  ret void
}

define i64 @entrypoint(i64 %arg1, i64 %arg2, void ()* %arg3, i64 %arg4) {
block_2460:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint void ()* %arg3 to i64
  store i64 %0, i64* %rdx
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %stack_var_8 = alloca i64
  %stack_var_0 = alloca i32
  %1 = alloca i64
  %2 = trunc i64 %arg4 to i32
  store i32 %2, i32* %stack_var_0
  store volatile i64 9312, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %3 = icmp eq i32 0, 0
  store i1 %3, i1* %zf
  %4 = icmp slt i32 0, 0
  store i1 %4, i1* %sf
  %5 = trunc i32 0 to i8
  %6 = call i8 @llvm.ctpop.i8(i8 %5)
  %7 = and i8 %6, 1
  %8 = icmp eq i8 %7, 0
  store i1 %8, i1* %pf
  %9 = zext i32 0 to i64
  store i64 %9, i64* %rbp
  store volatile i64 9314, i64* @assembly_address
  %10 = load i64* %rdx
  store i64 %10, i64* %r9
  store volatile i64 9317, i64* @assembly_address
  %11 = load i32* %stack_var_0
  %12 = sext i32 %11 to i64
  store i64 %12, i64* %rsi
  %13 = ptrtoint i64* %stack_var_8 to i64
  store i64 %13, i64* %rsp
  store volatile i64 9318, i64* @assembly_address
  %14 = ptrtoint i64* %stack_var_8 to i64
  store i64 %14, i64* %rdx
  store volatile i64 9321, i64* @assembly_address
  %15 = load i64* %rsp
  %16 = and i64 %15, -16
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %17 = icmp eq i64 %16, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %16, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %16 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i32* %stack_var_0 to i64
  store i64 %23, i64* %rsp
  store volatile i64 9325, i64* @assembly_address
  %24 = load i64* %rax
  store i64 %24, i64* %stack_var_-8
  %25 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %25, i64* %rsp
  store volatile i64 9326, i64* @assembly_address
  %26 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %26, i64* %stack_var_-16
  %27 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %27, i64* %rsp
  store volatile i64 9327, i64* @assembly_address
  store i64 68896, i64* %r8
  store volatile i64 9334, i64* @assembly_address
  store i64 68784, i64* %rcx
  store volatile i64 9341, i64* @assembly_address
  store i64 15714, i64* %rdi
  store volatile i64 9348, i64* @assembly_address
  %28 = load i64* %rdi
  %29 = load i64* %rsi
  %30 = trunc i64 %29 to i32
  %31 = load i64* %rdx
  %32 = inttoptr i64 %31 to i8**
  %33 = load i64* %rcx
  %34 = inttoptr i64 %33 to void ()*
  %35 = load i64* %r8
  %36 = inttoptr i64 %35 to void ()*
  %37 = load i64* %r9
  %38 = inttoptr i64 %37 to void ()*
  %39 = call i32 @__libc_start_main(i64 %28, i32 %30, i8** %32, void ()* %34, void ()* %36, void ()* %38)
  %40 = sext i32 %39 to i64
  store i64 %40, i64* %rax
  %41 = sext i32 %39 to i64
  store i64 %41, i64* %rax
  store volatile i64 9354, i64* @assembly_address
  %42 = call i64 @__asm_hlt()
  store i64 %42, i64* %rax
  store i64 %42, i64* %rax
  unreachable
}

declare i64 @156(i64, i64, i64, i32)

declare i64 @157(i64, i64, i64, i64)

define i64 @deregister_tm_clones() {
block_2490:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9360, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216560 to i64), i64* %rdi
  store volatile i64 9367, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 9368, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216560 to i64), i64* %rax
  store volatile i64 9375, i64* @assembly_address
  %2 = load i64* %rax
  %3 = load i64* %rdi
  %4 = sub i64 %2, %3
  %5 = and i64 %2, 15
  %6 = and i64 %3, 15
  %7 = sub i64 %5, %6
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %2, %3
  %10 = xor i64 %2, %3
  %11 = xor i64 %2, %4
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %4, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %4, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %4 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  store volatile i64 9378, i64* @assembly_address
  %20 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %20, i64* %rbp
  store volatile i64 9381, i64* @assembly_address
  %21 = load i1* %zf
  br i1 %21, label %block_24c0, label %block_24a7

block_24a7:                                       ; preds = %block_2490
  store volatile i64 9383, i64* @assembly_address
  %22 = load i64* @global_var_215fc0
  store i64 %22, i64* %rax
  store volatile i64 9390, i64* @assembly_address
  %23 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %24 = icmp eq i64 %23, 0
  store i1 %24, i1* %zf
  %25 = icmp slt i64 %23, 0
  store i1 %25, i1* %sf
  %26 = trunc i64 %23 to i8
  %27 = call i8 @llvm.ctpop.i8(i8 %26)
  %28 = and i8 %27, 1
  %29 = icmp eq i8 %28, 0
  store i1 %29, i1* %pf
  store volatile i64 9393, i64* @assembly_address
  %30 = load i1* %zf
  br i1 %30, label %block_24c0, label %block_24b3

block_24b3:                                       ; preds = %block_24a7
  store volatile i64 9395, i64* @assembly_address
  %31 = load i64* %stack_var_-8
  store i64 %31, i64* %rbp
  %32 = ptrtoint i64* %stack_var_0 to i64
  store i64 %32, i64* %rsp
  store volatile i64 9396, i64* @assembly_address
  %33 = load i64* %rdi
  %34 = inttoptr i64 %33 to i64*
  %35 = call i64 @_ITM_deregisterTMCloneTable(i64* %34)
  store i64 %35, i64* %rax
  store i64 %35, i64* %rax
  %36 = load i64* %rax
  %37 = load i64* %rax
  ret i64 %37

block_24c0:                                       ; preds = %block_24a7, %block_2490
  store volatile i64 9408, i64* @assembly_address
  %38 = load i64* %stack_var_-8
  store i64 %38, i64* %rbp
  %39 = ptrtoint i64* %stack_var_0 to i64
  store i64 %39, i64* %rsp
  store volatile i64 9409, i64* @assembly_address
  %40 = load i64* %rax
  %41 = load i64* %rax
  ret i64 %41
}

define i64 @register_tm_clones() {
block_24d0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9424, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216560 to i64), i64* %rdi
  store volatile i64 9431, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216560 to i64), i64* %rsi
  store volatile i64 9438, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 9439, i64* @assembly_address
  %2 = load i64* %rsi
  %3 = load i64* %rdi
  %4 = sub i64 %2, %3
  %5 = and i64 %2, 15
  %6 = and i64 %3, 15
  %7 = sub i64 %5, %6
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %2, %3
  %10 = xor i64 %2, %3
  %11 = xor i64 %2, %4
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %4, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %4, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %4 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  store i64 %4, i64* %rsi
  store volatile i64 9442, i64* @assembly_address
  %20 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %20, i64* %rbp
  store volatile i64 9445, i64* @assembly_address
  %21 = load i64* %rsi
  %22 = load i1* %of
  %23 = ashr i64 %21, 3
  %24 = icmp eq i64 %23, 0
  store i1 %24, i1* %zf
  %25 = icmp slt i64 %23, 0
  store i1 %25, i1* %sf
  %26 = trunc i64 %23 to i8
  %27 = call i8 @llvm.ctpop.i8(i8 %26)
  %28 = and i8 %27, 1
  %29 = icmp eq i8 %28, 0
  store i1 %29, i1* %pf
  store i64 %23, i64* %rsi
  %30 = and i64 4, %21
  %31 = icmp ne i64 %30, 0
  store i1 %31, i1* %cf
  %32 = select i1 false, i1 false, i1 %22
  store i1 %32, i1* %of
  store volatile i64 9449, i64* @assembly_address
  %33 = load i64* %rsi
  store i64 %33, i64* %rax
  store volatile i64 9452, i64* @assembly_address
  %34 = load i64* %rax
  %35 = load i1* %of
  %36 = lshr i64 %34, 63
  %37 = icmp eq i64 %36, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i64 %36, 0
  store i1 %38, i1* %sf
  %39 = trunc i64 %36 to i8
  %40 = call i8 @llvm.ctpop.i8(i8 %39)
  %41 = and i8 %40, 1
  %42 = icmp eq i8 %41, 0
  store i1 %42, i1* %pf
  store i64 %36, i64* %rax
  %43 = and i64 4611686018427387904, %34
  %44 = icmp ne i64 %43, 0
  store i1 %44, i1* %cf
  %45 = icmp slt i64 %34, 0
  %46 = select i1 false, i1 %45, i1 %35
  store i1 %46, i1* %of
  store volatile i64 9456, i64* @assembly_address
  %47 = load i64* %rsi
  %48 = load i64* %rax
  %49 = add i64 %47, %48
  %50 = and i64 %47, 15
  %51 = and i64 %48, 15
  %52 = add i64 %50, %51
  %53 = icmp ugt i64 %52, 15
  %54 = icmp ult i64 %49, %47
  %55 = xor i64 %47, %49
  %56 = xor i64 %48, %49
  %57 = and i64 %55, %56
  %58 = icmp slt i64 %57, 0
  store i1 %53, i1* %az
  store i1 %54, i1* %cf
  store i1 %58, i1* %of
  %59 = icmp eq i64 %49, 0
  store i1 %59, i1* %zf
  %60 = icmp slt i64 %49, 0
  store i1 %60, i1* %sf
  %61 = trunc i64 %49 to i8
  %62 = call i8 @llvm.ctpop.i8(i8 %61)
  %63 = and i8 %62, 1
  %64 = icmp eq i8 %63, 0
  store i1 %64, i1* %pf
  store i64 %49, i64* %rsi
  store volatile i64 9459, i64* @assembly_address
  %65 = load i64* %rsi
  %66 = load i1* %of
  %67 = ashr i64 %65, 1
  %68 = icmp eq i64 %67, 0
  store i1 %68, i1* %zf
  %69 = icmp slt i64 %67, 0
  store i1 %69, i1* %sf
  %70 = trunc i64 %67 to i8
  %71 = call i8 @llvm.ctpop.i8(i8 %70)
  %72 = and i8 %71, 1
  %73 = icmp eq i8 %72, 0
  store i1 %73, i1* %pf
  store i64 %67, i64* %rsi
  %74 = and i64 1, %65
  %75 = icmp ne i64 %74, 0
  store i1 %75, i1* %cf
  %76 = select i1 true, i1 false, i1 %66
  store i1 %76, i1* %of
  store volatile i64 9462, i64* @assembly_address
  %77 = load i1* %zf
  br i1 %77, label %block_2510, label %block_24f8

block_24f8:                                       ; preds = %block_24d0
  store volatile i64 9464, i64* @assembly_address
  %78 = load i64* @global_var_215fd8
  store i64 %78, i64* %rax
  store volatile i64 9471, i64* @assembly_address
  %79 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %80 = icmp eq i64 %79, 0
  store i1 %80, i1* %zf
  %81 = icmp slt i64 %79, 0
  store i1 %81, i1* %sf
  %82 = trunc i64 %79 to i8
  %83 = call i8 @llvm.ctpop.i8(i8 %82)
  %84 = and i8 %83, 1
  %85 = icmp eq i8 %84, 0
  store i1 %85, i1* %pf
  store volatile i64 9474, i64* @assembly_address
  %86 = load i1* %zf
  br i1 %86, label %block_2510, label %block_2504

block_2504:                                       ; preds = %block_24f8
  store volatile i64 9476, i64* @assembly_address
  %87 = load i64* %stack_var_-8
  store i64 %87, i64* %rbp
  %88 = ptrtoint i64* %stack_var_0 to i64
  store i64 %88, i64* %rsp
  store volatile i64 9477, i64* @assembly_address
  %89 = load i64* %rdi
  %90 = inttoptr i64 %89 to i64*
  %91 = load i64* %rsi
  %92 = call i64 @_ITM_registerTMCloneTable(i64* %90, i64 %91)
  store i64 %92, i64* %rax
  store i64 %92, i64* %rax
  %93 = load i64* %rax
  %94 = load i64* %rax
  ret i64 %94

block_2510:                                       ; preds = %block_24f8, %block_24d0
  store volatile i64 9488, i64* @assembly_address
  %95 = load i64* %stack_var_-8
  store i64 %95, i64* %rbp
  %96 = ptrtoint i64* %stack_var_0 to i64
  store i64 %96, i64* %rsp
  store volatile i64 9489, i64* @assembly_address
  %97 = load i64* %rax
  %98 = load i64* %rax
  ret i64 %98
}

define i64 @__do_global_dtors_aux() {
block_2520:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9504, i64* @assembly_address
  %0 = load i8* bitcast (i64* @global_var_216588 to i8*)
  %1 = and i8 %0, 15
  %2 = icmp ugt i8 %1, 15
  %3 = icmp ult i8 %0, 0
  %4 = xor i8 %0, 0
  %5 = and i8 %4, 0
  %6 = icmp slt i8 %5, 0
  store i1 %2, i1* %az
  store i1 %3, i1* %cf
  store i1 %6, i1* %of
  %7 = icmp eq i8 %0, 0
  store i1 %7, i1* %zf
  %8 = icmp slt i8 %0, 0
  store i1 %8, i1* %sf
  %9 = call i8 @llvm.ctpop.i8(i8 %0)
  %10 = and i8 %9, 1
  %11 = icmp eq i8 %10, 0
  store i1 %11, i1* %pf
  store volatile i64 9511, i64* @assembly_address
  %12 = load i1* %zf
  %13 = icmp eq i1 %12, false
  br i1 %13, label %block_2558, label %block_2529

block_2529:                                       ; preds = %block_2520
  store volatile i64 9513, i64* @assembly_address
  %14 = load i64* @global_var_215fe0
  %15 = and i64 %14, 15
  %16 = icmp ugt i64 %15, 15
  %17 = icmp ult i64 %14, 0
  %18 = xor i64 %14, 0
  %19 = and i64 %18, 0
  %20 = icmp slt i64 %19, 0
  store i1 %16, i1* %az
  store i1 %17, i1* %cf
  store i1 %20, i1* %of
  %21 = icmp eq i64 %14, 0
  store i1 %21, i1* %zf
  %22 = icmp slt i64 %14, 0
  store i1 %22, i1* %sf
  %23 = trunc i64 %14 to i8
  %24 = call i8 @llvm.ctpop.i8(i8 %23)
  %25 = and i8 %24, 1
  %26 = icmp eq i8 %25, 0
  store i1 %26, i1* %pf
  store volatile i64 9521, i64* @assembly_address
  %27 = load i64* %rbp
  store i64 %27, i64* %stack_var_-8
  %28 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %28, i64* %rsp
  store volatile i64 9522, i64* @assembly_address
  %29 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %29, i64* %rbp
  store volatile i64 9525, i64* @assembly_address
  %30 = load i1* %zf
  br i1 %30, label %block_2543, label %block_2537

block_2537:                                       ; preds = %block_2529
  store volatile i64 9527, i64* @assembly_address
  %31 = load i64* inttoptr (i64 2187272 to i64*)
  store i64 %31, i64* %rdi
  store volatile i64 9534, i64* @assembly_address
  %32 = load i64* %rdi
  %33 = inttoptr i64 %32 to i64*
  call void @__cxa_finalize(i64* %33)
  br label %block_2543

block_2543:                                       ; preds = %block_2537, %block_2529
  store volatile i64 9539, i64* @assembly_address
  %34 = call i64 @deregister_tm_clones()
  store i64 %34, i64* %rax
  store i64 %34, i64* %rax
  store i64 %34, i64* %rax
  store volatile i64 9544, i64* @assembly_address
  store i8 1, i8* bitcast (i64* @global_var_216588 to i8*)
  store volatile i64 9551, i64* @assembly_address
  %35 = load i64* %stack_var_-8
  store i64 %35, i64* %rbp
  %36 = ptrtoint i64* %stack_var_0 to i64
  store i64 %36, i64* %rsp
  store volatile i64 9552, i64* @assembly_address
  %37 = load i64* %rax
  %38 = load i64* %rax
  ret i64 %38

block_2558:                                       ; preds = %block_2520
  store volatile i64 9560, i64* @assembly_address
  %39 = load i64* %rax
  %40 = load i64* %rax
  ret i64 %40
}

define i64 @frame_dummy() {
block_2560:
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 9568, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 9569, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 9572, i64* @assembly_address
  %3 = load i64* %stack_var_-8
  store i64 %3, i64* %rbp
  %4 = ptrtoint i64* %stack_var_0 to i64
  store i64 %4, i64* %rsp
  store volatile i64 9573, i64* @assembly_address
  %5 = call i64 @register_tm_clones()
  store i64 %5, i64* %rax
  store i64 %5, i64* %rax
  store i64 %5, i64* %rax
  %6 = load i64* %rax
  %7 = load i64* %rax
  ret i64 %7
}

define i64 @bi_init(i32 %arg1) {
block_256a:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-8 = alloca i64
  store volatile i64 9578, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 9579, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 9582, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = trunc i64 %4 to i32
  store i32 %5, i32* %stack_var_-12
  store volatile i64 9585, i64* @assembly_address
  %6 = load i32* %stack_var_-12
  %7 = zext i32 %6 to i64
  store i64 %7, i64* %rax
  store volatile i64 9588, i64* @assembly_address
  %8 = load i64* %rax
  %9 = trunc i64 %8 to i32
  store i32 %9, i32* bitcast (i64* @global_var_21658c to i32*)
  store volatile i64 9594, i64* @assembly_address
  store i16 0, i16* bitcast (i64* @global_var_216590 to i16*)
  store volatile i64 9603, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216594 to i32*)
  store volatile i64 9613, i64* @assembly_address
  %10 = load i32* bitcast (i64* @global_var_21658c to i32*)
  %11 = zext i32 %10 to i64
  store i64 %11, i64* %rax
  store volatile i64 9619, i64* @assembly_address
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  %14 = sub i32 %13, -1
  %15 = and i32 %13, 15
  %16 = sub i32 %15, 15
  %17 = icmp ugt i32 %16, 15
  %18 = icmp ult i32 %13, -1
  %19 = xor i32 %13, -1
  %20 = xor i32 %13, %14
  %21 = and i32 %19, %20
  %22 = icmp slt i32 %21, 0
  store i1 %17, i1* %az
  store i1 %18, i1* %cf
  store i1 %22, i1* %of
  %23 = icmp eq i32 %14, 0
  store i1 %23, i1* %zf
  %24 = icmp slt i32 %14, 0
  store i1 %24, i1* %sf
  %25 = trunc i32 %14 to i8
  %26 = call i8 @llvm.ctpop.i8(i8 %25)
  %27 = and i8 %26, 1
  %28 = icmp eq i8 %27, 0
  store i1 %28, i1* %pf
  store volatile i64 9622, i64* @assembly_address
  %29 = load i1* %zf
  br i1 %29, label %block_25a6, label %block_2598

block_2598:                                       ; preds = %block_256a
  store volatile i64 9624, i64* @assembly_address
  store i64 60570, i64* %rax
  store volatile i64 9631, i64* @assembly_address
  %30 = load i64* %rax
  store i64 %30, i64* @global_var_21a420
  br label %block_25a6

block_25a6:                                       ; preds = %block_2598, %block_256a
  store volatile i64 9638, i64* @assembly_address
  store volatile i64 9639, i64* @assembly_address
  %31 = load i64* %stack_var_-8
  store i64 %31, i64* %rbp
  %32 = ptrtoint i64* %stack_var_0 to i64
  store i64 %32, i64* %rsp
  store volatile i64 9640, i64* @assembly_address
  %33 = load i64* %rax
  ret i64 %33
}

declare i64 @158(i64)

define i64 @send_bits(i32 %arg1, i64 %arg2) {
block_25a9:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i32
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i64
  %3 = alloca i32
  %4 = alloca i64
  store volatile i64 9641, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 9642, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 9645, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 16
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 16
  %13 = xor i64 %8, 16
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %23, i64* %rsp
  store volatile i64 9649, i64* @assembly_address
  %24 = load i64* %rdi
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-12
  store volatile i64 9652, i64* @assembly_address
  %26 = load i64* %rsi
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-16
  store volatile i64 9655, i64* @assembly_address
  store i64 16, i64* %rax
  store volatile i64 9660, i64* @assembly_address
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  %30 = load i32* %stack_var_-16
  %31 = sub i32 %29, %30
  %32 = and i32 %29, 15
  %33 = and i32 %30, 15
  %34 = sub i32 %32, %33
  %35 = icmp ugt i32 %34, 15
  %36 = icmp ult i32 %29, %30
  %37 = xor i32 %29, %30
  %38 = xor i32 %29, %31
  %39 = and i32 %37, %38
  %40 = icmp slt i32 %39, 0
  store i1 %35, i1* %az
  store i1 %36, i1* %cf
  store i1 %40, i1* %of
  %41 = icmp eq i32 %31, 0
  store i1 %41, i1* %zf
  %42 = icmp slt i32 %31, 0
  store i1 %42, i1* %sf
  %43 = trunc i32 %31 to i8
  %44 = call i8 @llvm.ctpop.i8(i8 %43)
  %45 = and i8 %44, 1
  %46 = icmp eq i8 %45, 0
  store i1 %46, i1* %pf
  %47 = zext i32 %31 to i64
  store i64 %47, i64* %rax
  store volatile i64 9663, i64* @assembly_address
  %48 = load i64* %rax
  %49 = trunc i64 %48 to i32
  %50 = zext i32 %49 to i64
  store i64 %50, i64* %rdx
  store volatile i64 9665, i64* @assembly_address
  %51 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %52 = zext i32 %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 9671, i64* @assembly_address
  %53 = load i64* %rdx
  %54 = trunc i64 %53 to i32
  %55 = load i64* %rax
  %56 = trunc i64 %55 to i32
  %57 = trunc i64 %53 to i32
  store i32 %57, i32* %3
  %58 = trunc i64 %55 to i32
  store i32 %58, i32* %1
  %59 = sub i32 %54, %56
  %60 = and i32 %54, 15
  %61 = and i32 %56, 15
  %62 = sub i32 %60, %61
  %63 = icmp ugt i32 %62, 15
  %64 = icmp ult i32 %54, %56
  %65 = xor i32 %54, %56
  %66 = xor i32 %54, %59
  %67 = and i32 %65, %66
  %68 = icmp slt i32 %67, 0
  store i1 %63, i1* %az
  store i1 %64, i1* %cf
  store i1 %68, i1* %of
  %69 = icmp eq i32 %59, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 %59, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 %59 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  store volatile i64 9673, i64* @assembly_address
  %75 = load i32* %3
  %76 = sext i32 %75 to i64
  %77 = load i32* %1
  %78 = sext i32 %77 to i64
  %79 = icmp sge i64 %76, %78
  br i1 %79, label %block_26f1, label %block_25cf

block_25cf:                                       ; preds = %block_25a9
  store volatile i64 9679, i64* @assembly_address
  %80 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %81 = zext i32 %80 to i64
  store i64 %81, i64* %rax
  store volatile i64 9685, i64* @assembly_address
  %82 = load i32* %stack_var_-12
  %83 = zext i32 %82 to i64
  store i64 %83, i64* %rdx
  store volatile i64 9688, i64* @assembly_address
  %84 = load i64* %rax
  %85 = trunc i64 %84 to i32
  %86 = zext i32 %85 to i64
  store i64 %86, i64* %rcx
  store volatile i64 9690, i64* @assembly_address
  %87 = load i64* %rdx
  %88 = trunc i64 %87 to i32
  %89 = load i64* %rcx
  %90 = trunc i64 %89 to i8
  %91 = zext i8 %90 to i32
  %92 = and i32 %91, 31
  %93 = load i1* %of
  %94 = icmp eq i32 %92, 0
  br i1 %94, label %112, label %95

; <label>:95                                      ; preds = %block_25cf
  %96 = shl i32 %88, %92
  %97 = icmp eq i32 %96, 0
  store i1 %97, i1* %zf
  %98 = icmp slt i32 %96, 0
  store i1 %98, i1* %sf
  %99 = trunc i32 %96 to i8
  %100 = call i8 @llvm.ctpop.i8(i8 %99)
  %101 = and i8 %100, 1
  %102 = icmp eq i8 %101, 0
  store i1 %102, i1* %pf
  %103 = zext i32 %96 to i64
  store i64 %103, i64* %rdx
  %104 = sub i32 %92, 1
  %105 = shl i32 %88, %104
  %106 = lshr i32 %105, 31
  %107 = trunc i32 %106 to i1
  store i1 %107, i1* %cf
  %108 = lshr i32 %96, 31
  %109 = icmp ne i32 %108, %106
  %110 = icmp eq i32 %92, 1
  %111 = select i1 %110, i1 %109, i1 %93
  store i1 %111, i1* %of
  br label %112

; <label>:112                                     ; preds = %block_25cf, %95
  store volatile i64 9692, i64* @assembly_address
  %113 = load i64* %rdx
  %114 = trunc i64 %113 to i32
  %115 = zext i32 %114 to i64
  store i64 %115, i64* %rax
  store volatile i64 9694, i64* @assembly_address
  %116 = load i64* %rax
  %117 = trunc i64 %116 to i32
  %118 = zext i32 %117 to i64
  store i64 %118, i64* %rdx
  store volatile i64 9696, i64* @assembly_address
  %119 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %120 = zext i16 %119 to i64
  store i64 %120, i64* %rax
  store volatile i64 9703, i64* @assembly_address
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i32
  %123 = load i64* %rdx
  %124 = trunc i64 %123 to i32
  %125 = or i32 %122, %124
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %126 = icmp eq i32 %125, 0
  store i1 %126, i1* %zf
  %127 = icmp slt i32 %125, 0
  store i1 %127, i1* %sf
  %128 = trunc i32 %125 to i8
  %129 = call i8 @llvm.ctpop.i8(i8 %128)
  %130 = and i8 %129, 1
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %pf
  %132 = zext i32 %125 to i64
  store i64 %132, i64* %rax
  store volatile i64 9705, i64* @assembly_address
  %133 = load i64* %rax
  %134 = trunc i64 %133 to i16
  store i16 %134, i16* bitcast (i64* @global_var_216590 to i16*)
  store volatile i64 9712, i64* @assembly_address
  %135 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %136 = zext i32 %135 to i64
  store i64 %136, i64* %rax
  store volatile i64 9718, i64* @assembly_address
  %137 = load i64* %rax
  %138 = trunc i64 %137 to i32
  %139 = sub i32 %138, 16381
  %140 = and i32 %138, 15
  %141 = sub i32 %140, 13
  %142 = icmp ugt i32 %141, 15
  %143 = icmp ult i32 %138, 16381
  %144 = xor i32 %138, 16381
  %145 = xor i32 %138, %139
  %146 = and i32 %144, %145
  %147 = icmp slt i32 %146, 0
  store i1 %142, i1* %az
  store i1 %143, i1* %cf
  store i1 %147, i1* %of
  %148 = icmp eq i32 %139, 0
  store i1 %148, i1* %zf
  %149 = icmp slt i32 %139, 0
  store i1 %149, i1* %sf
  %150 = trunc i32 %139 to i8
  %151 = call i8 @llvm.ctpop.i8(i8 %150)
  %152 = and i8 %151, 1
  %153 = icmp eq i8 %152, 0
  store i1 %153, i1* %pf
  store volatile i64 9723, i64* @assembly_address
  %154 = load i1* %cf
  %155 = load i1* %zf
  %156 = or i1 %154, %155
  %157 = icmp ne i1 %156, true
  br i1 %157, label %block_2649, label %block_25fd

block_25fd:                                       ; preds = %112
  store volatile i64 9725, i64* @assembly_address
  %158 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %159 = zext i16 %158 to i64
  store i64 %159, i64* %rcx
  store volatile i64 9732, i64* @assembly_address
  %160 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %161 = zext i32 %160 to i64
  store i64 %161, i64* %rax
  store volatile i64 9738, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 9741, i64* @assembly_address
  %162 = load i64* %rdx
  %163 = trunc i64 %162 to i32
  store i32 %163, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 9747, i64* @assembly_address
  %164 = load i64* %rax
  %165 = trunc i64 %164 to i32
  %166 = zext i32 %165 to i64
  store i64 %166, i64* %rdx
  store volatile i64 9749, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 9756, i64* @assembly_address
  %167 = load i64* %rcx
  %168 = trunc i64 %167 to i8
  %169 = load i64* %rdx
  %170 = load i64* %rax
  %171 = mul i64 %170, 1
  %172 = add i64 %169, %171
  %173 = inttoptr i64 %172 to i8*
  store i8 %168, i8* %173
  store volatile i64 9759, i64* @assembly_address
  %174 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %175 = zext i16 %174 to i64
  store i64 %175, i64* %rax
  store volatile i64 9766, i64* @assembly_address
  %176 = load i64* %rax
  %177 = trunc i64 %176 to i16
  %178 = load i1* %of
  %179 = lshr i16 %177, 8
  %180 = icmp eq i16 %179, 0
  store i1 %180, i1* %zf
  %181 = icmp slt i16 %179, 0
  store i1 %181, i1* %sf
  %182 = trunc i16 %179 to i8
  %183 = call i8 @llvm.ctpop.i8(i8 %182)
  %184 = and i8 %183, 1
  %185 = icmp eq i8 %184, 0
  store i1 %185, i1* %pf
  %186 = zext i16 %179 to i64
  %187 = load i64* %rax
  %188 = and i64 %187, -65536
  %189 = or i64 %188, %186
  store i64 %189, i64* %rax
  %190 = and i16 128, %177
  %191 = icmp ne i16 %190, 0
  store i1 %191, i1* %cf
  %192 = icmp slt i16 %177, 0
  %193 = select i1 false, i1 %192, i1 %178
  store i1 %193, i1* %of
  store volatile i64 9770, i64* @assembly_address
  %194 = load i64* %rax
  %195 = trunc i64 %194 to i32
  %196 = zext i32 %195 to i64
  store i64 %196, i64* %rcx
  store volatile i64 9772, i64* @assembly_address
  %197 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %198 = zext i32 %197 to i64
  store i64 %198, i64* %rax
  store volatile i64 9778, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 9781, i64* @assembly_address
  %199 = load i64* %rdx
  %200 = trunc i64 %199 to i32
  store i32 %200, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 9787, i64* @assembly_address
  %201 = load i64* %rax
  %202 = trunc i64 %201 to i32
  %203 = zext i32 %202 to i64
  store i64 %203, i64* %rdx
  store volatile i64 9789, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 9796, i64* @assembly_address
  %204 = load i64* %rcx
  %205 = trunc i64 %204 to i8
  %206 = load i64* %rdx
  %207 = load i64* %rax
  %208 = mul i64 %207, 1
  %209 = add i64 %206, %208
  %210 = inttoptr i64 %209 to i8*
  store i8 %205, i8* %210
  store volatile i64 9799, i64* @assembly_address
  br label %block_26b7

block_2649:                                       ; preds = %112
  store volatile i64 9801, i64* @assembly_address
  %211 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %212 = zext i16 %211 to i64
  store i64 %212, i64* %rcx
  store volatile i64 9808, i64* @assembly_address
  %213 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %214 = zext i32 %213 to i64
  store i64 %214, i64* %rax
  store volatile i64 9814, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 9817, i64* @assembly_address
  %215 = load i64* %rdx
  %216 = trunc i64 %215 to i32
  store i32 %216, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 9823, i64* @assembly_address
  %217 = load i64* %rax
  %218 = trunc i64 %217 to i32
  %219 = zext i32 %218 to i64
  store i64 %219, i64* %rdx
  store volatile i64 9825, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 9832, i64* @assembly_address
  %220 = load i64* %rcx
  %221 = trunc i64 %220 to i8
  %222 = load i64* %rdx
  %223 = load i64* %rax
  %224 = mul i64 %223, 1
  %225 = add i64 %222, %224
  %226 = inttoptr i64 %225 to i8*
  store i8 %221, i8* %226
  store volatile i64 9835, i64* @assembly_address
  %227 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %228 = zext i32 %227 to i64
  store i64 %228, i64* %rax
  store volatile i64 9841, i64* @assembly_address
  %229 = load i64* %rax
  %230 = trunc i64 %229 to i32
  %231 = sub i32 %230, 16384
  %232 = and i32 %230, 15
  %233 = icmp ugt i32 %232, 15
  %234 = icmp ult i32 %230, 16384
  %235 = xor i32 %230, 16384
  %236 = xor i32 %230, %231
  %237 = and i32 %235, %236
  %238 = icmp slt i32 %237, 0
  store i1 %233, i1* %az
  store i1 %234, i1* %cf
  store i1 %238, i1* %of
  %239 = icmp eq i32 %231, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i32 %231, 0
  store i1 %240, i1* %sf
  %241 = trunc i32 %231 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  store volatile i64 9846, i64* @assembly_address
  %245 = load i1* %zf
  %246 = icmp eq i1 %245, false
  br i1 %246, label %block_267d, label %block_2678

block_2678:                                       ; preds = %block_2649
  store volatile i64 9848, i64* @assembly_address
  %247 = load i64* %rdi
  %248 = load i64* %rsi
  %249 = load i64* %rdx
  %250 = load i64* %rcx
  %251 = trunc i64 %250 to i16
  %252 = call i64 @flush_outbuf(i64 %247, i64 %248, i64 %249, i16 %251)
  store i64 %252, i64* %rax
  store i64 %252, i64* %rax
  store i64 %252, i64* %rax
  br label %block_267d

block_267d:                                       ; preds = %block_2678, %block_2649
  store volatile i64 9853, i64* @assembly_address
  %253 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %254 = zext i16 %253 to i64
  store i64 %254, i64* %rax
  store volatile i64 9860, i64* @assembly_address
  %255 = load i64* %rax
  %256 = trunc i64 %255 to i16
  %257 = load i1* %of
  %258 = lshr i16 %256, 8
  %259 = icmp eq i16 %258, 0
  store i1 %259, i1* %zf
  %260 = icmp slt i16 %258, 0
  store i1 %260, i1* %sf
  %261 = trunc i16 %258 to i8
  %262 = call i8 @llvm.ctpop.i8(i8 %261)
  %263 = and i8 %262, 1
  %264 = icmp eq i8 %263, 0
  store i1 %264, i1* %pf
  %265 = zext i16 %258 to i64
  %266 = load i64* %rax
  %267 = and i64 %266, -65536
  %268 = or i64 %267, %265
  store i64 %268, i64* %rax
  %269 = and i16 128, %256
  %270 = icmp ne i16 %269, 0
  store i1 %270, i1* %cf
  %271 = icmp slt i16 %256, 0
  %272 = select i1 false, i1 %271, i1 %257
  store i1 %272, i1* %of
  store volatile i64 9864, i64* @assembly_address
  %273 = load i64* %rax
  %274 = trunc i64 %273 to i32
  %275 = zext i32 %274 to i64
  store i64 %275, i64* %rcx
  store volatile i64 9866, i64* @assembly_address
  %276 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %277 = zext i32 %276 to i64
  store i64 %277, i64* %rax
  store volatile i64 9872, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 9875, i64* @assembly_address
  %278 = load i64* %rdx
  %279 = trunc i64 %278 to i32
  store i32 %279, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 9881, i64* @assembly_address
  %280 = load i64* %rax
  %281 = trunc i64 %280 to i32
  %282 = zext i32 %281 to i64
  store i64 %282, i64* %rdx
  store volatile i64 9883, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 9890, i64* @assembly_address
  %283 = load i64* %rcx
  %284 = trunc i64 %283 to i8
  %285 = load i64* %rdx
  %286 = load i64* %rax
  %287 = mul i64 %286, 1
  %288 = add i64 %285, %287
  %289 = inttoptr i64 %288 to i8*
  store i8 %284, i8* %289
  store volatile i64 9893, i64* @assembly_address
  %290 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %291 = zext i32 %290 to i64
  store i64 %291, i64* %rax
  store volatile i64 9899, i64* @assembly_address
  %292 = load i64* %rax
  %293 = trunc i64 %292 to i32
  %294 = sub i32 %293, 16384
  %295 = and i32 %293, 15
  %296 = icmp ugt i32 %295, 15
  %297 = icmp ult i32 %293, 16384
  %298 = xor i32 %293, 16384
  %299 = xor i32 %293, %294
  %300 = and i32 %298, %299
  %301 = icmp slt i32 %300, 0
  store i1 %296, i1* %az
  store i1 %297, i1* %cf
  store i1 %301, i1* %of
  %302 = icmp eq i32 %294, 0
  store i1 %302, i1* %zf
  %303 = icmp slt i32 %294, 0
  store i1 %303, i1* %sf
  %304 = trunc i32 %294 to i8
  %305 = call i8 @llvm.ctpop.i8(i8 %304)
  %306 = and i8 %305, 1
  %307 = icmp eq i8 %306, 0
  store i1 %307, i1* %pf
  store volatile i64 9904, i64* @assembly_address
  %308 = load i1* %zf
  %309 = icmp eq i1 %308, false
  br i1 %309, label %block_26b7, label %block_26b2

block_26b2:                                       ; preds = %block_267d
  store volatile i64 9906, i64* @assembly_address
  %310 = load i64* %rdi
  %311 = load i64* %rsi
  %312 = load i64* %rdx
  %313 = load i64* %rcx
  %314 = trunc i64 %313 to i16
  %315 = call i64 @flush_outbuf(i64 %310, i64 %311, i64 %312, i16 %314)
  store i64 %315, i64* %rax
  store i64 %315, i64* %rax
  store i64 %315, i64* %rax
  br label %block_26b7

block_26b7:                                       ; preds = %block_26b2, %block_267d, %block_25fd
  store volatile i64 9911, i64* @assembly_address
  %316 = load i32* %stack_var_-12
  %317 = zext i32 %316 to i64
  store i64 %317, i64* %rax
  store volatile i64 9914, i64* @assembly_address
  %318 = load i64* %rax
  %319 = trunc i64 %318 to i16
  %320 = zext i16 %319 to i64
  store i64 %320, i64* %rdx
  store volatile i64 9917, i64* @assembly_address
  %321 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %322 = zext i32 %321 to i64
  store i64 %322, i64* %rax
  store volatile i64 9923, i64* @assembly_address
  store i64 16, i64* %rcx
  store volatile i64 9928, i64* @assembly_address
  %323 = load i64* %rcx
  %324 = trunc i64 %323 to i32
  %325 = load i64* %rax
  %326 = trunc i64 %325 to i32
  %327 = sub i32 %324, %326
  %328 = and i32 %324, 15
  %329 = and i32 %326, 15
  %330 = sub i32 %328, %329
  %331 = icmp ugt i32 %330, 15
  %332 = icmp ult i32 %324, %326
  %333 = xor i32 %324, %326
  %334 = xor i32 %324, %327
  %335 = and i32 %333, %334
  %336 = icmp slt i32 %335, 0
  store i1 %331, i1* %az
  store i1 %332, i1* %cf
  store i1 %336, i1* %of
  %337 = icmp eq i32 %327, 0
  store i1 %337, i1* %zf
  %338 = icmp slt i32 %327, 0
  store i1 %338, i1* %sf
  %339 = trunc i32 %327 to i8
  %340 = call i8 @llvm.ctpop.i8(i8 %339)
  %341 = and i8 %340, 1
  %342 = icmp eq i8 %341, 0
  store i1 %342, i1* %pf
  %343 = zext i32 %327 to i64
  store i64 %343, i64* %rcx
  store volatile i64 9930, i64* @assembly_address
  %344 = load i64* %rcx
  %345 = trunc i64 %344 to i32
  %346 = zext i32 %345 to i64
  store i64 %346, i64* %rax
  store volatile i64 9932, i64* @assembly_address
  %347 = load i64* %rax
  %348 = trunc i64 %347 to i32
  %349 = zext i32 %348 to i64
  store i64 %349, i64* %rcx
  store volatile i64 9934, i64* @assembly_address
  %350 = load i64* %rdx
  %351 = trunc i64 %350 to i32
  %352 = load i64* %rcx
  %353 = trunc i64 %352 to i8
  %354 = zext i8 %353 to i32
  %355 = and i32 %354, 31
  %356 = load i1* %of
  %357 = icmp eq i32 %355, 0
  br i1 %357, label %373, label %358

; <label>:358                                     ; preds = %block_26b7
  %359 = ashr i32 %351, %355
  %360 = icmp eq i32 %359, 0
  store i1 %360, i1* %zf
  %361 = icmp slt i32 %359, 0
  store i1 %361, i1* %sf
  %362 = trunc i32 %359 to i8
  %363 = call i8 @llvm.ctpop.i8(i8 %362)
  %364 = and i8 %363, 1
  %365 = icmp eq i8 %364, 0
  store i1 %365, i1* %pf
  %366 = zext i32 %359 to i64
  store i64 %366, i64* %rdx
  %367 = sub i32 %355, 1
  %368 = shl i32 1, %367
  %369 = and i32 %368, %351
  %370 = icmp ne i32 %369, 0
  store i1 %370, i1* %cf
  %371 = icmp eq i32 %355, 1
  %372 = select i1 %371, i1 false, i1 %356
  store i1 %372, i1* %of
  br label %373

; <label>:373                                     ; preds = %block_26b7, %358
  store volatile i64 9936, i64* @assembly_address
  %374 = load i64* %rdx
  %375 = trunc i64 %374 to i32
  %376 = zext i32 %375 to i64
  store i64 %376, i64* %rax
  store volatile i64 9938, i64* @assembly_address
  %377 = load i64* %rax
  %378 = trunc i64 %377 to i16
  store i16 %378, i16* bitcast (i64* @global_var_216590 to i16*)
  store volatile i64 9945, i64* @assembly_address
  %379 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %380 = zext i32 %379 to i64
  store i64 %380, i64* %rax
  store volatile i64 9951, i64* @assembly_address
  %381 = load i64* %rax
  %382 = trunc i64 %381 to i32
  %383 = zext i32 %382 to i64
  store i64 %383, i64* %rdx
  store volatile i64 9953, i64* @assembly_address
  %384 = load i32* %stack_var_-16
  %385 = zext i32 %384 to i64
  store i64 %385, i64* %rax
  store volatile i64 9956, i64* @assembly_address
  %386 = load i64* %rax
  %387 = trunc i64 %386 to i32
  %388 = load i64* %rdx
  %389 = trunc i64 %388 to i32
  %390 = add i32 %387, %389
  %391 = and i32 %387, 15
  %392 = and i32 %389, 15
  %393 = add i32 %391, %392
  %394 = icmp ugt i32 %393, 15
  %395 = icmp ult i32 %390, %387
  %396 = xor i32 %387, %390
  %397 = xor i32 %389, %390
  %398 = and i32 %396, %397
  %399 = icmp slt i32 %398, 0
  store i1 %394, i1* %az
  store i1 %395, i1* %cf
  store i1 %399, i1* %of
  %400 = icmp eq i32 %390, 0
  store i1 %400, i1* %zf
  %401 = icmp slt i32 %390, 0
  store i1 %401, i1* %sf
  %402 = trunc i32 %390 to i8
  %403 = call i8 @llvm.ctpop.i8(i8 %402)
  %404 = and i8 %403, 1
  %405 = icmp eq i8 %404, 0
  store i1 %405, i1* %pf
  %406 = zext i32 %390 to i64
  store i64 %406, i64* %rax
  store volatile i64 9958, i64* @assembly_address
  %407 = load i64* %rax
  %408 = trunc i64 %407 to i32
  %409 = sub i32 %408, 16
  %410 = and i32 %408, 15
  %411 = icmp ugt i32 %410, 15
  %412 = icmp ult i32 %408, 16
  %413 = xor i32 %408, 16
  %414 = xor i32 %408, %409
  %415 = and i32 %413, %414
  %416 = icmp slt i32 %415, 0
  store i1 %411, i1* %az
  store i1 %412, i1* %cf
  store i1 %416, i1* %of
  %417 = icmp eq i32 %409, 0
  store i1 %417, i1* %zf
  %418 = icmp slt i32 %409, 0
  store i1 %418, i1* %sf
  %419 = trunc i32 %409 to i8
  %420 = call i8 @llvm.ctpop.i8(i8 %419)
  %421 = and i8 %420, 1
  %422 = icmp eq i8 %421, 0
  store i1 %422, i1* %pf
  %423 = zext i32 %409 to i64
  store i64 %423, i64* %rax
  store volatile i64 9961, i64* @assembly_address
  %424 = load i64* %rax
  %425 = trunc i64 %424 to i32
  store i32 %425, i32* bitcast (i64* @global_var_216594 to i32*)
  store volatile i64 9967, i64* @assembly_address
  br label %block_2723

block_26f1:                                       ; preds = %block_25a9
  store volatile i64 9969, i64* @assembly_address
  %426 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %427 = zext i32 %426 to i64
  store i64 %427, i64* %rax
  store volatile i64 9975, i64* @assembly_address
  %428 = load i32* %stack_var_-12
  %429 = zext i32 %428 to i64
  store i64 %429, i64* %rdx
  store volatile i64 9978, i64* @assembly_address
  %430 = load i64* %rax
  %431 = trunc i64 %430 to i32
  %432 = zext i32 %431 to i64
  store i64 %432, i64* %rcx
  store volatile i64 9980, i64* @assembly_address
  %433 = load i64* %rdx
  %434 = trunc i64 %433 to i32
  %435 = load i64* %rcx
  %436 = trunc i64 %435 to i8
  %437 = zext i8 %436 to i32
  %438 = and i32 %437, 31
  %439 = load i1* %of
  %440 = icmp eq i32 %438, 0
  br i1 %440, label %458, label %441

; <label>:441                                     ; preds = %block_26f1
  %442 = shl i32 %434, %438
  %443 = icmp eq i32 %442, 0
  store i1 %443, i1* %zf
  %444 = icmp slt i32 %442, 0
  store i1 %444, i1* %sf
  %445 = trunc i32 %442 to i8
  %446 = call i8 @llvm.ctpop.i8(i8 %445)
  %447 = and i8 %446, 1
  %448 = icmp eq i8 %447, 0
  store i1 %448, i1* %pf
  %449 = zext i32 %442 to i64
  store i64 %449, i64* %rdx
  %450 = sub i32 %438, 1
  %451 = shl i32 %434, %450
  %452 = lshr i32 %451, 31
  %453 = trunc i32 %452 to i1
  store i1 %453, i1* %cf
  %454 = lshr i32 %442, 31
  %455 = icmp ne i32 %454, %452
  %456 = icmp eq i32 %438, 1
  %457 = select i1 %456, i1 %455, i1 %439
  store i1 %457, i1* %of
  br label %458

; <label>:458                                     ; preds = %block_26f1, %441
  store volatile i64 9982, i64* @assembly_address
  %459 = load i64* %rdx
  %460 = trunc i64 %459 to i32
  %461 = zext i32 %460 to i64
  store i64 %461, i64* %rax
  store volatile i64 9984, i64* @assembly_address
  %462 = load i64* %rax
  %463 = trunc i64 %462 to i32
  %464 = zext i32 %463 to i64
  store i64 %464, i64* %rdx
  store volatile i64 9986, i64* @assembly_address
  %465 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %466 = zext i16 %465 to i64
  store i64 %466, i64* %rax
  store volatile i64 9993, i64* @assembly_address
  %467 = load i64* %rax
  %468 = trunc i64 %467 to i32
  %469 = load i64* %rdx
  %470 = trunc i64 %469 to i32
  %471 = or i32 %468, %470
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %472 = icmp eq i32 %471, 0
  store i1 %472, i1* %zf
  %473 = icmp slt i32 %471, 0
  store i1 %473, i1* %sf
  %474 = trunc i32 %471 to i8
  %475 = call i8 @llvm.ctpop.i8(i8 %474)
  %476 = and i8 %475, 1
  %477 = icmp eq i8 %476, 0
  store i1 %477, i1* %pf
  %478 = zext i32 %471 to i64
  store i64 %478, i64* %rax
  store volatile i64 9995, i64* @assembly_address
  %479 = load i64* %rax
  %480 = trunc i64 %479 to i16
  store i16 %480, i16* bitcast (i64* @global_var_216590 to i16*)
  store volatile i64 10002, i64* @assembly_address
  %481 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %482 = zext i32 %481 to i64
  store i64 %482, i64* %rdx
  store volatile i64 10008, i64* @assembly_address
  %483 = load i32* %stack_var_-16
  %484 = zext i32 %483 to i64
  store i64 %484, i64* %rax
  store volatile i64 10011, i64* @assembly_address
  %485 = load i64* %rax
  %486 = trunc i64 %485 to i32
  %487 = load i64* %rdx
  %488 = trunc i64 %487 to i32
  %489 = add i32 %486, %488
  %490 = and i32 %486, 15
  %491 = and i32 %488, 15
  %492 = add i32 %490, %491
  %493 = icmp ugt i32 %492, 15
  %494 = icmp ult i32 %489, %486
  %495 = xor i32 %486, %489
  %496 = xor i32 %488, %489
  %497 = and i32 %495, %496
  %498 = icmp slt i32 %497, 0
  store i1 %493, i1* %az
  store i1 %494, i1* %cf
  store i1 %498, i1* %of
  %499 = icmp eq i32 %489, 0
  store i1 %499, i1* %zf
  %500 = icmp slt i32 %489, 0
  store i1 %500, i1* %sf
  %501 = trunc i32 %489 to i8
  %502 = call i8 @llvm.ctpop.i8(i8 %501)
  %503 = and i8 %502, 1
  %504 = icmp eq i8 %503, 0
  store i1 %504, i1* %pf
  %505 = zext i32 %489 to i64
  store i64 %505, i64* %rax
  store volatile i64 10013, i64* @assembly_address
  %506 = load i64* %rax
  %507 = trunc i64 %506 to i32
  store i32 %507, i32* bitcast (i64* @global_var_216594 to i32*)
  br label %block_2723

block_2723:                                       ; preds = %458, %373
  store volatile i64 10019, i64* @assembly_address
  store volatile i64 10020, i64* @assembly_address
  %508 = load i64* %stack_var_-8
  store i64 %508, i64* %rbp
  %509 = ptrtoint i64* %stack_var_0 to i64
  store i64 %509, i64* %rsp
  store volatile i64 10021, i64* @assembly_address
  %510 = load i64* %rax
  ret i64 %510
}

declare i64 @159(i64, i32)

declare i64 @160(i64, i64)

define i64 @bi_reverse(i32 %arg1, i64 %arg2) {
block_2726:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-24 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 10022, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 10023, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 10026, i64* @assembly_address
  %4 = load i64* %rbx
  store i64 %4, i64* %stack_var_-16
  %5 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %5, i64* %rsp
  store volatile i64 10027, i64* @assembly_address
  %6 = load i64* %rdi
  %7 = trunc i64 %6 to i32
  store i32 %7, i32* %stack_var_-20
  store volatile i64 10030, i64* @assembly_address
  %8 = load i64* %rsi
  %9 = trunc i64 %8 to i32
  store i32 %9, i32* %stack_var_-24
  store volatile i64 10033, i64* @assembly_address
  store i64 0, i64* %rbx
  br label %block_2736

block_2736:                                       ; preds = %block_2736, %block_2726
  store volatile i64 10038, i64* @assembly_address
  %10 = load i32* %stack_var_-20
  %11 = zext i32 %10 to i64
  store i64 %11, i64* %rax
  store volatile i64 10041, i64* @assembly_address
  %12 = load i64* %rax
  %13 = trunc i64 %12 to i32
  %14 = and i32 %13, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %15 = icmp eq i32 %14, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i32 %14, 0
  store i1 %16, i1* %sf
  %17 = trunc i32 %14 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = zext i32 %14 to i64
  store i64 %21, i64* %rax
  store volatile i64 10044, i64* @assembly_address
  %22 = load i64* %rbx
  %23 = trunc i64 %22 to i32
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i32
  %26 = or i32 %23, %25
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %27 = icmp eq i32 %26, 0
  store i1 %27, i1* %zf
  %28 = icmp slt i32 %26, 0
  store i1 %28, i1* %sf
  %29 = trunc i32 %26 to i8
  %30 = call i8 @llvm.ctpop.i8(i8 %29)
  %31 = and i8 %30, 1
  %32 = icmp eq i8 %31, 0
  store i1 %32, i1* %pf
  %33 = zext i32 %26 to i64
  store i64 %33, i64* %rbx
  store volatile i64 10046, i64* @assembly_address
  %34 = load i32* %stack_var_-20
  %35 = load i1* %of
  %36 = lshr i32 %34, 1
  %37 = icmp eq i32 %36, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i32 %36, 0
  store i1 %38, i1* %sf
  %39 = trunc i32 %36 to i8
  %40 = call i8 @llvm.ctpop.i8(i8 %39)
  %41 = and i8 %40, 1
  %42 = icmp eq i8 %41, 0
  store i1 %42, i1* %pf
  store i32 %36, i32* %stack_var_-20
  %43 = and i32 1, %34
  %44 = icmp ne i32 %43, 0
  store i1 %44, i1* %cf
  %45 = icmp slt i32 %34, 0
  %46 = select i1 true, i1 %45, i1 %35
  store i1 %46, i1* %of
  store volatile i64 10049, i64* @assembly_address
  %47 = load i64* %rbx
  %48 = trunc i64 %47 to i32
  %49 = load i64* %rbx
  %50 = trunc i64 %49 to i32
  %51 = add i32 %48, %50
  %52 = and i32 %48, 15
  %53 = and i32 %50, 15
  %54 = add i32 %52, %53
  %55 = icmp ugt i32 %54, 15
  %56 = icmp ult i32 %51, %48
  %57 = xor i32 %48, %51
  %58 = xor i32 %50, %51
  %59 = and i32 %57, %58
  %60 = icmp slt i32 %59, 0
  store i1 %55, i1* %az
  store i1 %56, i1* %cf
  store i1 %60, i1* %of
  %61 = icmp eq i32 %51, 0
  store i1 %61, i1* %zf
  %62 = icmp slt i32 %51, 0
  store i1 %62, i1* %sf
  %63 = trunc i32 %51 to i8
  %64 = call i8 @llvm.ctpop.i8(i8 %63)
  %65 = and i8 %64, 1
  %66 = icmp eq i8 %65, 0
  store i1 %66, i1* %pf
  %67 = zext i32 %51 to i64
  store i64 %67, i64* %rbx
  store volatile i64 10051, i64* @assembly_address
  %68 = load i32* %stack_var_-24
  %69 = sub i32 %68, 1
  %70 = and i32 %68, 15
  %71 = sub i32 %70, 1
  %72 = icmp ugt i32 %71, 15
  %73 = icmp ult i32 %68, 1
  %74 = xor i32 %68, 1
  %75 = xor i32 %68, %69
  %76 = and i32 %74, %75
  %77 = icmp slt i32 %76, 0
  store i1 %72, i1* %az
  store i1 %73, i1* %cf
  store i1 %77, i1* %of
  %78 = icmp eq i32 %69, 0
  store i1 %78, i1* %zf
  %79 = icmp slt i32 %69, 0
  store i1 %79, i1* %sf
  %80 = trunc i32 %69 to i8
  %81 = call i8 @llvm.ctpop.i8(i8 %80)
  %82 = and i8 %81, 1
  %83 = icmp eq i8 %82, 0
  store i1 %83, i1* %pf
  store i32 %69, i32* %stack_var_-24
  store volatile i64 10055, i64* @assembly_address
  %84 = load i32* %stack_var_-24
  %85 = and i32 %84, 15
  %86 = icmp ugt i32 %85, 15
  %87 = icmp ult i32 %84, 0
  %88 = xor i32 %84, 0
  %89 = and i32 %88, 0
  %90 = icmp slt i32 %89, 0
  store i1 %86, i1* %az
  store i1 %87, i1* %cf
  store i1 %90, i1* %of
  %91 = icmp eq i32 %84, 0
  store i1 %91, i1* %zf
  %92 = icmp slt i32 %84, 0
  store i1 %92, i1* %sf
  %93 = trunc i32 %84 to i8
  %94 = call i8 @llvm.ctpop.i8(i8 %93)
  %95 = and i8 %94, 1
  %96 = icmp eq i8 %95, 0
  store i1 %96, i1* %pf
  store volatile i64 10059, i64* @assembly_address
  %97 = load i1* %zf
  %98 = load i1* %sf
  %99 = load i1* %of
  %100 = icmp eq i1 %98, %99
  %101 = icmp eq i1 %97, false
  %102 = icmp eq i1 %100, %101
  br i1 %102, label %block_2736, label %block_274d

block_274d:                                       ; preds = %block_2736
  store volatile i64 10061, i64* @assembly_address
  %103 = load i64* %rbx
  %104 = trunc i64 %103 to i32
  %105 = zext i32 %104 to i64
  store i64 %105, i64* %rax
  store volatile i64 10063, i64* @assembly_address
  %106 = load i64* %rax
  %107 = trunc i64 %106 to i32
  %108 = load i1* %of
  %109 = lshr i32 %107, 1
  %110 = icmp eq i32 %109, 0
  store i1 %110, i1* %zf
  %111 = icmp slt i32 %109, 0
  store i1 %111, i1* %sf
  %112 = trunc i32 %109 to i8
  %113 = call i8 @llvm.ctpop.i8(i8 %112)
  %114 = and i8 %113, 1
  %115 = icmp eq i8 %114, 0
  store i1 %115, i1* %pf
  %116 = zext i32 %109 to i64
  store i64 %116, i64* %rax
  %117 = and i32 1, %107
  %118 = icmp ne i32 %117, 0
  store i1 %118, i1* %cf
  %119 = icmp slt i32 %107, 0
  %120 = select i1 true, i1 %119, i1 %108
  store i1 %120, i1* %of
  store volatile i64 10065, i64* @assembly_address
  %121 = load i64* %stack_var_-16
  store i64 %121, i64* %rbx
  %122 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %122, i64* %rsp
  store volatile i64 10066, i64* @assembly_address
  %123 = load i64* %stack_var_-8
  store i64 %123, i64* %rbp
  %124 = ptrtoint i64* %stack_var_0 to i64
  store i64 %124, i64* %rsp
  store volatile i64 10067, i64* @assembly_address
  %125 = load i64* %rax
  ret i64 %125
}

declare i64 @161(i64, i32)

declare i64 @162(i64, i64)

define i64 @bi_windup() {
block_2754:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  %0 = alloca i32
  %1 = alloca i32
  %2 = alloca i64
  store volatile i64 10068, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 10069, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 10072, i64* @assembly_address
  %6 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %7 = zext i32 %6 to i64
  store i64 %7, i64* %rax
  store volatile i64 10078, i64* @assembly_address
  %8 = load i64* %rax
  %9 = trunc i64 %8 to i32
  %10 = trunc i64 %8 to i32
  store i32 %10, i32* %1
  store i32 8, i32* %0
  %11 = sub i32 %9, 8
  %12 = and i32 %9, 15
  %13 = sub i32 %12, 8
  %14 = icmp ugt i32 %13, 15
  %15 = icmp ult i32 %9, 8
  %16 = xor i32 %9, 8
  %17 = xor i32 %9, %11
  %18 = and i32 %16, %17
  %19 = icmp slt i32 %18, 0
  store i1 %14, i1* %az
  store i1 %15, i1* %cf
  store i1 %19, i1* %of
  %20 = icmp eq i32 %11, 0
  store i1 %20, i1* %zf
  %21 = icmp slt i32 %11, 0
  store i1 %21, i1* %sf
  %22 = trunc i32 %11 to i8
  %23 = call i8 @llvm.ctpop.i8(i8 %22)
  %24 = and i8 %23, 1
  %25 = icmp eq i8 %24, 0
  store i1 %25, i1* %pf
  store volatile i64 10081, i64* @assembly_address
  %26 = load i32* %1
  %27 = sext i32 %26 to i64
  %28 = load i32* %0
  %29 = trunc i64 %27 to i32
  %30 = icmp sle i32 %29, %28
  br i1 %30, label %block_2833, label %block_2767

block_2767:                                       ; preds = %block_2754
  store volatile i64 10087, i64* @assembly_address
  %31 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %32 = zext i32 %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 10093, i64* @assembly_address
  %33 = load i64* %rax
  %34 = trunc i64 %33 to i32
  %35 = sub i32 %34, 16381
  %36 = and i32 %34, 15
  %37 = sub i32 %36, 13
  %38 = icmp ugt i32 %37, 15
  %39 = icmp ult i32 %34, 16381
  %40 = xor i32 %34, 16381
  %41 = xor i32 %34, %35
  %42 = and i32 %40, %41
  %43 = icmp slt i32 %42, 0
  store i1 %38, i1* %az
  store i1 %39, i1* %cf
  store i1 %43, i1* %of
  %44 = icmp eq i32 %35, 0
  store i1 %44, i1* %zf
  %45 = icmp slt i32 %35, 0
  store i1 %45, i1* %sf
  %46 = trunc i32 %35 to i8
  %47 = call i8 @llvm.ctpop.i8(i8 %46)
  %48 = and i8 %47, 1
  %49 = icmp eq i8 %48, 0
  store i1 %49, i1* %pf
  store volatile i64 10098, i64* @assembly_address
  %50 = load i1* %cf
  %51 = load i1* %zf
  %52 = or i1 %50, %51
  %53 = icmp ne i1 %52, true
  br i1 %53, label %block_27c3, label %block_2774

block_2774:                                       ; preds = %block_2767
  store volatile i64 10100, i64* @assembly_address
  %54 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %55 = zext i16 %54 to i64
  store i64 %55, i64* %rcx
  store volatile i64 10107, i64* @assembly_address
  %56 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %57 = zext i32 %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 10113, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10116, i64* @assembly_address
  %58 = load i64* %rdx
  %59 = trunc i64 %58 to i32
  store i32 %59, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10122, i64* @assembly_address
  %60 = load i64* %rax
  %61 = trunc i64 %60 to i32
  %62 = zext i32 %61 to i64
  store i64 %62, i64* %rdx
  store volatile i64 10124, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10131, i64* @assembly_address
  %63 = load i64* %rcx
  %64 = trunc i64 %63 to i8
  %65 = load i64* %rdx
  %66 = load i64* %rax
  %67 = mul i64 %66, 1
  %68 = add i64 %65, %67
  %69 = inttoptr i64 %68 to i8*
  store i8 %64, i8* %69
  store volatile i64 10134, i64* @assembly_address
  %70 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %71 = zext i16 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 10141, i64* @assembly_address
  %72 = load i64* %rax
  %73 = trunc i64 %72 to i16
  %74 = load i1* %of
  %75 = lshr i16 %73, 8
  %76 = icmp eq i16 %75, 0
  store i1 %76, i1* %zf
  %77 = icmp slt i16 %75, 0
  store i1 %77, i1* %sf
  %78 = trunc i16 %75 to i8
  %79 = call i8 @llvm.ctpop.i8(i8 %78)
  %80 = and i8 %79, 1
  %81 = icmp eq i8 %80, 0
  store i1 %81, i1* %pf
  %82 = zext i16 %75 to i64
  %83 = load i64* %rax
  %84 = and i64 %83, -65536
  %85 = or i64 %84, %82
  store i64 %85, i64* %rax
  %86 = and i16 128, %73
  %87 = icmp ne i16 %86, 0
  store i1 %87, i1* %cf
  %88 = icmp slt i16 %73, 0
  %89 = select i1 false, i1 %88, i1 %74
  store i1 %89, i1* %of
  store volatile i64 10145, i64* @assembly_address
  %90 = load i64* %rax
  %91 = trunc i64 %90 to i32
  %92 = zext i32 %91 to i64
  store i64 %92, i64* %rcx
  store volatile i64 10147, i64* @assembly_address
  %93 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %94 = zext i32 %93 to i64
  store i64 %94, i64* %rax
  store volatile i64 10153, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10156, i64* @assembly_address
  %95 = load i64* %rdx
  %96 = trunc i64 %95 to i32
  store i32 %96, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10162, i64* @assembly_address
  %97 = load i64* %rax
  %98 = trunc i64 %97 to i32
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rdx
  store volatile i64 10164, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10171, i64* @assembly_address
  %100 = load i64* %rcx
  %101 = trunc i64 %100 to i8
  %102 = load i64* %rdx
  %103 = load i64* %rax
  %104 = mul i64 %103, 1
  %105 = add i64 %102, %104
  %106 = inttoptr i64 %105 to i8*
  store i8 %101, i8* %106
  store volatile i64 10174, i64* @assembly_address
  br label %block_2871

block_27c3:                                       ; preds = %block_2767
  store volatile i64 10179, i64* @assembly_address
  %107 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %108 = zext i16 %107 to i64
  store i64 %108, i64* %rcx
  store volatile i64 10186, i64* @assembly_address
  %109 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %110 = zext i32 %109 to i64
  store i64 %110, i64* %rax
  store volatile i64 10192, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10195, i64* @assembly_address
  %111 = load i64* %rdx
  %112 = trunc i64 %111 to i32
  store i32 %112, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10201, i64* @assembly_address
  %113 = load i64* %rax
  %114 = trunc i64 %113 to i32
  %115 = zext i32 %114 to i64
  store i64 %115, i64* %rdx
  store volatile i64 10203, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10210, i64* @assembly_address
  %116 = load i64* %rcx
  %117 = trunc i64 %116 to i8
  %118 = load i64* %rdx
  %119 = load i64* %rax
  %120 = mul i64 %119, 1
  %121 = add i64 %118, %120
  %122 = inttoptr i64 %121 to i8*
  store i8 %117, i8* %122
  store volatile i64 10213, i64* @assembly_address
  %123 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %124 = zext i32 %123 to i64
  store i64 %124, i64* %rax
  store volatile i64 10219, i64* @assembly_address
  %125 = load i64* %rax
  %126 = trunc i64 %125 to i32
  %127 = sub i32 %126, 16384
  %128 = and i32 %126, 15
  %129 = icmp ugt i32 %128, 15
  %130 = icmp ult i32 %126, 16384
  %131 = xor i32 %126, 16384
  %132 = xor i32 %126, %127
  %133 = and i32 %131, %132
  %134 = icmp slt i32 %133, 0
  store i1 %129, i1* %az
  store i1 %130, i1* %cf
  store i1 %134, i1* %of
  %135 = icmp eq i32 %127, 0
  store i1 %135, i1* %zf
  %136 = icmp slt i32 %127, 0
  store i1 %136, i1* %sf
  %137 = trunc i32 %127 to i8
  %138 = call i8 @llvm.ctpop.i8(i8 %137)
  %139 = and i8 %138, 1
  %140 = icmp eq i8 %139, 0
  store i1 %140, i1* %pf
  store volatile i64 10224, i64* @assembly_address
  %141 = load i1* %zf
  %142 = icmp eq i1 %141, false
  br i1 %142, label %block_27f7, label %block_27f2

block_27f2:                                       ; preds = %block_27c3
  store volatile i64 10226, i64* @assembly_address
  %143 = load i64* %rdi
  %144 = load i64* %rsi
  %145 = load i64* %rdx
  %146 = load i64* %rcx
  %147 = trunc i64 %146 to i16
  %148 = call i64 @flush_outbuf(i64 %143, i64 %144, i64 %145, i16 %147)
  store i64 %148, i64* %rax
  store i64 %148, i64* %rax
  store i64 %148, i64* %rax
  br label %block_27f7

block_27f7:                                       ; preds = %block_27f2, %block_27c3
  store volatile i64 10231, i64* @assembly_address
  %149 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %150 = zext i16 %149 to i64
  store i64 %150, i64* %rax
  store volatile i64 10238, i64* @assembly_address
  %151 = load i64* %rax
  %152 = trunc i64 %151 to i16
  %153 = load i1* %of
  %154 = lshr i16 %152, 8
  %155 = icmp eq i16 %154, 0
  store i1 %155, i1* %zf
  %156 = icmp slt i16 %154, 0
  store i1 %156, i1* %sf
  %157 = trunc i16 %154 to i8
  %158 = call i8 @llvm.ctpop.i8(i8 %157)
  %159 = and i8 %158, 1
  %160 = icmp eq i8 %159, 0
  store i1 %160, i1* %pf
  %161 = zext i16 %154 to i64
  %162 = load i64* %rax
  %163 = and i64 %162, -65536
  %164 = or i64 %163, %161
  store i64 %164, i64* %rax
  %165 = and i16 128, %152
  %166 = icmp ne i16 %165, 0
  store i1 %166, i1* %cf
  %167 = icmp slt i16 %152, 0
  %168 = select i1 false, i1 %167, i1 %153
  store i1 %168, i1* %of
  store volatile i64 10242, i64* @assembly_address
  %169 = load i64* %rax
  %170 = trunc i64 %169 to i32
  %171 = zext i32 %170 to i64
  store i64 %171, i64* %rcx
  store volatile i64 10244, i64* @assembly_address
  %172 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %173 = zext i32 %172 to i64
  store i64 %173, i64* %rax
  store volatile i64 10250, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10253, i64* @assembly_address
  %174 = load i64* %rdx
  %175 = trunc i64 %174 to i32
  store i32 %175, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10259, i64* @assembly_address
  %176 = load i64* %rax
  %177 = trunc i64 %176 to i32
  %178 = zext i32 %177 to i64
  store i64 %178, i64* %rdx
  store volatile i64 10261, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10268, i64* @assembly_address
  %179 = load i64* %rcx
  %180 = trunc i64 %179 to i8
  %181 = load i64* %rdx
  %182 = load i64* %rax
  %183 = mul i64 %182, 1
  %184 = add i64 %181, %183
  %185 = inttoptr i64 %184 to i8*
  store i8 %180, i8* %185
  store volatile i64 10271, i64* @assembly_address
  %186 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %187 = zext i32 %186 to i64
  store i64 %187, i64* %rax
  store volatile i64 10277, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  %190 = sub i32 %189, 16384
  %191 = and i32 %189, 15
  %192 = icmp ugt i32 %191, 15
  %193 = icmp ult i32 %189, 16384
  %194 = xor i32 %189, 16384
  %195 = xor i32 %189, %190
  %196 = and i32 %194, %195
  %197 = icmp slt i32 %196, 0
  store i1 %192, i1* %az
  store i1 %193, i1* %cf
  store i1 %197, i1* %of
  %198 = icmp eq i32 %190, 0
  store i1 %198, i1* %zf
  %199 = icmp slt i32 %190, 0
  store i1 %199, i1* %sf
  %200 = trunc i32 %190 to i8
  %201 = call i8 @llvm.ctpop.i8(i8 %200)
  %202 = and i8 %201, 1
  %203 = icmp eq i8 %202, 0
  store i1 %203, i1* %pf
  store volatile i64 10282, i64* @assembly_address
  %204 = load i1* %zf
  %205 = icmp eq i1 %204, false
  br i1 %205, label %block_2871, label %block_282c

block_282c:                                       ; preds = %block_27f7
  store volatile i64 10284, i64* @assembly_address
  %206 = load i64* %rdi
  %207 = load i64* %rsi
  %208 = load i64* %rdx
  %209 = load i64* %rcx
  %210 = trunc i64 %209 to i16
  %211 = call i64 @flush_outbuf(i64 %206, i64 %207, i64 %208, i16 %210)
  store i64 %211, i64* %rax
  store i64 %211, i64* %rax
  store i64 %211, i64* %rax
  store volatile i64 10289, i64* @assembly_address
  br label %block_2871

block_2833:                                       ; preds = %block_2754
  store volatile i64 10291, i64* @assembly_address
  %212 = load i32* bitcast (i64* @global_var_216594 to i32*)
  %213 = zext i32 %212 to i64
  store i64 %213, i64* %rax
  store volatile i64 10297, i64* @assembly_address
  %214 = load i64* %rax
  %215 = trunc i64 %214 to i32
  %216 = load i64* %rax
  %217 = trunc i64 %216 to i32
  %218 = and i32 %215, %217
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %219 = icmp eq i32 %218, 0
  store i1 %219, i1* %zf
  %220 = icmp slt i32 %218, 0
  store i1 %220, i1* %sf
  %221 = trunc i32 %218 to i8
  %222 = call i8 @llvm.ctpop.i8(i8 %221)
  %223 = and i8 %222, 1
  %224 = icmp eq i8 %223, 0
  store i1 %224, i1* %pf
  store volatile i64 10299, i64* @assembly_address
  %225 = load i1* %zf
  %226 = load i1* %sf
  %227 = load i1* %of
  %228 = icmp ne i1 %226, %227
  %229 = or i1 %225, %228
  br i1 %229, label %block_2871, label %block_283d

block_283d:                                       ; preds = %block_2833
  store volatile i64 10301, i64* @assembly_address
  %230 = load i16* bitcast (i64* @global_var_216590 to i16*)
  %231 = zext i16 %230 to i64
  store i64 %231, i64* %rcx
  store volatile i64 10308, i64* @assembly_address
  %232 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %233 = zext i32 %232 to i64
  store i64 %233, i64* %rax
  store volatile i64 10314, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10317, i64* @assembly_address
  %234 = load i64* %rdx
  %235 = trunc i64 %234 to i32
  store i32 %235, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10323, i64* @assembly_address
  %236 = load i64* %rax
  %237 = trunc i64 %236 to i32
  %238 = zext i32 %237 to i64
  store i64 %238, i64* %rdx
  store volatile i64 10325, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10332, i64* @assembly_address
  %239 = load i64* %rcx
  %240 = trunc i64 %239 to i8
  %241 = load i64* %rdx
  %242 = load i64* %rax
  %243 = mul i64 %242, 1
  %244 = add i64 %241, %243
  %245 = inttoptr i64 %244 to i8*
  store i8 %240, i8* %245
  store volatile i64 10335, i64* @assembly_address
  %246 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %247 = zext i32 %246 to i64
  store i64 %247, i64* %rax
  store volatile i64 10341, i64* @assembly_address
  %248 = load i64* %rax
  %249 = trunc i64 %248 to i32
  %250 = sub i32 %249, 16384
  %251 = and i32 %249, 15
  %252 = icmp ugt i32 %251, 15
  %253 = icmp ult i32 %249, 16384
  %254 = xor i32 %249, 16384
  %255 = xor i32 %249, %250
  %256 = and i32 %254, %255
  %257 = icmp slt i32 %256, 0
  store i1 %252, i1* %az
  store i1 %253, i1* %cf
  store i1 %257, i1* %of
  %258 = icmp eq i32 %250, 0
  store i1 %258, i1* %zf
  %259 = icmp slt i32 %250, 0
  store i1 %259, i1* %sf
  %260 = trunc i32 %250 to i8
  %261 = call i8 @llvm.ctpop.i8(i8 %260)
  %262 = and i8 %261, 1
  %263 = icmp eq i8 %262, 0
  store i1 %263, i1* %pf
  store volatile i64 10346, i64* @assembly_address
  %264 = load i1* %zf
  %265 = icmp eq i1 %264, false
  br i1 %265, label %block_2871, label %block_286c

block_286c:                                       ; preds = %block_283d
  store volatile i64 10348, i64* @assembly_address
  %266 = load i64* %rdi
  %267 = load i64* %rsi
  %268 = load i64* %rdx
  %269 = load i64* %rcx
  %270 = trunc i64 %269 to i16
  %271 = call i64 @flush_outbuf(i64 %266, i64 %267, i64 %268, i16 %270)
  store i64 %271, i64* %rax
  store i64 %271, i64* %rax
  store i64 %271, i64* %rax
  br label %block_2871

block_2871:                                       ; preds = %block_286c, %block_283d, %block_2833, %block_282c, %block_27f7, %block_2774
  store volatile i64 10353, i64* @assembly_address
  store i16 0, i16* bitcast (i64* @global_var_216590 to i16*)
  store volatile i64 10362, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216594 to i32*)
  store volatile i64 10372, i64* @assembly_address
  store volatile i64 10373, i64* @assembly_address
  %272 = load i64* %stack_var_-8
  store i64 %272, i64* %rbp
  %273 = ptrtoint i64* %stack_var_0 to i64
  store i64 %273, i64* %rsp
  store volatile i64 10374, i64* @assembly_address
  %274 = load i64* %rax
  %275 = load i64* %rax
  ret i64 %275
}

define i64 @copy_block(i8* %arg1, i64 %arg2, i64 %arg3) {
block_2887:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-20 = alloca i32
  %stack_var_-16 = alloca i8*
  %1 = alloca i64
  %stack_var_-24 = alloca i32
  %2 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 10375, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 10376, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 10379, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 16
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 16
  %11 = xor i64 %6, 16
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %21, i64* %rsp
  store volatile i64 10383, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = inttoptr i64 %22 to i8*
  store i8* %23, i8** %stack_var_-16
  store volatile i64 10387, i64* @assembly_address
  %24 = load i64* %rsi
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-20
  store volatile i64 10390, i64* @assembly_address
  %26 = load i64* %rdx
  %27 = trunc i64 %26 to i32
  %28 = sext i32 %27 to i64
  %29 = trunc i64 %28 to i32
  store i32 %29, i32* %stack_var_-24
  store volatile i64 10393, i64* @assembly_address
  %30 = call i64 @bi_windup()
  store i64 %30, i64* %rax
  store i64 %30, i64* %rax
  store i64 %30, i64* %rax
  store volatile i64 10398, i64* @assembly_address
  %31 = load i32* %stack_var_-24
  %32 = sext i32 %31 to i64
  %33 = trunc i64 %32 to i32
  %34 = and i32 %33, 15
  %35 = icmp ugt i32 %34, 15
  %36 = icmp ult i32 %33, 0
  %37 = xor i32 %33, 0
  %38 = and i32 %37, 0
  %39 = icmp slt i32 %38, 0
  store i1 %35, i1* %az
  store i1 %36, i1* %cf
  store i1 %39, i1* %of
  %40 = icmp eq i32 %33, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i32 %33, 0
  store i1 %41, i1* %sf
  %42 = trunc i32 %33 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  store volatile i64 10402, i64* @assembly_address
  %46 = load i1* %zf
  br i1 %46, label %block_2a6b, label %block_28a8

block_28a8:                                       ; preds = %block_2887
  store volatile i64 10408, i64* @assembly_address
  %47 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rax
  store volatile i64 10414, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  %51 = sub i32 %50, 16381
  %52 = and i32 %50, 15
  %53 = sub i32 %52, 13
  %54 = icmp ugt i32 %53, 15
  %55 = icmp ult i32 %50, 16381
  %56 = xor i32 %50, 16381
  %57 = xor i32 %50, %51
  %58 = and i32 %56, %57
  %59 = icmp slt i32 %58, 0
  store i1 %54, i1* %az
  store i1 %55, i1* %cf
  store i1 %59, i1* %of
  %60 = icmp eq i32 %51, 0
  store i1 %60, i1* %zf
  %61 = icmp slt i32 %51, 0
  store i1 %61, i1* %sf
  %62 = trunc i32 %51 to i8
  %63 = call i8 @llvm.ctpop.i8(i8 %62)
  %64 = and i8 %63, 1
  %65 = icmp eq i8 %64, 0
  store i1 %65, i1* %pf
  store volatile i64 10419, i64* @assembly_address
  %66 = load i1* %cf
  %67 = load i1* %zf
  %68 = or i1 %66, %67
  %69 = icmp ne i1 %68, true
  br i1 %69, label %block_28fb, label %block_28b5

block_28b5:                                       ; preds = %block_28a8
  store volatile i64 10421, i64* @assembly_address
  %70 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %71 = zext i32 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 10427, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10430, i64* @assembly_address
  %72 = load i64* %rdx
  %73 = trunc i64 %72 to i32
  store i32 %73, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10436, i64* @assembly_address
  %74 = load i32* %stack_var_-20
  %75 = zext i32 %74 to i64
  store i64 %75, i64* %rdx
  store volatile i64 10439, i64* @assembly_address
  %76 = load i64* %rdx
  %77 = trunc i64 %76 to i32
  %78 = zext i32 %77 to i64
  store i64 %78, i64* %rcx
  store volatile i64 10441, i64* @assembly_address
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  %81 = zext i32 %80 to i64
  store i64 %81, i64* %rdx
  store volatile i64 10443, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10450, i64* @assembly_address
  %82 = load i64* %rcx
  %83 = trunc i64 %82 to i8
  %84 = load i64* %rdx
  %85 = load i64* %rax
  %86 = mul i64 %85, 1
  %87 = add i64 %84, %86
  %88 = inttoptr i64 %87 to i8*
  store i8 %83, i8* %88
  store volatile i64 10453, i64* @assembly_address
  %89 = load i32* %stack_var_-20
  %90 = zext i32 %89 to i64
  store i64 %90, i64* %rax
  store volatile i64 10456, i64* @assembly_address
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i16
  %93 = load i1* %of
  %94 = lshr i16 %92, 8
  %95 = icmp eq i16 %94, 0
  store i1 %95, i1* %zf
  %96 = icmp slt i16 %94, 0
  store i1 %96, i1* %sf
  %97 = trunc i16 %94 to i8
  %98 = call i8 @llvm.ctpop.i8(i8 %97)
  %99 = and i8 %98, 1
  %100 = icmp eq i8 %99, 0
  store i1 %100, i1* %pf
  %101 = zext i16 %94 to i64
  %102 = load i64* %rax
  %103 = and i64 %102, -65536
  %104 = or i64 %103, %101
  store i64 %104, i64* %rax
  %105 = and i16 128, %92
  %106 = icmp ne i16 %105, 0
  store i1 %106, i1* %cf
  %107 = icmp slt i16 %92, 0
  %108 = select i1 false, i1 %107, i1 %93
  store i1 %108, i1* %of
  store volatile i64 10460, i64* @assembly_address
  %109 = load i64* %rax
  %110 = trunc i64 %109 to i32
  %111 = zext i32 %110 to i64
  store i64 %111, i64* %rcx
  store volatile i64 10462, i64* @assembly_address
  %112 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %113 = zext i32 %112 to i64
  store i64 %113, i64* %rax
  store volatile i64 10468, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10471, i64* @assembly_address
  %114 = load i64* %rdx
  %115 = trunc i64 %114 to i32
  store i32 %115, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10477, i64* @assembly_address
  %116 = load i64* %rax
  %117 = trunc i64 %116 to i32
  %118 = zext i32 %117 to i64
  store i64 %118, i64* %rdx
  store volatile i64 10479, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10486, i64* @assembly_address
  %119 = load i64* %rcx
  %120 = trunc i64 %119 to i8
  %121 = load i64* %rdx
  %122 = load i64* %rax
  %123 = mul i64 %122, 1
  %124 = add i64 %121, %123
  %125 = inttoptr i64 %124 to i8*
  store i8 %120, i8* %125
  store volatile i64 10489, i64* @assembly_address
  br label %block_2963

block_28fb:                                       ; preds = %block_28a8
  store volatile i64 10491, i64* @assembly_address
  %126 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %127 = zext i32 %126 to i64
  store i64 %127, i64* %rax
  store volatile i64 10497, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10500, i64* @assembly_address
  %128 = load i64* %rdx
  %129 = trunc i64 %128 to i32
  store i32 %129, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10506, i64* @assembly_address
  %130 = load i32* %stack_var_-20
  %131 = zext i32 %130 to i64
  store i64 %131, i64* %rdx
  store volatile i64 10509, i64* @assembly_address
  %132 = load i64* %rdx
  %133 = trunc i64 %132 to i32
  %134 = zext i32 %133 to i64
  store i64 %134, i64* %rcx
  store volatile i64 10511, i64* @assembly_address
  %135 = load i64* %rax
  %136 = trunc i64 %135 to i32
  %137 = zext i32 %136 to i64
  store i64 %137, i64* %rdx
  store volatile i64 10513, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10520, i64* @assembly_address
  %138 = load i64* %rcx
  %139 = trunc i64 %138 to i8
  %140 = load i64* %rdx
  %141 = load i64* %rax
  %142 = mul i64 %141, 1
  %143 = add i64 %140, %142
  %144 = inttoptr i64 %143 to i8*
  store i8 %139, i8* %144
  store volatile i64 10523, i64* @assembly_address
  %145 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %146 = zext i32 %145 to i64
  store i64 %146, i64* %rax
  store volatile i64 10529, i64* @assembly_address
  %147 = load i64* %rax
  %148 = trunc i64 %147 to i32
  %149 = sub i32 %148, 16384
  %150 = and i32 %148, 15
  %151 = icmp ugt i32 %150, 15
  %152 = icmp ult i32 %148, 16384
  %153 = xor i32 %148, 16384
  %154 = xor i32 %148, %149
  %155 = and i32 %153, %154
  %156 = icmp slt i32 %155, 0
  store i1 %151, i1* %az
  store i1 %152, i1* %cf
  store i1 %156, i1* %of
  %157 = icmp eq i32 %149, 0
  store i1 %157, i1* %zf
  %158 = icmp slt i32 %149, 0
  store i1 %158, i1* %sf
  %159 = trunc i32 %149 to i8
  %160 = call i8 @llvm.ctpop.i8(i8 %159)
  %161 = and i8 %160, 1
  %162 = icmp eq i8 %161, 0
  store i1 %162, i1* %pf
  store volatile i64 10534, i64* @assembly_address
  %163 = load i1* %zf
  %164 = icmp eq i1 %163, false
  br i1 %164, label %block_292d, label %block_2928

block_2928:                                       ; preds = %block_28fb
  store volatile i64 10536, i64* @assembly_address
  %165 = load i64* %rdi
  %166 = load i64* %rsi
  %167 = load i64* %rdx
  %168 = load i64* %rcx
  %169 = trunc i64 %168 to i16
  %170 = call i64 @flush_outbuf(i64 %165, i64 %166, i64 %167, i16 %169)
  store i64 %170, i64* %rax
  store i64 %170, i64* %rax
  store i64 %170, i64* %rax
  br label %block_292d

block_292d:                                       ; preds = %block_2928, %block_28fb
  store volatile i64 10541, i64* @assembly_address
  %171 = load i32* %stack_var_-20
  %172 = zext i32 %171 to i64
  store i64 %172, i64* %rax
  store volatile i64 10544, i64* @assembly_address
  %173 = load i64* %rax
  %174 = trunc i64 %173 to i16
  %175 = load i1* %of
  %176 = lshr i16 %174, 8
  %177 = icmp eq i16 %176, 0
  store i1 %177, i1* %zf
  %178 = icmp slt i16 %176, 0
  store i1 %178, i1* %sf
  %179 = trunc i16 %176 to i8
  %180 = call i8 @llvm.ctpop.i8(i8 %179)
  %181 = and i8 %180, 1
  %182 = icmp eq i8 %181, 0
  store i1 %182, i1* %pf
  %183 = zext i16 %176 to i64
  %184 = load i64* %rax
  %185 = and i64 %184, -65536
  %186 = or i64 %185, %183
  store i64 %186, i64* %rax
  %187 = and i16 128, %174
  %188 = icmp ne i16 %187, 0
  store i1 %188, i1* %cf
  %189 = icmp slt i16 %174, 0
  %190 = select i1 false, i1 %189, i1 %175
  store i1 %190, i1* %of
  store volatile i64 10548, i64* @assembly_address
  %191 = load i64* %rax
  %192 = trunc i64 %191 to i32
  %193 = zext i32 %192 to i64
  store i64 %193, i64* %rcx
  store volatile i64 10550, i64* @assembly_address
  %194 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %195 = zext i32 %194 to i64
  store i64 %195, i64* %rax
  store volatile i64 10556, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10559, i64* @assembly_address
  %196 = load i64* %rdx
  %197 = trunc i64 %196 to i32
  store i32 %197, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10565, i64* @assembly_address
  %198 = load i64* %rax
  %199 = trunc i64 %198 to i32
  %200 = zext i32 %199 to i64
  store i64 %200, i64* %rdx
  store volatile i64 10567, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10574, i64* @assembly_address
  %201 = load i64* %rcx
  %202 = trunc i64 %201 to i8
  %203 = load i64* %rdx
  %204 = load i64* %rax
  %205 = mul i64 %204, 1
  %206 = add i64 %203, %205
  %207 = inttoptr i64 %206 to i8*
  store i8 %202, i8* %207
  store volatile i64 10577, i64* @assembly_address
  %208 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %209 = zext i32 %208 to i64
  store i64 %209, i64* %rax
  store volatile i64 10583, i64* @assembly_address
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  %212 = sub i32 %211, 16384
  %213 = and i32 %211, 15
  %214 = icmp ugt i32 %213, 15
  %215 = icmp ult i32 %211, 16384
  %216 = xor i32 %211, 16384
  %217 = xor i32 %211, %212
  %218 = and i32 %216, %217
  %219 = icmp slt i32 %218, 0
  store i1 %214, i1* %az
  store i1 %215, i1* %cf
  store i1 %219, i1* %of
  %220 = icmp eq i32 %212, 0
  store i1 %220, i1* %zf
  %221 = icmp slt i32 %212, 0
  store i1 %221, i1* %sf
  %222 = trunc i32 %212 to i8
  %223 = call i8 @llvm.ctpop.i8(i8 %222)
  %224 = and i8 %223, 1
  %225 = icmp eq i8 %224, 0
  store i1 %225, i1* %pf
  store volatile i64 10588, i64* @assembly_address
  %226 = load i1* %zf
  %227 = icmp eq i1 %226, false
  br i1 %227, label %block_2963, label %block_295e

block_295e:                                       ; preds = %block_292d
  store volatile i64 10590, i64* @assembly_address
  %228 = load i64* %rdi
  %229 = load i64* %rsi
  %230 = load i64* %rdx
  %231 = load i64* %rcx
  %232 = trunc i64 %231 to i16
  %233 = call i64 @flush_outbuf(i64 %228, i64 %229, i64 %230, i16 %232)
  store i64 %233, i64* %rax
  store i64 %233, i64* %rax
  store i64 %233, i64* %rax
  br label %block_2963

block_2963:                                       ; preds = %block_295e, %block_292d, %block_28b5
  store volatile i64 10595, i64* @assembly_address
  %234 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %235 = zext i32 %234 to i64
  store i64 %235, i64* %rax
  store volatile i64 10601, i64* @assembly_address
  %236 = load i64* %rax
  %237 = trunc i64 %236 to i32
  %238 = sub i32 %237, 16381
  %239 = and i32 %237, 15
  %240 = sub i32 %239, 13
  %241 = icmp ugt i32 %240, 15
  %242 = icmp ult i32 %237, 16381
  %243 = xor i32 %237, 16381
  %244 = xor i32 %237, %238
  %245 = and i32 %243, %244
  %246 = icmp slt i32 %245, 0
  store i1 %241, i1* %az
  store i1 %242, i1* %cf
  store i1 %246, i1* %of
  %247 = icmp eq i32 %238, 0
  store i1 %247, i1* %zf
  %248 = icmp slt i32 %238, 0
  store i1 %248, i1* %sf
  %249 = trunc i32 %238 to i8
  %250 = call i8 @llvm.ctpop.i8(i8 %249)
  %251 = and i8 %250, 1
  %252 = icmp eq i8 %251, 0
  store i1 %252, i1* %pf
  store volatile i64 10606, i64* @assembly_address
  %253 = load i1* %cf
  %254 = load i1* %zf
  %255 = or i1 %253, %254
  %256 = icmp ne i1 %255, true
  br i1 %256, label %block_29bf, label %block_2970

block_2970:                                       ; preds = %block_2963
  store volatile i64 10608, i64* @assembly_address
  %257 = load i32* %stack_var_-20
  %258 = zext i32 %257 to i64
  store i64 %258, i64* %rax
  store volatile i64 10611, i64* @assembly_address
  %259 = load i64* %rax
  %260 = trunc i64 %259 to i32
  %261 = zext i32 %260 to i64
  store i64 %261, i64* %rcx
  store volatile i64 10613, i64* @assembly_address
  %262 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %263 = zext i32 %262 to i64
  store i64 %263, i64* %rax
  store volatile i64 10619, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10622, i64* @assembly_address
  %264 = load i64* %rdx
  %265 = trunc i64 %264 to i32
  store i32 %265, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10628, i64* @assembly_address
  %266 = load i64* %rcx
  %267 = trunc i64 %266 to i32
  %268 = xor i32 %267, -1
  %269 = zext i32 %268 to i64
  store i64 %269, i64* %rcx
  store volatile i64 10630, i64* @assembly_address
  %270 = load i64* %rcx
  %271 = trunc i64 %270 to i32
  %272 = zext i32 %271 to i64
  store i64 %272, i64* %rdx
  store volatile i64 10632, i64* @assembly_address
  %273 = load i64* %rax
  %274 = trunc i64 %273 to i32
  %275 = zext i32 %274 to i64
  store i64 %275, i64* %rcx
  store volatile i64 10634, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10641, i64* @assembly_address
  %276 = load i64* %rdx
  %277 = trunc i64 %276 to i8
  %278 = load i64* %rcx
  %279 = load i64* %rax
  %280 = mul i64 %279, 1
  %281 = add i64 %278, %280
  %282 = inttoptr i64 %281 to i8*
  store i8 %277, i8* %282
  store volatile i64 10644, i64* @assembly_address
  %283 = load i32* %stack_var_-20
  %284 = zext i32 %283 to i64
  store i64 %284, i64* %rax
  store volatile i64 10647, i64* @assembly_address
  %285 = load i64* %rax
  %286 = trunc i64 %285 to i32
  %287 = xor i32 %286, -1
  %288 = zext i32 %287 to i64
  store i64 %288, i64* %rax
  store volatile i64 10649, i64* @assembly_address
  %289 = load i64* %rax
  %290 = trunc i64 %289 to i16
  %291 = load i1* %of
  %292 = lshr i16 %290, 8
  %293 = icmp eq i16 %292, 0
  store i1 %293, i1* %zf
  %294 = icmp slt i16 %292, 0
  store i1 %294, i1* %sf
  %295 = trunc i16 %292 to i8
  %296 = call i8 @llvm.ctpop.i8(i8 %295)
  %297 = and i8 %296, 1
  %298 = icmp eq i8 %297, 0
  store i1 %298, i1* %pf
  %299 = zext i16 %292 to i64
  %300 = load i64* %rax
  %301 = and i64 %300, -65536
  %302 = or i64 %301, %299
  store i64 %302, i64* %rax
  %303 = and i16 128, %290
  %304 = icmp ne i16 %303, 0
  store i1 %304, i1* %cf
  %305 = icmp slt i16 %290, 0
  %306 = select i1 false, i1 %305, i1 %291
  store i1 %306, i1* %of
  store volatile i64 10653, i64* @assembly_address
  %307 = load i64* %rax
  %308 = trunc i64 %307 to i32
  %309 = zext i32 %308 to i64
  store i64 %309, i64* %rcx
  store volatile i64 10655, i64* @assembly_address
  %310 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %311 = zext i32 %310 to i64
  store i64 %311, i64* %rax
  store volatile i64 10661, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10664, i64* @assembly_address
  %312 = load i64* %rdx
  %313 = trunc i64 %312 to i32
  store i32 %313, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10670, i64* @assembly_address
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i32
  %316 = zext i32 %315 to i64
  store i64 %316, i64* %rdx
  store volatile i64 10672, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10679, i64* @assembly_address
  %317 = load i64* %rcx
  %318 = trunc i64 %317 to i8
  %319 = load i64* %rdx
  %320 = load i64* %rax
  %321 = mul i64 %320, 1
  %322 = add i64 %319, %321
  %323 = inttoptr i64 %322 to i8*
  store i8 %318, i8* %323
  store volatile i64 10682, i64* @assembly_address
  br label %block_2a6b

block_29bf:                                       ; preds = %block_2963
  store volatile i64 10687, i64* @assembly_address
  %324 = load i32* %stack_var_-20
  %325 = zext i32 %324 to i64
  store i64 %325, i64* %rax
  store volatile i64 10690, i64* @assembly_address
  %326 = load i64* %rax
  %327 = trunc i64 %326 to i32
  %328 = zext i32 %327 to i64
  store i64 %328, i64* %rcx
  store volatile i64 10692, i64* @assembly_address
  %329 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %330 = zext i32 %329 to i64
  store i64 %330, i64* %rax
  store volatile i64 10698, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10701, i64* @assembly_address
  %331 = load i64* %rdx
  %332 = trunc i64 %331 to i32
  store i32 %332, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10707, i64* @assembly_address
  %333 = load i64* %rcx
  %334 = trunc i64 %333 to i32
  %335 = xor i32 %334, -1
  %336 = zext i32 %335 to i64
  store i64 %336, i64* %rcx
  store volatile i64 10709, i64* @assembly_address
  %337 = load i64* %rcx
  %338 = trunc i64 %337 to i32
  %339 = zext i32 %338 to i64
  store i64 %339, i64* %rdx
  store volatile i64 10711, i64* @assembly_address
  %340 = load i64* %rax
  %341 = trunc i64 %340 to i32
  %342 = zext i32 %341 to i64
  store i64 %342, i64* %rcx
  store volatile i64 10713, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10720, i64* @assembly_address
  %343 = load i64* %rdx
  %344 = trunc i64 %343 to i8
  %345 = load i64* %rcx
  %346 = load i64* %rax
  %347 = mul i64 %346, 1
  %348 = add i64 %345, %347
  %349 = inttoptr i64 %348 to i8*
  store i8 %344, i8* %349
  store volatile i64 10723, i64* @assembly_address
  %350 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %351 = zext i32 %350 to i64
  store i64 %351, i64* %rax
  store volatile i64 10729, i64* @assembly_address
  %352 = load i64* %rax
  %353 = trunc i64 %352 to i32
  %354 = sub i32 %353, 16384
  %355 = and i32 %353, 15
  %356 = icmp ugt i32 %355, 15
  %357 = icmp ult i32 %353, 16384
  %358 = xor i32 %353, 16384
  %359 = xor i32 %353, %354
  %360 = and i32 %358, %359
  %361 = icmp slt i32 %360, 0
  store i1 %356, i1* %az
  store i1 %357, i1* %cf
  store i1 %361, i1* %of
  %362 = icmp eq i32 %354, 0
  store i1 %362, i1* %zf
  %363 = icmp slt i32 %354, 0
  store i1 %363, i1* %sf
  %364 = trunc i32 %354 to i8
  %365 = call i8 @llvm.ctpop.i8(i8 %364)
  %366 = and i8 %365, 1
  %367 = icmp eq i8 %366, 0
  store i1 %367, i1* %pf
  store volatile i64 10734, i64* @assembly_address
  %368 = load i1* %zf
  %369 = icmp eq i1 %368, false
  br i1 %369, label %block_29f5, label %block_29f0

block_29f0:                                       ; preds = %block_29bf
  store volatile i64 10736, i64* @assembly_address
  %370 = load i64* %rdi
  %371 = load i64* %rsi
  %372 = load i64* %rdx
  %373 = load i64* %rcx
  %374 = trunc i64 %373 to i16
  %375 = call i64 @flush_outbuf(i64 %370, i64 %371, i64 %372, i16 %374)
  store i64 %375, i64* %rax
  store i64 %375, i64* %rax
  store i64 %375, i64* %rax
  br label %block_29f5

block_29f5:                                       ; preds = %block_29f0, %block_29bf
  store volatile i64 10741, i64* @assembly_address
  %376 = load i32* %stack_var_-20
  %377 = zext i32 %376 to i64
  store i64 %377, i64* %rax
  store volatile i64 10744, i64* @assembly_address
  %378 = load i64* %rax
  %379 = trunc i64 %378 to i32
  %380 = xor i32 %379, -1
  %381 = zext i32 %380 to i64
  store i64 %381, i64* %rax
  store volatile i64 10746, i64* @assembly_address
  %382 = load i64* %rax
  %383 = trunc i64 %382 to i16
  %384 = load i1* %of
  %385 = lshr i16 %383, 8
  %386 = icmp eq i16 %385, 0
  store i1 %386, i1* %zf
  %387 = icmp slt i16 %385, 0
  store i1 %387, i1* %sf
  %388 = trunc i16 %385 to i8
  %389 = call i8 @llvm.ctpop.i8(i8 %388)
  %390 = and i8 %389, 1
  %391 = icmp eq i8 %390, 0
  store i1 %391, i1* %pf
  %392 = zext i16 %385 to i64
  %393 = load i64* %rax
  %394 = and i64 %393, -65536
  %395 = or i64 %394, %392
  store i64 %395, i64* %rax
  %396 = and i16 128, %383
  %397 = icmp ne i16 %396, 0
  store i1 %397, i1* %cf
  %398 = icmp slt i16 %383, 0
  %399 = select i1 false, i1 %398, i1 %384
  store i1 %399, i1* %of
  store volatile i64 10750, i64* @assembly_address
  %400 = load i64* %rax
  %401 = trunc i64 %400 to i32
  %402 = zext i32 %401 to i64
  store i64 %402, i64* %rcx
  store volatile i64 10752, i64* @assembly_address
  %403 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %404 = zext i32 %403 to i64
  store i64 %404, i64* %rax
  store volatile i64 10758, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10761, i64* @assembly_address
  %405 = load i64* %rdx
  %406 = trunc i64 %405 to i32
  store i32 %406, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10767, i64* @assembly_address
  %407 = load i64* %rax
  %408 = trunc i64 %407 to i32
  %409 = zext i32 %408 to i64
  store i64 %409, i64* %rdx
  store volatile i64 10769, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10776, i64* @assembly_address
  %410 = load i64* %rcx
  %411 = trunc i64 %410 to i8
  %412 = load i64* %rdx
  %413 = load i64* %rax
  %414 = mul i64 %413, 1
  %415 = add i64 %412, %414
  %416 = inttoptr i64 %415 to i8*
  store i8 %411, i8* %416
  store volatile i64 10779, i64* @assembly_address
  %417 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %418 = zext i32 %417 to i64
  store i64 %418, i64* %rax
  store volatile i64 10785, i64* @assembly_address
  %419 = load i64* %rax
  %420 = trunc i64 %419 to i32
  %421 = sub i32 %420, 16384
  %422 = and i32 %420, 15
  %423 = icmp ugt i32 %422, 15
  %424 = icmp ult i32 %420, 16384
  %425 = xor i32 %420, 16384
  %426 = xor i32 %420, %421
  %427 = and i32 %425, %426
  %428 = icmp slt i32 %427, 0
  store i1 %423, i1* %az
  store i1 %424, i1* %cf
  store i1 %428, i1* %of
  %429 = icmp eq i32 %421, 0
  store i1 %429, i1* %zf
  %430 = icmp slt i32 %421, 0
  store i1 %430, i1* %sf
  %431 = trunc i32 %421 to i8
  %432 = call i8 @llvm.ctpop.i8(i8 %431)
  %433 = and i8 %432, 1
  %434 = icmp eq i8 %433, 0
  store i1 %434, i1* %pf
  store volatile i64 10790, i64* @assembly_address
  %435 = load i1* %zf
  %436 = icmp eq i1 %435, false
  br i1 %436, label %block_2a6b, label %block_2a28

block_2a28:                                       ; preds = %block_29f5
  store volatile i64 10792, i64* @assembly_address
  %437 = load i64* %rdi
  %438 = load i64* %rsi
  %439 = load i64* %rdx
  %440 = load i64* %rcx
  %441 = trunc i64 %440 to i16
  %442 = call i64 @flush_outbuf(i64 %437, i64 %438, i64 %439, i16 %441)
  store i64 %442, i64* %rax
  store i64 %442, i64* %rax
  store i64 %442, i64* %rax
  store volatile i64 10797, i64* @assembly_address
  br label %block_2a6b

block_2a2f:                                       ; preds = %block_2a6b
  store volatile i64 10799, i64* @assembly_address
  %443 = load i8** %stack_var_-16
  %444 = ptrtoint i8* %443 to i64
  store i64 %444, i64* %rax
  store volatile i64 10803, i64* @assembly_address
  %445 = load i64* %rax
  %446 = add i64 %445, 1
  store i64 %446, i64* %rdx
  store volatile i64 10807, i64* @assembly_address
  %447 = load i64* %rdx
  %448 = inttoptr i64 %447 to i8*
  store i8* %448, i8** %stack_var_-16
  store volatile i64 10811, i64* @assembly_address
  %449 = load i64* %rax
  %450 = inttoptr i64 %449 to i8*
  %451 = load i8* %450
  %452 = zext i8 %451 to i64
  store i64 %452, i64* %rcx
  store volatile i64 10814, i64* @assembly_address
  %453 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %454 = zext i32 %453 to i64
  store i64 %454, i64* %rax
  store volatile i64 10820, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 10823, i64* @assembly_address
  %455 = load i64* %rdx
  %456 = trunc i64 %455 to i32
  store i32 %456, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 10829, i64* @assembly_address
  %457 = load i64* %rax
  %458 = trunc i64 %457 to i32
  %459 = zext i32 %458 to i64
  store i64 %459, i64* %rdx
  store volatile i64 10831, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 10838, i64* @assembly_address
  %460 = load i64* %rcx
  %461 = trunc i64 %460 to i8
  %462 = load i64* %rdx
  %463 = load i64* %rax
  %464 = mul i64 %463, 1
  %465 = add i64 %462, %464
  %466 = inttoptr i64 %465 to i8*
  store i8 %461, i8* %466
  store volatile i64 10841, i64* @assembly_address
  %467 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %468 = zext i32 %467 to i64
  store i64 %468, i64* %rax
  store volatile i64 10847, i64* @assembly_address
  %469 = load i64* %rax
  %470 = trunc i64 %469 to i32
  %471 = sub i32 %470, 16384
  %472 = and i32 %470, 15
  %473 = icmp ugt i32 %472, 15
  %474 = icmp ult i32 %470, 16384
  %475 = xor i32 %470, 16384
  %476 = xor i32 %470, %471
  %477 = and i32 %475, %476
  %478 = icmp slt i32 %477, 0
  store i1 %473, i1* %az
  store i1 %474, i1* %cf
  store i1 %478, i1* %of
  %479 = icmp eq i32 %471, 0
  store i1 %479, i1* %zf
  %480 = icmp slt i32 %471, 0
  store i1 %480, i1* %sf
  %481 = trunc i32 %471 to i8
  %482 = call i8 @llvm.ctpop.i8(i8 %481)
  %483 = and i8 %482, 1
  %484 = icmp eq i8 %483, 0
  store i1 %484, i1* %pf
  store volatile i64 10852, i64* @assembly_address
  %485 = load i1* %zf
  %486 = icmp eq i1 %485, false
  br i1 %486, label %block_2a6b, label %block_2a66

block_2a66:                                       ; preds = %block_2a2f
  store volatile i64 10854, i64* @assembly_address
  %487 = load i64* %rdi
  %488 = load i64* %rsi
  %489 = load i64* %rdx
  %490 = load i64* %rcx
  %491 = trunc i64 %490 to i16
  %492 = call i64 @flush_outbuf(i64 %487, i64 %488, i64 %489, i16 %491)
  store i64 %492, i64* %rax
  store i64 %492, i64* %rax
  store i64 %492, i64* %rax
  br label %block_2a6b

block_2a6b:                                       ; preds = %block_2a66, %block_2a2f, %block_2a28, %block_29f5, %block_2970, %block_2887
  store volatile i64 10859, i64* @assembly_address
  %493 = load i32* %stack_var_-20
  %494 = zext i32 %493 to i64
  store i64 %494, i64* %rax
  store volatile i64 10862, i64* @assembly_address
  %495 = load i64* %rax
  %496 = add i64 %495, -1
  %497 = trunc i64 %496 to i32
  %498 = zext i32 %497 to i64
  store i64 %498, i64* %rdx
  store volatile i64 10865, i64* @assembly_address
  %499 = load i64* %rdx
  %500 = trunc i64 %499 to i32
  store i32 %500, i32* %stack_var_-20
  store volatile i64 10868, i64* @assembly_address
  %501 = load i64* %rax
  %502 = trunc i64 %501 to i32
  %503 = load i64* %rax
  %504 = trunc i64 %503 to i32
  %505 = and i32 %502, %504
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %506 = icmp eq i32 %505, 0
  store i1 %506, i1* %zf
  %507 = icmp slt i32 %505, 0
  store i1 %507, i1* %sf
  %508 = trunc i32 %505 to i8
  %509 = call i8 @llvm.ctpop.i8(i8 %508)
  %510 = and i8 %509, 1
  %511 = icmp eq i8 %510, 0
  store i1 %511, i1* %pf
  store volatile i64 10870, i64* @assembly_address
  %512 = load i1* %zf
  %513 = icmp eq i1 %512, false
  br i1 %513, label %block_2a2f, label %block_2a78

block_2a78:                                       ; preds = %block_2a6b
  store volatile i64 10872, i64* @assembly_address
  store volatile i64 10873, i64* @assembly_address
  %514 = load i64* %stack_var_-8
  store i64 %514, i64* %rbp
  %515 = ptrtoint i64* %stack_var_0 to i64
  store i64 %515, i64* %rsp
  store volatile i64 10874, i64* @assembly_address
  %516 = load i64* %rax
  ret i64 %516
}

declare i64 @163(i64, i32, i64)

declare i64 @164(i64, i64, i32)

declare i64 @165(i64, i64, i64)

define i64 @lm_init(i32 %arg1, i16* %arg2) {
block_2a7b:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i16* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i16*
  %2 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i32
  store volatile i64 10875, i64* @assembly_address
  %7 = load i64* %rbp
  store i64 %7, i64* %stack_var_-8
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rsp
  store volatile i64 10876, i64* @assembly_address
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rbp
  store volatile i64 10879, i64* @assembly_address
  %10 = load i64* %rbx
  store i64 %10, i64* %stack_var_-16
  %11 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %11, i64* %rsp
  store volatile i64 10880, i64* @assembly_address
  %12 = load i64* %rsp
  %13 = sub i64 %12, 24
  %14 = and i64 %12, 15
  %15 = sub i64 %14, 8
  %16 = icmp ugt i64 %15, 15
  %17 = icmp ult i64 %12, 24
  %18 = xor i64 %12, 24
  %19 = xor i64 %12, %13
  %20 = and i64 %18, %19
  %21 = icmp slt i64 %20, 0
  store i1 %16, i1* %az
  store i1 %17, i1* %cf
  store i1 %21, i1* %of
  %22 = icmp eq i64 %13, 0
  store i1 %22, i1* %zf
  %23 = icmp slt i64 %13, 0
  store i1 %23, i1* %sf
  %24 = trunc i64 %13 to i8
  %25 = call i8 @llvm.ctpop.i8(i8 %24)
  %26 = and i8 %25, 1
  %27 = icmp eq i8 %26, 0
  store i1 %27, i1* %pf
  %28 = ptrtoint i16** %stack_var_-40 to i64
  store i64 %28, i64* %rsp
  store volatile i64 10884, i64* @assembly_address
  %29 = load i64* %rdi
  %30 = trunc i64 %29 to i32
  store i32 %30, i32* %stack_var_-28
  store volatile i64 10887, i64* @assembly_address
  %31 = load i64* %rsi
  %32 = inttoptr i64 %31 to i16*
  store i16* %32, i16** %stack_var_-40
  store volatile i64 10891, i64* @assembly_address
  %33 = load i32* %stack_var_-28
  %34 = and i32 %33, 15
  %35 = icmp ugt i32 %34, 15
  %36 = icmp ult i32 %33, 0
  %37 = xor i32 %33, 0
  %38 = and i32 %37, 0
  %39 = icmp slt i32 %38, 0
  store i1 %35, i1* %az
  store i1 %36, i1* %cf
  store i1 %39, i1* %of
  store i32 %33, i32* %6
  store i32 0, i32* %5
  %40 = icmp eq i32 %33, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i32 %33, 0
  store i1 %41, i1* %sf
  %42 = trunc i32 %33 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  store volatile i64 10895, i64* @assembly_address
  %46 = load i32* %6
  %47 = load i32* %5
  %48 = icmp sle i32 %46, %47
  br i1 %48, label %block_2a97, label %block_2a91

block_2a91:                                       ; preds = %block_2a7b
  store volatile i64 10897, i64* @assembly_address
  %49 = load i32* %stack_var_-28
  store i32 %49, i32* %4
  store i32 9, i32* %3
  %50 = sub i32 %49, 9
  %51 = and i32 %49, 15
  %52 = sub i32 %51, 9
  %53 = icmp ugt i32 %52, 15
  %54 = icmp ult i32 %49, 9
  %55 = xor i32 %49, 9
  %56 = xor i32 %49, %50
  %57 = and i32 %55, %56
  %58 = icmp slt i32 %57, 0
  store i1 %53, i1* %az
  store i1 %54, i1* %cf
  store i1 %58, i1* %of
  %59 = icmp eq i32 %50, 0
  store i1 %59, i1* %zf
  %60 = icmp slt i32 %50, 0
  store i1 %60, i1* %sf
  %61 = trunc i32 %50 to i8
  %62 = call i8 @llvm.ctpop.i8(i8 %61)
  %63 = and i8 %62, 1
  %64 = icmp eq i8 %63, 0
  store i1 %64, i1* %pf
  store volatile i64 10901, i64* @assembly_address
  %65 = load i32* %4
  %66 = load i32* %3
  %67 = icmp sle i32 %65, %66
  br i1 %67, label %block_2aa3, label %block_2a97

block_2a97:                                       ; preds = %block_2a91, %block_2a7b
  store volatile i64 10903, i64* @assembly_address
  store i64 ptrtoint ([15 x i8]* @global_var_10d84 to i64), i64* %rdi
  store volatile i64 10910, i64* @assembly_address
  %68 = load i64* %rdi
  %69 = inttoptr i64 %68 to i8*
  %70 = call i64 @gzip_error(i8* %69)
  store i64 %70, i64* %rax
  store i64 %70, i64* %rax
  unreachable

block_2aa3:                                       ; preds = %block_2a91
  store volatile i64 10915, i64* @assembly_address
  %71 = load i32* %stack_var_-28
  %72 = zext i32 %71 to i64
  store i64 %72, i64* %rax
  store volatile i64 10918, i64* @assembly_address
  %73 = load i64* %rax
  %74 = trunc i64 %73 to i32
  store i32 %74, i32* bitcast (i64* @global_var_2165a8 to i32*)
  store volatile i64 10924, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_22a880 to i64), i64* %rax
  store volatile i64 10931, i64* @assembly_address
  store i64 65536, i64* %rdx
  store volatile i64 10936, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 10941, i64* @assembly_address
  %75 = load i64* %rax
  store i64 %75, i64* %rdi
  store volatile i64 10944, i64* @assembly_address
  %76 = load i64* %rdi
  %77 = inttoptr i64 %76 to i64*
  %78 = load i64* %rsi
  %79 = trunc i64 %78 to i32
  %80 = load i64* %rdx
  %81 = trunc i64 %80 to i32
  %82 = call i64* @memset(i64* %77, i32 %79, i32 %81)
  %83 = ptrtoint i64* %82 to i64
  store i64 %83, i64* %rax
  %84 = ptrtoint i64* %82 to i64
  store i64 %84, i64* %rax
  store volatile i64 10949, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 10954, i64* @assembly_address
  %85 = load i64* %rax
  store i64 %85, i64* @global_var_2165b8
  store volatile i64 10961, i64* @assembly_address
  store i64 0, i64* @global_var_2165b0
  store volatile i64 10972, i64* @assembly_address
  %86 = load i32* %stack_var_-28
  %87 = zext i32 %86 to i64
  store i64 %87, i64* %rax
  store volatile i64 10975, i64* @assembly_address
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  %90 = sext i32 %89 to i64
  store i64 %90, i64* %rax
  store volatile i64 10977, i64* @assembly_address
  %91 = load i64* %rax
  %92 = mul i64 %91, 8
  store i64 %92, i64* %rdx
  store volatile i64 10985, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216042 to i64), i64* %rax
  store volatile i64 10992, i64* @assembly_address
  %93 = load i64* %rdx
  %94 = load i64* %rax
  %95 = mul i64 %94, 1
  %96 = add i64 %93, %95
  %97 = inttoptr i64 %96 to i16*
  %98 = load i16* %97
  %99 = zext i16 %98 to i64
  store i64 %99, i64* %rax
  store volatile i64 10996, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i16
  %102 = zext i16 %101 to i64
  store i64 %102, i64* %rax
  store volatile i64 10999, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  store i32 %104, i32* bitcast (i64* @global_var_2165a4 to i32*)
  store volatile i64 11005, i64* @assembly_address
  %105 = load i32* %stack_var_-28
  %106 = zext i32 %105 to i64
  store i64 %106, i64* %rax
  store volatile i64 11008, i64* @assembly_address
  %107 = load i64* %rax
  %108 = trunc i64 %107 to i32
  %109 = sext i32 %108 to i64
  store i64 %109, i64* %rax
  store volatile i64 11010, i64* @assembly_address
  %110 = load i64* %rax
  %111 = mul i64 %110, 8
  store i64 %111, i64* %rdx
  store volatile i64 11018, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216040 to i64), i64* %rax
  store volatile i64 11025, i64* @assembly_address
  %112 = load i64* %rdx
  %113 = load i64* %rax
  %114 = mul i64 %113, 1
  %115 = add i64 %112, %114
  %116 = inttoptr i64 %115 to i16*
  %117 = load i16* %116
  %118 = zext i16 %117 to i64
  store i64 %118, i64* %rax
  store volatile i64 11029, i64* @assembly_address
  %119 = load i64* %rax
  %120 = trunc i64 %119 to i16
  %121 = zext i16 %120 to i64
  store i64 %121, i64* %rax
  store volatile i64 11032, i64* @assembly_address
  %122 = load i64* %rax
  %123 = trunc i64 %122 to i32
  store i32 %123, i32* bitcast (i64* @global_var_21a438 to i32*)
  store volatile i64 11038, i64* @assembly_address
  %124 = load i32* %stack_var_-28
  %125 = zext i32 %124 to i64
  store i64 %125, i64* %rax
  store volatile i64 11041, i64* @assembly_address
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i32
  %128 = sext i32 %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 11043, i64* @assembly_address
  %129 = load i64* %rax
  %130 = mul i64 %129, 8
  store i64 %130, i64* %rdx
  store volatile i64 11051, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216044 to i64), i64* %rax
  store volatile i64 11058, i64* @assembly_address
  %131 = load i64* %rdx
  %132 = load i64* %rax
  %133 = mul i64 %132, 1
  %134 = add i64 %131, %133
  %135 = inttoptr i64 %134 to i16*
  %136 = load i16* %135
  %137 = zext i16 %136 to i64
  store i64 %137, i64* %rax
  store volatile i64 11062, i64* @assembly_address
  %138 = load i64* %rax
  %139 = trunc i64 %138 to i16
  %140 = zext i16 %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 11065, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  store i32 %142, i32* bitcast (i64* @global_var_2165c0 to i32*)
  store volatile i64 11071, i64* @assembly_address
  %143 = load i32* %stack_var_-28
  %144 = zext i32 %143 to i64
  store i64 %144, i64* %rax
  store volatile i64 11074, i64* @assembly_address
  %145 = load i64* %rax
  %146 = trunc i64 %145 to i32
  %147 = sext i32 %146 to i64
  store i64 %147, i64* %rax
  store volatile i64 11076, i64* @assembly_address
  %148 = load i64* %rax
  %149 = mul i64 %148, 8
  store i64 %149, i64* %rdx
  store volatile i64 11084, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216046 to i64), i64* %rax
  store volatile i64 11091, i64* @assembly_address
  %150 = load i64* %rdx
  %151 = load i64* %rax
  %152 = mul i64 %151, 1
  %153 = add i64 %150, %152
  %154 = inttoptr i64 %153 to i16*
  %155 = load i16* %154
  %156 = zext i16 %155 to i64
  store i64 %156, i64* %rax
  store volatile i64 11095, i64* @assembly_address
  %157 = load i64* %rax
  %158 = trunc i64 %157 to i16
  %159 = zext i16 %158 to i64
  store i64 %159, i64* %rax
  store volatile i64 11098, i64* @assembly_address
  %160 = load i64* %rax
  %161 = trunc i64 %160 to i32
  store i32 %161, i32* bitcast (i64* @global_var_21a43c to i32*)
  store volatile i64 11104, i64* @assembly_address
  %162 = load i32* %stack_var_-28
  %163 = sub i32 %162, 1
  %164 = and i32 %162, 15
  %165 = sub i32 %164, 1
  %166 = icmp ugt i32 %165, 15
  %167 = icmp ult i32 %162, 1
  %168 = xor i32 %162, 1
  %169 = xor i32 %162, %163
  %170 = and i32 %168, %169
  %171 = icmp slt i32 %170, 0
  store i1 %166, i1* %az
  store i1 %167, i1* %cf
  store i1 %171, i1* %of
  %172 = icmp eq i32 %163, 0
  store i1 %172, i1* %zf
  %173 = icmp slt i32 %163, 0
  store i1 %173, i1* %sf
  %174 = trunc i32 %163 to i8
  %175 = call i8 @llvm.ctpop.i8(i8 %174)
  %176 = and i8 %175, 1
  %177 = icmp eq i8 %176, 0
  store i1 %177, i1* %pf
  store volatile i64 11108, i64* @assembly_address
  %178 = load i1* %zf
  %179 = icmp eq i1 %178, false
  br i1 %179, label %block_2b7b, label %block_2b66

block_2b66:                                       ; preds = %block_2aa3
  store volatile i64 11110, i64* @assembly_address
  %180 = load i16** %stack_var_-40
  %181 = ptrtoint i16* %180 to i64
  store i64 %181, i64* %rax
  store volatile i64 11114, i64* @assembly_address
  %182 = load i64* %rax
  %183 = inttoptr i64 %182 to i16*
  %184 = load i16* %183
  %185 = zext i16 %184 to i64
  store i64 %185, i64* %rax
  store volatile i64 11117, i64* @assembly_address
  %186 = load i64* %rax
  %187 = trunc i64 %186 to i32
  %188 = or i32 %187, 4
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %189 = icmp eq i32 %188, 0
  store i1 %189, i1* %zf
  %190 = icmp slt i32 %188, 0
  store i1 %190, i1* %sf
  %191 = trunc i32 %188 to i8
  %192 = call i8 @llvm.ctpop.i8(i8 %191)
  %193 = and i8 %192, 1
  %194 = icmp eq i8 %193, 0
  store i1 %194, i1* %pf
  %195 = zext i32 %188 to i64
  store i64 %195, i64* %rax
  store volatile i64 11120, i64* @assembly_address
  %196 = load i64* %rax
  %197 = trunc i64 %196 to i32
  %198 = zext i32 %197 to i64
  store i64 %198, i64* %rdx
  store volatile i64 11122, i64* @assembly_address
  %199 = load i16** %stack_var_-40
  %200 = ptrtoint i16* %199 to i64
  store i64 %200, i64* %rax
  store volatile i64 11126, i64* @assembly_address
  %201 = load i64* %rdx
  %202 = trunc i64 %201 to i16
  %203 = load i64* %rax
  %204 = inttoptr i64 %203 to i16*
  store i16 %202, i16* %204
  store volatile i64 11129, i64* @assembly_address
  br label %block_2b94

block_2b7b:                                       ; preds = %block_2aa3
  store volatile i64 11131, i64* @assembly_address
  %205 = load i32* %stack_var_-28
  %206 = sub i32 %205, 9
  %207 = and i32 %205, 15
  %208 = sub i32 %207, 9
  %209 = icmp ugt i32 %208, 15
  %210 = icmp ult i32 %205, 9
  %211 = xor i32 %205, 9
  %212 = xor i32 %205, %206
  %213 = and i32 %211, %212
  %214 = icmp slt i32 %213, 0
  store i1 %209, i1* %az
  store i1 %210, i1* %cf
  store i1 %214, i1* %of
  %215 = icmp eq i32 %206, 0
  store i1 %215, i1* %zf
  %216 = icmp slt i32 %206, 0
  store i1 %216, i1* %sf
  %217 = trunc i32 %206 to i8
  %218 = call i8 @llvm.ctpop.i8(i8 %217)
  %219 = and i8 %218, 1
  %220 = icmp eq i8 %219, 0
  store i1 %220, i1* %pf
  store volatile i64 11135, i64* @assembly_address
  %221 = load i1* %zf
  %222 = icmp eq i1 %221, false
  br i1 %222, label %block_2b94, label %block_2b81

block_2b81:                                       ; preds = %block_2b7b
  store volatile i64 11137, i64* @assembly_address
  %223 = load i16** %stack_var_-40
  %224 = ptrtoint i16* %223 to i64
  store i64 %224, i64* %rax
  store volatile i64 11141, i64* @assembly_address
  %225 = load i64* %rax
  %226 = inttoptr i64 %225 to i16*
  %227 = load i16* %226
  %228 = zext i16 %227 to i64
  store i64 %228, i64* %rax
  store volatile i64 11144, i64* @assembly_address
  %229 = load i64* %rax
  %230 = trunc i64 %229 to i32
  %231 = or i32 %230, 2
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %232 = icmp eq i32 %231, 0
  store i1 %232, i1* %zf
  %233 = icmp slt i32 %231, 0
  store i1 %233, i1* %sf
  %234 = trunc i32 %231 to i8
  %235 = call i8 @llvm.ctpop.i8(i8 %234)
  %236 = and i8 %235, 1
  %237 = icmp eq i8 %236, 0
  store i1 %237, i1* %pf
  %238 = zext i32 %231 to i64
  store i64 %238, i64* %rax
  store volatile i64 11147, i64* @assembly_address
  %239 = load i64* %rax
  %240 = trunc i64 %239 to i32
  %241 = zext i32 %240 to i64
  store i64 %241, i64* %rdx
  store volatile i64 11149, i64* @assembly_address
  %242 = load i16** %stack_var_-40
  %243 = ptrtoint i16* %242 to i64
  store i64 %243, i64* %rax
  store volatile i64 11153, i64* @assembly_address
  %244 = load i64* %rdx
  %245 = trunc i64 %244 to i16
  %246 = load i64* %rax
  %247 = inttoptr i64 %246 to i16*
  store i16 %245, i16* %247
  br label %block_2b94

block_2b94:                                       ; preds = %block_2b81, %block_2b7b, %block_2b66
  store volatile i64 11156, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 11166, i64* @assembly_address
  store i64 0, i64* @global_var_21a430
  store volatile i64 11177, i64* @assembly_address
  %248 = load i64* @global_var_21a420
  store i64 %248, i64* %rax
  store volatile i64 11184, i64* @assembly_address
  store i64 65536, i64* %rsi
  store volatile i64 11189, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdi
  %249 = load i64* %rdi
  call void @__ppdasm_undefined_function__store_0(i64 %249)
  store volatile i64 11196, i64* @assembly_address
  %250 = load i64* %rax
  %251 = call i64 @__pseudo_call(i64 %250)
  store i64 %251, i64* %rax
  store volatile i64 11198, i64* @assembly_address
  %252 = load i64* %rax
  %253 = trunc i64 %252 to i32
  store i32 %253, i32* bitcast (i64* @global_var_2165a0 to i32*)
  store volatile i64 11204, i64* @assembly_address
  %254 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %255 = zext i32 %254 to i64
  store i64 %255, i64* %rax
  store volatile i64 11210, i64* @assembly_address
  %256 = load i64* %rax
  %257 = trunc i64 %256 to i32
  %258 = load i64* %rax
  %259 = trunc i64 %258 to i32
  %260 = and i32 %257, %259
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %261 = icmp eq i32 %260, 0
  store i1 %261, i1* %zf
  %262 = icmp slt i32 %260, 0
  store i1 %262, i1* %sf
  %263 = trunc i32 %260 to i8
  %264 = call i8 @llvm.ctpop.i8(i8 %263)
  %265 = and i8 %264, 1
  %266 = icmp eq i8 %265, 0
  store i1 %266, i1* %pf
  store volatile i64 11212, i64* @assembly_address
  %267 = load i1* %zf
  br i1 %267, label %block_2bd9, label %block_2bce

block_2bce:                                       ; preds = %block_2b94
  store volatile i64 11214, i64* @assembly_address
  %268 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %269 = zext i32 %268 to i64
  store i64 %269, i64* %rax
  store volatile i64 11220, i64* @assembly_address
  %270 = load i64* %rax
  %271 = trunc i64 %270 to i32
  %272 = sub i32 %271, -1
  %273 = and i32 %271, 15
  %274 = sub i32 %273, 15
  %275 = icmp ugt i32 %274, 15
  %276 = icmp ult i32 %271, -1
  %277 = xor i32 %271, -1
  %278 = xor i32 %271, %272
  %279 = and i32 %277, %278
  %280 = icmp slt i32 %279, 0
  store i1 %275, i1* %az
  store i1 %276, i1* %cf
  store i1 %280, i1* %of
  %281 = icmp eq i32 %272, 0
  store i1 %281, i1* %zf
  %282 = icmp slt i32 %272, 0
  store i1 %282, i1* %sf
  %283 = trunc i32 %272 to i8
  %284 = call i8 @llvm.ctpop.i8(i8 %283)
  %285 = and i8 %284, 1
  %286 = icmp eq i8 %285, 0
  store i1 %286, i1* %pf
  store volatile i64 11223, i64* @assembly_address
  %287 = load i1* %zf
  %288 = icmp eq i1 %287, false
  br i1 %288, label %block_2bef, label %block_2bd9

block_2bd9:                                       ; preds = %block_2bce, %block_2b94
  store volatile i64 11225, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21659c to i32*)
  store volatile i64 11235, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_2165a0 to i32*)
  store volatile i64 11245, i64* @assembly_address
  br label %block_2c58

block_2bef:                                       ; preds = %block_2bce
  store volatile i64 11247, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21659c to i32*)
  store volatile i64 11257, i64* @assembly_address
  br label %block_2c00

block_2bfb:                                       ; preds = %block_2c0d
  store volatile i64 11259, i64* @assembly_address
  %289 = call i64 @fill_window()
  store i64 %289, i64* %rax
  store i64 %289, i64* %rax
  store i64 %289, i64* %rax
  br label %block_2c00

block_2c00:                                       ; preds = %block_2bfb, %block_2bef
  store volatile i64 11264, i64* @assembly_address
  %290 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %291 = zext i32 %290 to i64
  store i64 %291, i64* %rax
  store volatile i64 11270, i64* @assembly_address
  %292 = load i64* %rax
  %293 = trunc i64 %292 to i32
  %294 = sub i32 %293, 261
  %295 = and i32 %293, 15
  %296 = sub i32 %295, 5
  %297 = icmp ugt i32 %296, 15
  %298 = icmp ult i32 %293, 261
  %299 = xor i32 %293, 261
  %300 = xor i32 %293, %294
  %301 = and i32 %299, %300
  %302 = icmp slt i32 %301, 0
  store i1 %297, i1* %az
  store i1 %298, i1* %cf
  store i1 %302, i1* %of
  %303 = icmp eq i32 %294, 0
  store i1 %303, i1* %zf
  %304 = icmp slt i32 %294, 0
  store i1 %304, i1* %sf
  %305 = trunc i32 %294 to i8
  %306 = call i8 @llvm.ctpop.i8(i8 %305)
  %307 = and i8 %306, 1
  %308 = icmp eq i8 %307, 0
  store i1 %308, i1* %pf
  store volatile i64 11275, i64* @assembly_address
  %309 = load i1* %cf
  %310 = load i1* %zf
  %311 = or i1 %309, %310
  %312 = icmp ne i1 %311, true
  br i1 %312, label %block_2c17, label %block_2c0d

block_2c0d:                                       ; preds = %block_2c00
  store volatile i64 11277, i64* @assembly_address
  %313 = load i32* bitcast (i64* @global_var_21659c to i32*)
  %314 = zext i32 %313 to i64
  store i64 %314, i64* %rax
  store volatile i64 11283, i64* @assembly_address
  %315 = load i64* %rax
  %316 = trunc i64 %315 to i32
  %317 = load i64* %rax
  %318 = trunc i64 %317 to i32
  %319 = and i32 %316, %318
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %320 = icmp eq i32 %319, 0
  store i1 %320, i1* %zf
  %321 = icmp slt i32 %319, 0
  store i1 %321, i1* %sf
  %322 = trunc i32 %319 to i8
  %323 = call i8 @llvm.ctpop.i8(i8 %322)
  %324 = and i8 %323, 1
  %325 = icmp eq i8 %324, 0
  store i1 %325, i1* %pf
  store volatile i64 11285, i64* @assembly_address
  %326 = load i1* %zf
  br i1 %326, label %block_2bfb, label %block_2c17

block_2c17:                                       ; preds = %block_2c0d, %block_2c00
  store volatile i64 11287, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 11297, i64* @assembly_address
  store i64 0, i64* %rbx
  store volatile i64 11302, i64* @assembly_address
  br label %block_2c53

block_2c28:                                       ; preds = %block_2c53
  store volatile i64 11304, i64* @assembly_address
  %327 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %328 = zext i32 %327 to i64
  store i64 %328, i64* %rax
  store volatile i64 11310, i64* @assembly_address
  %329 = load i64* %rax
  %330 = trunc i64 %329 to i32
  %331 = load i1* %of
  %332 = shl i32 %330, 5
  %333 = icmp eq i32 %332, 0
  store i1 %333, i1* %zf
  %334 = icmp slt i32 %332, 0
  store i1 %334, i1* %sf
  %335 = trunc i32 %332 to i8
  %336 = call i8 @llvm.ctpop.i8(i8 %335)
  %337 = and i8 %336, 1
  %338 = icmp eq i8 %337, 0
  store i1 %338, i1* %pf
  %339 = zext i32 %332 to i64
  store i64 %339, i64* %rax
  %340 = shl i32 %330, 4
  %341 = lshr i32 %340, 31
  %342 = trunc i32 %341 to i1
  store i1 %342, i1* %cf
  %343 = lshr i32 %332, 31
  %344 = icmp ne i32 %343, %341
  %345 = select i1 false, i1 %344, i1 %331
  store i1 %345, i1* %of
  store volatile i64 11313, i64* @assembly_address
  %346 = load i64* %rax
  %347 = trunc i64 %346 to i32
  %348 = zext i32 %347 to i64
  store i64 %348, i64* %rcx
  store volatile i64 11315, i64* @assembly_address
  %349 = load i64* %rbx
  %350 = trunc i64 %349 to i32
  %351 = zext i32 %350 to i64
  store i64 %351, i64* %rdx
  store volatile i64 11317, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 11324, i64* @assembly_address
  %352 = load i64* %rdx
  %353 = load i64* %rax
  %354 = mul i64 %353, 1
  %355 = add i64 %352, %354
  %356 = inttoptr i64 %355 to i8*
  %357 = load i8* %356
  %358 = zext i8 %357 to i64
  store i64 %358, i64* %rax
  store volatile i64 11328, i64* @assembly_address
  %359 = load i64* %rax
  %360 = trunc i64 %359 to i8
  %361 = zext i8 %360 to i64
  store i64 %361, i64* %rax
  store volatile i64 11331, i64* @assembly_address
  %362 = load i64* %rax
  %363 = trunc i64 %362 to i32
  %364 = load i64* %rcx
  %365 = trunc i64 %364 to i32
  %366 = xor i32 %363, %365
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %367 = icmp eq i32 %366, 0
  store i1 %367, i1* %zf
  %368 = icmp slt i32 %366, 0
  store i1 %368, i1* %sf
  %369 = trunc i32 %366 to i8
  %370 = call i8 @llvm.ctpop.i8(i8 %369)
  %371 = and i8 %370, 1
  %372 = icmp eq i8 %371, 0
  store i1 %372, i1* %pf
  %373 = zext i32 %366 to i64
  store i64 %373, i64* %rax
  store volatile i64 11333, i64* @assembly_address
  %374 = load i64* %rax
  %375 = trunc i64 %374 to i32
  %376 = and i32 %375, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %377 = icmp eq i32 %376, 0
  store i1 %377, i1* %zf
  %378 = icmp slt i32 %376, 0
  store i1 %378, i1* %sf
  %379 = trunc i32 %376 to i8
  %380 = call i8 @llvm.ctpop.i8(i8 %379)
  %381 = and i8 %380, 1
  %382 = icmp eq i8 %381, 0
  store i1 %382, i1* %pf
  %383 = zext i32 %376 to i64
  store i64 %383, i64* %rax
  store volatile i64 11338, i64* @assembly_address
  %384 = load i64* %rax
  %385 = trunc i64 %384 to i32
  store i32 %385, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 11344, i64* @assembly_address
  %386 = load i64* %rbx
  %387 = trunc i64 %386 to i32
  %388 = add i32 %387, 1
  %389 = and i32 %387, 15
  %390 = add i32 %389, 1
  %391 = icmp ugt i32 %390, 15
  %392 = icmp ult i32 %388, %387
  %393 = xor i32 %387, %388
  %394 = xor i32 1, %388
  %395 = and i32 %393, %394
  %396 = icmp slt i32 %395, 0
  store i1 %391, i1* %az
  store i1 %392, i1* %cf
  store i1 %396, i1* %of
  %397 = icmp eq i32 %388, 0
  store i1 %397, i1* %zf
  %398 = icmp slt i32 %388, 0
  store i1 %398, i1* %sf
  %399 = trunc i32 %388 to i8
  %400 = call i8 @llvm.ctpop.i8(i8 %399)
  %401 = and i8 %400, 1
  %402 = icmp eq i8 %401, 0
  store i1 %402, i1* %pf
  %403 = zext i32 %388 to i64
  store i64 %403, i64* %rbx
  br label %block_2c53

block_2c53:                                       ; preds = %block_2c28, %block_2c17
  store volatile i64 11347, i64* @assembly_address
  %404 = load i64* %rbx
  %405 = trunc i64 %404 to i32
  %406 = sub i32 %405, 1
  %407 = and i32 %405, 15
  %408 = sub i32 %407, 1
  %409 = icmp ugt i32 %408, 15
  %410 = icmp ult i32 %405, 1
  %411 = xor i32 %405, 1
  %412 = xor i32 %405, %406
  %413 = and i32 %411, %412
  %414 = icmp slt i32 %413, 0
  store i1 %409, i1* %az
  store i1 %410, i1* %cf
  store i1 %414, i1* %of
  %415 = icmp eq i32 %406, 0
  store i1 %415, i1* %zf
  %416 = icmp slt i32 %406, 0
  store i1 %416, i1* %sf
  %417 = trunc i32 %406 to i8
  %418 = call i8 @llvm.ctpop.i8(i8 %417)
  %419 = and i8 %418, 1
  %420 = icmp eq i8 %419, 0
  store i1 %420, i1* %pf
  store volatile i64 11350, i64* @assembly_address
  %421 = load i1* %cf
  %422 = load i1* %zf
  %423 = or i1 %421, %422
  br i1 %423, label %block_2c28, label %block_2c58

block_2c58:                                       ; preds = %block_2c53, %block_2bd9
  store volatile i64 11352, i64* @assembly_address
  %424 = load i64* %rsp
  %425 = add i64 %424, 24
  %426 = and i64 %424, 15
  %427 = add i64 %426, 8
  %428 = icmp ugt i64 %427, 15
  %429 = icmp ult i64 %425, %424
  %430 = xor i64 %424, %425
  %431 = xor i64 24, %425
  %432 = and i64 %430, %431
  %433 = icmp slt i64 %432, 0
  store i1 %428, i1* %az
  store i1 %429, i1* %cf
  store i1 %433, i1* %of
  %434 = icmp eq i64 %425, 0
  store i1 %434, i1* %zf
  %435 = icmp slt i64 %425, 0
  store i1 %435, i1* %sf
  %436 = trunc i64 %425 to i8
  %437 = call i8 @llvm.ctpop.i8(i8 %436)
  %438 = and i8 %437, 1
  %439 = icmp eq i8 %438, 0
  store i1 %439, i1* %pf
  %440 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %440, i64* %rsp
  store volatile i64 11356, i64* @assembly_address
  %441 = load i64* %stack_var_-16
  store i64 %441, i64* %rbx
  %442 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %442, i64* %rsp
  store volatile i64 11357, i64* @assembly_address
  %443 = load i64* %stack_var_-8
  store i64 %443, i64* %rbp
  %444 = ptrtoint i64* %stack_var_0 to i64
  store i64 %444, i64* %rsp
  store volatile i64 11358, i64* @assembly_address
  %445 = load i64* %rax
  ret i64 %445
}

declare i64 @166(i64, i16*)

define i64 @longest_match(i32 %arg1) {
block_2c5f:
  %r15 = alloca i64
  %r14 = alloca i64
  %r13 = alloca i64
  %r12 = alloca i64
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-52 = alloca i32
  %stack_var_-56 = alloca i8*
  %1 = alloca i32
  %stack_var_-60 = alloca i32
  %stack_var_-68 = alloca i32
  %stack_var_-48 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i64
  %4 = alloca i64
  %5 = alloca i8*
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i64
  store volatile i64 11359, i64* @assembly_address
  %9 = load i64* %rbp
  store i64 %9, i64* %stack_var_-8
  %10 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %10, i64* %rsp
  store volatile i64 11360, i64* @assembly_address
  %11 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %11, i64* %rbp
  store volatile i64 11363, i64* @assembly_address
  %12 = load i64* %r15
  store i64 %12, i64* %stack_var_-16
  %13 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %13, i64* %rsp
  store volatile i64 11365, i64* @assembly_address
  %14 = load i64* %r14
  store i64 %14, i64* %stack_var_-24
  %15 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %15, i64* %rsp
  store volatile i64 11367, i64* @assembly_address
  %16 = load i64* %r13
  store i64 %16, i64* %stack_var_-32
  %17 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %17, i64* %rsp
  store volatile i64 11369, i64* @assembly_address
  %18 = load i64* %r12
  store i64 %18, i64* %stack_var_-40
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 11371, i64* @assembly_address
  %20 = load i64* %rbx
  store i64 %20, i64* %stack_var_-48
  %21 = ptrtoint i64* %stack_var_-48 to i64
  store i64 %21, i64* %rsp
  store volatile i64 11372, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-68
  store volatile i64 11375, i64* @assembly_address
  %24 = load i32* bitcast (i64* @global_var_21a43c to i32*)
  %25 = zext i32 %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 11381, i64* @assembly_address
  %26 = load i64* %rax
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-60
  store volatile i64 11384, i64* @assembly_address
  %28 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %29 = zext i32 %28 to i64
  store i64 %29, i64* %rax
  store volatile i64 11390, i64* @assembly_address
  %30 = load i64* %rax
  %31 = trunc i64 %30 to i32
  %32 = zext i32 %31 to i64
  store i64 %32, i64* %rdx
  store volatile i64 11392, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 11399, i64* @assembly_address
  %33 = load i64* %rdx
  %34 = load i64* %rax
  %35 = mul i64 %34, 1
  %36 = add i64 %33, %35
  store i64 %36, i64* %rbx
  store volatile i64 11403, i64* @assembly_address
  %37 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %38 = zext i32 %37 to i64
  store i64 %38, i64* %rax
  store volatile i64 11409, i64* @assembly_address
  %39 = load i64* %rax
  %40 = trunc i64 %39 to i32
  %41 = inttoptr i32 %40 to i8*
  store i8* %41, i8** %stack_var_-56
  store volatile i64 11412, i64* @assembly_address
  %42 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %43 = zext i32 %42 to i64
  store i64 %43, i64* %rax
  store volatile i64 11418, i64* @assembly_address
  store i64 32506, i64* %rdx
  store volatile i64 11423, i64* @assembly_address
  %44 = load i64* %rax
  %45 = trunc i64 %44 to i32
  %46 = sub i32 %45, 32506
  %47 = and i32 %45, 15
  %48 = sub i32 %47, 10
  %49 = icmp ugt i32 %48, 15
  %50 = icmp ult i32 %45, 32506
  %51 = xor i32 %45, 32506
  %52 = xor i32 %45, %46
  %53 = and i32 %51, %52
  %54 = icmp slt i32 %53, 0
  store i1 %49, i1* %az
  store i1 %50, i1* %cf
  store i1 %54, i1* %of
  %55 = icmp eq i32 %46, 0
  store i1 %55, i1* %zf
  %56 = icmp slt i32 %46, 0
  store i1 %56, i1* %sf
  %57 = trunc i32 %46 to i8
  %58 = call i8 @llvm.ctpop.i8(i8 %57)
  %59 = and i8 %58, 1
  %60 = icmp eq i8 %59, 0
  store i1 %60, i1* %pf
  store volatile i64 11428, i64* @assembly_address
  %61 = load i1* %cf
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i32
  %64 = load i64* %rdx
  %65 = trunc i64 %64 to i32
  %66 = select i1 %61, i32 %65, i32 %63
  %67 = zext i32 %66 to i64
  store i64 %67, i64* %rax
  store volatile i64 11431, i64* @assembly_address
  %68 = load i64* %rax
  %69 = trunc i64 %68 to i32
  %70 = sub i32 %69, 32506
  %71 = and i32 %69, 15
  %72 = sub i32 %71, 10
  %73 = icmp ugt i32 %72, 15
  %74 = icmp ult i32 %69, 32506
  %75 = xor i32 %69, 32506
  %76 = xor i32 %69, %70
  %77 = and i32 %75, %76
  %78 = icmp slt i32 %77, 0
  store i1 %73, i1* %az
  store i1 %74, i1* %cf
  store i1 %78, i1* %of
  %79 = icmp eq i32 %70, 0
  store i1 %79, i1* %zf
  %80 = icmp slt i32 %70, 0
  store i1 %80, i1* %sf
  %81 = trunc i32 %70 to i8
  %82 = call i8 @llvm.ctpop.i8(i8 %81)
  %83 = and i8 %82, 1
  %84 = icmp eq i8 %83, 0
  store i1 %84, i1* %pf
  %85 = zext i32 %70 to i64
  store i64 %85, i64* %rax
  store volatile i64 11436, i64* @assembly_address
  %86 = load i64* %rax
  %87 = trunc i64 %86 to i32
  store i32 %87, i32* %stack_var_-52
  store volatile i64 11439, i64* @assembly_address
  %88 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %89 = zext i32 %88 to i64
  store i64 %89, i64* %rax
  store volatile i64 11445, i64* @assembly_address
  %90 = load i64* %rax
  %91 = trunc i64 %90 to i32
  %92 = zext i32 %91 to i64
  store i64 %92, i64* %rax
  store volatile i64 11447, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a52a to i64), i64* %rdx
  store volatile i64 11454, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 11461, i64* @assembly_address
  %93 = load i64* %rdx
  %94 = load i64* %rax
  %95 = mul i64 %94, 1
  %96 = add i64 %93, %95
  store i64 %96, i64* %r13
  store volatile i64 11465, i64* @assembly_address
  %97 = load i8** %stack_var_-56
  %98 = ptrtoint i8* %97 to i32
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rax
  store volatile i64 11468, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i32
  %102 = sext i32 %101 to i64
  store i64 %102, i64* %rax
  store volatile i64 11470, i64* @assembly_address
  %103 = load i64* %rax
  %104 = sub i64 %103, 1
  %105 = and i64 %103, 15
  %106 = sub i64 %105, 1
  %107 = icmp ugt i64 %106, 15
  %108 = icmp ult i64 %103, 1
  %109 = xor i64 %103, 1
  %110 = xor i64 %103, %104
  %111 = and i64 %109, %110
  %112 = icmp slt i64 %111, 0
  store i1 %107, i1* %az
  store i1 %108, i1* %cf
  store i1 %112, i1* %of
  %113 = icmp eq i64 %104, 0
  store i1 %113, i1* %zf
  %114 = icmp slt i64 %104, 0
  store i1 %114, i1* %sf
  %115 = trunc i64 %104 to i8
  %116 = call i8 @llvm.ctpop.i8(i8 %115)
  %117 = and i8 %116, 1
  %118 = icmp eq i8 %117, 0
  store i1 %118, i1* %pf
  store i64 %104, i64* %rax
  store volatile i64 11474, i64* @assembly_address
  %119 = load i64* %rax
  %120 = load i64* %rbx
  %121 = add i64 %119, %120
  %122 = and i64 %119, 15
  %123 = and i64 %120, 15
  %124 = add i64 %122, %123
  %125 = icmp ugt i64 %124, 15
  %126 = icmp ult i64 %121, %119
  %127 = xor i64 %119, %121
  %128 = xor i64 %120, %121
  %129 = and i64 %127, %128
  %130 = icmp slt i64 %129, 0
  store i1 %125, i1* %az
  store i1 %126, i1* %cf
  store i1 %130, i1* %of
  %131 = icmp eq i64 %121, 0
  store i1 %131, i1* %zf
  %132 = icmp slt i64 %121, 0
  store i1 %132, i1* %sf
  %133 = trunc i64 %121 to i8
  %134 = call i8 @llvm.ctpop.i8(i8 %133)
  %135 = and i8 %134, 1
  %136 = icmp eq i8 %135, 0
  store i1 %136, i1* %pf
  store i64 %121, i64* %rax
  store volatile i64 11477, i64* @assembly_address
  %137 = load i64* %rax
  %138 = inttoptr i64 %137 to i8*
  %139 = load i8* %138
  %140 = zext i8 %139 to i64
  store i64 %140, i64* %r14
  store volatile i64 11481, i64* @assembly_address
  %141 = load i8** %stack_var_-56
  %142 = ptrtoint i8* %141 to i32
  %143 = zext i32 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 11484, i64* @assembly_address
  %144 = load i64* %rax
  %145 = trunc i64 %144 to i32
  %146 = sext i32 %145 to i64
  store i64 %146, i64* %rax
  store volatile i64 11486, i64* @assembly_address
  %147 = load i64* %rax
  %148 = load i64* %rbx
  %149 = add i64 %147, %148
  %150 = and i64 %147, 15
  %151 = and i64 %148, 15
  %152 = add i64 %150, %151
  %153 = icmp ugt i64 %152, 15
  %154 = icmp ult i64 %149, %147
  %155 = xor i64 %147, %149
  %156 = xor i64 %148, %149
  %157 = and i64 %155, %156
  %158 = icmp slt i64 %157, 0
  store i1 %153, i1* %az
  store i1 %154, i1* %cf
  store i1 %158, i1* %of
  %159 = icmp eq i64 %149, 0
  store i1 %159, i1* %zf
  %160 = icmp slt i64 %149, 0
  store i1 %160, i1* %sf
  %161 = trunc i64 %149 to i8
  %162 = call i8 @llvm.ctpop.i8(i8 %161)
  %163 = and i8 %162, 1
  %164 = icmp eq i8 %163, 0
  store i1 %164, i1* %pf
  store i64 %149, i64* %rax
  store volatile i64 11489, i64* @assembly_address
  %165 = load i64* %rax
  %166 = inttoptr i64 %165 to i8*
  %167 = load i8* %166
  %168 = zext i8 %167 to i64
  store i64 %168, i64* %r15
  store volatile i64 11493, i64* @assembly_address
  %169 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %170 = zext i32 %169 to i64
  store i64 %170, i64* %rdx
  store volatile i64 11499, i64* @assembly_address
  %171 = load i32* bitcast (i64* @global_var_21a438 to i32*)
  %172 = zext i32 %171 to i64
  store i64 %172, i64* %rax
  store volatile i64 11505, i64* @assembly_address
  %173 = load i64* %rdx
  %174 = trunc i64 %173 to i32
  %175 = load i64* %rax
  %176 = trunc i64 %175 to i32
  %177 = sub i32 %174, %176
  %178 = and i32 %174, 15
  %179 = and i32 %176, 15
  %180 = sub i32 %178, %179
  %181 = icmp ugt i32 %180, 15
  %182 = icmp ult i32 %174, %176
  %183 = xor i32 %174, %176
  %184 = xor i32 %174, %177
  %185 = and i32 %183, %184
  %186 = icmp slt i32 %185, 0
  store i1 %181, i1* %az
  store i1 %182, i1* %cf
  store i1 %186, i1* %of
  %187 = icmp eq i32 %177, 0
  store i1 %187, i1* %zf
  %188 = icmp slt i32 %177, 0
  store i1 %188, i1* %sf
  %189 = trunc i32 %177 to i8
  %190 = call i8 @llvm.ctpop.i8(i8 %189)
  %191 = and i8 %190, 1
  %192 = icmp eq i8 %191, 0
  store i1 %192, i1* %pf
  store volatile i64 11507, i64* @assembly_address
  %193 = load i1* %cf
  br i1 %193, label %block_2cf9, label %block_2cf5

block_2cf5:                                       ; preds = %block_2c5f
  store volatile i64 11509, i64* @assembly_address
  %194 = load i32* %stack_var_-60
  %195 = load i1* %of
  %196 = lshr i32 %194, 2
  %197 = icmp eq i32 %196, 0
  store i1 %197, i1* %zf
  %198 = icmp slt i32 %196, 0
  store i1 %198, i1* %sf
  %199 = trunc i32 %196 to i8
  %200 = call i8 @llvm.ctpop.i8(i8 %199)
  %201 = and i8 %200, 1
  %202 = icmp eq i8 %201, 0
  store i1 %202, i1* %pf
  store i32 %196, i32* %stack_var_-60
  %203 = and i32 2, %194
  %204 = icmp ne i32 %203, 0
  store i1 %204, i1* %cf
  %205 = icmp slt i32 %194, 0
  %206 = select i1 false, i1 %205, i1 %195
  store i1 %206, i1* %of
  br label %block_2cf9

block_2cf9:                                       ; preds = %block_2e95, %block_2cf5, %block_2c5f
  store volatile i64 11513, i64* @assembly_address
  %207 = load i32* %stack_var_-68
  %208 = zext i32 %207 to i64
  store i64 %208, i64* %rdx
  store volatile i64 11516, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 11523, i64* @assembly_address
  %209 = load i64* %rdx
  %210 = load i64* %rax
  %211 = mul i64 %210, 1
  %212 = add i64 %209, %211
  store i64 %212, i64* %r12
  store volatile i64 11527, i64* @assembly_address
  %213 = load i8** %stack_var_-56
  %214 = ptrtoint i8* %213 to i32
  %215 = zext i32 %214 to i64
  store i64 %215, i64* %rax
  store volatile i64 11530, i64* @assembly_address
  %216 = load i64* %rax
  %217 = trunc i64 %216 to i32
  %218 = sext i32 %217 to i64
  store i64 %218, i64* %rax
  store volatile i64 11532, i64* @assembly_address
  %219 = load i64* %rax
  %220 = load i64* %r12
  %221 = add i64 %219, %220
  %222 = and i64 %219, 15
  %223 = and i64 %220, 15
  %224 = add i64 %222, %223
  %225 = icmp ugt i64 %224, 15
  %226 = icmp ult i64 %221, %219
  %227 = xor i64 %219, %221
  %228 = xor i64 %220, %221
  %229 = and i64 %227, %228
  %230 = icmp slt i64 %229, 0
  store i1 %225, i1* %az
  store i1 %226, i1* %cf
  store i1 %230, i1* %of
  %231 = icmp eq i64 %221, 0
  store i1 %231, i1* %zf
  %232 = icmp slt i64 %221, 0
  store i1 %232, i1* %sf
  %233 = trunc i64 %221 to i8
  %234 = call i8 @llvm.ctpop.i8(i8 %233)
  %235 = and i8 %234, 1
  %236 = icmp eq i8 %235, 0
  store i1 %236, i1* %pf
  store i64 %221, i64* %rax
  store volatile i64 11535, i64* @assembly_address
  %237 = load i64* %rax
  %238 = inttoptr i64 %237 to i8*
  %239 = load i8* %238
  %240 = zext i8 %239 to i64
  store i64 %240, i64* %rax
  store volatile i64 11538, i64* @assembly_address
  %241 = load i64* %r15
  %242 = trunc i64 %241 to i8
  %243 = load i64* %rax
  %244 = trunc i64 %243 to i8
  %245 = sub i8 %242, %244
  %246 = and i8 %242, 15
  %247 = and i8 %244, 15
  %248 = sub i8 %246, %247
  %249 = icmp ugt i8 %248, 15
  %250 = icmp ult i8 %242, %244
  %251 = xor i8 %242, %244
  %252 = xor i8 %242, %245
  %253 = and i8 %251, %252
  %254 = icmp slt i8 %253, 0
  store i1 %249, i1* %az
  store i1 %250, i1* %cf
  store i1 %254, i1* %of
  %255 = icmp eq i8 %245, 0
  store i1 %255, i1* %zf
  %256 = icmp slt i8 %245, 0
  store i1 %256, i1* %sf
  %257 = call i8 @llvm.ctpop.i8(i8 %245)
  %258 = and i8 %257, 1
  %259 = icmp eq i8 %258, 0
  store i1 %259, i1* %pf
  store volatile i64 11541, i64* @assembly_address
  %260 = load i1* %zf
  %261 = icmp eq i1 %260, false
  br i1 %261, label %block_2e6d, label %block_2d1b

block_2d1b:                                       ; preds = %block_2cf9
  store volatile i64 11547, i64* @assembly_address
  %262 = load i8** %stack_var_-56
  %263 = ptrtoint i8* %262 to i32
  %264 = zext i32 %263 to i64
  store i64 %264, i64* %rax
  store volatile i64 11550, i64* @assembly_address
  %265 = load i64* %rax
  %266 = trunc i64 %265 to i32
  %267 = sext i32 %266 to i64
  store i64 %267, i64* %rax
  store volatile i64 11552, i64* @assembly_address
  %268 = load i64* %rax
  %269 = sub i64 %268, 1
  %270 = and i64 %268, 15
  %271 = sub i64 %270, 1
  %272 = icmp ugt i64 %271, 15
  %273 = icmp ult i64 %268, 1
  %274 = xor i64 %268, 1
  %275 = xor i64 %268, %269
  %276 = and i64 %274, %275
  %277 = icmp slt i64 %276, 0
  store i1 %272, i1* %az
  store i1 %273, i1* %cf
  store i1 %277, i1* %of
  %278 = icmp eq i64 %269, 0
  store i1 %278, i1* %zf
  %279 = icmp slt i64 %269, 0
  store i1 %279, i1* %sf
  %280 = trunc i64 %269 to i8
  %281 = call i8 @llvm.ctpop.i8(i8 %280)
  %282 = and i8 %281, 1
  %283 = icmp eq i8 %282, 0
  store i1 %283, i1* %pf
  store i64 %269, i64* %rax
  store volatile i64 11556, i64* @assembly_address
  %284 = load i64* %rax
  %285 = load i64* %r12
  %286 = add i64 %284, %285
  %287 = and i64 %284, 15
  %288 = and i64 %285, 15
  %289 = add i64 %287, %288
  %290 = icmp ugt i64 %289, 15
  %291 = icmp ult i64 %286, %284
  %292 = xor i64 %284, %286
  %293 = xor i64 %285, %286
  %294 = and i64 %292, %293
  %295 = icmp slt i64 %294, 0
  store i1 %290, i1* %az
  store i1 %291, i1* %cf
  store i1 %295, i1* %of
  %296 = icmp eq i64 %286, 0
  store i1 %296, i1* %zf
  %297 = icmp slt i64 %286, 0
  store i1 %297, i1* %sf
  %298 = trunc i64 %286 to i8
  %299 = call i8 @llvm.ctpop.i8(i8 %298)
  %300 = and i8 %299, 1
  %301 = icmp eq i8 %300, 0
  store i1 %301, i1* %pf
  store i64 %286, i64* %rax
  store volatile i64 11559, i64* @assembly_address
  %302 = load i64* %rax
  %303 = inttoptr i64 %302 to i8*
  %304 = load i8* %303
  %305 = zext i8 %304 to i64
  store i64 %305, i64* %rax
  store volatile i64 11562, i64* @assembly_address
  %306 = load i64* %r14
  %307 = trunc i64 %306 to i8
  %308 = load i64* %rax
  %309 = trunc i64 %308 to i8
  %310 = sub i8 %307, %309
  %311 = and i8 %307, 15
  %312 = and i8 %309, 15
  %313 = sub i8 %311, %312
  %314 = icmp ugt i8 %313, 15
  %315 = icmp ult i8 %307, %309
  %316 = xor i8 %307, %309
  %317 = xor i8 %307, %310
  %318 = and i8 %316, %317
  %319 = icmp slt i8 %318, 0
  store i1 %314, i1* %az
  store i1 %315, i1* %cf
  store i1 %319, i1* %of
  %320 = icmp eq i8 %310, 0
  store i1 %320, i1* %zf
  %321 = icmp slt i8 %310, 0
  store i1 %321, i1* %sf
  %322 = call i8 @llvm.ctpop.i8(i8 %310)
  %323 = and i8 %322, 1
  %324 = icmp eq i8 %323, 0
  store i1 %324, i1* %pf
  store volatile i64 11565, i64* @assembly_address
  %325 = load i1* %zf
  %326 = icmp eq i1 %325, false
  br i1 %326, label %block_2e6d, label %block_2d33

block_2d33:                                       ; preds = %block_2d1b
  store volatile i64 11571, i64* @assembly_address
  %327 = load i64* %r12
  %328 = inttoptr i64 %327 to i8*
  %329 = load i8* %328
  %330 = zext i8 %329 to i64
  store i64 %330, i64* %rdx
  store volatile i64 11576, i64* @assembly_address
  %331 = load i64* %rbx
  %332 = inttoptr i64 %331 to i8*
  %333 = load i8* %332
  %334 = zext i8 %333 to i64
  store i64 %334, i64* %rax
  store volatile i64 11579, i64* @assembly_address
  %335 = load i64* %rdx
  %336 = trunc i64 %335 to i8
  %337 = load i64* %rax
  %338 = trunc i64 %337 to i8
  %339 = sub i8 %336, %338
  %340 = and i8 %336, 15
  %341 = and i8 %338, 15
  %342 = sub i8 %340, %341
  %343 = icmp ugt i8 %342, 15
  %344 = icmp ult i8 %336, %338
  %345 = xor i8 %336, %338
  %346 = xor i8 %336, %339
  %347 = and i8 %345, %346
  %348 = icmp slt i8 %347, 0
  store i1 %343, i1* %az
  store i1 %344, i1* %cf
  store i1 %348, i1* %of
  %349 = icmp eq i8 %339, 0
  store i1 %349, i1* %zf
  %350 = icmp slt i8 %339, 0
  store i1 %350, i1* %sf
  %351 = call i8 @llvm.ctpop.i8(i8 %339)
  %352 = and i8 %351, 1
  %353 = icmp eq i8 %352, 0
  store i1 %353, i1* %pf
  store volatile i64 11581, i64* @assembly_address
  %354 = load i1* %zf
  %355 = icmp eq i1 %354, false
  br i1 %355, label %block_2e6d, label %block_2d43

block_2d43:                                       ; preds = %block_2d33
  store volatile i64 11587, i64* @assembly_address
  %356 = load i64* %r12
  %357 = add i64 %356, 1
  %358 = and i64 %356, 15
  %359 = add i64 %358, 1
  %360 = icmp ugt i64 %359, 15
  %361 = icmp ult i64 %357, %356
  %362 = xor i64 %356, %357
  %363 = xor i64 1, %357
  %364 = and i64 %362, %363
  %365 = icmp slt i64 %364, 0
  store i1 %360, i1* %az
  store i1 %361, i1* %cf
  store i1 %365, i1* %of
  %366 = icmp eq i64 %357, 0
  store i1 %366, i1* %zf
  %367 = icmp slt i64 %357, 0
  store i1 %367, i1* %sf
  %368 = trunc i64 %357 to i8
  %369 = call i8 @llvm.ctpop.i8(i8 %368)
  %370 = and i8 %369, 1
  %371 = icmp eq i8 %370, 0
  store i1 %371, i1* %pf
  store i64 %357, i64* %r12
  store volatile i64 11591, i64* @assembly_address
  %372 = load i64* %r12
  %373 = inttoptr i64 %372 to i8*
  %374 = load i8* %373
  %375 = zext i8 %374 to i64
  store i64 %375, i64* %rdx
  store volatile i64 11596, i64* @assembly_address
  %376 = load i64* %rbx
  %377 = add i64 %376, 1
  store i64 %377, i64* %rax
  store volatile i64 11600, i64* @assembly_address
  %378 = load i64* %rax
  %379 = inttoptr i64 %378 to i8*
  %380 = load i8* %379
  %381 = zext i8 %380 to i64
  store i64 %381, i64* %rax
  store volatile i64 11603, i64* @assembly_address
  %382 = load i64* %rdx
  %383 = trunc i64 %382 to i8
  %384 = load i64* %rax
  %385 = trunc i64 %384 to i8
  %386 = sub i8 %383, %385
  %387 = and i8 %383, 15
  %388 = and i8 %385, 15
  %389 = sub i8 %387, %388
  %390 = icmp ugt i8 %389, 15
  %391 = icmp ult i8 %383, %385
  %392 = xor i8 %383, %385
  %393 = xor i8 %383, %386
  %394 = and i8 %392, %393
  %395 = icmp slt i8 %394, 0
  store i1 %390, i1* %az
  store i1 %391, i1* %cf
  store i1 %395, i1* %of
  %396 = icmp eq i8 %386, 0
  store i1 %396, i1* %zf
  %397 = icmp slt i8 %386, 0
  store i1 %397, i1* %sf
  %398 = call i8 @llvm.ctpop.i8(i8 %386)
  %399 = and i8 %398, 1
  %400 = icmp eq i8 %399, 0
  store i1 %400, i1* %pf
  store volatile i64 11605, i64* @assembly_address
  %401 = load i1* %zf
  %402 = icmp eq i1 %401, false
  br i1 %402, label %block_2e6d, label %block_2d5b

block_2d5b:                                       ; preds = %block_2d43
  store volatile i64 11611, i64* @assembly_address
  %403 = load i64* %rbx
  %404 = add i64 %403, 2
  %405 = and i64 %403, 15
  %406 = add i64 %405, 2
  %407 = icmp ugt i64 %406, 15
  %408 = icmp ult i64 %404, %403
  %409 = xor i64 %403, %404
  %410 = xor i64 2, %404
  %411 = and i64 %409, %410
  %412 = icmp slt i64 %411, 0
  store i1 %407, i1* %az
  store i1 %408, i1* %cf
  store i1 %412, i1* %of
  %413 = icmp eq i64 %404, 0
  store i1 %413, i1* %zf
  %414 = icmp slt i64 %404, 0
  store i1 %414, i1* %sf
  %415 = trunc i64 %404 to i8
  %416 = call i8 @llvm.ctpop.i8(i8 %415)
  %417 = and i8 %416, 1
  %418 = icmp eq i8 %417, 0
  store i1 %418, i1* %pf
  store i64 %404, i64* %rbx
  store volatile i64 11615, i64* @assembly_address
  %419 = load i64* %r12
  %420 = add i64 %419, 1
  %421 = and i64 %419, 15
  %422 = add i64 %421, 1
  %423 = icmp ugt i64 %422, 15
  %424 = icmp ult i64 %420, %419
  %425 = xor i64 %419, %420
  %426 = xor i64 1, %420
  %427 = and i64 %425, %426
  %428 = icmp slt i64 %427, 0
  store i1 %423, i1* %az
  store i1 %424, i1* %cf
  store i1 %428, i1* %of
  %429 = icmp eq i64 %420, 0
  store i1 %429, i1* %zf
  %430 = icmp slt i64 %420, 0
  store i1 %430, i1* %sf
  %431 = trunc i64 %420 to i8
  %432 = call i8 @llvm.ctpop.i8(i8 %431)
  %433 = and i8 %432, 1
  %434 = icmp eq i8 %433, 0
  store i1 %434, i1* %pf
  store i64 %420, i64* %r12
  br label %block_2d63

block_2d63:                                       ; preds = %block_2e0b, %block_2d5b
  store volatile i64 11619, i64* @assembly_address
  %435 = load i64* %rbx
  %436 = add i64 %435, 1
  %437 = and i64 %435, 15
  %438 = add i64 %437, 1
  %439 = icmp ugt i64 %438, 15
  %440 = icmp ult i64 %436, %435
  %441 = xor i64 %435, %436
  %442 = xor i64 1, %436
  %443 = and i64 %441, %442
  %444 = icmp slt i64 %443, 0
  store i1 %439, i1* %az
  store i1 %440, i1* %cf
  store i1 %444, i1* %of
  %445 = icmp eq i64 %436, 0
  store i1 %445, i1* %zf
  %446 = icmp slt i64 %436, 0
  store i1 %446, i1* %sf
  %447 = trunc i64 %436 to i8
  %448 = call i8 @llvm.ctpop.i8(i8 %447)
  %449 = and i8 %448, 1
  %450 = icmp eq i8 %449, 0
  store i1 %450, i1* %pf
  store i64 %436, i64* %rbx
  store volatile i64 11623, i64* @assembly_address
  %451 = load i64* %rbx
  %452 = inttoptr i64 %451 to i8*
  %453 = load i8* %452
  %454 = zext i8 %453 to i64
  store i64 %454, i64* %rdx
  store volatile i64 11626, i64* @assembly_address
  %455 = load i64* %r12
  %456 = add i64 %455, 1
  %457 = and i64 %455, 15
  %458 = add i64 %457, 1
  %459 = icmp ugt i64 %458, 15
  %460 = icmp ult i64 %456, %455
  %461 = xor i64 %455, %456
  %462 = xor i64 1, %456
  %463 = and i64 %461, %462
  %464 = icmp slt i64 %463, 0
  store i1 %459, i1* %az
  store i1 %460, i1* %cf
  store i1 %464, i1* %of
  %465 = icmp eq i64 %456, 0
  store i1 %465, i1* %zf
  %466 = icmp slt i64 %456, 0
  store i1 %466, i1* %sf
  %467 = trunc i64 %456 to i8
  %468 = call i8 @llvm.ctpop.i8(i8 %467)
  %469 = and i8 %468, 1
  %470 = icmp eq i8 %469, 0
  store i1 %470, i1* %pf
  store i64 %456, i64* %r12
  store volatile i64 11630, i64* @assembly_address
  %471 = load i64* %r12
  %472 = inttoptr i64 %471 to i8*
  %473 = load i8* %472
  %474 = zext i8 %473 to i64
  store i64 %474, i64* %rax
  store volatile i64 11635, i64* @assembly_address
  %475 = load i64* %rdx
  %476 = trunc i64 %475 to i8
  %477 = load i64* %rax
  %478 = trunc i64 %477 to i8
  %479 = sub i8 %476, %478
  %480 = and i8 %476, 15
  %481 = and i8 %478, 15
  %482 = sub i8 %480, %481
  %483 = icmp ugt i8 %482, 15
  %484 = icmp ult i8 %476, %478
  %485 = xor i8 %476, %478
  %486 = xor i8 %476, %479
  %487 = and i8 %485, %486
  %488 = icmp slt i8 %487, 0
  store i1 %483, i1* %az
  store i1 %484, i1* %cf
  store i1 %488, i1* %of
  %489 = icmp eq i8 %479, 0
  store i1 %489, i1* %zf
  %490 = icmp slt i8 %479, 0
  store i1 %490, i1* %sf
  %491 = call i8 @llvm.ctpop.i8(i8 %479)
  %492 = and i8 %491, 1
  %493 = icmp eq i8 %492, 0
  store i1 %493, i1* %pf
  store volatile i64 11637, i64* @assembly_address
  %494 = load i1* %zf
  %495 = icmp eq i1 %494, false
  br i1 %495, label %block_2e14, label %block_2d7b

block_2d7b:                                       ; preds = %block_2d63
  store volatile i64 11643, i64* @assembly_address
  %496 = load i64* %rbx
  %497 = add i64 %496, 1
  %498 = and i64 %496, 15
  %499 = add i64 %498, 1
  %500 = icmp ugt i64 %499, 15
  %501 = icmp ult i64 %497, %496
  %502 = xor i64 %496, %497
  %503 = xor i64 1, %497
  %504 = and i64 %502, %503
  %505 = icmp slt i64 %504, 0
  store i1 %500, i1* %az
  store i1 %501, i1* %cf
  store i1 %505, i1* %of
  %506 = icmp eq i64 %497, 0
  store i1 %506, i1* %zf
  %507 = icmp slt i64 %497, 0
  store i1 %507, i1* %sf
  %508 = trunc i64 %497 to i8
  %509 = call i8 @llvm.ctpop.i8(i8 %508)
  %510 = and i8 %509, 1
  %511 = icmp eq i8 %510, 0
  store i1 %511, i1* %pf
  store i64 %497, i64* %rbx
  store volatile i64 11647, i64* @assembly_address
  %512 = load i64* %rbx
  %513 = inttoptr i64 %512 to i8*
  %514 = load i8* %513
  %515 = zext i8 %514 to i64
  store i64 %515, i64* %rdx
  store volatile i64 11650, i64* @assembly_address
  %516 = load i64* %r12
  %517 = add i64 %516, 1
  %518 = and i64 %516, 15
  %519 = add i64 %518, 1
  %520 = icmp ugt i64 %519, 15
  %521 = icmp ult i64 %517, %516
  %522 = xor i64 %516, %517
  %523 = xor i64 1, %517
  %524 = and i64 %522, %523
  %525 = icmp slt i64 %524, 0
  store i1 %520, i1* %az
  store i1 %521, i1* %cf
  store i1 %525, i1* %of
  %526 = icmp eq i64 %517, 0
  store i1 %526, i1* %zf
  %527 = icmp slt i64 %517, 0
  store i1 %527, i1* %sf
  %528 = trunc i64 %517 to i8
  %529 = call i8 @llvm.ctpop.i8(i8 %528)
  %530 = and i8 %529, 1
  %531 = icmp eq i8 %530, 0
  store i1 %531, i1* %pf
  store i64 %517, i64* %r12
  store volatile i64 11654, i64* @assembly_address
  %532 = load i64* %r12
  %533 = inttoptr i64 %532 to i8*
  %534 = load i8* %533
  %535 = zext i8 %534 to i64
  store i64 %535, i64* %rax
  store volatile i64 11659, i64* @assembly_address
  %536 = load i64* %rdx
  %537 = trunc i64 %536 to i8
  %538 = load i64* %rax
  %539 = trunc i64 %538 to i8
  %540 = sub i8 %537, %539
  %541 = and i8 %537, 15
  %542 = and i8 %539, 15
  %543 = sub i8 %541, %542
  %544 = icmp ugt i8 %543, 15
  %545 = icmp ult i8 %537, %539
  %546 = xor i8 %537, %539
  %547 = xor i8 %537, %540
  %548 = and i8 %546, %547
  %549 = icmp slt i8 %548, 0
  store i1 %544, i1* %az
  store i1 %545, i1* %cf
  store i1 %549, i1* %of
  %550 = icmp eq i8 %540, 0
  store i1 %550, i1* %zf
  %551 = icmp slt i8 %540, 0
  store i1 %551, i1* %sf
  %552 = call i8 @llvm.ctpop.i8(i8 %540)
  %553 = and i8 %552, 1
  %554 = icmp eq i8 %553, 0
  store i1 %554, i1* %pf
  store volatile i64 11661, i64* @assembly_address
  %555 = load i1* %zf
  %556 = icmp eq i1 %555, false
  br i1 %556, label %block_2e14, label %block_2d93

block_2d93:                                       ; preds = %block_2d7b
  store volatile i64 11667, i64* @assembly_address
  %557 = load i64* %rbx
  %558 = add i64 %557, 1
  %559 = and i64 %557, 15
  %560 = add i64 %559, 1
  %561 = icmp ugt i64 %560, 15
  %562 = icmp ult i64 %558, %557
  %563 = xor i64 %557, %558
  %564 = xor i64 1, %558
  %565 = and i64 %563, %564
  %566 = icmp slt i64 %565, 0
  store i1 %561, i1* %az
  store i1 %562, i1* %cf
  store i1 %566, i1* %of
  %567 = icmp eq i64 %558, 0
  store i1 %567, i1* %zf
  %568 = icmp slt i64 %558, 0
  store i1 %568, i1* %sf
  %569 = trunc i64 %558 to i8
  %570 = call i8 @llvm.ctpop.i8(i8 %569)
  %571 = and i8 %570, 1
  %572 = icmp eq i8 %571, 0
  store i1 %572, i1* %pf
  store i64 %558, i64* %rbx
  store volatile i64 11671, i64* @assembly_address
  %573 = load i64* %rbx
  %574 = inttoptr i64 %573 to i8*
  %575 = load i8* %574
  %576 = zext i8 %575 to i64
  store i64 %576, i64* %rdx
  store volatile i64 11674, i64* @assembly_address
  %577 = load i64* %r12
  %578 = add i64 %577, 1
  %579 = and i64 %577, 15
  %580 = add i64 %579, 1
  %581 = icmp ugt i64 %580, 15
  %582 = icmp ult i64 %578, %577
  %583 = xor i64 %577, %578
  %584 = xor i64 1, %578
  %585 = and i64 %583, %584
  %586 = icmp slt i64 %585, 0
  store i1 %581, i1* %az
  store i1 %582, i1* %cf
  store i1 %586, i1* %of
  %587 = icmp eq i64 %578, 0
  store i1 %587, i1* %zf
  %588 = icmp slt i64 %578, 0
  store i1 %588, i1* %sf
  %589 = trunc i64 %578 to i8
  %590 = call i8 @llvm.ctpop.i8(i8 %589)
  %591 = and i8 %590, 1
  %592 = icmp eq i8 %591, 0
  store i1 %592, i1* %pf
  store i64 %578, i64* %r12
  store volatile i64 11678, i64* @assembly_address
  %593 = load i64* %r12
  %594 = inttoptr i64 %593 to i8*
  %595 = load i8* %594
  %596 = zext i8 %595 to i64
  store i64 %596, i64* %rax
  store volatile i64 11683, i64* @assembly_address
  %597 = load i64* %rdx
  %598 = trunc i64 %597 to i8
  %599 = load i64* %rax
  %600 = trunc i64 %599 to i8
  %601 = sub i8 %598, %600
  %602 = and i8 %598, 15
  %603 = and i8 %600, 15
  %604 = sub i8 %602, %603
  %605 = icmp ugt i8 %604, 15
  %606 = icmp ult i8 %598, %600
  %607 = xor i8 %598, %600
  %608 = xor i8 %598, %601
  %609 = and i8 %607, %608
  %610 = icmp slt i8 %609, 0
  store i1 %605, i1* %az
  store i1 %606, i1* %cf
  store i1 %610, i1* %of
  %611 = icmp eq i8 %601, 0
  store i1 %611, i1* %zf
  %612 = icmp slt i8 %601, 0
  store i1 %612, i1* %sf
  %613 = call i8 @llvm.ctpop.i8(i8 %601)
  %614 = and i8 %613, 1
  %615 = icmp eq i8 %614, 0
  store i1 %615, i1* %pf
  store volatile i64 11685, i64* @assembly_address
  %616 = load i1* %zf
  %617 = icmp eq i1 %616, false
  br i1 %617, label %block_2e14, label %block_2da7

block_2da7:                                       ; preds = %block_2d93
  store volatile i64 11687, i64* @assembly_address
  %618 = load i64* %rbx
  %619 = add i64 %618, 1
  %620 = and i64 %618, 15
  %621 = add i64 %620, 1
  %622 = icmp ugt i64 %621, 15
  %623 = icmp ult i64 %619, %618
  %624 = xor i64 %618, %619
  %625 = xor i64 1, %619
  %626 = and i64 %624, %625
  %627 = icmp slt i64 %626, 0
  store i1 %622, i1* %az
  store i1 %623, i1* %cf
  store i1 %627, i1* %of
  %628 = icmp eq i64 %619, 0
  store i1 %628, i1* %zf
  %629 = icmp slt i64 %619, 0
  store i1 %629, i1* %sf
  %630 = trunc i64 %619 to i8
  %631 = call i8 @llvm.ctpop.i8(i8 %630)
  %632 = and i8 %631, 1
  %633 = icmp eq i8 %632, 0
  store i1 %633, i1* %pf
  store i64 %619, i64* %rbx
  store volatile i64 11691, i64* @assembly_address
  %634 = load i64* %rbx
  %635 = inttoptr i64 %634 to i8*
  %636 = load i8* %635
  %637 = zext i8 %636 to i64
  store i64 %637, i64* %rdx
  store volatile i64 11694, i64* @assembly_address
  %638 = load i64* %r12
  %639 = add i64 %638, 1
  %640 = and i64 %638, 15
  %641 = add i64 %640, 1
  %642 = icmp ugt i64 %641, 15
  %643 = icmp ult i64 %639, %638
  %644 = xor i64 %638, %639
  %645 = xor i64 1, %639
  %646 = and i64 %644, %645
  %647 = icmp slt i64 %646, 0
  store i1 %642, i1* %az
  store i1 %643, i1* %cf
  store i1 %647, i1* %of
  %648 = icmp eq i64 %639, 0
  store i1 %648, i1* %zf
  %649 = icmp slt i64 %639, 0
  store i1 %649, i1* %sf
  %650 = trunc i64 %639 to i8
  %651 = call i8 @llvm.ctpop.i8(i8 %650)
  %652 = and i8 %651, 1
  %653 = icmp eq i8 %652, 0
  store i1 %653, i1* %pf
  store i64 %639, i64* %r12
  store volatile i64 11698, i64* @assembly_address
  %654 = load i64* %r12
  %655 = inttoptr i64 %654 to i8*
  %656 = load i8* %655
  %657 = zext i8 %656 to i64
  store i64 %657, i64* %rax
  store volatile i64 11703, i64* @assembly_address
  %658 = load i64* %rdx
  %659 = trunc i64 %658 to i8
  %660 = load i64* %rax
  %661 = trunc i64 %660 to i8
  %662 = sub i8 %659, %661
  %663 = and i8 %659, 15
  %664 = and i8 %661, 15
  %665 = sub i8 %663, %664
  %666 = icmp ugt i8 %665, 15
  %667 = icmp ult i8 %659, %661
  %668 = xor i8 %659, %661
  %669 = xor i8 %659, %662
  %670 = and i8 %668, %669
  %671 = icmp slt i8 %670, 0
  store i1 %666, i1* %az
  store i1 %667, i1* %cf
  store i1 %671, i1* %of
  %672 = icmp eq i8 %662, 0
  store i1 %672, i1* %zf
  %673 = icmp slt i8 %662, 0
  store i1 %673, i1* %sf
  %674 = call i8 @llvm.ctpop.i8(i8 %662)
  %675 = and i8 %674, 1
  %676 = icmp eq i8 %675, 0
  store i1 %676, i1* %pf
  store volatile i64 11705, i64* @assembly_address
  %677 = load i1* %zf
  %678 = icmp eq i1 %677, false
  br i1 %678, label %block_2e14, label %block_2dbb

block_2dbb:                                       ; preds = %block_2da7
  store volatile i64 11707, i64* @assembly_address
  %679 = load i64* %rbx
  %680 = add i64 %679, 1
  %681 = and i64 %679, 15
  %682 = add i64 %681, 1
  %683 = icmp ugt i64 %682, 15
  %684 = icmp ult i64 %680, %679
  %685 = xor i64 %679, %680
  %686 = xor i64 1, %680
  %687 = and i64 %685, %686
  %688 = icmp slt i64 %687, 0
  store i1 %683, i1* %az
  store i1 %684, i1* %cf
  store i1 %688, i1* %of
  %689 = icmp eq i64 %680, 0
  store i1 %689, i1* %zf
  %690 = icmp slt i64 %680, 0
  store i1 %690, i1* %sf
  %691 = trunc i64 %680 to i8
  %692 = call i8 @llvm.ctpop.i8(i8 %691)
  %693 = and i8 %692, 1
  %694 = icmp eq i8 %693, 0
  store i1 %694, i1* %pf
  store i64 %680, i64* %rbx
  store volatile i64 11711, i64* @assembly_address
  %695 = load i64* %rbx
  %696 = inttoptr i64 %695 to i8*
  %697 = load i8* %696
  %698 = zext i8 %697 to i64
  store i64 %698, i64* %rdx
  store volatile i64 11714, i64* @assembly_address
  %699 = load i64* %r12
  %700 = add i64 %699, 1
  %701 = and i64 %699, 15
  %702 = add i64 %701, 1
  %703 = icmp ugt i64 %702, 15
  %704 = icmp ult i64 %700, %699
  %705 = xor i64 %699, %700
  %706 = xor i64 1, %700
  %707 = and i64 %705, %706
  %708 = icmp slt i64 %707, 0
  store i1 %703, i1* %az
  store i1 %704, i1* %cf
  store i1 %708, i1* %of
  %709 = icmp eq i64 %700, 0
  store i1 %709, i1* %zf
  %710 = icmp slt i64 %700, 0
  store i1 %710, i1* %sf
  %711 = trunc i64 %700 to i8
  %712 = call i8 @llvm.ctpop.i8(i8 %711)
  %713 = and i8 %712, 1
  %714 = icmp eq i8 %713, 0
  store i1 %714, i1* %pf
  store i64 %700, i64* %r12
  store volatile i64 11718, i64* @assembly_address
  %715 = load i64* %r12
  %716 = inttoptr i64 %715 to i8*
  %717 = load i8* %716
  %718 = zext i8 %717 to i64
  store i64 %718, i64* %rax
  store volatile i64 11723, i64* @assembly_address
  %719 = load i64* %rdx
  %720 = trunc i64 %719 to i8
  %721 = load i64* %rax
  %722 = trunc i64 %721 to i8
  %723 = sub i8 %720, %722
  %724 = and i8 %720, 15
  %725 = and i8 %722, 15
  %726 = sub i8 %724, %725
  %727 = icmp ugt i8 %726, 15
  %728 = icmp ult i8 %720, %722
  %729 = xor i8 %720, %722
  %730 = xor i8 %720, %723
  %731 = and i8 %729, %730
  %732 = icmp slt i8 %731, 0
  store i1 %727, i1* %az
  store i1 %728, i1* %cf
  store i1 %732, i1* %of
  %733 = icmp eq i8 %723, 0
  store i1 %733, i1* %zf
  %734 = icmp slt i8 %723, 0
  store i1 %734, i1* %sf
  %735 = call i8 @llvm.ctpop.i8(i8 %723)
  %736 = and i8 %735, 1
  %737 = icmp eq i8 %736, 0
  store i1 %737, i1* %pf
  store volatile i64 11725, i64* @assembly_address
  %738 = load i1* %zf
  %739 = icmp eq i1 %738, false
  br i1 %739, label %block_2e14, label %block_2dcf

block_2dcf:                                       ; preds = %block_2dbb
  store volatile i64 11727, i64* @assembly_address
  %740 = load i64* %rbx
  %741 = add i64 %740, 1
  %742 = and i64 %740, 15
  %743 = add i64 %742, 1
  %744 = icmp ugt i64 %743, 15
  %745 = icmp ult i64 %741, %740
  %746 = xor i64 %740, %741
  %747 = xor i64 1, %741
  %748 = and i64 %746, %747
  %749 = icmp slt i64 %748, 0
  store i1 %744, i1* %az
  store i1 %745, i1* %cf
  store i1 %749, i1* %of
  %750 = icmp eq i64 %741, 0
  store i1 %750, i1* %zf
  %751 = icmp slt i64 %741, 0
  store i1 %751, i1* %sf
  %752 = trunc i64 %741 to i8
  %753 = call i8 @llvm.ctpop.i8(i8 %752)
  %754 = and i8 %753, 1
  %755 = icmp eq i8 %754, 0
  store i1 %755, i1* %pf
  store i64 %741, i64* %rbx
  store volatile i64 11731, i64* @assembly_address
  %756 = load i64* %rbx
  %757 = inttoptr i64 %756 to i8*
  %758 = load i8* %757
  %759 = zext i8 %758 to i64
  store i64 %759, i64* %rdx
  store volatile i64 11734, i64* @assembly_address
  %760 = load i64* %r12
  %761 = add i64 %760, 1
  %762 = and i64 %760, 15
  %763 = add i64 %762, 1
  %764 = icmp ugt i64 %763, 15
  %765 = icmp ult i64 %761, %760
  %766 = xor i64 %760, %761
  %767 = xor i64 1, %761
  %768 = and i64 %766, %767
  %769 = icmp slt i64 %768, 0
  store i1 %764, i1* %az
  store i1 %765, i1* %cf
  store i1 %769, i1* %of
  %770 = icmp eq i64 %761, 0
  store i1 %770, i1* %zf
  %771 = icmp slt i64 %761, 0
  store i1 %771, i1* %sf
  %772 = trunc i64 %761 to i8
  %773 = call i8 @llvm.ctpop.i8(i8 %772)
  %774 = and i8 %773, 1
  %775 = icmp eq i8 %774, 0
  store i1 %775, i1* %pf
  store i64 %761, i64* %r12
  store volatile i64 11738, i64* @assembly_address
  %776 = load i64* %r12
  %777 = inttoptr i64 %776 to i8*
  %778 = load i8* %777
  %779 = zext i8 %778 to i64
  store i64 %779, i64* %rax
  store volatile i64 11743, i64* @assembly_address
  %780 = load i64* %rdx
  %781 = trunc i64 %780 to i8
  %782 = load i64* %rax
  %783 = trunc i64 %782 to i8
  %784 = sub i8 %781, %783
  %785 = and i8 %781, 15
  %786 = and i8 %783, 15
  %787 = sub i8 %785, %786
  %788 = icmp ugt i8 %787, 15
  %789 = icmp ult i8 %781, %783
  %790 = xor i8 %781, %783
  %791 = xor i8 %781, %784
  %792 = and i8 %790, %791
  %793 = icmp slt i8 %792, 0
  store i1 %788, i1* %az
  store i1 %789, i1* %cf
  store i1 %793, i1* %of
  %794 = icmp eq i8 %784, 0
  store i1 %794, i1* %zf
  %795 = icmp slt i8 %784, 0
  store i1 %795, i1* %sf
  %796 = call i8 @llvm.ctpop.i8(i8 %784)
  %797 = and i8 %796, 1
  %798 = icmp eq i8 %797, 0
  store i1 %798, i1* %pf
  store volatile i64 11745, i64* @assembly_address
  %799 = load i1* %zf
  %800 = icmp eq i1 %799, false
  br i1 %800, label %block_2e14, label %block_2de3

block_2de3:                                       ; preds = %block_2dcf
  store volatile i64 11747, i64* @assembly_address
  %801 = load i64* %rbx
  %802 = add i64 %801, 1
  %803 = and i64 %801, 15
  %804 = add i64 %803, 1
  %805 = icmp ugt i64 %804, 15
  %806 = icmp ult i64 %802, %801
  %807 = xor i64 %801, %802
  %808 = xor i64 1, %802
  %809 = and i64 %807, %808
  %810 = icmp slt i64 %809, 0
  store i1 %805, i1* %az
  store i1 %806, i1* %cf
  store i1 %810, i1* %of
  %811 = icmp eq i64 %802, 0
  store i1 %811, i1* %zf
  %812 = icmp slt i64 %802, 0
  store i1 %812, i1* %sf
  %813 = trunc i64 %802 to i8
  %814 = call i8 @llvm.ctpop.i8(i8 %813)
  %815 = and i8 %814, 1
  %816 = icmp eq i8 %815, 0
  store i1 %816, i1* %pf
  store i64 %802, i64* %rbx
  store volatile i64 11751, i64* @assembly_address
  %817 = load i64* %rbx
  %818 = inttoptr i64 %817 to i8*
  %819 = load i8* %818
  %820 = zext i8 %819 to i64
  store i64 %820, i64* %rdx
  store volatile i64 11754, i64* @assembly_address
  %821 = load i64* %r12
  %822 = add i64 %821, 1
  %823 = and i64 %821, 15
  %824 = add i64 %823, 1
  %825 = icmp ugt i64 %824, 15
  %826 = icmp ult i64 %822, %821
  %827 = xor i64 %821, %822
  %828 = xor i64 1, %822
  %829 = and i64 %827, %828
  %830 = icmp slt i64 %829, 0
  store i1 %825, i1* %az
  store i1 %826, i1* %cf
  store i1 %830, i1* %of
  %831 = icmp eq i64 %822, 0
  store i1 %831, i1* %zf
  %832 = icmp slt i64 %822, 0
  store i1 %832, i1* %sf
  %833 = trunc i64 %822 to i8
  %834 = call i8 @llvm.ctpop.i8(i8 %833)
  %835 = and i8 %834, 1
  %836 = icmp eq i8 %835, 0
  store i1 %836, i1* %pf
  store i64 %822, i64* %r12
  store volatile i64 11758, i64* @assembly_address
  %837 = load i64* %r12
  %838 = inttoptr i64 %837 to i8*
  %839 = load i8* %838
  %840 = zext i8 %839 to i64
  store i64 %840, i64* %rax
  store volatile i64 11763, i64* @assembly_address
  %841 = load i64* %rdx
  %842 = trunc i64 %841 to i8
  %843 = load i64* %rax
  %844 = trunc i64 %843 to i8
  %845 = sub i8 %842, %844
  %846 = and i8 %842, 15
  %847 = and i8 %844, 15
  %848 = sub i8 %846, %847
  %849 = icmp ugt i8 %848, 15
  %850 = icmp ult i8 %842, %844
  %851 = xor i8 %842, %844
  %852 = xor i8 %842, %845
  %853 = and i8 %851, %852
  %854 = icmp slt i8 %853, 0
  store i1 %849, i1* %az
  store i1 %850, i1* %cf
  store i1 %854, i1* %of
  %855 = icmp eq i8 %845, 0
  store i1 %855, i1* %zf
  %856 = icmp slt i8 %845, 0
  store i1 %856, i1* %sf
  %857 = call i8 @llvm.ctpop.i8(i8 %845)
  %858 = and i8 %857, 1
  %859 = icmp eq i8 %858, 0
  store i1 %859, i1* %pf
  store volatile i64 11765, i64* @assembly_address
  %860 = load i1* %zf
  %861 = icmp eq i1 %860, false
  br i1 %861, label %block_2e14, label %block_2df7

block_2df7:                                       ; preds = %block_2de3
  store volatile i64 11767, i64* @assembly_address
  %862 = load i64* %rbx
  %863 = add i64 %862, 1
  %864 = and i64 %862, 15
  %865 = add i64 %864, 1
  %866 = icmp ugt i64 %865, 15
  %867 = icmp ult i64 %863, %862
  %868 = xor i64 %862, %863
  %869 = xor i64 1, %863
  %870 = and i64 %868, %869
  %871 = icmp slt i64 %870, 0
  store i1 %866, i1* %az
  store i1 %867, i1* %cf
  store i1 %871, i1* %of
  %872 = icmp eq i64 %863, 0
  store i1 %872, i1* %zf
  %873 = icmp slt i64 %863, 0
  store i1 %873, i1* %sf
  %874 = trunc i64 %863 to i8
  %875 = call i8 @llvm.ctpop.i8(i8 %874)
  %876 = and i8 %875, 1
  %877 = icmp eq i8 %876, 0
  store i1 %877, i1* %pf
  store i64 %863, i64* %rbx
  store volatile i64 11771, i64* @assembly_address
  %878 = load i64* %rbx
  %879 = inttoptr i64 %878 to i8*
  %880 = load i8* %879
  %881 = zext i8 %880 to i64
  store i64 %881, i64* %rdx
  store volatile i64 11774, i64* @assembly_address
  %882 = load i64* %r12
  %883 = add i64 %882, 1
  %884 = and i64 %882, 15
  %885 = add i64 %884, 1
  %886 = icmp ugt i64 %885, 15
  %887 = icmp ult i64 %883, %882
  %888 = xor i64 %882, %883
  %889 = xor i64 1, %883
  %890 = and i64 %888, %889
  %891 = icmp slt i64 %890, 0
  store i1 %886, i1* %az
  store i1 %887, i1* %cf
  store i1 %891, i1* %of
  %892 = icmp eq i64 %883, 0
  store i1 %892, i1* %zf
  %893 = icmp slt i64 %883, 0
  store i1 %893, i1* %sf
  %894 = trunc i64 %883 to i8
  %895 = call i8 @llvm.ctpop.i8(i8 %894)
  %896 = and i8 %895, 1
  %897 = icmp eq i8 %896, 0
  store i1 %897, i1* %pf
  store i64 %883, i64* %r12
  store volatile i64 11778, i64* @assembly_address
  %898 = load i64* %r12
  %899 = inttoptr i64 %898 to i8*
  %900 = load i8* %899
  %901 = zext i8 %900 to i64
  store i64 %901, i64* %rax
  store volatile i64 11783, i64* @assembly_address
  %902 = load i64* %rdx
  %903 = trunc i64 %902 to i8
  %904 = load i64* %rax
  %905 = trunc i64 %904 to i8
  %906 = sub i8 %903, %905
  %907 = and i8 %903, 15
  %908 = and i8 %905, 15
  %909 = sub i8 %907, %908
  %910 = icmp ugt i8 %909, 15
  %911 = icmp ult i8 %903, %905
  %912 = xor i8 %903, %905
  %913 = xor i8 %903, %906
  %914 = and i8 %912, %913
  %915 = icmp slt i8 %914, 0
  store i1 %910, i1* %az
  store i1 %911, i1* %cf
  store i1 %915, i1* %of
  %916 = icmp eq i8 %906, 0
  store i1 %916, i1* %zf
  %917 = icmp slt i8 %906, 0
  store i1 %917, i1* %sf
  %918 = call i8 @llvm.ctpop.i8(i8 %906)
  %919 = and i8 %918, 1
  %920 = icmp eq i8 %919, 0
  store i1 %920, i1* %pf
  store volatile i64 11785, i64* @assembly_address
  %921 = load i1* %zf
  %922 = icmp eq i1 %921, false
  br i1 %922, label %block_2e14, label %block_2e0b

block_2e0b:                                       ; preds = %block_2df7
  store volatile i64 11787, i64* @assembly_address
  %923 = load i64* %rbx
  %924 = load i64* %r13
  %925 = sub i64 %923, %924
  %926 = and i64 %923, 15
  %927 = and i64 %924, 15
  %928 = sub i64 %926, %927
  %929 = icmp ugt i64 %928, 15
  %930 = icmp ult i64 %923, %924
  %931 = xor i64 %923, %924
  %932 = xor i64 %923, %925
  %933 = and i64 %931, %932
  %934 = icmp slt i64 %933, 0
  store i1 %929, i1* %az
  store i1 %930, i1* %cf
  store i1 %934, i1* %of
  %935 = icmp eq i64 %925, 0
  store i1 %935, i1* %zf
  %936 = icmp slt i64 %925, 0
  store i1 %936, i1* %sf
  %937 = trunc i64 %925 to i8
  %938 = call i8 @llvm.ctpop.i8(i8 %937)
  %939 = and i8 %938, 1
  %940 = icmp eq i8 %939, 0
  store i1 %940, i1* %pf
  store volatile i64 11790, i64* @assembly_address
  %941 = load i1* %cf
  br i1 %941, label %block_2d63, label %block_2e14

block_2e14:                                       ; preds = %block_2e0b, %block_2df7, %block_2de3, %block_2dcf, %block_2dbb, %block_2da7, %block_2d93, %block_2d7b, %block_2d63
  store volatile i64 11796, i64* @assembly_address
  %942 = load i64* %r13
  store i64 %942, i64* %rdx
  store volatile i64 11799, i64* @assembly_address
  %943 = load i64* %rbx
  store i64 %943, i64* %rax
  store volatile i64 11802, i64* @assembly_address
  %944 = load i64* %rdx
  %945 = load i64* %rax
  %946 = sub i64 %944, %945
  %947 = and i64 %944, 15
  %948 = and i64 %945, 15
  %949 = sub i64 %947, %948
  %950 = icmp ugt i64 %949, 15
  %951 = icmp ult i64 %944, %945
  %952 = xor i64 %944, %945
  %953 = xor i64 %944, %946
  %954 = and i64 %952, %953
  %955 = icmp slt i64 %954, 0
  store i1 %950, i1* %az
  store i1 %951, i1* %cf
  store i1 %955, i1* %of
  %956 = icmp eq i64 %946, 0
  store i1 %956, i1* %zf
  %957 = icmp slt i64 %946, 0
  store i1 %957, i1* %sf
  %958 = trunc i64 %946 to i8
  %959 = call i8 @llvm.ctpop.i8(i8 %958)
  %960 = and i8 %959, 1
  %961 = icmp eq i8 %960, 0
  store i1 %961, i1* %pf
  store i64 %946, i64* %rdx
  store volatile i64 11805, i64* @assembly_address
  %962 = load i64* %rdx
  store i64 %962, i64* %rax
  store volatile i64 11808, i64* @assembly_address
  store i64 258, i64* %rdx
  store volatile i64 11813, i64* @assembly_address
  %963 = load i64* %rdx
  %964 = trunc i64 %963 to i32
  %965 = load i64* %rax
  %966 = trunc i64 %965 to i32
  %967 = sub i32 %964, %966
  %968 = and i32 %964, 15
  %969 = and i32 %966, 15
  %970 = sub i32 %968, %969
  %971 = icmp ugt i32 %970, 15
  %972 = icmp ult i32 %964, %966
  %973 = xor i32 %964, %966
  %974 = xor i32 %964, %967
  %975 = and i32 %973, %974
  %976 = icmp slt i32 %975, 0
  store i1 %971, i1* %az
  store i1 %972, i1* %cf
  store i1 %976, i1* %of
  %977 = icmp eq i32 %967, 0
  store i1 %977, i1* %zf
  %978 = icmp slt i32 %967, 0
  store i1 %978, i1* %sf
  %979 = trunc i32 %967 to i8
  %980 = call i8 @llvm.ctpop.i8(i8 %979)
  %981 = and i8 %980, 1
  %982 = icmp eq i8 %981, 0
  store i1 %982, i1* %pf
  %983 = zext i32 %967 to i64
  store i64 %983, i64* %rdx
  store volatile i64 11815, i64* @assembly_address
  %984 = load i64* %rdx
  %985 = trunc i64 %984 to i32
  %986 = zext i32 %985 to i64
  store i64 %986, i64* %r12
  store volatile i64 11818, i64* @assembly_address
  %987 = load i64* %r13
  %988 = add i64 %987, -258
  store i64 %988, i64* %rbx
  store volatile i64 11825, i64* @assembly_address
  %989 = load i64* %r12
  %990 = trunc i64 %989 to i32
  %991 = load i8** %stack_var_-56
  %992 = ptrtoint i8* %991 to i32
  %993 = trunc i64 %989 to i32
  store i32 %993, i32* %7
  store i8* %991, i8** %5
  %994 = sub i32 %990, %992
  %995 = and i32 %990, 15
  %996 = and i32 %992, 15
  %997 = sub i32 %995, %996
  %998 = icmp ugt i32 %997, 15
  %999 = icmp ult i32 %990, %992
  %1000 = xor i32 %990, %992
  %1001 = xor i32 %990, %994
  %1002 = and i32 %1000, %1001
  %1003 = icmp slt i32 %1002, 0
  store i1 %998, i1* %az
  store i1 %999, i1* %cf
  store i1 %1003, i1* %of
  %1004 = icmp eq i32 %994, 0
  store i1 %1004, i1* %zf
  %1005 = icmp slt i32 %994, 0
  store i1 %1005, i1* %sf
  %1006 = trunc i32 %994 to i8
  %1007 = call i8 @llvm.ctpop.i8(i8 %1006)
  %1008 = and i8 %1007, 1
  %1009 = icmp eq i8 %1008, 0
  store i1 %1009, i1* %pf
  store volatile i64 11829, i64* @assembly_address
  %1010 = load i32* %7
  %1011 = sext i32 %1010 to i64
  %1012 = load i8** %5
  %1013 = ptrtoint i8* %1012 to i32
  %1014 = trunc i64 %1011 to i32
  %1015 = icmp sle i32 %1014, %1013
  br i1 %1015, label %block_2e6e, label %block_2e37

block_2e37:                                       ; preds = %block_2e14
  store volatile i64 11831, i64* @assembly_address
  %1016 = load i32* %stack_var_-68
  %1017 = zext i32 %1016 to i64
  store i64 %1017, i64* %rax
  store volatile i64 11834, i64* @assembly_address
  %1018 = load i64* %rax
  %1019 = trunc i64 %1018 to i32
  store i32 %1019, i32* bitcast (i64* @global_var_21a444 to i32*)
  store volatile i64 11840, i64* @assembly_address
  %1020 = load i64* %r12
  %1021 = trunc i64 %1020 to i32
  %1022 = inttoptr i32 %1021 to i8*
  store i8* %1022, i8** %stack_var_-56
  store volatile i64 11844, i64* @assembly_address
  %1023 = load i32* bitcast (i64* @global_var_2165c0 to i32*)
  %1024 = zext i32 %1023 to i64
  store i64 %1024, i64* %rax
  store volatile i64 11850, i64* @assembly_address
  %1025 = load i64* %r12
  %1026 = trunc i64 %1025 to i32
  %1027 = load i64* %rax
  %1028 = trunc i64 %1027 to i32
  store i64 %1025, i64* %4
  %1029 = trunc i64 %1027 to i32
  store i32 %1029, i32* %2
  %1030 = sub i32 %1026, %1028
  %1031 = and i32 %1026, 15
  %1032 = and i32 %1028, 15
  %1033 = sub i32 %1031, %1032
  %1034 = icmp ugt i32 %1033, 15
  %1035 = icmp ult i32 %1026, %1028
  %1036 = xor i32 %1026, %1028
  %1037 = xor i32 %1026, %1030
  %1038 = and i32 %1036, %1037
  %1039 = icmp slt i32 %1038, 0
  store i1 %1034, i1* %az
  store i1 %1035, i1* %cf
  store i1 %1039, i1* %of
  %1040 = icmp eq i32 %1030, 0
  store i1 %1040, i1* %zf
  %1041 = icmp slt i32 %1030, 0
  store i1 %1041, i1* %sf
  %1042 = trunc i32 %1030 to i8
  %1043 = call i8 @llvm.ctpop.i8(i8 %1042)
  %1044 = and i8 %1043, 1
  %1045 = icmp eq i8 %1044, 0
  store i1 %1045, i1* %pf
  store volatile i64 11853, i64* @assembly_address
  %1046 = load i64* %4
  %1047 = load i32* %2
  %1048 = sext i32 %1047 to i64
  %1049 = icmp sge i64 %1046, %1048
  br i1 %1049, label %block_2ea5, label %block_2e4f

block_2e4f:                                       ; preds = %block_2e37
  store volatile i64 11855, i64* @assembly_address
  %1050 = load i8** %stack_var_-56
  %1051 = ptrtoint i8* %1050 to i32
  %1052 = zext i32 %1051 to i64
  store i64 %1052, i64* %rax
  store volatile i64 11858, i64* @assembly_address
  %1053 = load i64* %rax
  %1054 = trunc i64 %1053 to i32
  %1055 = sext i32 %1054 to i64
  store i64 %1055, i64* %rax
  store volatile i64 11860, i64* @assembly_address
  %1056 = load i64* %rax
  %1057 = sub i64 %1056, 1
  %1058 = and i64 %1056, 15
  %1059 = sub i64 %1058, 1
  %1060 = icmp ugt i64 %1059, 15
  %1061 = icmp ult i64 %1056, 1
  %1062 = xor i64 %1056, 1
  %1063 = xor i64 %1056, %1057
  %1064 = and i64 %1062, %1063
  %1065 = icmp slt i64 %1064, 0
  store i1 %1060, i1* %az
  store i1 %1061, i1* %cf
  store i1 %1065, i1* %of
  %1066 = icmp eq i64 %1057, 0
  store i1 %1066, i1* %zf
  %1067 = icmp slt i64 %1057, 0
  store i1 %1067, i1* %sf
  %1068 = trunc i64 %1057 to i8
  %1069 = call i8 @llvm.ctpop.i8(i8 %1068)
  %1070 = and i8 %1069, 1
  %1071 = icmp eq i8 %1070, 0
  store i1 %1071, i1* %pf
  store i64 %1057, i64* %rax
  store volatile i64 11864, i64* @assembly_address
  %1072 = load i64* %rax
  %1073 = load i64* %rbx
  %1074 = add i64 %1072, %1073
  %1075 = and i64 %1072, 15
  %1076 = and i64 %1073, 15
  %1077 = add i64 %1075, %1076
  %1078 = icmp ugt i64 %1077, 15
  %1079 = icmp ult i64 %1074, %1072
  %1080 = xor i64 %1072, %1074
  %1081 = xor i64 %1073, %1074
  %1082 = and i64 %1080, %1081
  %1083 = icmp slt i64 %1082, 0
  store i1 %1078, i1* %az
  store i1 %1079, i1* %cf
  store i1 %1083, i1* %of
  %1084 = icmp eq i64 %1074, 0
  store i1 %1084, i1* %zf
  %1085 = icmp slt i64 %1074, 0
  store i1 %1085, i1* %sf
  %1086 = trunc i64 %1074 to i8
  %1087 = call i8 @llvm.ctpop.i8(i8 %1086)
  %1088 = and i8 %1087, 1
  %1089 = icmp eq i8 %1088, 0
  store i1 %1089, i1* %pf
  store i64 %1074, i64* %rax
  store volatile i64 11867, i64* @assembly_address
  %1090 = load i64* %rax
  %1091 = inttoptr i64 %1090 to i8*
  %1092 = load i8* %1091
  %1093 = zext i8 %1092 to i64
  store i64 %1093, i64* %r14
  store volatile i64 11871, i64* @assembly_address
  %1094 = load i8** %stack_var_-56
  %1095 = ptrtoint i8* %1094 to i32
  %1096 = zext i32 %1095 to i64
  store i64 %1096, i64* %rax
  store volatile i64 11874, i64* @assembly_address
  %1097 = load i64* %rax
  %1098 = trunc i64 %1097 to i32
  %1099 = sext i32 %1098 to i64
  store i64 %1099, i64* %rax
  store volatile i64 11876, i64* @assembly_address
  %1100 = load i64* %rax
  %1101 = load i64* %rbx
  %1102 = add i64 %1100, %1101
  %1103 = and i64 %1100, 15
  %1104 = and i64 %1101, 15
  %1105 = add i64 %1103, %1104
  %1106 = icmp ugt i64 %1105, 15
  %1107 = icmp ult i64 %1102, %1100
  %1108 = xor i64 %1100, %1102
  %1109 = xor i64 %1101, %1102
  %1110 = and i64 %1108, %1109
  %1111 = icmp slt i64 %1110, 0
  store i1 %1106, i1* %az
  store i1 %1107, i1* %cf
  store i1 %1111, i1* %of
  %1112 = icmp eq i64 %1102, 0
  store i1 %1112, i1* %zf
  %1113 = icmp slt i64 %1102, 0
  store i1 %1113, i1* %sf
  %1114 = trunc i64 %1102 to i8
  %1115 = call i8 @llvm.ctpop.i8(i8 %1114)
  %1116 = and i8 %1115, 1
  %1117 = icmp eq i8 %1116, 0
  store i1 %1117, i1* %pf
  store i64 %1102, i64* %rax
  store volatile i64 11879, i64* @assembly_address
  %1118 = load i64* %rax
  %1119 = inttoptr i64 %1118 to i8*
  %1120 = load i8* %1119
  %1121 = zext i8 %1120 to i64
  store i64 %1121, i64* %r15
  store volatile i64 11883, i64* @assembly_address
  br label %block_2e6e

block_2e6d:                                       ; preds = %block_2d43, %block_2d33, %block_2d1b, %block_2cf9
  store volatile i64 11885, i64* @assembly_address
  br label %block_2e6e

block_2e6e:                                       ; preds = %block_2e6d, %block_2e4f, %block_2e14
  store volatile i64 11886, i64* @assembly_address
  %1122 = load i32* %stack_var_-68
  %1123 = zext i32 %1122 to i64
  store i64 %1123, i64* %rax
  store volatile i64 11889, i64* @assembly_address
  %1124 = load i64* %rax
  %1125 = trunc i64 %1124 to i32
  %1126 = and i32 %1125, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1127 = icmp eq i32 %1126, 0
  store i1 %1127, i1* %zf
  %1128 = icmp slt i32 %1126, 0
  store i1 %1128, i1* %sf
  %1129 = trunc i32 %1126 to i8
  %1130 = call i8 @llvm.ctpop.i8(i8 %1129)
  %1131 = and i8 %1130, 1
  %1132 = icmp eq i8 %1131, 0
  store i1 %1132, i1* %pf
  %1133 = zext i32 %1126 to i64
  store i64 %1133, i64* %rax
  store volatile i64 11894, i64* @assembly_address
  %1134 = load i64* %rax
  %1135 = trunc i64 %1134 to i32
  %1136 = zext i32 %1135 to i64
  store i64 %1136, i64* %rax
  store volatile i64 11896, i64* @assembly_address
  %1137 = load i64* %rax
  %1138 = load i64* %rax
  %1139 = mul i64 %1138, 1
  %1140 = add i64 %1137, %1139
  store i64 %1140, i64* %rdx
  store volatile i64 11900, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 11907, i64* @assembly_address
  %1141 = load i64* %rdx
  %1142 = load i64* %rax
  %1143 = mul i64 %1142, 1
  %1144 = add i64 %1141, %1143
  %1145 = inttoptr i64 %1144 to i16*
  %1146 = load i16* %1145
  %1147 = zext i16 %1146 to i64
  store i64 %1147, i64* %rax
  store volatile i64 11911, i64* @assembly_address
  %1148 = load i64* %rax
  %1149 = trunc i64 %1148 to i16
  %1150 = zext i16 %1149 to i64
  store i64 %1150, i64* %rax
  store volatile i64 11914, i64* @assembly_address
  %1151 = load i64* %rax
  %1152 = trunc i64 %1151 to i32
  store i32 %1152, i32* %stack_var_-68
  store volatile i64 11917, i64* @assembly_address
  %1153 = load i32* %stack_var_-68
  %1154 = zext i32 %1153 to i64
  store i64 %1154, i64* %rax
  store volatile i64 11920, i64* @assembly_address
  %1155 = load i64* %rax
  %1156 = trunc i64 %1155 to i32
  %1157 = load i32* %stack_var_-52
  %1158 = sub i32 %1156, %1157
  %1159 = and i32 %1156, 15
  %1160 = and i32 %1157, 15
  %1161 = sub i32 %1159, %1160
  %1162 = icmp ugt i32 %1161, 15
  %1163 = icmp ult i32 %1156, %1157
  %1164 = xor i32 %1156, %1157
  %1165 = xor i32 %1156, %1158
  %1166 = and i32 %1164, %1165
  %1167 = icmp slt i32 %1166, 0
  store i1 %1162, i1* %az
  store i1 %1163, i1* %cf
  store i1 %1167, i1* %of
  %1168 = icmp eq i32 %1158, 0
  store i1 %1168, i1* %zf
  %1169 = icmp slt i32 %1158, 0
  store i1 %1169, i1* %sf
  %1170 = trunc i32 %1158 to i8
  %1171 = call i8 @llvm.ctpop.i8(i8 %1170)
  %1172 = and i8 %1171, 1
  %1173 = icmp eq i8 %1172, 0
  store i1 %1173, i1* %pf
  store volatile i64 11923, i64* @assembly_address
  %1174 = load i1* %cf
  %1175 = load i1* %zf
  %1176 = or i1 %1174, %1175
  br i1 %1176, label %block_2ea6, label %block_2e95

block_2e95:                                       ; preds = %block_2e6e
  store volatile i64 11925, i64* @assembly_address
  %1177 = load i32* %stack_var_-60
  %1178 = sub i32 %1177, 1
  %1179 = and i32 %1177, 15
  %1180 = sub i32 %1179, 1
  %1181 = icmp ugt i32 %1180, 15
  %1182 = icmp ult i32 %1177, 1
  %1183 = xor i32 %1177, 1
  %1184 = xor i32 %1177, %1178
  %1185 = and i32 %1183, %1184
  %1186 = icmp slt i32 %1185, 0
  store i1 %1181, i1* %az
  store i1 %1182, i1* %cf
  store i1 %1186, i1* %of
  %1187 = icmp eq i32 %1178, 0
  store i1 %1187, i1* %zf
  %1188 = icmp slt i32 %1178, 0
  store i1 %1188, i1* %sf
  %1189 = trunc i32 %1178 to i8
  %1190 = call i8 @llvm.ctpop.i8(i8 %1189)
  %1191 = and i8 %1190, 1
  %1192 = icmp eq i8 %1191, 0
  store i1 %1192, i1* %pf
  store i32 %1178, i32* %stack_var_-60
  store volatile i64 11929, i64* @assembly_address
  %1193 = load i32* %stack_var_-60
  %1194 = and i32 %1193, 15
  %1195 = icmp ugt i32 %1194, 15
  %1196 = icmp ult i32 %1193, 0
  %1197 = xor i32 %1193, 0
  %1198 = and i32 %1197, 0
  %1199 = icmp slt i32 %1198, 0
  store i1 %1195, i1* %az
  store i1 %1196, i1* %cf
  store i1 %1199, i1* %of
  %1200 = icmp eq i32 %1193, 0
  store i1 %1200, i1* %zf
  %1201 = icmp slt i32 %1193, 0
  store i1 %1201, i1* %sf
  %1202 = trunc i32 %1193 to i8
  %1203 = call i8 @llvm.ctpop.i8(i8 %1202)
  %1204 = and i8 %1203, 1
  %1205 = icmp eq i8 %1204, 0
  store i1 %1205, i1* %pf
  store volatile i64 11933, i64* @assembly_address
  %1206 = load i1* %zf
  %1207 = icmp eq i1 %1206, false
  br i1 %1207, label %block_2cf9, label %block_2ea3

block_2ea3:                                       ; preds = %block_2e95
  store volatile i64 11939, i64* @assembly_address
  br label %block_2ea6

block_2ea5:                                       ; preds = %block_2e37
  store volatile i64 11941, i64* @assembly_address
  br label %block_2ea6

block_2ea6:                                       ; preds = %block_2ea5, %block_2ea3, %block_2e6e
  store volatile i64 11942, i64* @assembly_address
  %1208 = load i8** %stack_var_-56
  %1209 = ptrtoint i8* %1208 to i32
  %1210 = zext i32 %1209 to i64
  store i64 %1210, i64* %rax
  store volatile i64 11945, i64* @assembly_address
  %1211 = load i64* %stack_var_-48
  store i64 %1211, i64* %rbx
  %1212 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %1212, i64* %rsp
  store volatile i64 11946, i64* @assembly_address
  %1213 = load i64* %stack_var_-40
  store i64 %1213, i64* %r12
  %1214 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %1214, i64* %rsp
  store volatile i64 11948, i64* @assembly_address
  %1215 = load i64* %stack_var_-32
  store i64 %1215, i64* %r13
  %1216 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %1216, i64* %rsp
  store volatile i64 11950, i64* @assembly_address
  %1217 = load i64* %stack_var_-24
  store i64 %1217, i64* %r14
  %1218 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %1218, i64* %rsp
  store volatile i64 11952, i64* @assembly_address
  %1219 = load i64* %stack_var_-16
  store i64 %1219, i64* %r15
  %1220 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1220, i64* %rsp
  store volatile i64 11954, i64* @assembly_address
  %1221 = load i64* %stack_var_-8
  store i64 %1221, i64* %rbp
  %1222 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1222, i64* %rsp
  store volatile i64 11955, i64* @assembly_address
  %1223 = load i64* %rax
  ret i64 %1223
}

declare i64 @167(i64)

define i64 @fill_window() {
block_2eb4:
  %r12 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i64*
  %0 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 11956, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 11957, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 11960, i64* @assembly_address
  %4 = load i64* %r12
  store i64 %4, i64* %stack_var_-16
  %5 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %5, i64* %rsp
  store volatile i64 11962, i64* @assembly_address
  %6 = load i64* %rbx
  store i64 %6, i64* %stack_var_-24
  %7 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %7, i64* %rsp
  store volatile i64 11963, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 16
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 16
  %13 = xor i64 %8, 16
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %23, i64* %rsp
  store volatile i64 11967, i64* @assembly_address
  %24 = load i64* @global_var_216020
  store i64 %24, i64* %rax
  store volatile i64 11974, i64* @assembly_address
  %25 = load i64* %rax
  %26 = trunc i64 %25 to i32
  %27 = zext i32 %26 to i64
  store i64 %27, i64* %rdx
  store volatile i64 11976, i64* @assembly_address
  %28 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %29 = zext i32 %28 to i64
  store i64 %29, i64* %rax
  store volatile i64 11982, i64* @assembly_address
  %30 = load i64* %rdx
  %31 = trunc i64 %30 to i32
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i32
  %34 = sub i32 %31, %33
  %35 = and i32 %31, 15
  %36 = and i32 %33, 15
  %37 = sub i32 %35, %36
  %38 = icmp ugt i32 %37, 15
  %39 = icmp ult i32 %31, %33
  %40 = xor i32 %31, %33
  %41 = xor i32 %31, %34
  %42 = and i32 %40, %41
  %43 = icmp slt i32 %42, 0
  store i1 %38, i1* %az
  store i1 %39, i1* %cf
  store i1 %43, i1* %of
  %44 = icmp eq i32 %34, 0
  store i1 %44, i1* %zf
  %45 = icmp slt i32 %34, 0
  store i1 %45, i1* %sf
  %46 = trunc i32 %34 to i8
  %47 = call i8 @llvm.ctpop.i8(i8 %46)
  %48 = and i8 %47, 1
  %49 = icmp eq i8 %48, 0
  store i1 %49, i1* %pf
  %50 = zext i32 %34 to i64
  store i64 %50, i64* %rdx
  store volatile i64 11984, i64* @assembly_address
  %51 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %52 = zext i32 %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 11990, i64* @assembly_address
  %53 = load i64* %rdx
  %54 = trunc i64 %53 to i32
  %55 = load i64* %rax
  %56 = trunc i64 %55 to i32
  %57 = sub i32 %54, %56
  %58 = and i32 %54, 15
  %59 = and i32 %56, 15
  %60 = sub i32 %58, %59
  %61 = icmp ugt i32 %60, 15
  %62 = icmp ult i32 %54, %56
  %63 = xor i32 %54, %56
  %64 = xor i32 %54, %57
  %65 = and i32 %63, %64
  %66 = icmp slt i32 %65, 0
  store i1 %61, i1* %az
  store i1 %62, i1* %cf
  store i1 %66, i1* %of
  %67 = icmp eq i32 %57, 0
  store i1 %67, i1* %zf
  %68 = icmp slt i32 %57, 0
  store i1 %68, i1* %sf
  %69 = trunc i32 %57 to i8
  %70 = call i8 @llvm.ctpop.i8(i8 %69)
  %71 = and i8 %70, 1
  %72 = icmp eq i8 %71, 0
  store i1 %72, i1* %pf
  %73 = zext i32 %57 to i64
  store i64 %73, i64* %rdx
  store volatile i64 11992, i64* @assembly_address
  %74 = load i64* %rdx
  %75 = trunc i64 %74 to i32
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 11994, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i32
  %79 = inttoptr i32 %78 to i64*
  store i64* %79, i64** %stack_var_-28
  store volatile i64 11997, i64* @assembly_address
  %80 = load i64** %stack_var_-28
  %81 = ptrtoint i64* %80 to i32
  %82 = sub i32 %81, -1
  %83 = and i32 %81, 15
  %84 = sub i32 %83, 15
  %85 = icmp ugt i32 %84, 15
  %86 = icmp ult i32 %81, -1
  %87 = xor i32 %81, -1
  %88 = xor i32 %81, %82
  %89 = and i32 %87, %88
  %90 = icmp slt i32 %89, 0
  store i1 %85, i1* %az
  store i1 %86, i1* %cf
  store i1 %90, i1* %of
  %91 = icmp eq i32 %82, 0
  store i1 %91, i1* %zf
  %92 = icmp slt i32 %82, 0
  store i1 %92, i1* %sf
  %93 = trunc i32 %82 to i8
  %94 = call i8 @llvm.ctpop.i8(i8 %93)
  %95 = and i8 %94, 1
  %96 = icmp eq i8 %95, 0
  store i1 %96, i1* %pf
  store volatile i64 12001, i64* @assembly_address
  %97 = load i1* %zf
  %98 = icmp eq i1 %97, false
  br i1 %98, label %block_2eec, label %block_2ee3

block_2ee3:                                       ; preds = %block_2eb4
  store volatile i64 12003, i64* @assembly_address
  %99 = load i64** %stack_var_-28
  %100 = ptrtoint i64* %99 to i32
  %101 = sub i32 %100, 1
  %102 = and i32 %100, 15
  %103 = sub i32 %102, 1
  %104 = icmp ugt i32 %103, 15
  %105 = icmp ult i32 %100, 1
  %106 = xor i32 %100, 1
  %107 = xor i32 %100, %101
  %108 = and i32 %106, %107
  %109 = icmp slt i32 %108, 0
  store i1 %104, i1* %az
  store i1 %105, i1* %cf
  store i1 %109, i1* %of
  %110 = icmp eq i32 %101, 0
  store i1 %110, i1* %zf
  %111 = icmp slt i32 %101, 0
  store i1 %111, i1* %sf
  %112 = trunc i32 %101 to i8
  %113 = call i8 @llvm.ctpop.i8(i8 %112)
  %114 = and i8 %113, 1
  %115 = icmp eq i8 %114, 0
  store i1 %115, i1* %pf
  %116 = inttoptr i32 %101 to i64*
  store i64* %116, i64** %stack_var_-28
  store volatile i64 12007, i64* @assembly_address
  br label %block_302e

block_2eec:                                       ; preds = %block_2eb4
  store volatile i64 12012, i64* @assembly_address
  %117 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %118 = zext i32 %117 to i64
  store i64 %118, i64* %rax
  store volatile i64 12018, i64* @assembly_address
  %119 = load i64* %rax
  %120 = trunc i64 %119 to i32
  %121 = sub i32 %120, 65273
  %122 = and i32 %120, 15
  %123 = sub i32 %122, 9
  %124 = icmp ugt i32 %123, 15
  %125 = icmp ult i32 %120, 65273
  %126 = xor i32 %120, 65273
  %127 = xor i32 %120, %121
  %128 = and i32 %126, %127
  %129 = icmp slt i32 %128, 0
  store i1 %124, i1* %az
  store i1 %125, i1* %cf
  store i1 %129, i1* %of
  %130 = icmp eq i32 %121, 0
  store i1 %130, i1* %zf
  %131 = icmp slt i32 %121, 0
  store i1 %131, i1* %sf
  %132 = trunc i32 %121 to i8
  %133 = call i8 @llvm.ctpop.i8(i8 %132)
  %134 = and i8 %133, 1
  %135 = icmp eq i8 %134, 0
  store i1 %135, i1* %pf
  store volatile i64 12023, i64* @assembly_address
  %136 = load i1* %cf
  %137 = load i1* %zf
  %138 = or i1 %136, %137
  br i1 %138, label %block_302e, label %block_2efd

block_2efd:                                       ; preds = %block_2eec
  store volatile i64 12029, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2574c0 to i64), i64* %rax
  store volatile i64 12036, i64* @assembly_address
  store i64 32768, i64* %rdx
  store volatile i64 12041, i64* @assembly_address
  %139 = load i64* %rax
  store i64 %139, i64* %rsi
  store volatile i64 12044, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdi
  store volatile i64 12051, i64* @assembly_address
  %140 = load i64* %rdi
  %141 = inttoptr i64 %140 to i64*
  %142 = load i64* %rsi
  %143 = inttoptr i64 %142 to i64*
  %144 = load i64* %rdx
  %145 = trunc i64 %144 to i32
  %146 = call i64* @memcpy(i64* %141, i64* %143, i32 %145)
  %147 = ptrtoint i64* %146 to i64
  store i64 %147, i64* %rax
  %148 = ptrtoint i64* %146 to i64
  store i64 %148, i64* %rax
  store volatile i64 12056, i64* @assembly_address
  %149 = load i32* bitcast (i64* @global_var_21a444 to i32*)
  %150 = zext i32 %149 to i64
  store i64 %150, i64* %rax
  store volatile i64 12062, i64* @assembly_address
  %151 = load i64* %rax
  %152 = trunc i64 %151 to i32
  %153 = sub i32 %152, 32768
  %154 = and i32 %152, 15
  %155 = icmp ugt i32 %154, 15
  %156 = icmp ult i32 %152, 32768
  %157 = xor i32 %152, 32768
  %158 = xor i32 %152, %153
  %159 = and i32 %157, %158
  %160 = icmp slt i32 %159, 0
  store i1 %155, i1* %az
  store i1 %156, i1* %cf
  store i1 %160, i1* %of
  %161 = icmp eq i32 %153, 0
  store i1 %161, i1* %zf
  %162 = icmp slt i32 %153, 0
  store i1 %162, i1* %sf
  %163 = trunc i32 %153 to i8
  %164 = call i8 @llvm.ctpop.i8(i8 %163)
  %165 = and i8 %164, 1
  %166 = icmp eq i8 %165, 0
  store i1 %166, i1* %pf
  %167 = zext i32 %153 to i64
  store i64 %167, i64* %rax
  store volatile i64 12067, i64* @assembly_address
  %168 = load i64* %rax
  %169 = trunc i64 %168 to i32
  store i32 %169, i32* bitcast (i64* @global_var_21a444 to i32*)
  store volatile i64 12073, i64* @assembly_address
  %170 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %171 = zext i32 %170 to i64
  store i64 %171, i64* %rax
  store volatile i64 12079, i64* @assembly_address
  %172 = load i64* %rax
  %173 = trunc i64 %172 to i32
  %174 = sub i32 %173, 32768
  %175 = and i32 %173, 15
  %176 = icmp ugt i32 %175, 15
  %177 = icmp ult i32 %173, 32768
  %178 = xor i32 %173, 32768
  %179 = xor i32 %173, %174
  %180 = and i32 %178, %179
  %181 = icmp slt i32 %180, 0
  store i1 %176, i1* %az
  store i1 %177, i1* %cf
  store i1 %181, i1* %of
  %182 = icmp eq i32 %174, 0
  store i1 %182, i1* %zf
  %183 = icmp slt i32 %174, 0
  store i1 %183, i1* %sf
  %184 = trunc i32 %174 to i8
  %185 = call i8 @llvm.ctpop.i8(i8 %184)
  %186 = and i8 %185, 1
  %187 = icmp eq i8 %186, 0
  store i1 %187, i1* %pf
  %188 = zext i32 %174 to i64
  store i64 %188, i64* %rax
  store volatile i64 12084, i64* @assembly_address
  %189 = load i64* %rax
  %190 = trunc i64 %189 to i32
  store i32 %190, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 12090, i64* @assembly_address
  %191 = load i64* @global_var_2165b8
  store i64 %191, i64* %rdx
  store volatile i64 12097, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 12102, i64* @assembly_address
  %192 = load i64* %rdx
  %193 = load i64* %rax
  %194 = sub i64 %192, %193
  %195 = and i64 %192, 15
  %196 = and i64 %193, 15
  %197 = sub i64 %195, %196
  %198 = icmp ugt i64 %197, 15
  %199 = icmp ult i64 %192, %193
  %200 = xor i64 %192, %193
  %201 = xor i64 %192, %194
  %202 = and i64 %200, %201
  %203 = icmp slt i64 %202, 0
  store i1 %198, i1* %az
  store i1 %199, i1* %cf
  store i1 %203, i1* %of
  %204 = icmp eq i64 %194, 0
  store i1 %204, i1* %zf
  %205 = icmp slt i64 %194, 0
  store i1 %205, i1* %sf
  %206 = trunc i64 %194 to i8
  %207 = call i8 @llvm.ctpop.i8(i8 %206)
  %208 = and i8 %207, 1
  %209 = icmp eq i8 %208, 0
  store i1 %209, i1* %pf
  store volatile i64 12105, i64* @assembly_address
  %210 = load i1* %zf
  br i1 %210, label %block_2f5f, label %block_2f4b

block_2f4b:                                       ; preds = %block_2efd
  store volatile i64 12107, i64* @assembly_address
  %211 = load i64* @global_var_2165b8
  store i64 %211, i64* %rax
  store volatile i64 12114, i64* @assembly_address
  %212 = load i64* %rax
  %213 = sub i64 %212, 32768
  %214 = and i64 %212, 15
  %215 = icmp ugt i64 %214, 15
  %216 = icmp ult i64 %212, 32768
  %217 = xor i64 %212, 32768
  %218 = xor i64 %212, %213
  %219 = and i64 %217, %218
  %220 = icmp slt i64 %219, 0
  store i1 %215, i1* %az
  store i1 %216, i1* %cf
  store i1 %220, i1* %of
  %221 = icmp eq i64 %213, 0
  store i1 %221, i1* %zf
  %222 = icmp slt i64 %213, 0
  store i1 %222, i1* %sf
  %223 = trunc i64 %213 to i8
  %224 = call i8 @llvm.ctpop.i8(i8 %223)
  %225 = and i8 %224, 1
  %226 = icmp eq i8 %225, 0
  store i1 %226, i1* %pf
  store i64 %213, i64* %rax
  store volatile i64 12120, i64* @assembly_address
  %227 = load i64* %rax
  store i64 %227, i64* @global_var_2165b8
  br label %block_2f5f

block_2f5f:                                       ; preds = %block_2f4b, %block_2efd
  store volatile i64 12127, i64* @assembly_address
  %228 = load i64* @global_var_21a430
  store i64 %228, i64* %rax
  store volatile i64 12134, i64* @assembly_address
  %229 = load i64* %rax
  %230 = sub i64 %229, 32768
  %231 = and i64 %229, 15
  %232 = icmp ugt i64 %231, 15
  %233 = icmp ult i64 %229, 32768
  %234 = xor i64 %229, 32768
  %235 = xor i64 %229, %230
  %236 = and i64 %234, %235
  %237 = icmp slt i64 %236, 0
  store i1 %232, i1* %az
  store i1 %233, i1* %cf
  store i1 %237, i1* %of
  %238 = icmp eq i64 %230, 0
  store i1 %238, i1* %zf
  %239 = icmp slt i64 %230, 0
  store i1 %239, i1* %sf
  %240 = trunc i64 %230 to i8
  %241 = call i8 @llvm.ctpop.i8(i8 %240)
  %242 = and i8 %241, 1
  %243 = icmp eq i8 %242, 0
  store i1 %243, i1* %pf
  store i64 %230, i64* %rax
  store volatile i64 12140, i64* @assembly_address
  %244 = load i64* %rax
  store i64 %244, i64* @global_var_21a430
  store volatile i64 12147, i64* @assembly_address
  store i64 0, i64* %rbx
  store volatile i64 12152, i64* @assembly_address
  br label %block_2fcd

block_2f7a:                                       ; preds = %block_2fcd
  store volatile i64 12154, i64* @assembly_address
  %245 = load i64* %rbx
  %246 = trunc i64 %245 to i32
  %247 = zext i32 %246 to i64
  store i64 %247, i64* %rax
  store volatile i64 12156, i64* @assembly_address
  %248 = load i64* %rax
  %249 = add i64 %248, 32768
  %250 = and i64 %248, 15
  %251 = icmp ugt i64 %250, 15
  %252 = icmp ult i64 %249, %248
  %253 = xor i64 %248, %249
  %254 = xor i64 32768, %249
  %255 = and i64 %253, %254
  %256 = icmp slt i64 %255, 0
  store i1 %251, i1* %az
  store i1 %252, i1* %cf
  store i1 %256, i1* %of
  %257 = icmp eq i64 %249, 0
  store i1 %257, i1* %zf
  %258 = icmp slt i64 %249, 0
  store i1 %258, i1* %sf
  %259 = trunc i64 %249 to i8
  %260 = call i8 @llvm.ctpop.i8(i8 %259)
  %261 = and i8 %260, 1
  %262 = icmp eq i8 %261, 0
  store i1 %262, i1* %pf
  store i64 %249, i64* %rax
  store volatile i64 12162, i64* @assembly_address
  %263 = load i64* %rax
  %264 = load i64* %rax
  %265 = mul i64 %264, 1
  %266 = add i64 %263, %265
  store i64 %266, i64* %rdx
  store volatile i64 12166, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 12173, i64* @assembly_address
  %267 = load i64* %rax
  %268 = load i64* %rdx
  %269 = add i64 %267, %268
  %270 = and i64 %267, 15
  %271 = and i64 %268, 15
  %272 = add i64 %270, %271
  %273 = icmp ugt i64 %272, 15
  %274 = icmp ult i64 %269, %267
  %275 = xor i64 %267, %269
  %276 = xor i64 %268, %269
  %277 = and i64 %275, %276
  %278 = icmp slt i64 %277, 0
  store i1 %273, i1* %az
  store i1 %274, i1* %cf
  store i1 %278, i1* %of
  %279 = icmp eq i64 %269, 0
  store i1 %279, i1* %zf
  %280 = icmp slt i64 %269, 0
  store i1 %280, i1* %sf
  %281 = trunc i64 %269 to i8
  %282 = call i8 @llvm.ctpop.i8(i8 %281)
  %283 = and i8 %282, 1
  %284 = icmp eq i8 %283, 0
  store i1 %284, i1* %pf
  store i64 %269, i64* %rax
  store volatile i64 12176, i64* @assembly_address
  %285 = load i64* %rax
  %286 = inttoptr i64 %285 to i16*
  %287 = load i16* %286
  %288 = zext i16 %287 to i64
  store i64 %288, i64* %rax
  store volatile i64 12179, i64* @assembly_address
  %289 = load i64* %rax
  %290 = trunc i64 %289 to i16
  %291 = zext i16 %290 to i64
  store i64 %291, i64* %r12
  store volatile i64 12183, i64* @assembly_address
  %292 = load i64* %r12
  %293 = trunc i64 %292 to i32
  %294 = sub i32 %293, 32767
  %295 = and i32 %293, 15
  %296 = sub i32 %295, 15
  %297 = icmp ugt i32 %296, 15
  %298 = icmp ult i32 %293, 32767
  %299 = xor i32 %293, 32767
  %300 = xor i32 %293, %294
  %301 = and i32 %299, %300
  %302 = icmp slt i32 %301, 0
  store i1 %297, i1* %az
  store i1 %298, i1* %cf
  store i1 %302, i1* %of
  %303 = icmp eq i32 %294, 0
  store i1 %303, i1* %zf
  %304 = icmp slt i32 %294, 0
  store i1 %304, i1* %sf
  %305 = trunc i32 %294 to i8
  %306 = call i8 @llvm.ctpop.i8(i8 %305)
  %307 = and i8 %306, 1
  %308 = icmp eq i8 %307, 0
  store i1 %308, i1* %pf
  store volatile i64 12190, i64* @assembly_address
  %309 = load i1* %cf
  %310 = load i1* %zf
  %311 = or i1 %309, %310
  br i1 %311, label %block_2fac, label %block_2fa0

block_2fa0:                                       ; preds = %block_2f7a
  store volatile i64 12192, i64* @assembly_address
  %312 = load i64* %r12
  %313 = add i64 %312, -32768
  %314 = trunc i64 %313 to i32
  %315 = zext i32 %314 to i64
  store i64 %315, i64* %rax
  store volatile i64 12200, i64* @assembly_address
  %316 = load i64* %rax
  %317 = trunc i64 %316 to i32
  %318 = zext i32 %317 to i64
  store i64 %318, i64* %rdx
  store volatile i64 12202, i64* @assembly_address
  br label %block_2fb1

block_2fac:                                       ; preds = %block_2f7a
  store volatile i64 12204, i64* @assembly_address
  store i64 0, i64* %rdx
  br label %block_2fb1

block_2fb1:                                       ; preds = %block_2fac, %block_2fa0
  store volatile i64 12209, i64* @assembly_address
  %319 = load i64* %rbx
  %320 = trunc i64 %319 to i32
  %321 = zext i32 %320 to i64
  store i64 %321, i64* %rax
  store volatile i64 12211, i64* @assembly_address
  %322 = load i64* %rax
  %323 = add i64 %322, 32768
  %324 = and i64 %322, 15
  %325 = icmp ugt i64 %324, 15
  %326 = icmp ult i64 %323, %322
  %327 = xor i64 %322, %323
  %328 = xor i64 32768, %323
  %329 = and i64 %327, %328
  %330 = icmp slt i64 %329, 0
  store i1 %325, i1* %az
  store i1 %326, i1* %cf
  store i1 %330, i1* %of
  %331 = icmp eq i64 %323, 0
  store i1 %331, i1* %zf
  %332 = icmp slt i64 %323, 0
  store i1 %332, i1* %sf
  %333 = trunc i64 %323 to i8
  %334 = call i8 @llvm.ctpop.i8(i8 %333)
  %335 = and i8 %334, 1
  %336 = icmp eq i8 %335, 0
  store i1 %336, i1* %pf
  store i64 %323, i64* %rax
  store volatile i64 12217, i64* @assembly_address
  %337 = load i64* %rax
  %338 = load i64* %rax
  %339 = mul i64 %338, 1
  %340 = add i64 %337, %339
  store i64 %340, i64* %rcx
  store volatile i64 12221, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 12228, i64* @assembly_address
  %341 = load i64* %rax
  %342 = load i64* %rcx
  %343 = add i64 %341, %342
  %344 = and i64 %341, 15
  %345 = and i64 %342, 15
  %346 = add i64 %344, %345
  %347 = icmp ugt i64 %346, 15
  %348 = icmp ult i64 %343, %341
  %349 = xor i64 %341, %343
  %350 = xor i64 %342, %343
  %351 = and i64 %349, %350
  %352 = icmp slt i64 %351, 0
  store i1 %347, i1* %az
  store i1 %348, i1* %cf
  store i1 %352, i1* %of
  %353 = icmp eq i64 %343, 0
  store i1 %353, i1* %zf
  %354 = icmp slt i64 %343, 0
  store i1 %354, i1* %sf
  %355 = trunc i64 %343 to i8
  %356 = call i8 @llvm.ctpop.i8(i8 %355)
  %357 = and i8 %356, 1
  %358 = icmp eq i8 %357, 0
  store i1 %358, i1* %pf
  store i64 %343, i64* %rax
  store volatile i64 12231, i64* @assembly_address
  %359 = load i64* %rdx
  %360 = trunc i64 %359 to i16
  %361 = load i64* %rax
  %362 = inttoptr i64 %361 to i16*
  store i16 %360, i16* %362
  store volatile i64 12234, i64* @assembly_address
  %363 = load i64* %rbx
  %364 = trunc i64 %363 to i32
  %365 = add i32 %364, 1
  %366 = and i32 %364, 15
  %367 = add i32 %366, 1
  %368 = icmp ugt i32 %367, 15
  %369 = icmp ult i32 %365, %364
  %370 = xor i32 %364, %365
  %371 = xor i32 1, %365
  %372 = and i32 %370, %371
  %373 = icmp slt i32 %372, 0
  store i1 %368, i1* %az
  store i1 %369, i1* %cf
  store i1 %373, i1* %of
  %374 = icmp eq i32 %365, 0
  store i1 %374, i1* %zf
  %375 = icmp slt i32 %365, 0
  store i1 %375, i1* %sf
  %376 = trunc i32 %365 to i8
  %377 = call i8 @llvm.ctpop.i8(i8 %376)
  %378 = and i8 %377, 1
  %379 = icmp eq i8 %378, 0
  store i1 %379, i1* %pf
  %380 = zext i32 %365 to i64
  store i64 %380, i64* %rbx
  br label %block_2fcd

block_2fcd:                                       ; preds = %block_2fb1, %block_2f5f
  store volatile i64 12237, i64* @assembly_address
  %381 = load i64* %rbx
  %382 = trunc i64 %381 to i32
  %383 = sub i32 %382, 32767
  %384 = and i32 %382, 15
  %385 = sub i32 %384, 15
  %386 = icmp ugt i32 %385, 15
  %387 = icmp ult i32 %382, 32767
  %388 = xor i32 %382, 32767
  %389 = xor i32 %382, %383
  %390 = and i32 %388, %389
  %391 = icmp slt i32 %390, 0
  store i1 %386, i1* %az
  store i1 %387, i1* %cf
  store i1 %391, i1* %of
  %392 = icmp eq i32 %383, 0
  store i1 %392, i1* %zf
  %393 = icmp slt i32 %383, 0
  store i1 %393, i1* %sf
  %394 = trunc i32 %383 to i8
  %395 = call i8 @llvm.ctpop.i8(i8 %394)
  %396 = and i8 %395, 1
  %397 = icmp eq i8 %396, 0
  store i1 %397, i1* %pf
  store volatile i64 12243, i64* @assembly_address
  %398 = load i1* %cf
  %399 = load i1* %zf
  %400 = or i1 %398, %399
  br i1 %400, label %block_2f7a, label %block_2fd5

block_2fd5:                                       ; preds = %block_2fcd
  store volatile i64 12245, i64* @assembly_address
  store i64 0, i64* %rbx
  store volatile i64 12250, i64* @assembly_address
  br label %block_301f

block_2fdc:                                       ; preds = %block_301f
  store volatile i64 12252, i64* @assembly_address
  %401 = load i64* %rbx
  %402 = trunc i64 %401 to i32
  %403 = zext i32 %402 to i64
  store i64 %403, i64* %rax
  store volatile i64 12254, i64* @assembly_address
  %404 = load i64* %rax
  %405 = load i64* %rax
  %406 = mul i64 %405, 1
  %407 = add i64 %404, %406
  store i64 %407, i64* %rdx
  store volatile i64 12258, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 12265, i64* @assembly_address
  %408 = load i64* %rdx
  %409 = load i64* %rax
  %410 = mul i64 %409, 1
  %411 = add i64 %408, %410
  %412 = inttoptr i64 %411 to i16*
  %413 = load i16* %412
  %414 = zext i16 %413 to i64
  store i64 %414, i64* %rax
  store volatile i64 12269, i64* @assembly_address
  %415 = load i64* %rax
  %416 = trunc i64 %415 to i16
  %417 = zext i16 %416 to i64
  store i64 %417, i64* %r12
  store volatile i64 12273, i64* @assembly_address
  %418 = load i64* %r12
  %419 = trunc i64 %418 to i32
  %420 = sub i32 %419, 32767
  %421 = and i32 %419, 15
  %422 = sub i32 %421, 15
  %423 = icmp ugt i32 %422, 15
  %424 = icmp ult i32 %419, 32767
  %425 = xor i32 %419, 32767
  %426 = xor i32 %419, %420
  %427 = and i32 %425, %426
  %428 = icmp slt i32 %427, 0
  store i1 %423, i1* %az
  store i1 %424, i1* %cf
  store i1 %428, i1* %of
  %429 = icmp eq i32 %420, 0
  store i1 %429, i1* %zf
  %430 = icmp slt i32 %420, 0
  store i1 %430, i1* %sf
  %431 = trunc i32 %420 to i8
  %432 = call i8 @llvm.ctpop.i8(i8 %431)
  %433 = and i8 %432, 1
  %434 = icmp eq i8 %433, 0
  store i1 %434, i1* %pf
  store volatile i64 12280, i64* @assembly_address
  %435 = load i1* %cf
  %436 = load i1* %zf
  %437 = or i1 %435, %436
  br i1 %437, label %block_3006, label %block_2ffa

block_2ffa:                                       ; preds = %block_2fdc
  store volatile i64 12282, i64* @assembly_address
  %438 = load i64* %r12
  %439 = add i64 %438, -32768
  %440 = trunc i64 %439 to i32
  %441 = zext i32 %440 to i64
  store i64 %441, i64* %rax
  store volatile i64 12290, i64* @assembly_address
  %442 = load i64* %rax
  %443 = trunc i64 %442 to i32
  %444 = zext i32 %443 to i64
  store i64 %444, i64* %rcx
  store volatile i64 12292, i64* @assembly_address
  br label %block_300b

block_3006:                                       ; preds = %block_2fdc
  store volatile i64 12294, i64* @assembly_address
  store i64 0, i64* %rcx
  br label %block_300b

block_300b:                                       ; preds = %block_3006, %block_2ffa
  store volatile i64 12299, i64* @assembly_address
  %445 = load i64* %rbx
  %446 = trunc i64 %445 to i32
  %447 = zext i32 %446 to i64
  store i64 %447, i64* %rax
  store volatile i64 12301, i64* @assembly_address
  %448 = load i64* %rax
  %449 = load i64* %rax
  %450 = mul i64 %449, 1
  %451 = add i64 %448, %450
  store i64 %451, i64* %rdx
  store volatile i64 12305, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 12312, i64* @assembly_address
  %452 = load i64* %rcx
  %453 = trunc i64 %452 to i16
  %454 = load i64* %rdx
  %455 = load i64* %rax
  %456 = mul i64 %455, 1
  %457 = add i64 %454, %456
  %458 = inttoptr i64 %457 to i16*
  store i16 %453, i16* %458
  store volatile i64 12316, i64* @assembly_address
  %459 = load i64* %rbx
  %460 = trunc i64 %459 to i32
  %461 = add i32 %460, 1
  %462 = and i32 %460, 15
  %463 = add i32 %462, 1
  %464 = icmp ugt i32 %463, 15
  %465 = icmp ult i32 %461, %460
  %466 = xor i32 %460, %461
  %467 = xor i32 1, %461
  %468 = and i32 %466, %467
  %469 = icmp slt i32 %468, 0
  store i1 %464, i1* %az
  store i1 %465, i1* %cf
  store i1 %469, i1* %of
  %470 = icmp eq i32 %461, 0
  store i1 %470, i1* %zf
  %471 = icmp slt i32 %461, 0
  store i1 %471, i1* %sf
  %472 = trunc i32 %461 to i8
  %473 = call i8 @llvm.ctpop.i8(i8 %472)
  %474 = and i8 %473, 1
  %475 = icmp eq i8 %474, 0
  store i1 %475, i1* %pf
  %476 = zext i32 %461 to i64
  store i64 %476, i64* %rbx
  br label %block_301f

block_301f:                                       ; preds = %block_300b, %block_2fd5
  store volatile i64 12319, i64* @assembly_address
  %477 = load i64* %rbx
  %478 = trunc i64 %477 to i32
  %479 = sub i32 %478, 32767
  %480 = and i32 %478, 15
  %481 = sub i32 %480, 15
  %482 = icmp ugt i32 %481, 15
  %483 = icmp ult i32 %478, 32767
  %484 = xor i32 %478, 32767
  %485 = xor i32 %478, %479
  %486 = and i32 %484, %485
  %487 = icmp slt i32 %486, 0
  store i1 %482, i1* %az
  store i1 %483, i1* %cf
  store i1 %487, i1* %of
  %488 = icmp eq i32 %479, 0
  store i1 %488, i1* %zf
  %489 = icmp slt i32 %479, 0
  store i1 %489, i1* %sf
  %490 = trunc i32 %479 to i8
  %491 = call i8 @llvm.ctpop.i8(i8 %490)
  %492 = and i8 %491, 1
  %493 = icmp eq i8 %492, 0
  store i1 %493, i1* %pf
  store volatile i64 12325, i64* @assembly_address
  %494 = load i1* %cf
  %495 = load i1* %zf
  %496 = or i1 %494, %495
  br i1 %496, label %block_2fdc, label %block_3027

block_3027:                                       ; preds = %block_301f
  store volatile i64 12327, i64* @assembly_address
  %497 = load i64** %stack_var_-28
  %498 = ptrtoint i64* %497 to i32
  %499 = add i32 %498, 32768
  %500 = and i32 %498, 15
  %501 = icmp ugt i32 %500, 15
  %502 = icmp ult i32 %499, %498
  %503 = xor i32 %498, %499
  %504 = xor i32 32768, %499
  %505 = and i32 %503, %504
  %506 = icmp slt i32 %505, 0
  store i1 %501, i1* %az
  store i1 %502, i1* %cf
  store i1 %506, i1* %of
  %507 = icmp eq i32 %499, 0
  store i1 %507, i1* %zf
  %508 = icmp slt i32 %499, 0
  store i1 %508, i1* %sf
  %509 = trunc i32 %499 to i8
  %510 = call i8 @llvm.ctpop.i8(i8 %509)
  %511 = and i8 %510, 1
  %512 = icmp eq i8 %511, 0
  store i1 %512, i1* %pf
  %513 = inttoptr i32 %499 to i64*
  store i64* %513, i64** %stack_var_-28
  br label %block_302e

block_302e:                                       ; preds = %block_3027, %block_2eec, %block_2ee3
  store volatile i64 12334, i64* @assembly_address
  %514 = load i32* bitcast (i64* @global_var_21659c to i32*)
  %515 = zext i32 %514 to i64
  store i64 %515, i64* %rax
  store volatile i64 12340, i64* @assembly_address
  %516 = load i64* %rax
  %517 = trunc i64 %516 to i32
  %518 = load i64* %rax
  %519 = trunc i64 %518 to i32
  %520 = and i32 %517, %519
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %521 = icmp eq i32 %520, 0
  store i1 %521, i1* %zf
  %522 = icmp slt i32 %520, 0
  store i1 %522, i1* %sf
  %523 = trunc i32 %520 to i8
  %524 = call i8 @llvm.ctpop.i8(i8 %523)
  %525 = and i8 %524, 1
  %526 = icmp eq i8 %525, 0
  store i1 %526, i1* %pf
  store volatile i64 12342, i64* @assembly_address
  %527 = load i1* %zf
  %528 = icmp eq i1 %527, false
  br i1 %528, label %block_30be, label %block_303c

block_303c:                                       ; preds = %block_302e
  store volatile i64 12348, i64* @assembly_address
  %529 = load i64* @global_var_21a420
  store i64 %529, i64* %rax
  store volatile i64 12355, i64* @assembly_address
  %530 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %531 = zext i32 %530 to i64
  store i64 %531, i64* %rdx
  store volatile i64 12361, i64* @assembly_address
  %532 = load i64* %rdx
  %533 = trunc i64 %532 to i32
  %534 = zext i32 %533 to i64
  store i64 %534, i64* %rcx
  store volatile i64 12363, i64* @assembly_address
  %535 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %536 = zext i32 %535 to i64
  store i64 %536, i64* %rdx
  store volatile i64 12369, i64* @assembly_address
  %537 = load i64* %rdx
  %538 = trunc i64 %537 to i32
  %539 = zext i32 %538 to i64
  store i64 %539, i64* %rdx
  store volatile i64 12371, i64* @assembly_address
  %540 = load i64* %rcx
  %541 = load i64* %rdx
  %542 = add i64 %540, %541
  %543 = and i64 %540, 15
  %544 = and i64 %541, 15
  %545 = add i64 %543, %544
  %546 = icmp ugt i64 %545, 15
  %547 = icmp ult i64 %542, %540
  %548 = xor i64 %540, %542
  %549 = xor i64 %541, %542
  %550 = and i64 %548, %549
  %551 = icmp slt i64 %550, 0
  store i1 %546, i1* %az
  store i1 %547, i1* %cf
  store i1 %551, i1* %of
  %552 = icmp eq i64 %542, 0
  store i1 %552, i1* %zf
  %553 = icmp slt i64 %542, 0
  store i1 %553, i1* %sf
  %554 = trunc i64 %542 to i8
  %555 = call i8 @llvm.ctpop.i8(i8 %554)
  %556 = and i8 %555, 1
  %557 = icmp eq i8 %556, 0
  store i1 %557, i1* %pf
  store i64 %542, i64* %rcx
  store volatile i64 12374, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdx
  store volatile i64 12381, i64* @assembly_address
  %558 = load i64* %rcx
  %559 = load i64* %rdx
  %560 = add i64 %558, %559
  %561 = and i64 %558, 15
  %562 = and i64 %559, 15
  %563 = add i64 %561, %562
  %564 = icmp ugt i64 %563, 15
  %565 = icmp ult i64 %560, %558
  %566 = xor i64 %558, %560
  %567 = xor i64 %559, %560
  %568 = and i64 %566, %567
  %569 = icmp slt i64 %568, 0
  store i1 %564, i1* %az
  store i1 %565, i1* %cf
  store i1 %569, i1* %of
  %570 = icmp eq i64 %560, 0
  store i1 %570, i1* %zf
  %571 = icmp slt i64 %560, 0
  store i1 %571, i1* %sf
  %572 = trunc i64 %560 to i8
  %573 = call i8 @llvm.ctpop.i8(i8 %572)
  %574 = and i8 %573, 1
  %575 = icmp eq i8 %574, 0
  store i1 %575, i1* %pf
  store i64 %560, i64* %rcx
  store volatile i64 12384, i64* @assembly_address
  %576 = load i64** %stack_var_-28
  %577 = ptrtoint i64* %576 to i32
  %578 = zext i32 %577 to i64
  store i64 %578, i64* %rdx
  store volatile i64 12387, i64* @assembly_address
  %579 = load i64* %rdx
  %580 = trunc i64 %579 to i32
  %581 = zext i32 %580 to i64
  store i64 %581, i64* %rsi
  store volatile i64 12389, i64* @assembly_address
  %582 = load i64* %rcx
  store i64 %582, i64* %rdi
  store volatile i64 12392, i64* @assembly_address
  %583 = load i64* %rax
  %584 = call i64 @__pseudo_call(i64 %583)
  store i64 %584, i64* %rax
  store volatile i64 12394, i64* @assembly_address
  %585 = load i64* %rax
  %586 = trunc i64 %585 to i32
  %587 = zext i32 %586 to i64
  store i64 %587, i64* %rbx
  store volatile i64 12396, i64* @assembly_address
  %588 = load i64* %rbx
  %589 = trunc i64 %588 to i32
  %590 = load i64* %rbx
  %591 = trunc i64 %590 to i32
  %592 = and i32 %589, %591
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %593 = icmp eq i32 %592, 0
  store i1 %593, i1* %zf
  %594 = icmp slt i32 %592, 0
  store i1 %594, i1* %sf
  %595 = trunc i32 %592 to i8
  %596 = call i8 @llvm.ctpop.i8(i8 %595)
  %597 = and i8 %596, 1
  %598 = icmp eq i8 %597, 0
  store i1 %598, i1* %pf
  store volatile i64 12398, i64* @assembly_address
  %599 = load i1* %zf
  br i1 %599, label %block_3075, label %block_3070

block_3070:                                       ; preds = %block_303c
  store volatile i64 12400, i64* @assembly_address
  %600 = load i64* %rbx
  %601 = trunc i64 %600 to i32
  %602 = sub i32 %601, -1
  %603 = and i32 %601, 15
  %604 = sub i32 %603, 15
  %605 = icmp ugt i32 %604, 15
  %606 = icmp ult i32 %601, -1
  %607 = xor i32 %601, -1
  %608 = xor i32 %601, %602
  %609 = and i32 %607, %608
  %610 = icmp slt i32 %609, 0
  store i1 %605, i1* %az
  store i1 %606, i1* %cf
  store i1 %610, i1* %of
  %611 = icmp eq i32 %602, 0
  store i1 %611, i1* %zf
  %612 = icmp slt i32 %602, 0
  store i1 %612, i1* %sf
  %613 = trunc i32 %602 to i8
  %614 = call i8 @llvm.ctpop.i8(i8 %613)
  %615 = and i8 %614, 1
  %616 = icmp eq i8 %615, 0
  store i1 %616, i1* %pf
  store volatile i64 12403, i64* @assembly_address
  %617 = load i1* %zf
  %618 = icmp eq i1 %617, false
  br i1 %618, label %block_30b0, label %block_3075

block_3075:                                       ; preds = %block_3070, %block_303c
  store volatile i64 12405, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21659c to i32*)
  store volatile i64 12415, i64* @assembly_address
  %619 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %620 = zext i32 %619 to i64
  store i64 %620, i64* %rax
  store volatile i64 12421, i64* @assembly_address
  %621 = load i64* %rax
  %622 = trunc i64 %621 to i32
  %623 = zext i32 %622 to i64
  store i64 %623, i64* %rdx
  store volatile i64 12423, i64* @assembly_address
  %624 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %625 = zext i32 %624 to i64
  store i64 %625, i64* %rax
  store volatile i64 12429, i64* @assembly_address
  %626 = load i64* %rax
  %627 = trunc i64 %626 to i32
  %628 = zext i32 %627 to i64
  store i64 %628, i64* %rax
  store volatile i64 12431, i64* @assembly_address
  %629 = load i64* %rdx
  %630 = load i64* %rax
  %631 = add i64 %629, %630
  %632 = and i64 %629, 15
  %633 = and i64 %630, 15
  %634 = add i64 %632, %633
  %635 = icmp ugt i64 %634, 15
  %636 = icmp ult i64 %631, %629
  %637 = xor i64 %629, %631
  %638 = xor i64 %630, %631
  %639 = and i64 %637, %638
  %640 = icmp slt i64 %639, 0
  store i1 %635, i1* %az
  store i1 %636, i1* %cf
  store i1 %640, i1* %of
  %641 = icmp eq i64 %631, 0
  store i1 %641, i1* %zf
  %642 = icmp slt i64 %631, 0
  store i1 %642, i1* %sf
  %643 = trunc i64 %631 to i8
  %644 = call i8 @llvm.ctpop.i8(i8 %643)
  %645 = and i8 %644, 1
  %646 = icmp eq i8 %645, 0
  store i1 %646, i1* %pf
  store i64 %631, i64* %rdx
  store volatile i64 12434, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 12441, i64* @assembly_address
  %647 = load i64* %rax
  %648 = load i64* %rdx
  %649 = add i64 %647, %648
  %650 = and i64 %647, 15
  %651 = and i64 %648, 15
  %652 = add i64 %650, %651
  %653 = icmp ugt i64 %652, 15
  %654 = icmp ult i64 %649, %647
  %655 = xor i64 %647, %649
  %656 = xor i64 %648, %649
  %657 = and i64 %655, %656
  %658 = icmp slt i64 %657, 0
  store i1 %653, i1* %az
  store i1 %654, i1* %cf
  store i1 %658, i1* %of
  %659 = icmp eq i64 %649, 0
  store i1 %659, i1* %zf
  %660 = icmp slt i64 %649, 0
  store i1 %660, i1* %sf
  %661 = trunc i64 %649 to i8
  %662 = call i8 @llvm.ctpop.i8(i8 %661)
  %663 = and i8 %662, 1
  %664 = icmp eq i8 %663, 0
  store i1 %664, i1* %pf
  store i64 %649, i64* %rax
  store volatile i64 12444, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 12449, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 12454, i64* @assembly_address
  %665 = load i64* %rax
  store i64 %665, i64* %rdi
  store volatile i64 12457, i64* @assembly_address
  %666 = load i64* %rdi
  %667 = inttoptr i64 %666 to i64*
  %668 = load i64* %rsi
  %669 = trunc i64 %668 to i32
  %670 = load i64* %rdx
  %671 = trunc i64 %670 to i32
  %672 = call i64* @memset(i64* %667, i32 %669, i32 %671)
  %673 = ptrtoint i64* %672 to i64
  store i64 %673, i64* %rax
  %674 = ptrtoint i64* %672 to i64
  store i64 %674, i64* %rax
  store volatile i64 12462, i64* @assembly_address
  br label %block_30be

block_30b0:                                       ; preds = %block_3070
  store volatile i64 12464, i64* @assembly_address
  %675 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %676 = zext i32 %675 to i64
  store i64 %676, i64* %rax
  store volatile i64 12470, i64* @assembly_address
  %677 = load i64* %rax
  %678 = trunc i64 %677 to i32
  %679 = load i64* %rbx
  %680 = trunc i64 %679 to i32
  %681 = add i32 %678, %680
  %682 = and i32 %678, 15
  %683 = and i32 %680, 15
  %684 = add i32 %682, %683
  %685 = icmp ugt i32 %684, 15
  %686 = icmp ult i32 %681, %678
  %687 = xor i32 %678, %681
  %688 = xor i32 %680, %681
  %689 = and i32 %687, %688
  %690 = icmp slt i32 %689, 0
  store i1 %685, i1* %az
  store i1 %686, i1* %cf
  store i1 %690, i1* %of
  %691 = icmp eq i32 %681, 0
  store i1 %691, i1* %zf
  %692 = icmp slt i32 %681, 0
  store i1 %692, i1* %sf
  %693 = trunc i32 %681 to i8
  %694 = call i8 @llvm.ctpop.i8(i8 %693)
  %695 = and i8 %694, 1
  %696 = icmp eq i8 %695, 0
  store i1 %696, i1* %pf
  %697 = zext i32 %681 to i64
  store i64 %697, i64* %rax
  store volatile i64 12472, i64* @assembly_address
  %698 = load i64* %rax
  %699 = trunc i64 %698 to i32
  store i32 %699, i32* bitcast (i64* @global_var_2165a0 to i32*)
  br label %block_30be

block_30be:                                       ; preds = %block_30b0, %block_3075, %block_302e
  store volatile i64 12478, i64* @assembly_address
  store volatile i64 12479, i64* @assembly_address
  %700 = load i64* %rsp
  %701 = add i64 %700, 16
  %702 = and i64 %700, 15
  %703 = icmp ugt i64 %702, 15
  %704 = icmp ult i64 %701, %700
  %705 = xor i64 %700, %701
  %706 = xor i64 16, %701
  %707 = and i64 %705, %706
  %708 = icmp slt i64 %707, 0
  store i1 %703, i1* %az
  store i1 %704, i1* %cf
  store i1 %708, i1* %of
  %709 = icmp eq i64 %701, 0
  store i1 %709, i1* %zf
  %710 = icmp slt i64 %701, 0
  store i1 %710, i1* %sf
  %711 = trunc i64 %701 to i8
  %712 = call i8 @llvm.ctpop.i8(i8 %711)
  %713 = and i8 %712, 1
  %714 = icmp eq i8 %713, 0
  store i1 %714, i1* %pf
  %715 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %715, i64* %rsp
  store volatile i64 12483, i64* @assembly_address
  %716 = load i64* %stack_var_-24
  store i64 %716, i64* %rbx
  %717 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %717, i64* %rsp
  store volatile i64 12484, i64* @assembly_address
  %718 = load i64* %stack_var_-16
  store i64 %718, i64* %r12
  %719 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %719, i64* %rsp
  store volatile i64 12486, i64* @assembly_address
  %720 = load i64* %stack_var_-8
  store i64 %720, i64* %rbp
  %721 = ptrtoint i64* %stack_var_0 to i64
  store i64 %721, i64* %rsp
  store volatile i64 12487, i64* @assembly_address
  %722 = load i64* %rax
  %723 = load i64* %rax
  ret i64 %723
}

define i64 @rsync_roll(i32 %arg1, i64 %arg2) {
block_30c8:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i8*
  %1 = alloca i32
  %stack_var_-32 = alloca i8*
  %2 = alloca i32
  %stack_var_-28 = alloca i8*
  %3 = alloca i32
  %stack_var_-8 = alloca i64
  store volatile i64 12488, i64* @assembly_address
  %4 = load i64* %rbp
  store i64 %4, i64* %stack_var_-8
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rsp
  store volatile i64 12489, i64* @assembly_address
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rbp
  store volatile i64 12492, i64* @assembly_address
  %7 = load i64* %rdi
  %8 = trunc i64 %7 to i32
  %9 = inttoptr i32 %8 to i8*
  store i8* %9, i8** %stack_var_-28
  store volatile i64 12495, i64* @assembly_address
  %10 = load i64* %rsi
  %11 = trunc i64 %10 to i32
  %12 = inttoptr i32 %11 to i8*
  store i8* %12, i8** %stack_var_-32
  store volatile i64 12498, i64* @assembly_address
  %13 = load i8** %stack_var_-28
  %14 = ptrtoint i8* %13 to i32
  %15 = sub i32 %14, 4095
  %16 = and i32 %14, 15
  %17 = sub i32 %16, 15
  %18 = icmp ugt i32 %17, 15
  %19 = icmp ult i32 %14, 4095
  %20 = xor i32 %14, 4095
  %21 = xor i32 %14, %15
  %22 = and i32 %20, %21
  %23 = icmp slt i32 %22, 0
  store i1 %18, i1* %az
  store i1 %19, i1* %cf
  store i1 %23, i1* %of
  %24 = icmp eq i32 %15, 0
  store i1 %24, i1* %zf
  %25 = icmp slt i32 %15, 0
  store i1 %25, i1* %sf
  %26 = trunc i32 %15 to i8
  %27 = call i8 @llvm.ctpop.i8(i8 %26)
  %28 = and i8 %27, 1
  %29 = icmp eq i8 %28, 0
  store i1 %29, i1* %pf
  store volatile i64 12505, i64* @assembly_address
  %30 = load i1* %cf
  %31 = load i1* %zf
  %32 = or i1 %30, %31
  %33 = icmp ne i1 %32, true
  br i1 %33, label %block_313a, label %block_30db

block_30db:                                       ; preds = %block_30c8
  store volatile i64 12507, i64* @assembly_address
  %34 = load i8** %stack_var_-28
  %35 = ptrtoint i8* %34 to i32
  %36 = zext i32 %35 to i64
  store i64 %36, i64* %rax
  store volatile i64 12510, i64* @assembly_address
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i32
  %39 = inttoptr i32 %38 to i8*
  store i8* %39, i8** %stack_var_-12
  store volatile i64 12513, i64* @assembly_address
  br label %block_311a

block_30e3:                                       ; preds = %block_311a
  store volatile i64 12515, i64* @assembly_address
  %40 = load i8** %stack_var_-28
  %41 = ptrtoint i8* %40 to i32
  %42 = zext i32 %41 to i64
  store i64 %42, i64* %rdx
  store volatile i64 12518, i64* @assembly_address
  %43 = load i8** %stack_var_-32
  %44 = ptrtoint i8* %43 to i32
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 12521, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = load i64* %rdx
  %49 = trunc i64 %48 to i32
  %50 = add i32 %47, %49
  %51 = and i32 %47, 15
  %52 = and i32 %49, 15
  %53 = add i32 %51, %52
  %54 = icmp ugt i32 %53, 15
  %55 = icmp ult i32 %50, %47
  %56 = xor i32 %47, %50
  %57 = xor i32 %49, %50
  %58 = and i32 %56, %57
  %59 = icmp slt i32 %58, 0
  store i1 %54, i1* %az
  store i1 %55, i1* %cf
  store i1 %59, i1* %of
  %60 = icmp eq i32 %50, 0
  store i1 %60, i1* %zf
  %61 = icmp slt i32 %50, 0
  store i1 %61, i1* %sf
  %62 = trunc i32 %50 to i8
  %63 = call i8 @llvm.ctpop.i8(i8 %62)
  %64 = and i8 %63, 1
  %65 = icmp eq i8 %64, 0
  store i1 %65, i1* %pf
  %66 = zext i32 %50 to i64
  store i64 %66, i64* %rax
  store volatile i64 12523, i64* @assembly_address
  %67 = load i8** %stack_var_-12
  %68 = ptrtoint i8* %67 to i32
  %69 = load i64* %rax
  %70 = trunc i64 %69 to i32
  %71 = sub i32 %68, %70
  %72 = and i32 %68, 15
  %73 = and i32 %70, 15
  %74 = sub i32 %72, %73
  %75 = icmp ugt i32 %74, 15
  %76 = icmp ult i32 %68, %70
  %77 = xor i32 %68, %70
  %78 = xor i32 %68, %71
  %79 = and i32 %77, %78
  %80 = icmp slt i32 %79, 0
  store i1 %75, i1* %az
  store i1 %76, i1* %cf
  store i1 %80, i1* %of
  %81 = icmp eq i32 %71, 0
  store i1 %81, i1* %zf
  %82 = icmp slt i32 %71, 0
  store i1 %82, i1* %sf
  %83 = trunc i32 %71 to i8
  %84 = call i8 @llvm.ctpop.i8(i8 %83)
  %85 = and i8 %84, 1
  %86 = icmp eq i8 %85, 0
  store i1 %86, i1* %pf
  store volatile i64 12526, i64* @assembly_address
  %87 = load i1* %zf
  br i1 %87, label %block_31d3, label %block_30f4

block_30f4:                                       ; preds = %block_30e3
  store volatile i64 12532, i64* @assembly_address
  %88 = load i8** %stack_var_-12
  %89 = ptrtoint i8* %88 to i32
  %90 = zext i32 %89 to i64
  store i64 %90, i64* %rdx
  store volatile i64 12535, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 12542, i64* @assembly_address
  %91 = load i64* %rdx
  %92 = load i64* %rax
  %93 = mul i64 %92, 1
  %94 = add i64 %91, %93
  %95 = inttoptr i64 %94 to i8*
  %96 = load i8* %95
  %97 = zext i8 %96 to i64
  store i64 %97, i64* %rax
  store volatile i64 12546, i64* @assembly_address
  %98 = load i64* %rax
  %99 = trunc i64 %98 to i8
  %100 = zext i8 %99 to i64
  store i64 %100, i64* %rdx
  store volatile i64 12549, i64* @assembly_address
  %101 = load i64* @global_var_2165b0
  store i64 %101, i64* %rax
  store volatile i64 12556, i64* @assembly_address
  %102 = load i64* %rax
  %103 = load i64* %rdx
  %104 = add i64 %102, %103
  %105 = and i64 %102, 15
  %106 = and i64 %103, 15
  %107 = add i64 %105, %106
  %108 = icmp ugt i64 %107, 15
  %109 = icmp ult i64 %104, %102
  %110 = xor i64 %102, %104
  %111 = xor i64 %103, %104
  %112 = and i64 %110, %111
  %113 = icmp slt i64 %112, 0
  store i1 %108, i1* %az
  store i1 %109, i1* %cf
  store i1 %113, i1* %of
  %114 = icmp eq i64 %104, 0
  store i1 %114, i1* %zf
  %115 = icmp slt i64 %104, 0
  store i1 %115, i1* %sf
  %116 = trunc i64 %104 to i8
  %117 = call i8 @llvm.ctpop.i8(i8 %116)
  %118 = and i8 %117, 1
  %119 = icmp eq i8 %118, 0
  store i1 %119, i1* %pf
  store i64 %104, i64* %rax
  store volatile i64 12559, i64* @assembly_address
  %120 = load i64* %rax
  store i64 %120, i64* @global_var_2165b0
  store volatile i64 12566, i64* @assembly_address
  %121 = load i8** %stack_var_-12
  %122 = ptrtoint i8* %121 to i32
  %123 = add i32 %122, 1
  %124 = and i32 %122, 15
  %125 = add i32 %124, 1
  %126 = icmp ugt i32 %125, 15
  %127 = icmp ult i32 %123, %122
  %128 = xor i32 %122, %123
  %129 = xor i32 1, %123
  %130 = and i32 %128, %129
  %131 = icmp slt i32 %130, 0
  store i1 %126, i1* %az
  store i1 %127, i1* %cf
  store i1 %131, i1* %of
  %132 = icmp eq i32 %123, 0
  store i1 %132, i1* %zf
  %133 = icmp slt i32 %123, 0
  store i1 %133, i1* %sf
  %134 = trunc i32 %123 to i8
  %135 = call i8 @llvm.ctpop.i8(i8 %134)
  %136 = and i8 %135, 1
  %137 = icmp eq i8 %136, 0
  store i1 %137, i1* %pf
  %138 = inttoptr i32 %123 to i8*
  store i8* %138, i8** %stack_var_-12
  br label %block_311a

block_311a:                                       ; preds = %block_30f4, %block_30db
  store volatile i64 12570, i64* @assembly_address
  %139 = load i8** %stack_var_-12
  %140 = ptrtoint i8* %139 to i32
  %141 = sub i32 %140, 4095
  %142 = and i32 %140, 15
  %143 = sub i32 %142, 15
  %144 = icmp ugt i32 %143, 15
  %145 = icmp ult i32 %140, 4095
  %146 = xor i32 %140, 4095
  %147 = xor i32 %140, %141
  %148 = and i32 %146, %147
  %149 = icmp slt i32 %148, 0
  store i1 %144, i1* %az
  store i1 %145, i1* %cf
  store i1 %149, i1* %of
  %150 = icmp eq i32 %141, 0
  store i1 %150, i1* %zf
  %151 = icmp slt i32 %141, 0
  store i1 %151, i1* %sf
  %152 = trunc i32 %141 to i8
  %153 = call i8 @llvm.ctpop.i8(i8 %152)
  %154 = and i8 %153, 1
  %155 = icmp eq i8 %154, 0
  store i1 %155, i1* %pf
  store volatile i64 12577, i64* @assembly_address
  %156 = load i1* %cf
  %157 = load i1* %zf
  %158 = or i1 %156, %157
  br i1 %158, label %block_30e3, label %block_3123

block_3123:                                       ; preds = %block_311a
  store volatile i64 12579, i64* @assembly_address
  %159 = load i8** %stack_var_-28
  %160 = ptrtoint i8* %159 to i32
  %161 = zext i32 %160 to i64
  store i64 %161, i64* %rdx
  store volatile i64 12582, i64* @assembly_address
  %162 = load i8** %stack_var_-32
  %163 = ptrtoint i8* %162 to i32
  %164 = zext i32 %163 to i64
  store i64 %164, i64* %rax
  store volatile i64 12585, i64* @assembly_address
  %165 = load i64* %rax
  %166 = trunc i64 %165 to i32
  %167 = load i64* %rdx
  %168 = trunc i64 %167 to i32
  %169 = add i32 %166, %168
  %170 = and i32 %166, 15
  %171 = and i32 %168, 15
  %172 = add i32 %170, %171
  %173 = icmp ugt i32 %172, 15
  %174 = icmp ult i32 %169, %166
  %175 = xor i32 %166, %169
  %176 = xor i32 %168, %169
  %177 = and i32 %175, %176
  %178 = icmp slt i32 %177, 0
  store i1 %173, i1* %az
  store i1 %174, i1* %cf
  store i1 %178, i1* %of
  %179 = icmp eq i32 %169, 0
  store i1 %179, i1* %zf
  %180 = icmp slt i32 %169, 0
  store i1 %180, i1* %sf
  %181 = trunc i32 %169 to i8
  %182 = call i8 @llvm.ctpop.i8(i8 %181)
  %183 = and i8 %182, 1
  %184 = icmp eq i8 %183, 0
  store i1 %184, i1* %pf
  %185 = zext i32 %169 to i64
  store i64 %185, i64* %rax
  store volatile i64 12587, i64* @assembly_address
  %186 = load i64* %rax
  %187 = trunc i64 %186 to i32
  %188 = sub i32 %187, ptrtoint ([2 x i8]* @global_var_1000 to i32)
  %189 = and i32 %187, 15
  %190 = icmp ugt i32 %189, 15
  %191 = icmp ult i32 %187, 4096
  %192 = xor i32 %187, 4096
  %193 = xor i32 %187, %188
  %194 = and i32 %192, %193
  %195 = icmp slt i32 %194, 0
  store i1 %190, i1* %az
  store i1 %191, i1* %cf
  store i1 %195, i1* %of
  %196 = icmp eq i32 %188, 0
  store i1 %196, i1* %zf
  %197 = icmp slt i32 %188, 0
  store i1 %197, i1* %sf
  %198 = trunc i32 %188 to i8
  %199 = call i8 @llvm.ctpop.i8(i8 %198)
  %200 = and i8 %199, 1
  %201 = icmp eq i8 %200, 0
  store i1 %201, i1* %pf
  %202 = zext i32 %188 to i64
  store i64 %202, i64* %rax
  store volatile i64 12592, i64* @assembly_address
  %203 = load i64* %rax
  %204 = trunc i64 %203 to i32
  %205 = inttoptr i32 %204 to i8*
  store i8* %205, i8** %stack_var_-32
  store volatile i64 12595, i64* @assembly_address
  %206 = inttoptr i32 ptrtoint ([2 x i8]* @global_var_1000 to i32) to i8*
  store i8* %206, i8** %stack_var_-28
  br label %block_313a

block_313a:                                       ; preds = %block_3123, %block_30c8
  store volatile i64 12602, i64* @assembly_address
  %207 = load i8** %stack_var_-28
  %208 = ptrtoint i8* %207 to i32
  %209 = zext i32 %208 to i64
  store i64 %209, i64* %rax
  store volatile i64 12605, i64* @assembly_address
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  %212 = inttoptr i32 %211 to i8*
  store i8* %212, i8** %stack_var_-12
  store volatile i64 12608, i64* @assembly_address
  br label %block_31c0

block_3142:                                       ; preds = %block_31c0
  store volatile i64 12610, i64* @assembly_address
  %213 = load i8** %stack_var_-12
  %214 = ptrtoint i8* %213 to i32
  %215 = zext i32 %214 to i64
  store i64 %215, i64* %rdx
  store volatile i64 12613, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 12620, i64* @assembly_address
  %216 = load i64* %rdx
  %217 = load i64* %rax
  %218 = mul i64 %217, 1
  %219 = add i64 %216, %218
  %220 = inttoptr i64 %219 to i8*
  %221 = load i8* %220
  %222 = zext i8 %221 to i64
  store i64 %222, i64* %rax
  store volatile i64 12624, i64* @assembly_address
  %223 = load i64* %rax
  %224 = trunc i64 %223 to i8
  %225 = zext i8 %224 to i64
  store i64 %225, i64* %rdx
  store volatile i64 12627, i64* @assembly_address
  %226 = load i64* @global_var_2165b0
  store i64 %226, i64* %rax
  store volatile i64 12634, i64* @assembly_address
  %227 = load i64* %rax
  %228 = load i64* %rdx
  %229 = add i64 %227, %228
  %230 = and i64 %227, 15
  %231 = and i64 %228, 15
  %232 = add i64 %230, %231
  %233 = icmp ugt i64 %232, 15
  %234 = icmp ult i64 %229, %227
  %235 = xor i64 %227, %229
  %236 = xor i64 %228, %229
  %237 = and i64 %235, %236
  %238 = icmp slt i64 %237, 0
  store i1 %233, i1* %az
  store i1 %234, i1* %cf
  store i1 %238, i1* %of
  %239 = icmp eq i64 %229, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i64 %229, 0
  store i1 %240, i1* %sf
  %241 = trunc i64 %229 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  store i64 %229, i64* %rax
  store volatile i64 12637, i64* @assembly_address
  %245 = load i64* %rax
  store i64 %245, i64* @global_var_2165b0
  store volatile i64 12644, i64* @assembly_address
  %246 = load i64* @global_var_2165b0
  store i64 %246, i64* %rdx
  store volatile i64 12651, i64* @assembly_address
  %247 = load i8** %stack_var_-12
  %248 = ptrtoint i8* %247 to i32
  %249 = zext i32 %248 to i64
  store i64 %249, i64* %rax
  store volatile i64 12654, i64* @assembly_address
  %250 = load i64* %rax
  %251 = trunc i64 %250 to i32
  %252 = sub i32 %251, ptrtoint ([2 x i8]* @global_var_1000 to i32)
  %253 = and i32 %251, 15
  %254 = icmp ugt i32 %253, 15
  %255 = icmp ult i32 %251, 4096
  %256 = xor i32 %251, 4096
  %257 = xor i32 %251, %252
  %258 = and i32 %256, %257
  %259 = icmp slt i32 %258, 0
  store i1 %254, i1* %az
  store i1 %255, i1* %cf
  store i1 %259, i1* %of
  %260 = icmp eq i32 %252, 0
  store i1 %260, i1* %zf
  %261 = icmp slt i32 %252, 0
  store i1 %261, i1* %sf
  %262 = trunc i32 %252 to i8
  %263 = call i8 @llvm.ctpop.i8(i8 %262)
  %264 = and i8 %263, 1
  %265 = icmp eq i8 %264, 0
  store i1 %265, i1* %pf
  %266 = zext i32 %252 to i64
  store i64 %266, i64* %rax
  store volatile i64 12659, i64* @assembly_address
  %267 = load i64* %rax
  %268 = trunc i64 %267 to i32
  %269 = zext i32 %268 to i64
  store i64 %269, i64* %rcx
  store volatile i64 12661, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 12668, i64* @assembly_address
  %270 = load i64* %rcx
  %271 = load i64* %rax
  %272 = mul i64 %271, 1
  %273 = add i64 %270, %272
  %274 = inttoptr i64 %273 to i8*
  %275 = load i8* %274
  %276 = zext i8 %275 to i64
  store i64 %276, i64* %rax
  store volatile i64 12672, i64* @assembly_address
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i8
  %279 = zext i8 %278 to i64
  store i64 %279, i64* %rax
  store volatile i64 12675, i64* @assembly_address
  %280 = load i64* %rdx
  %281 = load i64* %rax
  %282 = sub i64 %280, %281
  %283 = and i64 %280, 15
  %284 = and i64 %281, 15
  %285 = sub i64 %283, %284
  %286 = icmp ugt i64 %285, 15
  %287 = icmp ult i64 %280, %281
  %288 = xor i64 %280, %281
  %289 = xor i64 %280, %282
  %290 = and i64 %288, %289
  %291 = icmp slt i64 %290, 0
  store i1 %286, i1* %az
  store i1 %287, i1* %cf
  store i1 %291, i1* %of
  %292 = icmp eq i64 %282, 0
  store i1 %292, i1* %zf
  %293 = icmp slt i64 %282, 0
  store i1 %293, i1* %sf
  %294 = trunc i64 %282 to i8
  %295 = call i8 @llvm.ctpop.i8(i8 %294)
  %296 = and i8 %295, 1
  %297 = icmp eq i8 %296, 0
  store i1 %297, i1* %pf
  store i64 %282, i64* %rdx
  store volatile i64 12678, i64* @assembly_address
  %298 = load i64* %rdx
  store i64 %298, i64* %rax
  store volatile i64 12681, i64* @assembly_address
  %299 = load i64* %rax
  store i64 %299, i64* @global_var_2165b0
  store volatile i64 12688, i64* @assembly_address
  %300 = load i64* @global_var_2165b8
  store i64 %300, i64* %rdx
  store volatile i64 12695, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 12700, i64* @assembly_address
  %301 = load i64* %rdx
  %302 = load i64* %rax
  %303 = sub i64 %301, %302
  %304 = and i64 %301, 15
  %305 = and i64 %302, 15
  %306 = sub i64 %304, %305
  %307 = icmp ugt i64 %306, 15
  %308 = icmp ult i64 %301, %302
  %309 = xor i64 %301, %302
  %310 = xor i64 %301, %303
  %311 = and i64 %309, %310
  %312 = icmp slt i64 %311, 0
  store i1 %307, i1* %az
  store i1 %308, i1* %cf
  store i1 %312, i1* %of
  %313 = icmp eq i64 %303, 0
  store i1 %313, i1* %zf
  %314 = icmp slt i64 %303, 0
  store i1 %314, i1* %sf
  %315 = trunc i64 %303 to i8
  %316 = call i8 @llvm.ctpop.i8(i8 %315)
  %317 = and i8 %316, 1
  %318 = icmp eq i8 %317, 0
  store i1 %318, i1* %pf
  store volatile i64 12703, i64* @assembly_address
  %319 = load i1* %zf
  %320 = icmp eq i1 %319, false
  br i1 %320, label %block_31bc, label %block_31a1

block_31a1:                                       ; preds = %block_3142
  store volatile i64 12705, i64* @assembly_address
  %321 = load i64* @global_var_2165b0
  store i64 %321, i64* %rax
  store volatile i64 12712, i64* @assembly_address
  %322 = load i64* %rax
  %323 = trunc i64 %322 to i32
  %324 = and i32 %323, 4095
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %325 = icmp eq i32 %324, 0
  store i1 %325, i1* %zf
  %326 = icmp slt i32 %324, 0
  store i1 %326, i1* %sf
  %327 = trunc i32 %324 to i8
  %328 = call i8 @llvm.ctpop.i8(i8 %327)
  %329 = and i8 %328, 1
  %330 = icmp eq i8 %329, 0
  store i1 %330, i1* %pf
  %331 = zext i32 %324 to i64
  store i64 %331, i64* %rax
  store volatile i64 12717, i64* @assembly_address
  %332 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %333 = icmp eq i64 %332, 0
  store i1 %333, i1* %zf
  %334 = icmp slt i64 %332, 0
  store i1 %334, i1* %sf
  %335 = trunc i64 %332 to i8
  %336 = call i8 @llvm.ctpop.i8(i8 %335)
  %337 = and i8 %336, 1
  %338 = icmp eq i8 %337, 0
  store i1 %338, i1* %pf
  store volatile i64 12720, i64* @assembly_address
  %339 = load i1* %zf
  %340 = icmp eq i1 %339, false
  br i1 %340, label %block_31bc, label %block_31b2

block_31b2:                                       ; preds = %block_31a1
  store volatile i64 12722, i64* @assembly_address
  %341 = load i8** %stack_var_-12
  %342 = ptrtoint i8* %341 to i32
  %343 = zext i32 %342 to i64
  store i64 %343, i64* %rax
  store volatile i64 12725, i64* @assembly_address
  %344 = load i64* %rax
  store i64 %344, i64* @global_var_2165b8
  br label %block_31bc

block_31bc:                                       ; preds = %block_31b2, %block_31a1, %block_3142
  store volatile i64 12732, i64* @assembly_address
  %345 = load i8** %stack_var_-12
  %346 = ptrtoint i8* %345 to i32
  %347 = add i32 %346, 1
  %348 = and i32 %346, 15
  %349 = add i32 %348, 1
  %350 = icmp ugt i32 %349, 15
  %351 = icmp ult i32 %347, %346
  %352 = xor i32 %346, %347
  %353 = xor i32 1, %347
  %354 = and i32 %352, %353
  %355 = icmp slt i32 %354, 0
  store i1 %350, i1* %az
  store i1 %351, i1* %cf
  store i1 %355, i1* %of
  %356 = icmp eq i32 %347, 0
  store i1 %356, i1* %zf
  %357 = icmp slt i32 %347, 0
  store i1 %357, i1* %sf
  %358 = trunc i32 %347 to i8
  %359 = call i8 @llvm.ctpop.i8(i8 %358)
  %360 = and i8 %359, 1
  %361 = icmp eq i8 %360, 0
  store i1 %361, i1* %pf
  %362 = inttoptr i32 %347 to i8*
  store i8* %362, i8** %stack_var_-12
  br label %block_31c0

block_31c0:                                       ; preds = %block_31bc, %block_313a
  store volatile i64 12736, i64* @assembly_address
  %363 = load i8** %stack_var_-28
  %364 = ptrtoint i8* %363 to i32
  %365 = zext i32 %364 to i64
  store i64 %365, i64* %rdx
  store volatile i64 12739, i64* @assembly_address
  %366 = load i8** %stack_var_-32
  %367 = ptrtoint i8* %366 to i32
  %368 = zext i32 %367 to i64
  store i64 %368, i64* %rax
  store volatile i64 12742, i64* @assembly_address
  %369 = load i64* %rax
  %370 = trunc i64 %369 to i32
  %371 = load i64* %rdx
  %372 = trunc i64 %371 to i32
  %373 = add i32 %370, %372
  %374 = and i32 %370, 15
  %375 = and i32 %372, 15
  %376 = add i32 %374, %375
  %377 = icmp ugt i32 %376, 15
  %378 = icmp ult i32 %373, %370
  %379 = xor i32 %370, %373
  %380 = xor i32 %372, %373
  %381 = and i32 %379, %380
  %382 = icmp slt i32 %381, 0
  store i1 %377, i1* %az
  store i1 %378, i1* %cf
  store i1 %382, i1* %of
  %383 = icmp eq i32 %373, 0
  store i1 %383, i1* %zf
  %384 = icmp slt i32 %373, 0
  store i1 %384, i1* %sf
  %385 = trunc i32 %373 to i8
  %386 = call i8 @llvm.ctpop.i8(i8 %385)
  %387 = and i8 %386, 1
  %388 = icmp eq i8 %387, 0
  store i1 %388, i1* %pf
  %389 = zext i32 %373 to i64
  store i64 %389, i64* %rax
  store volatile i64 12744, i64* @assembly_address
  %390 = load i8** %stack_var_-12
  %391 = ptrtoint i8* %390 to i32
  %392 = load i64* %rax
  %393 = trunc i64 %392 to i32
  %394 = sub i32 %391, %393
  %395 = and i32 %391, 15
  %396 = and i32 %393, 15
  %397 = sub i32 %395, %396
  %398 = icmp ugt i32 %397, 15
  %399 = icmp ult i32 %391, %393
  %400 = xor i32 %391, %393
  %401 = xor i32 %391, %394
  %402 = and i32 %400, %401
  %403 = icmp slt i32 %402, 0
  store i1 %398, i1* %az
  store i1 %399, i1* %cf
  store i1 %403, i1* %of
  %404 = icmp eq i32 %394, 0
  store i1 %404, i1* %zf
  %405 = icmp slt i32 %394, 0
  store i1 %405, i1* %sf
  %406 = trunc i32 %394 to i8
  %407 = call i8 @llvm.ctpop.i8(i8 %406)
  %408 = and i8 %407, 1
  %409 = icmp eq i8 %408, 0
  store i1 %409, i1* %pf
  store volatile i64 12747, i64* @assembly_address
  %410 = load i1* %cf
  br i1 %410, label %block_3142, label %block_31d1

block_31d1:                                       ; preds = %block_31c0
  store volatile i64 12753, i64* @assembly_address
  br label %block_31d4

block_31d3:                                       ; preds = %block_30e3
  store volatile i64 12755, i64* @assembly_address
  br label %block_31d4

block_31d4:                                       ; preds = %block_31d3, %block_31d1
  store volatile i64 12756, i64* @assembly_address
  %411 = load i64* %stack_var_-8
  store i64 %411, i64* %rbp
  %412 = ptrtoint i64* %stack_var_0 to i64
  store i64 %412, i64* %rsp
  store volatile i64 12757, i64* @assembly_address
  %413 = load i64* %rax
  ret i64 %413
}

declare i64 @168(i64, i32)

declare i64 @169(i64, i64)

define i64 @deflate_fast(%z_stream_s* %arg1, i32 %arg2) {
block_31d6:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint %z_stream_s* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 12758, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 12759, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 12762, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 12766, i64* @assembly_address
  store i32 0, i32* %stack_var_-20
  store volatile i64 12773, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 12780, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_21a440 to i32*)
  store volatile i64 12790, i64* @assembly_address
  br label %block_35a0

block_31fb:                                       ; preds = %block_35a0
  store volatile i64 12795, i64* @assembly_address
  %21 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %22 = zext i32 %21 to i64
  store i64 %22, i64* %rax
  store volatile i64 12801, i64* @assembly_address
  %23 = load i64* %rax
  %24 = trunc i64 %23 to i32
  %25 = load i1* %of
  %26 = shl i32 %24, 5
  %27 = icmp eq i32 %26, 0
  store i1 %27, i1* %zf
  %28 = icmp slt i32 %26, 0
  store i1 %28, i1* %sf
  %29 = trunc i32 %26 to i8
  %30 = call i8 @llvm.ctpop.i8(i8 %29)
  %31 = and i8 %30, 1
  %32 = icmp eq i8 %31, 0
  store i1 %32, i1* %pf
  %33 = zext i32 %26 to i64
  store i64 %33, i64* %rax
  %34 = shl i32 %24, 4
  %35 = lshr i32 %34, 31
  %36 = trunc i32 %35 to i1
  store i1 %36, i1* %cf
  %37 = lshr i32 %26, 31
  %38 = icmp ne i32 %37, %35
  %39 = select i1 false, i1 %38, i1 %25
  store i1 %39, i1* %of
  store volatile i64 12804, i64* @assembly_address
  %40 = load i64* %rax
  %41 = trunc i64 %40 to i32
  %42 = zext i32 %41 to i64
  store i64 %42, i64* %rdx
  store volatile i64 12806, i64* @assembly_address
  %43 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %44 = zext i32 %43 to i64
  store i64 %44, i64* %rax
  store volatile i64 12812, i64* @assembly_address
  %45 = load i64* %rax
  %46 = trunc i64 %45 to i32
  %47 = add i32 %46, 2
  %48 = and i32 %46, 15
  %49 = add i32 %48, 2
  %50 = icmp ugt i32 %49, 15
  %51 = icmp ult i32 %47, %46
  %52 = xor i32 %46, %47
  %53 = xor i32 2, %47
  %54 = and i32 %52, %53
  %55 = icmp slt i32 %54, 0
  store i1 %50, i1* %az
  store i1 %51, i1* %cf
  store i1 %55, i1* %of
  %56 = icmp eq i32 %47, 0
  store i1 %56, i1* %zf
  %57 = icmp slt i32 %47, 0
  store i1 %57, i1* %sf
  %58 = trunc i32 %47 to i8
  %59 = call i8 @llvm.ctpop.i8(i8 %58)
  %60 = and i8 %59, 1
  %61 = icmp eq i8 %60, 0
  store i1 %61, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a42a to i64), i64* %rax
  store volatile i64 12815, i64* @assembly_address
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i32
  %64 = zext i32 %63 to i64
  store i64 %64, i64* %rcx
  store volatile i64 12817, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 12824, i64* @assembly_address
  %65 = load i64* %rcx
  %66 = load i64* %rax
  %67 = mul i64 %66, 1
  %68 = add i64 %65, %67
  %69 = inttoptr i64 %68 to i8*
  %70 = load i8* %69
  %71 = zext i8 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 12828, i64* @assembly_address
  %72 = load i64* %rax
  %73 = trunc i64 %72 to i8
  %74 = zext i8 %73 to i64
  store i64 %74, i64* %rax
  store volatile i64 12831, i64* @assembly_address
  %75 = load i64* %rax
  %76 = trunc i64 %75 to i32
  %77 = load i64* %rdx
  %78 = trunc i64 %77 to i32
  %79 = xor i32 %76, %78
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %80 = icmp eq i32 %79, 0
  store i1 %80, i1* %zf
  %81 = icmp slt i32 %79, 0
  store i1 %81, i1* %sf
  %82 = trunc i32 %79 to i8
  %83 = call i8 @llvm.ctpop.i8(i8 %82)
  %84 = and i8 %83, 1
  %85 = icmp eq i8 %84, 0
  store i1 %85, i1* %pf
  %86 = zext i32 %79 to i64
  store i64 %86, i64* %rax
  store volatile i64 12833, i64* @assembly_address
  %87 = load i64* %rax
  %88 = trunc i64 %87 to i32
  %89 = and i32 %88, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %90 = icmp eq i32 %89, 0
  store i1 %90, i1* %zf
  %91 = icmp slt i32 %89, 0
  store i1 %91, i1* %sf
  %92 = trunc i32 %89 to i8
  %93 = call i8 @llvm.ctpop.i8(i8 %92)
  %94 = and i8 %93, 1
  %95 = icmp eq i8 %94, 0
  store i1 %95, i1* %pf
  %96 = zext i32 %89 to i64
  store i64 %96, i64* %rax
  store volatile i64 12838, i64* @assembly_address
  %97 = load i64* %rax
  %98 = trunc i64 %97 to i32
  store i32 %98, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 12844, i64* @assembly_address
  %99 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %100 = zext i32 %99 to i64
  store i64 %100, i64* %rax
  store volatile i64 12850, i64* @assembly_address
  %101 = load i64* %rax
  %102 = trunc i64 %101 to i32
  %103 = zext i32 %102 to i64
  store i64 %103, i64* %rax
  store volatile i64 12852, i64* @assembly_address
  %104 = load i64* %rax
  %105 = add i64 %104, 32768
  %106 = and i64 %104, 15
  %107 = icmp ugt i64 %106, 15
  %108 = icmp ult i64 %105, %104
  %109 = xor i64 %104, %105
  %110 = xor i64 32768, %105
  %111 = and i64 %109, %110
  %112 = icmp slt i64 %111, 0
  store i1 %107, i1* %az
  store i1 %108, i1* %cf
  store i1 %112, i1* %of
  %113 = icmp eq i64 %105, 0
  store i1 %113, i1* %zf
  %114 = icmp slt i64 %105, 0
  store i1 %114, i1* %sf
  %115 = trunc i64 %105 to i8
  %116 = call i8 @llvm.ctpop.i8(i8 %115)
  %117 = and i8 %116, 1
  %118 = icmp eq i8 %117, 0
  store i1 %118, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 12858, i64* @assembly_address
  %119 = load i64* %rax
  %120 = load i64* %rax
  %121 = mul i64 %120, 1
  %122 = add i64 %119, %121
  store i64 %122, i64* %rdx
  store volatile i64 12862, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 12869, i64* @assembly_address
  %123 = load i64* %rax
  %124 = load i64* %rdx
  %125 = add i64 %123, %124
  %126 = and i64 %123, 15
  %127 = and i64 %124, 15
  %128 = add i64 %126, %127
  %129 = icmp ugt i64 %128, 15
  %130 = icmp ult i64 %125, %123
  %131 = xor i64 %123, %125
  %132 = xor i64 %124, %125
  %133 = and i64 %131, %132
  %134 = icmp slt i64 %133, 0
  store i1 %129, i1* %az
  store i1 %130, i1* %cf
  store i1 %134, i1* %of
  %135 = icmp eq i64 %125, 0
  store i1 %135, i1* %zf
  %136 = icmp slt i64 %125, 0
  store i1 %136, i1* %sf
  %137 = trunc i64 %125 to i8
  %138 = call i8 @llvm.ctpop.i8(i8 %137)
  %139 = and i8 %138, 1
  %140 = icmp eq i8 %139, 0
  store i1 %140, i1* %pf
  store i64 %125, i64* %rax
  store volatile i64 12872, i64* @assembly_address
  %141 = load i64* %rax
  %142 = inttoptr i64 %141 to i16*
  %143 = load i16* %142
  %144 = zext i16 %143 to i64
  store i64 %144, i64* %rax
  store volatile i64 12875, i64* @assembly_address
  %145 = load i64* %rax
  %146 = trunc i64 %145 to i16
  %147 = zext i16 %146 to i64
  store i64 %147, i64* %rax
  store volatile i64 12878, i64* @assembly_address
  %148 = load i64* %rax
  %149 = trunc i64 %148 to i32
  store i32 %149, i32* %stack_var_-12
  store volatile i64 12881, i64* @assembly_address
  %150 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %151 = zext i32 %150 to i64
  store i64 %151, i64* %rax
  store volatile i64 12887, i64* @assembly_address
  %152 = load i64* %rax
  %153 = trunc i64 %152 to i32
  %154 = and i32 %153, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %155 = icmp eq i32 %154, 0
  store i1 %155, i1* %zf
  %156 = icmp slt i32 %154, 0
  store i1 %156, i1* %sf
  %157 = trunc i32 %154 to i8
  %158 = call i8 @llvm.ctpop.i8(i8 %157)
  %159 = and i8 %158, 1
  %160 = icmp eq i8 %159, 0
  store i1 %160, i1* %pf
  %161 = zext i32 %154 to i64
  store i64 %161, i64* %rax
  store volatile i64 12892, i64* @assembly_address
  %162 = load i64* %rax
  %163 = trunc i64 %162 to i32
  %164 = zext i32 %163 to i64
  store i64 %164, i64* %rdx
  store volatile i64 12894, i64* @assembly_address
  %165 = load i32* %stack_var_-12
  %166 = zext i32 %165 to i64
  store i64 %166, i64* %rax
  store volatile i64 12897, i64* @assembly_address
  %167 = load i64* %rax
  %168 = trunc i64 %167 to i32
  %169 = zext i32 %168 to i64
  store i64 %169, i64* %rcx
  store volatile i64 12899, i64* @assembly_address
  %170 = load i64* %rdx
  %171 = trunc i64 %170 to i32
  %172 = zext i32 %171 to i64
  store i64 %172, i64* %rax
  store volatile i64 12901, i64* @assembly_address
  %173 = load i64* %rax
  %174 = load i64* %rax
  %175 = mul i64 %174, 1
  %176 = add i64 %173, %175
  store i64 %176, i64* %rdx
  store volatile i64 12905, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 12912, i64* @assembly_address
  %177 = load i64* %rcx
  %178 = trunc i64 %177 to i16
  %179 = load i64* %rdx
  %180 = load i64* %rax
  %181 = mul i64 %180, 1
  %182 = add i64 %179, %181
  %183 = inttoptr i64 %182 to i16*
  store i16 %178, i16* %183
  store volatile i64 12916, i64* @assembly_address
  %184 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %185 = zext i32 %184 to i64
  store i64 %185, i64* %rdx
  store volatile i64 12922, i64* @assembly_address
  %186 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %187 = zext i32 %186 to i64
  store i64 %187, i64* %rax
  store volatile i64 12928, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  %190 = zext i32 %189 to i64
  store i64 %190, i64* %rax
  store volatile i64 12930, i64* @assembly_address
  %191 = load i64* %rax
  %192 = add i64 %191, 32768
  %193 = and i64 %191, 15
  %194 = icmp ugt i64 %193, 15
  %195 = icmp ult i64 %192, %191
  %196 = xor i64 %191, %192
  %197 = xor i64 32768, %192
  %198 = and i64 %196, %197
  %199 = icmp slt i64 %198, 0
  store i1 %194, i1* %az
  store i1 %195, i1* %cf
  store i1 %199, i1* %of
  %200 = icmp eq i64 %192, 0
  store i1 %200, i1* %zf
  %201 = icmp slt i64 %192, 0
  store i1 %201, i1* %sf
  %202 = trunc i64 %192 to i8
  %203 = call i8 @llvm.ctpop.i8(i8 %202)
  %204 = and i8 %203, 1
  %205 = icmp eq i8 %204, 0
  store i1 %205, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 12936, i64* @assembly_address
  %206 = load i64* %rax
  %207 = load i64* %rax
  %208 = mul i64 %207, 1
  %209 = add i64 %206, %208
  store i64 %209, i64* %rcx
  store volatile i64 12940, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 12947, i64* @assembly_address
  %210 = load i64* %rax
  %211 = load i64* %rcx
  %212 = add i64 %210, %211
  %213 = and i64 %210, 15
  %214 = and i64 %211, 15
  %215 = add i64 %213, %214
  %216 = icmp ugt i64 %215, 15
  %217 = icmp ult i64 %212, %210
  %218 = xor i64 %210, %212
  %219 = xor i64 %211, %212
  %220 = and i64 %218, %219
  %221 = icmp slt i64 %220, 0
  store i1 %216, i1* %az
  store i1 %217, i1* %cf
  store i1 %221, i1* %of
  %222 = icmp eq i64 %212, 0
  store i1 %222, i1* %zf
  %223 = icmp slt i64 %212, 0
  store i1 %223, i1* %sf
  %224 = trunc i64 %212 to i8
  %225 = call i8 @llvm.ctpop.i8(i8 %224)
  %226 = and i8 %225, 1
  %227 = icmp eq i8 %226, 0
  store i1 %227, i1* %pf
  store i64 %212, i64* %rax
  store volatile i64 12950, i64* @assembly_address
  %228 = load i64* %rdx
  %229 = trunc i64 %228 to i16
  %230 = load i64* %rax
  %231 = inttoptr i64 %230 to i16*
  store i16 %229, i16* %231
  store volatile i64 12953, i64* @assembly_address
  %232 = load i32* %stack_var_-12
  %233 = and i32 %232, 15
  %234 = icmp ugt i32 %233, 15
  %235 = icmp ult i32 %232, 0
  %236 = xor i32 %232, 0
  %237 = and i32 %236, 0
  %238 = icmp slt i32 %237, 0
  store i1 %234, i1* %az
  store i1 %235, i1* %cf
  store i1 %238, i1* %of
  %239 = icmp eq i32 %232, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i32 %232, 0
  store i1 %240, i1* %sf
  %241 = trunc i32 %232 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  store volatile i64 12957, i64* @assembly_address
  %245 = load i1* %zf
  br i1 %245, label %block_32ea, label %block_329f

block_329f:                                       ; preds = %block_31fb
  store volatile i64 12959, i64* @assembly_address
  %246 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %247 = zext i32 %246 to i64
  store i64 %247, i64* %rax
  store volatile i64 12965, i64* @assembly_address
  %248 = load i64* %rax
  %249 = trunc i64 %248 to i32
  %250 = load i32* %stack_var_-12
  %251 = sub i32 %249, %250
  %252 = and i32 %249, 15
  %253 = and i32 %250, 15
  %254 = sub i32 %252, %253
  %255 = icmp ugt i32 %254, 15
  %256 = icmp ult i32 %249, %250
  %257 = xor i32 %249, %250
  %258 = xor i32 %249, %251
  %259 = and i32 %257, %258
  %260 = icmp slt i32 %259, 0
  store i1 %255, i1* %az
  store i1 %256, i1* %cf
  store i1 %260, i1* %of
  %261 = icmp eq i32 %251, 0
  store i1 %261, i1* %zf
  %262 = icmp slt i32 %251, 0
  store i1 %262, i1* %sf
  %263 = trunc i32 %251 to i8
  %264 = call i8 @llvm.ctpop.i8(i8 %263)
  %265 = and i8 %264, 1
  %266 = icmp eq i8 %265, 0
  store i1 %266, i1* %pf
  %267 = zext i32 %251 to i64
  store i64 %267, i64* %rax
  store volatile i64 12968, i64* @assembly_address
  %268 = load i64* %rax
  %269 = trunc i64 %268 to i32
  %270 = sub i32 %269, 32506
  %271 = and i32 %269, 15
  %272 = sub i32 %271, 10
  %273 = icmp ugt i32 %272, 15
  %274 = icmp ult i32 %269, 32506
  %275 = xor i32 %269, 32506
  %276 = xor i32 %269, %270
  %277 = and i32 %275, %276
  %278 = icmp slt i32 %277, 0
  store i1 %273, i1* %az
  store i1 %274, i1* %cf
  store i1 %278, i1* %of
  %279 = icmp eq i32 %270, 0
  store i1 %279, i1* %zf
  %280 = icmp slt i32 %270, 0
  store i1 %280, i1* %sf
  %281 = trunc i32 %270 to i8
  %282 = call i8 @llvm.ctpop.i8(i8 %281)
  %283 = and i8 %282, 1
  %284 = icmp eq i8 %283, 0
  store i1 %284, i1* %pf
  store volatile i64 12973, i64* @assembly_address
  %285 = load i1* %cf
  %286 = load i1* %zf
  %287 = or i1 %285, %286
  %288 = icmp ne i1 %287, true
  br i1 %288, label %block_32ea, label %block_32af

block_32af:                                       ; preds = %block_329f
  store volatile i64 12975, i64* @assembly_address
  %289 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %290 = zext i32 %289 to i64
  store i64 %290, i64* %rax
  store volatile i64 12981, i64* @assembly_address
  %291 = load i64* %rax
  %292 = trunc i64 %291 to i32
  %293 = zext i32 %292 to i64
  store i64 %293, i64* %rdx
  store volatile i64 12983, i64* @assembly_address
  %294 = load i64* @global_var_216020
  store i64 %294, i64* %rax
  store volatile i64 12990, i64* @assembly_address
  %295 = load i64* %rax
  %296 = sub i64 %295, 262
  %297 = and i64 %295, 15
  %298 = sub i64 %297, 6
  %299 = icmp ugt i64 %298, 15
  %300 = icmp ult i64 %295, 262
  %301 = xor i64 %295, 262
  %302 = xor i64 %295, %296
  %303 = and i64 %301, %302
  %304 = icmp slt i64 %303, 0
  store i1 %299, i1* %az
  store i1 %300, i1* %cf
  store i1 %304, i1* %of
  %305 = icmp eq i64 %296, 0
  store i1 %305, i1* %zf
  %306 = icmp slt i64 %296, 0
  store i1 %306, i1* %sf
  %307 = trunc i64 %296 to i8
  %308 = call i8 @llvm.ctpop.i8(i8 %307)
  %309 = and i8 %308, 1
  %310 = icmp eq i8 %309, 0
  store i1 %310, i1* %pf
  store i64 %296, i64* %rax
  store volatile i64 12996, i64* @assembly_address
  %311 = load i64* %rdx
  %312 = load i64* %rax
  %313 = sub i64 %311, %312
  %314 = and i64 %311, 15
  %315 = and i64 %312, 15
  %316 = sub i64 %314, %315
  %317 = icmp ugt i64 %316, 15
  %318 = icmp ult i64 %311, %312
  %319 = xor i64 %311, %312
  %320 = xor i64 %311, %313
  %321 = and i64 %319, %320
  %322 = icmp slt i64 %321, 0
  store i1 %317, i1* %az
  store i1 %318, i1* %cf
  store i1 %322, i1* %of
  %323 = icmp eq i64 %313, 0
  store i1 %323, i1* %zf
  %324 = icmp slt i64 %313, 0
  store i1 %324, i1* %sf
  %325 = trunc i64 %313 to i8
  %326 = call i8 @llvm.ctpop.i8(i8 %325)
  %327 = and i8 %326, 1
  %328 = icmp eq i8 %327, 0
  store i1 %328, i1* %pf
  store volatile i64 12999, i64* @assembly_address
  %329 = load i1* %cf
  %330 = load i1* %zf
  %331 = or i1 %329, %330
  %332 = icmp ne i1 %331, true
  br i1 %332, label %block_32ea, label %block_32c9

block_32c9:                                       ; preds = %block_32af
  store volatile i64 13001, i64* @assembly_address
  %333 = load i32* %stack_var_-12
  %334 = zext i32 %333 to i64
  store i64 %334, i64* %rax
  store volatile i64 13004, i64* @assembly_address
  %335 = load i64* %rax
  %336 = trunc i64 %335 to i32
  %337 = zext i32 %336 to i64
  store i64 %337, i64* %rdi
  store volatile i64 13006, i64* @assembly_address
  %338 = load i64* %rdi
  %339 = trunc i64 %338 to i32
  %340 = call i64 @longest_match(i32 %339)
  store i64 %340, i64* %rax
  store i64 %340, i64* %rax
  store volatile i64 13011, i64* @assembly_address
  %341 = load i64* %rax
  %342 = trunc i64 %341 to i32
  store i32 %342, i32* %stack_var_-16
  store volatile i64 13014, i64* @assembly_address
  %343 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %344 = zext i32 %343 to i64
  store i64 %344, i64* %rax
  store volatile i64 13020, i64* @assembly_address
  %345 = load i32* %stack_var_-16
  %346 = load i64* %rax
  %347 = trunc i64 %346 to i32
  %348 = sub i32 %345, %347
  %349 = and i32 %345, 15
  %350 = and i32 %347, 15
  %351 = sub i32 %349, %350
  %352 = icmp ugt i32 %351, 15
  %353 = icmp ult i32 %345, %347
  %354 = xor i32 %345, %347
  %355 = xor i32 %345, %348
  %356 = and i32 %354, %355
  %357 = icmp slt i32 %356, 0
  store i1 %352, i1* %az
  store i1 %353, i1* %cf
  store i1 %357, i1* %of
  %358 = icmp eq i32 %348, 0
  store i1 %358, i1* %zf
  %359 = icmp slt i32 %348, 0
  store i1 %359, i1* %sf
  %360 = trunc i32 %348 to i8
  %361 = call i8 @llvm.ctpop.i8(i8 %360)
  %362 = and i8 %361, 1
  %363 = icmp eq i8 %362, 0
  store i1 %363, i1* %pf
  store volatile i64 13023, i64* @assembly_address
  %364 = load i1* %cf
  %365 = load i1* %zf
  %366 = or i1 %364, %365
  br i1 %366, label %block_32ea, label %block_32e1

block_32e1:                                       ; preds = %block_32c9
  store volatile i64 13025, i64* @assembly_address
  %367 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %368 = zext i32 %367 to i64
  store i64 %368, i64* %rax
  store volatile i64 13031, i64* @assembly_address
  %369 = load i64* %rax
  %370 = trunc i64 %369 to i32
  store i32 %370, i32* %stack_var_-16
  br label %block_32ea

block_32ea:                                       ; preds = %block_32e1, %block_32c9, %block_32af, %block_329f, %block_31fb
  store volatile i64 13034, i64* @assembly_address
  %371 = load i32* %stack_var_-16
  %372 = sub i32 %371, 2
  %373 = and i32 %371, 15
  %374 = sub i32 %373, 2
  %375 = icmp ugt i32 %374, 15
  %376 = icmp ult i32 %371, 2
  %377 = xor i32 %371, 2
  %378 = xor i32 %371, %372
  %379 = and i32 %377, %378
  %380 = icmp slt i32 %379, 0
  store i1 %375, i1* %az
  store i1 %376, i1* %cf
  store i1 %380, i1* %of
  %381 = icmp eq i32 %372, 0
  store i1 %381, i1* %zf
  %382 = icmp slt i32 %372, 0
  store i1 %382, i1* %sf
  %383 = trunc i32 %372 to i8
  %384 = call i8 @llvm.ctpop.i8(i8 %383)
  %385 = and i8 %384, 1
  %386 = icmp eq i8 %385, 0
  store i1 %386, i1* %pf
  store volatile i64 13038, i64* @assembly_address
  %387 = load i1* %cf
  %388 = load i1* %zf
  %389 = or i1 %387, %388
  br i1 %389, label %block_348c, label %block_32f4

block_32f4:                                       ; preds = %block_32ea
  store volatile i64 13044, i64* @assembly_address
  %390 = load i32* %stack_var_-16
  %391 = zext i32 %390 to i64
  store i64 %391, i64* %rax
  store volatile i64 13047, i64* @assembly_address
  %392 = load i64* %rax
  %393 = trunc i64 %392 to i32
  %394 = sub i32 %393, 3
  %395 = and i32 %393, 15
  %396 = sub i32 %395, 3
  %397 = icmp ugt i32 %396, 15
  %398 = icmp ult i32 %393, 3
  %399 = xor i32 %393, 3
  %400 = xor i32 %393, %394
  %401 = and i32 %399, %400
  %402 = icmp slt i32 %401, 0
  store i1 %397, i1* %az
  store i1 %398, i1* %cf
  store i1 %402, i1* %of
  %403 = icmp eq i32 %394, 0
  store i1 %403, i1* %zf
  %404 = icmp slt i32 %394, 0
  store i1 %404, i1* %sf
  %405 = trunc i32 %394 to i8
  %406 = call i8 @llvm.ctpop.i8(i8 %405)
  %407 = and i8 %406, 1
  %408 = icmp eq i8 %407, 0
  store i1 %408, i1* %pf
  %409 = zext i32 %394 to i64
  store i64 %409, i64* %rax
  store volatile i64 13050, i64* @assembly_address
  %410 = load i64* %rax
  %411 = trunc i64 %410 to i32
  %412 = zext i32 %411 to i64
  store i64 %412, i64* %rcx
  store volatile i64 13052, i64* @assembly_address
  %413 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %414 = zext i32 %413 to i64
  store i64 %414, i64* %rdx
  store volatile i64 13058, i64* @assembly_address
  %415 = load i32* bitcast (i64* @global_var_21a444 to i32*)
  %416 = zext i32 %415 to i64
  store i64 %416, i64* %rax
  store volatile i64 13064, i64* @assembly_address
  %417 = load i64* %rdx
  %418 = trunc i64 %417 to i32
  %419 = load i64* %rax
  %420 = trunc i64 %419 to i32
  %421 = sub i32 %418, %420
  %422 = and i32 %418, 15
  %423 = and i32 %420, 15
  %424 = sub i32 %422, %423
  %425 = icmp ugt i32 %424, 15
  %426 = icmp ult i32 %418, %420
  %427 = xor i32 %418, %420
  %428 = xor i32 %418, %421
  %429 = and i32 %427, %428
  %430 = icmp slt i32 %429, 0
  store i1 %425, i1* %az
  store i1 %426, i1* %cf
  store i1 %430, i1* %of
  %431 = icmp eq i32 %421, 0
  store i1 %431, i1* %zf
  %432 = icmp slt i32 %421, 0
  store i1 %432, i1* %sf
  %433 = trunc i32 %421 to i8
  %434 = call i8 @llvm.ctpop.i8(i8 %433)
  %435 = and i8 %434, 1
  %436 = icmp eq i8 %435, 0
  store i1 %436, i1* %pf
  %437 = zext i32 %421 to i64
  store i64 %437, i64* %rdx
  store volatile i64 13066, i64* @assembly_address
  %438 = load i64* %rdx
  %439 = trunc i64 %438 to i32
  %440 = zext i32 %439 to i64
  store i64 %440, i64* %rax
  store volatile i64 13068, i64* @assembly_address
  %441 = load i64* %rcx
  %442 = trunc i64 %441 to i32
  %443 = zext i32 %442 to i64
  store i64 %443, i64* %rsi
  store volatile i64 13070, i64* @assembly_address
  %444 = load i64* %rax
  %445 = trunc i64 %444 to i32
  %446 = zext i32 %445 to i64
  store i64 %446, i64* %rdi
  store volatile i64 13072, i64* @assembly_address
  %447 = load i64* %rdi
  %448 = load i64* %rsi
  %449 = trunc i64 %447 to i32
  %450 = call i64 @ct_tally(i32 %449, i64 %448)
  store i64 %450, i64* %rax
  store i64 %450, i64* %rax
  store volatile i64 13077, i64* @assembly_address
  %451 = load i64* %rax
  %452 = trunc i64 %451 to i32
  store i32 %452, i32* %stack_var_-20
  store volatile i64 13080, i64* @assembly_address
  %453 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %454 = zext i32 %453 to i64
  store i64 %454, i64* %rax
  store volatile i64 13086, i64* @assembly_address
  %455 = load i64* %rax
  %456 = trunc i64 %455 to i32
  %457 = load i32* %stack_var_-16
  %458 = sub i32 %456, %457
  %459 = and i32 %456, 15
  %460 = and i32 %457, 15
  %461 = sub i32 %459, %460
  %462 = icmp ugt i32 %461, 15
  %463 = icmp ult i32 %456, %457
  %464 = xor i32 %456, %457
  %465 = xor i32 %456, %458
  %466 = and i32 %464, %465
  %467 = icmp slt i32 %466, 0
  store i1 %462, i1* %az
  store i1 %463, i1* %cf
  store i1 %467, i1* %of
  %468 = icmp eq i32 %458, 0
  store i1 %468, i1* %zf
  %469 = icmp slt i32 %458, 0
  store i1 %469, i1* %sf
  %470 = trunc i32 %458 to i8
  %471 = call i8 @llvm.ctpop.i8(i8 %470)
  %472 = and i8 %471, 1
  %473 = icmp eq i8 %472, 0
  store i1 %473, i1* %pf
  %474 = zext i32 %458 to i64
  store i64 %474, i64* %rax
  store volatile i64 13089, i64* @assembly_address
  %475 = load i64* %rax
  %476 = trunc i64 %475 to i32
  store i32 %476, i32* bitcast (i64* @global_var_2165a0 to i32*)
  store volatile i64 13095, i64* @assembly_address
  %477 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %478 = zext i32 %477 to i64
  store i64 %478, i64* %rax
  store volatile i64 13101, i64* @assembly_address
  %479 = load i64* %rax
  %480 = trunc i64 %479 to i32
  %481 = load i64* %rax
  %482 = trunc i64 %481 to i32
  %483 = and i32 %480, %482
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %484 = icmp eq i32 %483, 0
  store i1 %484, i1* %zf
  %485 = icmp slt i32 %483, 0
  store i1 %485, i1* %sf
  %486 = trunc i32 %483 to i8
  %487 = call i8 @llvm.ctpop.i8(i8 %486)
  %488 = and i8 %487, 1
  %489 = icmp eq i8 %488, 0
  store i1 %489, i1* %pf
  store volatile i64 13103, i64* @assembly_address
  %490 = load i1* %zf
  br i1 %490, label %block_3343, label %block_3331

block_3331:                                       ; preds = %block_32f4
  store volatile i64 13105, i64* @assembly_address
  %491 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %492 = zext i32 %491 to i64
  store i64 %492, i64* %rax
  store volatile i64 13111, i64* @assembly_address
  %493 = load i32* %stack_var_-16
  %494 = zext i32 %493 to i64
  store i64 %494, i64* %rdx
  store volatile i64 13114, i64* @assembly_address
  %495 = load i64* %rdx
  %496 = trunc i64 %495 to i32
  %497 = zext i32 %496 to i64
  store i64 %497, i64* %rsi
  store volatile i64 13116, i64* @assembly_address
  %498 = load i64* %rax
  %499 = trunc i64 %498 to i32
  %500 = zext i32 %499 to i64
  store i64 %500, i64* %rdi
  store volatile i64 13118, i64* @assembly_address
  %501 = load i64* %rdi
  %502 = load i64* %rsi
  %503 = trunc i64 %501 to i32
  %504 = call i64 @rsync_roll(i32 %503, i64 %502)
  store i64 %504, i64* %rax
  store i64 %504, i64* %rax
  br label %block_3343

block_3343:                                       ; preds = %block_3331, %block_32f4
  store volatile i64 13123, i64* @assembly_address
  %505 = load i32* bitcast (i64* @global_var_2165a4 to i32*)
  %506 = zext i32 %505 to i64
  store i64 %506, i64* %rax
  store volatile i64 13129, i64* @assembly_address
  %507 = load i32* %stack_var_-16
  %508 = load i64* %rax
  %509 = trunc i64 %508 to i32
  %510 = sub i32 %507, %509
  %511 = and i32 %507, 15
  %512 = and i32 %509, 15
  %513 = sub i32 %511, %512
  %514 = icmp ugt i32 %513, 15
  %515 = icmp ult i32 %507, %509
  %516 = xor i32 %507, %509
  %517 = xor i32 %507, %510
  %518 = and i32 %516, %517
  %519 = icmp slt i32 %518, 0
  store i1 %514, i1* %az
  store i1 %515, i1* %cf
  store i1 %519, i1* %of
  %520 = icmp eq i32 %510, 0
  store i1 %520, i1* %zf
  %521 = icmp slt i32 %510, 0
  store i1 %521, i1* %sf
  %522 = trunc i32 %510 to i8
  %523 = call i8 @llvm.ctpop.i8(i8 %522)
  %524 = and i8 %523, 1
  %525 = icmp eq i8 %524, 0
  store i1 %525, i1* %pf
  store volatile i64 13132, i64* @assembly_address
  %526 = load i1* %cf
  %527 = load i1* %zf
  %528 = or i1 %526, %527
  %529 = icmp ne i1 %528, true
  br i1 %529, label %block_3425, label %block_3352

block_3352:                                       ; preds = %block_3343
  store volatile i64 13138, i64* @assembly_address
  %530 = load i32* %stack_var_-16
  %531 = sub i32 %530, 1
  %532 = and i32 %530, 15
  %533 = sub i32 %532, 1
  %534 = icmp ugt i32 %533, 15
  %535 = icmp ult i32 %530, 1
  %536 = xor i32 %530, 1
  %537 = xor i32 %530, %531
  %538 = and i32 %536, %537
  %539 = icmp slt i32 %538, 0
  store i1 %534, i1* %az
  store i1 %535, i1* %cf
  store i1 %539, i1* %of
  %540 = icmp eq i32 %531, 0
  store i1 %540, i1* %zf
  %541 = icmp slt i32 %531, 0
  store i1 %541, i1* %sf
  %542 = trunc i32 %531 to i8
  %543 = call i8 @llvm.ctpop.i8(i8 %542)
  %544 = and i8 %543, 1
  %545 = icmp eq i8 %544, 0
  store i1 %545, i1* %pf
  store i32 %531, i32* %stack_var_-16
  br label %block_3356

block_3356:                                       ; preds = %block_3356, %block_3352
  store volatile i64 13142, i64* @assembly_address
  %546 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %547 = zext i32 %546 to i64
  store i64 %547, i64* %rax
  store volatile i64 13148, i64* @assembly_address
  %548 = load i64* %rax
  %549 = trunc i64 %548 to i32
  %550 = add i32 %549, 1
  %551 = and i32 %549, 15
  %552 = add i32 %551, 1
  %553 = icmp ugt i32 %552, 15
  %554 = icmp ult i32 %550, %549
  %555 = xor i32 %549, %550
  %556 = xor i32 1, %550
  %557 = and i32 %555, %556
  %558 = icmp slt i32 %557, 0
  store i1 %553, i1* %az
  store i1 %554, i1* %cf
  store i1 %558, i1* %of
  %559 = icmp eq i32 %550, 0
  store i1 %559, i1* %zf
  %560 = icmp slt i32 %550, 0
  store i1 %560, i1* %sf
  %561 = trunc i32 %550 to i8
  %562 = call i8 @llvm.ctpop.i8(i8 %561)
  %563 = and i8 %562, 1
  %564 = icmp eq i8 %563, 0
  store i1 %564, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 13151, i64* @assembly_address
  %565 = load i64* %rax
  %566 = trunc i64 %565 to i32
  store i32 %566, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 13157, i64* @assembly_address
  %567 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %568 = zext i32 %567 to i64
  store i64 %568, i64* %rax
  store volatile i64 13163, i64* @assembly_address
  %569 = load i64* %rax
  %570 = trunc i64 %569 to i32
  %571 = load i1* %of
  %572 = shl i32 %570, 5
  %573 = icmp eq i32 %572, 0
  store i1 %573, i1* %zf
  %574 = icmp slt i32 %572, 0
  store i1 %574, i1* %sf
  %575 = trunc i32 %572 to i8
  %576 = call i8 @llvm.ctpop.i8(i8 %575)
  %577 = and i8 %576, 1
  %578 = icmp eq i8 %577, 0
  store i1 %578, i1* %pf
  %579 = zext i32 %572 to i64
  store i64 %579, i64* %rax
  %580 = shl i32 %570, 4
  %581 = lshr i32 %580, 31
  %582 = trunc i32 %581 to i1
  store i1 %582, i1* %cf
  %583 = lshr i32 %572, 31
  %584 = icmp ne i32 %583, %581
  %585 = select i1 false, i1 %584, i1 %571
  store i1 %585, i1* %of
  store volatile i64 13166, i64* @assembly_address
  %586 = load i64* %rax
  %587 = trunc i64 %586 to i32
  %588 = zext i32 %587 to i64
  store i64 %588, i64* %rdx
  store volatile i64 13168, i64* @assembly_address
  %589 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %590 = zext i32 %589 to i64
  store i64 %590, i64* %rax
  store volatile i64 13174, i64* @assembly_address
  %591 = load i64* %rax
  %592 = trunc i64 %591 to i32
  %593 = add i32 %592, 2
  %594 = and i32 %592, 15
  %595 = add i32 %594, 2
  %596 = icmp ugt i32 %595, 15
  %597 = icmp ult i32 %593, %592
  %598 = xor i32 %592, %593
  %599 = xor i32 2, %593
  %600 = and i32 %598, %599
  %601 = icmp slt i32 %600, 0
  store i1 %596, i1* %az
  store i1 %597, i1* %cf
  store i1 %601, i1* %of
  %602 = icmp eq i32 %593, 0
  store i1 %602, i1* %zf
  %603 = icmp slt i32 %593, 0
  store i1 %603, i1* %sf
  %604 = trunc i32 %593 to i8
  %605 = call i8 @llvm.ctpop.i8(i8 %604)
  %606 = and i8 %605, 1
  %607 = icmp eq i8 %606, 0
  store i1 %607, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a42a to i64), i64* %rax
  store volatile i64 13177, i64* @assembly_address
  %608 = load i64* %rax
  %609 = trunc i64 %608 to i32
  %610 = zext i32 %609 to i64
  store i64 %610, i64* %rcx
  store volatile i64 13179, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13186, i64* @assembly_address
  %611 = load i64* %rcx
  %612 = load i64* %rax
  %613 = mul i64 %612, 1
  %614 = add i64 %611, %613
  %615 = inttoptr i64 %614 to i8*
  %616 = load i8* %615
  %617 = zext i8 %616 to i64
  store i64 %617, i64* %rax
  store volatile i64 13190, i64* @assembly_address
  %618 = load i64* %rax
  %619 = trunc i64 %618 to i8
  %620 = zext i8 %619 to i64
  store i64 %620, i64* %rax
  store volatile i64 13193, i64* @assembly_address
  %621 = load i64* %rax
  %622 = trunc i64 %621 to i32
  %623 = load i64* %rdx
  %624 = trunc i64 %623 to i32
  %625 = xor i32 %622, %624
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %626 = icmp eq i32 %625, 0
  store i1 %626, i1* %zf
  %627 = icmp slt i32 %625, 0
  store i1 %627, i1* %sf
  %628 = trunc i32 %625 to i8
  %629 = call i8 @llvm.ctpop.i8(i8 %628)
  %630 = and i8 %629, 1
  %631 = icmp eq i8 %630, 0
  store i1 %631, i1* %pf
  %632 = zext i32 %625 to i64
  store i64 %632, i64* %rax
  store volatile i64 13195, i64* @assembly_address
  %633 = load i64* %rax
  %634 = trunc i64 %633 to i32
  %635 = and i32 %634, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %636 = icmp eq i32 %635, 0
  store i1 %636, i1* %zf
  %637 = icmp slt i32 %635, 0
  store i1 %637, i1* %sf
  %638 = trunc i32 %635 to i8
  %639 = call i8 @llvm.ctpop.i8(i8 %638)
  %640 = and i8 %639, 1
  %641 = icmp eq i8 %640, 0
  store i1 %641, i1* %pf
  %642 = zext i32 %635 to i64
  store i64 %642, i64* %rax
  store volatile i64 13200, i64* @assembly_address
  %643 = load i64* %rax
  %644 = trunc i64 %643 to i32
  store i32 %644, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 13206, i64* @assembly_address
  %645 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %646 = zext i32 %645 to i64
  store i64 %646, i64* %rax
  store volatile i64 13212, i64* @assembly_address
  %647 = load i64* %rax
  %648 = trunc i64 %647 to i32
  %649 = zext i32 %648 to i64
  store i64 %649, i64* %rax
  store volatile i64 13214, i64* @assembly_address
  %650 = load i64* %rax
  %651 = add i64 %650, 32768
  %652 = and i64 %650, 15
  %653 = icmp ugt i64 %652, 15
  %654 = icmp ult i64 %651, %650
  %655 = xor i64 %650, %651
  %656 = xor i64 32768, %651
  %657 = and i64 %655, %656
  %658 = icmp slt i64 %657, 0
  store i1 %653, i1* %az
  store i1 %654, i1* %cf
  store i1 %658, i1* %of
  %659 = icmp eq i64 %651, 0
  store i1 %659, i1* %zf
  %660 = icmp slt i64 %651, 0
  store i1 %660, i1* %sf
  %661 = trunc i64 %651 to i8
  %662 = call i8 @llvm.ctpop.i8(i8 %661)
  %663 = and i8 %662, 1
  %664 = icmp eq i8 %663, 0
  store i1 %664, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 13220, i64* @assembly_address
  %665 = load i64* %rax
  %666 = load i64* %rax
  %667 = mul i64 %666, 1
  %668 = add i64 %665, %667
  store i64 %668, i64* %rdx
  store volatile i64 13224, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 13231, i64* @assembly_address
  %669 = load i64* %rax
  %670 = load i64* %rdx
  %671 = add i64 %669, %670
  %672 = and i64 %669, 15
  %673 = and i64 %670, 15
  %674 = add i64 %672, %673
  %675 = icmp ugt i64 %674, 15
  %676 = icmp ult i64 %671, %669
  %677 = xor i64 %669, %671
  %678 = xor i64 %670, %671
  %679 = and i64 %677, %678
  %680 = icmp slt i64 %679, 0
  store i1 %675, i1* %az
  store i1 %676, i1* %cf
  store i1 %680, i1* %of
  %681 = icmp eq i64 %671, 0
  store i1 %681, i1* %zf
  %682 = icmp slt i64 %671, 0
  store i1 %682, i1* %sf
  %683 = trunc i64 %671 to i8
  %684 = call i8 @llvm.ctpop.i8(i8 %683)
  %685 = and i8 %684, 1
  %686 = icmp eq i8 %685, 0
  store i1 %686, i1* %pf
  store i64 %671, i64* %rax
  store volatile i64 13234, i64* @assembly_address
  %687 = load i64* %rax
  %688 = inttoptr i64 %687 to i16*
  %689 = load i16* %688
  %690 = zext i16 %689 to i64
  store i64 %690, i64* %rax
  store volatile i64 13237, i64* @assembly_address
  %691 = load i64* %rax
  %692 = trunc i64 %691 to i16
  %693 = zext i16 %692 to i64
  store i64 %693, i64* %rax
  store volatile i64 13240, i64* @assembly_address
  %694 = load i64* %rax
  %695 = trunc i64 %694 to i32
  store i32 %695, i32* %stack_var_-12
  store volatile i64 13243, i64* @assembly_address
  %696 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %697 = zext i32 %696 to i64
  store i64 %697, i64* %rax
  store volatile i64 13249, i64* @assembly_address
  %698 = load i64* %rax
  %699 = trunc i64 %698 to i32
  %700 = and i32 %699, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %701 = icmp eq i32 %700, 0
  store i1 %701, i1* %zf
  %702 = icmp slt i32 %700, 0
  store i1 %702, i1* %sf
  %703 = trunc i32 %700 to i8
  %704 = call i8 @llvm.ctpop.i8(i8 %703)
  %705 = and i8 %704, 1
  %706 = icmp eq i8 %705, 0
  store i1 %706, i1* %pf
  %707 = zext i32 %700 to i64
  store i64 %707, i64* %rax
  store volatile i64 13254, i64* @assembly_address
  %708 = load i64* %rax
  %709 = trunc i64 %708 to i32
  %710 = zext i32 %709 to i64
  store i64 %710, i64* %rdx
  store volatile i64 13256, i64* @assembly_address
  %711 = load i32* %stack_var_-12
  %712 = zext i32 %711 to i64
  store i64 %712, i64* %rax
  store volatile i64 13259, i64* @assembly_address
  %713 = load i64* %rax
  %714 = trunc i64 %713 to i32
  %715 = zext i32 %714 to i64
  store i64 %715, i64* %rcx
  store volatile i64 13261, i64* @assembly_address
  %716 = load i64* %rdx
  %717 = trunc i64 %716 to i32
  %718 = zext i32 %717 to i64
  store i64 %718, i64* %rax
  store volatile i64 13263, i64* @assembly_address
  %719 = load i64* %rax
  %720 = load i64* %rax
  %721 = mul i64 %720, 1
  %722 = add i64 %719, %721
  store i64 %722, i64* %rdx
  store volatile i64 13267, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 13274, i64* @assembly_address
  %723 = load i64* %rcx
  %724 = trunc i64 %723 to i16
  %725 = load i64* %rdx
  %726 = load i64* %rax
  %727 = mul i64 %726, 1
  %728 = add i64 %725, %727
  %729 = inttoptr i64 %728 to i16*
  store i16 %724, i16* %729
  store volatile i64 13278, i64* @assembly_address
  %730 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %731 = zext i32 %730 to i64
  store i64 %731, i64* %rdx
  store volatile i64 13284, i64* @assembly_address
  %732 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %733 = zext i32 %732 to i64
  store i64 %733, i64* %rax
  store volatile i64 13290, i64* @assembly_address
  %734 = load i64* %rax
  %735 = trunc i64 %734 to i32
  %736 = zext i32 %735 to i64
  store i64 %736, i64* %rax
  store volatile i64 13292, i64* @assembly_address
  %737 = load i64* %rax
  %738 = add i64 %737, 32768
  %739 = and i64 %737, 15
  %740 = icmp ugt i64 %739, 15
  %741 = icmp ult i64 %738, %737
  %742 = xor i64 %737, %738
  %743 = xor i64 32768, %738
  %744 = and i64 %742, %743
  %745 = icmp slt i64 %744, 0
  store i1 %740, i1* %az
  store i1 %741, i1* %cf
  store i1 %745, i1* %of
  %746 = icmp eq i64 %738, 0
  store i1 %746, i1* %zf
  %747 = icmp slt i64 %738, 0
  store i1 %747, i1* %sf
  %748 = trunc i64 %738 to i8
  %749 = call i8 @llvm.ctpop.i8(i8 %748)
  %750 = and i8 %749, 1
  %751 = icmp eq i8 %750, 0
  store i1 %751, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 13298, i64* @assembly_address
  %752 = load i64* %rax
  %753 = load i64* %rax
  %754 = mul i64 %753, 1
  %755 = add i64 %752, %754
  store i64 %755, i64* %rcx
  store volatile i64 13302, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 13309, i64* @assembly_address
  %756 = load i64* %rax
  %757 = load i64* %rcx
  %758 = add i64 %756, %757
  %759 = and i64 %756, 15
  %760 = and i64 %757, 15
  %761 = add i64 %759, %760
  %762 = icmp ugt i64 %761, 15
  %763 = icmp ult i64 %758, %756
  %764 = xor i64 %756, %758
  %765 = xor i64 %757, %758
  %766 = and i64 %764, %765
  %767 = icmp slt i64 %766, 0
  store i1 %762, i1* %az
  store i1 %763, i1* %cf
  store i1 %767, i1* %of
  %768 = icmp eq i64 %758, 0
  store i1 %768, i1* %zf
  %769 = icmp slt i64 %758, 0
  store i1 %769, i1* %sf
  %770 = trunc i64 %758 to i8
  %771 = call i8 @llvm.ctpop.i8(i8 %770)
  %772 = and i8 %771, 1
  %773 = icmp eq i8 %772, 0
  store i1 %773, i1* %pf
  store i64 %758, i64* %rax
  store volatile i64 13312, i64* @assembly_address
  %774 = load i64* %rdx
  %775 = trunc i64 %774 to i16
  %776 = load i64* %rax
  %777 = inttoptr i64 %776 to i16*
  store i16 %775, i16* %777
  store volatile i64 13315, i64* @assembly_address
  %778 = load i32* %stack_var_-16
  %779 = sub i32 %778, 1
  %780 = and i32 %778, 15
  %781 = sub i32 %780, 1
  %782 = icmp ugt i32 %781, 15
  %783 = icmp ult i32 %778, 1
  %784 = xor i32 %778, 1
  %785 = xor i32 %778, %779
  %786 = and i32 %784, %785
  %787 = icmp slt i32 %786, 0
  store i1 %782, i1* %az
  store i1 %783, i1* %cf
  store i1 %787, i1* %of
  %788 = icmp eq i32 %779, 0
  store i1 %788, i1* %zf
  %789 = icmp slt i32 %779, 0
  store i1 %789, i1* %sf
  %790 = trunc i32 %779 to i8
  %791 = call i8 @llvm.ctpop.i8(i8 %790)
  %792 = and i8 %791, 1
  %793 = icmp eq i8 %792, 0
  store i1 %793, i1* %pf
  store i32 %779, i32* %stack_var_-16
  store volatile i64 13319, i64* @assembly_address
  %794 = load i32* %stack_var_-16
  %795 = and i32 %794, 15
  %796 = icmp ugt i32 %795, 15
  %797 = icmp ult i32 %794, 0
  %798 = xor i32 %794, 0
  %799 = and i32 %798, 0
  %800 = icmp slt i32 %799, 0
  store i1 %796, i1* %az
  store i1 %797, i1* %cf
  store i1 %800, i1* %of
  %801 = icmp eq i32 %794, 0
  store i1 %801, i1* %zf
  %802 = icmp slt i32 %794, 0
  store i1 %802, i1* %sf
  %803 = trunc i32 %794 to i8
  %804 = call i8 @llvm.ctpop.i8(i8 %803)
  %805 = and i8 %804, 1
  %806 = icmp eq i8 %805, 0
  store i1 %806, i1* %pf
  store volatile i64 13323, i64* @assembly_address
  %807 = load i1* %zf
  %808 = icmp eq i1 %807, false
  br i1 %808, label %block_3356, label %block_3411

block_3411:                                       ; preds = %block_3356
  store volatile i64 13329, i64* @assembly_address
  %809 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %810 = zext i32 %809 to i64
  store i64 %810, i64* %rax
  store volatile i64 13335, i64* @assembly_address
  %811 = load i64* %rax
  %812 = trunc i64 %811 to i32
  %813 = add i32 %812, 1
  %814 = and i32 %812, 15
  %815 = add i32 %814, 1
  %816 = icmp ugt i32 %815, 15
  %817 = icmp ult i32 %813, %812
  %818 = xor i32 %812, %813
  %819 = xor i32 1, %813
  %820 = and i32 %818, %819
  %821 = icmp slt i32 %820, 0
  store i1 %816, i1* %az
  store i1 %817, i1* %cf
  store i1 %821, i1* %of
  %822 = icmp eq i32 %813, 0
  store i1 %822, i1* %zf
  %823 = icmp slt i32 %813, 0
  store i1 %823, i1* %sf
  %824 = trunc i32 %813 to i8
  %825 = call i8 @llvm.ctpop.i8(i8 %824)
  %826 = and i8 %825, 1
  %827 = icmp eq i8 %826, 0
  store i1 %827, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 13338, i64* @assembly_address
  %828 = load i64* %rax
  %829 = trunc i64 %828 to i32
  store i32 %829, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 13344, i64* @assembly_address
  br label %block_34eb

block_3425:                                       ; preds = %block_3343
  store volatile i64 13349, i64* @assembly_address
  %830 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %831 = zext i32 %830 to i64
  store i64 %831, i64* %rdx
  store volatile i64 13355, i64* @assembly_address
  %832 = load i32* %stack_var_-16
  %833 = zext i32 %832 to i64
  store i64 %833, i64* %rax
  store volatile i64 13358, i64* @assembly_address
  %834 = load i64* %rax
  %835 = trunc i64 %834 to i32
  %836 = load i64* %rdx
  %837 = trunc i64 %836 to i32
  %838 = add i32 %835, %837
  %839 = and i32 %835, 15
  %840 = and i32 %837, 15
  %841 = add i32 %839, %840
  %842 = icmp ugt i32 %841, 15
  %843 = icmp ult i32 %838, %835
  %844 = xor i32 %835, %838
  %845 = xor i32 %837, %838
  %846 = and i32 %844, %845
  %847 = icmp slt i32 %846, 0
  store i1 %842, i1* %az
  store i1 %843, i1* %cf
  store i1 %847, i1* %of
  %848 = icmp eq i32 %838, 0
  store i1 %848, i1* %zf
  %849 = icmp slt i32 %838, 0
  store i1 %849, i1* %sf
  %850 = trunc i32 %838 to i8
  %851 = call i8 @llvm.ctpop.i8(i8 %850)
  %852 = and i8 %851, 1
  %853 = icmp eq i8 %852, 0
  store i1 %853, i1* %pf
  %854 = zext i32 %838 to i64
  store i64 %854, i64* %rax
  store volatile i64 13360, i64* @assembly_address
  %855 = load i64* %rax
  %856 = trunc i64 %855 to i32
  store i32 %856, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 13366, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 13373, i64* @assembly_address
  %857 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %858 = zext i32 %857 to i64
  store i64 %858, i64* %rax
  store volatile i64 13379, i64* @assembly_address
  %859 = load i64* %rax
  %860 = trunc i64 %859 to i32
  %861 = zext i32 %860 to i64
  store i64 %861, i64* %rdx
  store volatile i64 13381, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13388, i64* @assembly_address
  %862 = load i64* %rdx
  %863 = load i64* %rax
  %864 = mul i64 %863, 1
  %865 = add i64 %862, %864
  %866 = inttoptr i64 %865 to i8*
  %867 = load i8* %866
  %868 = zext i8 %867 to i64
  store i64 %868, i64* %rax
  store volatile i64 13392, i64* @assembly_address
  %869 = load i64* %rax
  %870 = trunc i64 %869 to i8
  %871 = zext i8 %870 to i64
  store i64 %871, i64* %rax
  store volatile i64 13395, i64* @assembly_address
  %872 = load i64* %rax
  %873 = trunc i64 %872 to i32
  store i32 %873, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 13401, i64* @assembly_address
  %874 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %875 = zext i32 %874 to i64
  store i64 %875, i64* %rax
  store volatile i64 13407, i64* @assembly_address
  %876 = load i64* %rax
  %877 = trunc i64 %876 to i32
  %878 = load i1* %of
  %879 = shl i32 %877, 5
  %880 = icmp eq i32 %879, 0
  store i1 %880, i1* %zf
  %881 = icmp slt i32 %879, 0
  store i1 %881, i1* %sf
  %882 = trunc i32 %879 to i8
  %883 = call i8 @llvm.ctpop.i8(i8 %882)
  %884 = and i8 %883, 1
  %885 = icmp eq i8 %884, 0
  store i1 %885, i1* %pf
  %886 = zext i32 %879 to i64
  store i64 %886, i64* %rax
  %887 = shl i32 %877, 4
  %888 = lshr i32 %887, 31
  %889 = trunc i32 %888 to i1
  store i1 %889, i1* %cf
  %890 = lshr i32 %879, 31
  %891 = icmp ne i32 %890, %888
  %892 = select i1 false, i1 %891, i1 %878
  store i1 %892, i1* %of
  store volatile i64 13410, i64* @assembly_address
  %893 = load i64* %rax
  %894 = trunc i64 %893 to i32
  %895 = zext i32 %894 to i64
  store i64 %895, i64* %rdx
  store volatile i64 13412, i64* @assembly_address
  %896 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %897 = zext i32 %896 to i64
  store i64 %897, i64* %rax
  store volatile i64 13418, i64* @assembly_address
  %898 = load i64* %rax
  %899 = trunc i64 %898 to i32
  %900 = add i32 %899, 1
  %901 = and i32 %899, 15
  %902 = add i32 %901, 1
  %903 = icmp ugt i32 %902, 15
  %904 = icmp ult i32 %900, %899
  %905 = xor i32 %899, %900
  %906 = xor i32 1, %900
  %907 = and i32 %905, %906
  %908 = icmp slt i32 %907, 0
  store i1 %903, i1* %az
  store i1 %904, i1* %cf
  store i1 %908, i1* %of
  %909 = icmp eq i32 %900, 0
  store i1 %909, i1* %zf
  %910 = icmp slt i32 %900, 0
  store i1 %910, i1* %sf
  %911 = trunc i32 %900 to i8
  %912 = call i8 @llvm.ctpop.i8(i8 %911)
  %913 = and i8 %912, 1
  %914 = icmp eq i8 %913, 0
  store i1 %914, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 13421, i64* @assembly_address
  %915 = load i64* %rax
  %916 = trunc i64 %915 to i32
  %917 = zext i32 %916 to i64
  store i64 %917, i64* %rcx
  store volatile i64 13423, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13430, i64* @assembly_address
  %918 = load i64* %rcx
  %919 = load i64* %rax
  %920 = mul i64 %919, 1
  %921 = add i64 %918, %920
  %922 = inttoptr i64 %921 to i8*
  %923 = load i8* %922
  %924 = zext i8 %923 to i64
  store i64 %924, i64* %rax
  store volatile i64 13434, i64* @assembly_address
  %925 = load i64* %rax
  %926 = trunc i64 %925 to i8
  %927 = zext i8 %926 to i64
  store i64 %927, i64* %rax
  store volatile i64 13437, i64* @assembly_address
  %928 = load i64* %rax
  %929 = trunc i64 %928 to i32
  %930 = load i64* %rdx
  %931 = trunc i64 %930 to i32
  %932 = xor i32 %929, %931
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %933 = icmp eq i32 %932, 0
  store i1 %933, i1* %zf
  %934 = icmp slt i32 %932, 0
  store i1 %934, i1* %sf
  %935 = trunc i32 %932 to i8
  %936 = call i8 @llvm.ctpop.i8(i8 %935)
  %937 = and i8 %936, 1
  %938 = icmp eq i8 %937, 0
  store i1 %938, i1* %pf
  %939 = zext i32 %932 to i64
  store i64 %939, i64* %rax
  store volatile i64 13439, i64* @assembly_address
  %940 = load i64* %rax
  %941 = trunc i64 %940 to i32
  %942 = and i32 %941, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %943 = icmp eq i32 %942, 0
  store i1 %943, i1* %zf
  %944 = icmp slt i32 %942, 0
  store i1 %944, i1* %sf
  %945 = trunc i32 %942 to i8
  %946 = call i8 @llvm.ctpop.i8(i8 %945)
  %947 = and i8 %946, 1
  %948 = icmp eq i8 %947, 0
  store i1 %948, i1* %pf
  %949 = zext i32 %942 to i64
  store i64 %949, i64* %rax
  store volatile i64 13444, i64* @assembly_address
  %950 = load i64* %rax
  %951 = trunc i64 %950 to i32
  store i32 %951, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 13450, i64* @assembly_address
  br label %block_34eb

block_348c:                                       ; preds = %block_32ea
  store volatile i64 13452, i64* @assembly_address
  %952 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %953 = zext i32 %952 to i64
  store i64 %953, i64* %rax
  store volatile i64 13458, i64* @assembly_address
  %954 = load i64* %rax
  %955 = trunc i64 %954 to i32
  %956 = zext i32 %955 to i64
  store i64 %956, i64* %rdx
  store volatile i64 13460, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13467, i64* @assembly_address
  %957 = load i64* %rdx
  %958 = load i64* %rax
  %959 = mul i64 %958, 1
  %960 = add i64 %957, %959
  %961 = inttoptr i64 %960 to i8*
  %962 = load i8* %961
  %963 = zext i8 %962 to i64
  store i64 %963, i64* %rax
  store volatile i64 13471, i64* @assembly_address
  %964 = load i64* %rax
  %965 = trunc i64 %964 to i8
  %966 = zext i8 %965 to i64
  store i64 %966, i64* %rax
  store volatile i64 13474, i64* @assembly_address
  %967 = load i64* %rax
  %968 = trunc i64 %967 to i32
  %969 = zext i32 %968 to i64
  store i64 %969, i64* %rsi
  store volatile i64 13476, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 13481, i64* @assembly_address
  %970 = load i64* %rdi
  %971 = load i64* %rsi
  %972 = trunc i64 %970 to i32
  %973 = call i64 @ct_tally(i32 %972, i64 %971)
  store i64 %973, i64* %rax
  store i64 %973, i64* %rax
  store volatile i64 13486, i64* @assembly_address
  %974 = load i64* %rax
  %975 = trunc i64 %974 to i32
  store i32 %975, i32* %stack_var_-20
  store volatile i64 13489, i64* @assembly_address
  %976 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %977 = zext i32 %976 to i64
  store i64 %977, i64* %rax
  store volatile i64 13495, i64* @assembly_address
  %978 = load i64* %rax
  %979 = trunc i64 %978 to i32
  %980 = load i64* %rax
  %981 = trunc i64 %980 to i32
  %982 = and i32 %979, %981
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %983 = icmp eq i32 %982, 0
  store i1 %983, i1* %zf
  %984 = icmp slt i32 %982, 0
  store i1 %984, i1* %sf
  %985 = trunc i32 %982 to i8
  %986 = call i8 @llvm.ctpop.i8(i8 %985)
  %987 = and i8 %986, 1
  %988 = icmp eq i8 %987, 0
  store i1 %988, i1* %pf
  store volatile i64 13497, i64* @assembly_address
  %989 = load i1* %zf
  br i1 %989, label %block_34cd, label %block_34bb

block_34bb:                                       ; preds = %block_348c
  store volatile i64 13499, i64* @assembly_address
  %990 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %991 = zext i32 %990 to i64
  store i64 %991, i64* %rax
  store volatile i64 13505, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 13510, i64* @assembly_address
  %992 = load i64* %rax
  %993 = trunc i64 %992 to i32
  %994 = zext i32 %993 to i64
  store i64 %994, i64* %rdi
  store volatile i64 13512, i64* @assembly_address
  %995 = load i64* %rdi
  %996 = load i64* %rsi
  %997 = trunc i64 %995 to i32
  %998 = call i64 @rsync_roll(i32 %997, i64 %996)
  store i64 %998, i64* %rax
  store i64 %998, i64* %rax
  br label %block_34cd

block_34cd:                                       ; preds = %block_34bb, %block_348c
  store volatile i64 13517, i64* @assembly_address
  %999 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %1000 = zext i32 %999 to i64
  store i64 %1000, i64* %rax
  store volatile i64 13523, i64* @assembly_address
  %1001 = load i64* %rax
  %1002 = trunc i64 %1001 to i32
  %1003 = sub i32 %1002, 1
  %1004 = and i32 %1002, 15
  %1005 = sub i32 %1004, 1
  %1006 = icmp ugt i32 %1005, 15
  %1007 = icmp ult i32 %1002, 1
  %1008 = xor i32 %1002, 1
  %1009 = xor i32 %1002, %1003
  %1010 = and i32 %1008, %1009
  %1011 = icmp slt i32 %1010, 0
  store i1 %1006, i1* %az
  store i1 %1007, i1* %cf
  store i1 %1011, i1* %of
  %1012 = icmp eq i32 %1003, 0
  store i1 %1012, i1* %zf
  %1013 = icmp slt i32 %1003, 0
  store i1 %1013, i1* %sf
  %1014 = trunc i32 %1003 to i8
  %1015 = call i8 @llvm.ctpop.i8(i8 %1014)
  %1016 = and i8 %1015, 1
  %1017 = icmp eq i8 %1016, 0
  store i1 %1017, i1* %pf
  %1018 = zext i32 %1003 to i64
  store i64 %1018, i64* %rax
  store volatile i64 13526, i64* @assembly_address
  %1019 = load i64* %rax
  %1020 = trunc i64 %1019 to i32
  store i32 %1020, i32* bitcast (i64* @global_var_2165a0 to i32*)
  store volatile i64 13532, i64* @assembly_address
  %1021 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1022 = zext i32 %1021 to i64
  store i64 %1022, i64* %rax
  store volatile i64 13538, i64* @assembly_address
  %1023 = load i64* %rax
  %1024 = trunc i64 %1023 to i32
  %1025 = add i32 %1024, 1
  %1026 = and i32 %1024, 15
  %1027 = add i32 %1026, 1
  %1028 = icmp ugt i32 %1027, 15
  %1029 = icmp ult i32 %1025, %1024
  %1030 = xor i32 %1024, %1025
  %1031 = xor i32 1, %1025
  %1032 = and i32 %1030, %1031
  %1033 = icmp slt i32 %1032, 0
  store i1 %1028, i1* %az
  store i1 %1029, i1* %cf
  store i1 %1033, i1* %of
  %1034 = icmp eq i32 %1025, 0
  store i1 %1034, i1* %zf
  %1035 = icmp slt i32 %1025, 0
  store i1 %1035, i1* %sf
  %1036 = trunc i32 %1025 to i8
  %1037 = call i8 @llvm.ctpop.i8(i8 %1036)
  %1038 = and i8 %1037, 1
  %1039 = icmp eq i8 %1038, 0
  store i1 %1039, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 13541, i64* @assembly_address
  %1040 = load i64* %rax
  %1041 = trunc i64 %1040 to i32
  store i32 %1041, i32* bitcast (i64* @global_var_21a428 to i32*)
  br label %block_34eb

block_34eb:                                       ; preds = %block_34cd, %block_3425, %block_3411
  store volatile i64 13547, i64* @assembly_address
  %1042 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %1043 = zext i32 %1042 to i64
  store i64 %1043, i64* %rax
  store volatile i64 13553, i64* @assembly_address
  %1044 = load i64* %rax
  %1045 = trunc i64 %1044 to i32
  %1046 = load i64* %rax
  %1047 = trunc i64 %1046 to i32
  %1048 = and i32 %1045, %1047
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1049 = icmp eq i32 %1048, 0
  store i1 %1049, i1* %zf
  %1050 = icmp slt i32 %1048, 0
  store i1 %1050, i1* %sf
  %1051 = trunc i32 %1048 to i8
  %1052 = call i8 @llvm.ctpop.i8(i8 %1051)
  %1053 = and i8 %1052, 1
  %1054 = icmp eq i8 %1053, 0
  store i1 %1054, i1* %pf
  store volatile i64 13555, i64* @assembly_address
  %1055 = load i1* %zf
  br i1 %1055, label %block_351c, label %block_34f5

block_34f5:                                       ; preds = %block_34eb
  store volatile i64 13557, i64* @assembly_address
  %1056 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1057 = zext i32 %1056 to i64
  store i64 %1057, i64* %rax
  store volatile i64 13563, i64* @assembly_address
  %1058 = load i64* %rax
  %1059 = trunc i64 %1058 to i32
  %1060 = zext i32 %1059 to i64
  store i64 %1060, i64* %rdx
  store volatile i64 13565, i64* @assembly_address
  %1061 = load i64* @global_var_2165b8
  store i64 %1061, i64* %rax
  store volatile i64 13572, i64* @assembly_address
  %1062 = load i64* %rdx
  %1063 = load i64* %rax
  %1064 = sub i64 %1062, %1063
  %1065 = and i64 %1062, 15
  %1066 = and i64 %1063, 15
  %1067 = sub i64 %1065, %1066
  %1068 = icmp ugt i64 %1067, 15
  %1069 = icmp ult i64 %1062, %1063
  %1070 = xor i64 %1062, %1063
  %1071 = xor i64 %1062, %1064
  %1072 = and i64 %1070, %1071
  %1073 = icmp slt i64 %1072, 0
  store i1 %1068, i1* %az
  store i1 %1069, i1* %cf
  store i1 %1073, i1* %of
  %1074 = icmp eq i64 %1064, 0
  store i1 %1074, i1* %zf
  %1075 = icmp slt i64 %1064, 0
  store i1 %1075, i1* %sf
  %1076 = trunc i64 %1064 to i8
  %1077 = call i8 @llvm.ctpop.i8(i8 %1076)
  %1078 = and i8 %1077, 1
  %1079 = icmp eq i8 %1078, 0
  store i1 %1079, i1* %pf
  store volatile i64 13575, i64* @assembly_address
  %1080 = load i1* %cf
  %1081 = load i1* %zf
  %1082 = or i1 %1080, %1081
  br i1 %1082, label %block_351c, label %block_3509

block_3509:                                       ; preds = %block_34f5
  store volatile i64 13577, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 13582, i64* @assembly_address
  %1083 = load i64* %rax
  store i64 %1083, i64* @global_var_2165b8
  store volatile i64 13589, i64* @assembly_address
  store i32 2, i32* %stack_var_-20
  br label %block_351c

block_351c:                                       ; preds = %block_3509, %block_34f5, %block_34eb
  store volatile i64 13596, i64* @assembly_address
  %1084 = load i32* %stack_var_-20
  %1085 = and i32 %1084, 15
  %1086 = icmp ugt i32 %1085, 15
  %1087 = icmp ult i32 %1084, 0
  %1088 = xor i32 %1084, 0
  %1089 = and i32 %1088, 0
  %1090 = icmp slt i32 %1089, 0
  store i1 %1086, i1* %az
  store i1 %1087, i1* %cf
  store i1 %1090, i1* %of
  %1091 = icmp eq i32 %1084, 0
  store i1 %1091, i1* %zf
  %1092 = icmp slt i32 %1084, 0
  store i1 %1092, i1* %sf
  %1093 = trunc i32 %1084 to i8
  %1094 = call i8 @llvm.ctpop.i8(i8 %1093)
  %1095 = and i8 %1094, 1
  %1096 = icmp eq i8 %1095, 0
  store i1 %1096, i1* %pf
  store volatile i64 13600, i64* @assembly_address
  %1097 = load i1* %zf
  br i1 %1097, label %block_3589, label %block_3522

block_3522:                                       ; preds = %block_351c
  store volatile i64 13602, i64* @assembly_address
  %1098 = load i32* %stack_var_-20
  %1099 = zext i32 %1098 to i64
  store i64 %1099, i64* %rax
  store volatile i64 13605, i64* @assembly_address
  %1100 = load i64* %rax
  %1101 = add i64 %1100, -1
  %1102 = trunc i64 %1101 to i32
  %1103 = zext i32 %1102 to i64
  store i64 %1103, i64* %rdx
  store volatile i64 13608, i64* @assembly_address
  %1104 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1105 = zext i32 %1104 to i64
  store i64 %1105, i64* %rax
  store volatile i64 13614, i64* @assembly_address
  %1106 = load i64* %rax
  %1107 = trunc i64 %1106 to i32
  %1108 = zext i32 %1107 to i64
  store i64 %1108, i64* %rcx
  store volatile i64 13616, i64* @assembly_address
  %1109 = load i64* @global_var_21a430
  store i64 %1109, i64* %rax
  store volatile i64 13623, i64* @assembly_address
  %1110 = load i64* %rcx
  %1111 = load i64* %rax
  %1112 = sub i64 %1110, %1111
  %1113 = and i64 %1110, 15
  %1114 = and i64 %1111, 15
  %1115 = sub i64 %1113, %1114
  %1116 = icmp ugt i64 %1115, 15
  %1117 = icmp ult i64 %1110, %1111
  %1118 = xor i64 %1110, %1111
  %1119 = xor i64 %1110, %1112
  %1120 = and i64 %1118, %1119
  %1121 = icmp slt i64 %1120, 0
  store i1 %1116, i1* %az
  store i1 %1117, i1* %cf
  store i1 %1121, i1* %of
  %1122 = icmp eq i64 %1112, 0
  store i1 %1122, i1* %zf
  %1123 = icmp slt i64 %1112, 0
  store i1 %1123, i1* %sf
  %1124 = trunc i64 %1112 to i8
  %1125 = call i8 @llvm.ctpop.i8(i8 %1124)
  %1126 = and i8 %1125, 1
  %1127 = icmp eq i8 %1126, 0
  store i1 %1127, i1* %pf
  store i64 %1112, i64* %rcx
  store volatile i64 13626, i64* @assembly_address
  %1128 = load i64* %rcx
  store i64 %1128, i64* %rax
  store volatile i64 13629, i64* @assembly_address
  %1129 = load i64* %rax
  store i64 %1129, i64* %rsi
  store volatile i64 13632, i64* @assembly_address
  %1130 = load i64* @global_var_21a430
  store i64 %1130, i64* %rax
  store volatile i64 13639, i64* @assembly_address
  %1131 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1132 = icmp eq i64 %1131, 0
  store i1 %1132, i1* %zf
  %1133 = icmp slt i64 %1131, 0
  store i1 %1133, i1* %sf
  %1134 = trunc i64 %1131 to i8
  %1135 = call i8 @llvm.ctpop.i8(i8 %1134)
  %1136 = and i8 %1135, 1
  %1137 = icmp eq i8 %1136, 0
  store i1 %1137, i1* %pf
  store volatile i64 13642, i64* @assembly_address
  %1138 = load i1* %sf
  br i1 %1138, label %block_3561, label %block_354c

block_354c:                                       ; preds = %block_3522
  store volatile i64 13644, i64* @assembly_address
  %1139 = load i64* @global_var_21a430
  store i64 %1139, i64* %rax
  store volatile i64 13651, i64* @assembly_address
  %1140 = load i64* %rax
  %1141 = trunc i64 %1140 to i32
  %1142 = zext i32 %1141 to i64
  store i64 %1142, i64* %rcx
  store volatile i64 13653, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13660, i64* @assembly_address
  %1143 = load i64* %rax
  %1144 = load i64* %rcx
  %1145 = add i64 %1143, %1144
  %1146 = and i64 %1143, 15
  %1147 = and i64 %1144, 15
  %1148 = add i64 %1146, %1147
  %1149 = icmp ugt i64 %1148, 15
  %1150 = icmp ult i64 %1145, %1143
  %1151 = xor i64 %1143, %1145
  %1152 = xor i64 %1144, %1145
  %1153 = and i64 %1151, %1152
  %1154 = icmp slt i64 %1153, 0
  store i1 %1149, i1* %az
  store i1 %1150, i1* %cf
  store i1 %1154, i1* %of
  %1155 = icmp eq i64 %1145, 0
  store i1 %1155, i1* %zf
  %1156 = icmp slt i64 %1145, 0
  store i1 %1156, i1* %sf
  %1157 = trunc i64 %1145 to i8
  %1158 = call i8 @llvm.ctpop.i8(i8 %1157)
  %1159 = and i8 %1158, 1
  %1160 = icmp eq i8 %1159, 0
  store i1 %1160, i1* %pf
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13663, i64* @assembly_address
  br label %block_3566

block_3561:                                       ; preds = %block_3522
  store volatile i64 13665, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_3566

block_3566:                                       ; preds = %block_3561, %block_354c
  store volatile i64 13670, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 13675, i64* @assembly_address
  %1161 = load i64* %rax
  store i64 %1161, i64* %rdi
  store volatile i64 13678, i64* @assembly_address
  %1162 = load i64* %rdi
  %1163 = load i64* %rsi
  %1164 = load i64* %rdx
  %1165 = load i64* %rcx
  %1166 = trunc i64 %1163 to i32
  %1167 = call i64 @flush_block(i64 %1162, i32 %1166, i64 %1164, i64 %1165)
  store i64 %1167, i64* %rax
  store i64 %1167, i64* %rax
  store volatile i64 13683, i64* @assembly_address
  %1168 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1169 = zext i32 %1168 to i64
  store i64 %1169, i64* %rax
  store volatile i64 13689, i64* @assembly_address
  %1170 = load i64* %rax
  %1171 = trunc i64 %1170 to i32
  %1172 = zext i32 %1171 to i64
  store i64 %1172, i64* %rax
  store volatile i64 13691, i64* @assembly_address
  %1173 = load i64* %rax
  store i64 %1173, i64* @global_var_21a430
  store volatile i64 13698, i64* @assembly_address
  br label %block_3589

block_3584:                                       ; preds = %block_3596
  store volatile i64 13700, i64* @assembly_address
  %1174 = call i64 @fill_window()
  store i64 %1174, i64* %rax
  store i64 %1174, i64* %rax
  store i64 %1174, i64* %rax
  br label %block_3589

block_3589:                                       ; preds = %block_3584, %block_3566, %block_351c
  store volatile i64 13705, i64* @assembly_address
  %1175 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %1176 = zext i32 %1175 to i64
  store i64 %1176, i64* %rax
  store volatile i64 13711, i64* @assembly_address
  %1177 = load i64* %rax
  %1178 = trunc i64 %1177 to i32
  %1179 = sub i32 %1178, 261
  %1180 = and i32 %1178, 15
  %1181 = sub i32 %1180, 5
  %1182 = icmp ugt i32 %1181, 15
  %1183 = icmp ult i32 %1178, 261
  %1184 = xor i32 %1178, 261
  %1185 = xor i32 %1178, %1179
  %1186 = and i32 %1184, %1185
  %1187 = icmp slt i32 %1186, 0
  store i1 %1182, i1* %az
  store i1 %1183, i1* %cf
  store i1 %1187, i1* %of
  %1188 = icmp eq i32 %1179, 0
  store i1 %1188, i1* %zf
  %1189 = icmp slt i32 %1179, 0
  store i1 %1189, i1* %sf
  %1190 = trunc i32 %1179 to i8
  %1191 = call i8 @llvm.ctpop.i8(i8 %1190)
  %1192 = and i8 %1191, 1
  %1193 = icmp eq i8 %1192, 0
  store i1 %1193, i1* %pf
  store volatile i64 13716, i64* @assembly_address
  %1194 = load i1* %cf
  %1195 = load i1* %zf
  %1196 = or i1 %1194, %1195
  %1197 = icmp ne i1 %1196, true
  br i1 %1197, label %block_35a0, label %block_3596

block_3596:                                       ; preds = %block_3589
  store volatile i64 13718, i64* @assembly_address
  %1198 = load i32* bitcast (i64* @global_var_21659c to i32*)
  %1199 = zext i32 %1198 to i64
  store i64 %1199, i64* %rax
  store volatile i64 13724, i64* @assembly_address
  %1200 = load i64* %rax
  %1201 = trunc i64 %1200 to i32
  %1202 = load i64* %rax
  %1203 = trunc i64 %1202 to i32
  %1204 = and i32 %1201, %1203
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1205 = icmp eq i32 %1204, 0
  store i1 %1205, i1* %zf
  %1206 = icmp slt i32 %1204, 0
  store i1 %1206, i1* %sf
  %1207 = trunc i32 %1204 to i8
  %1208 = call i8 @llvm.ctpop.i8(i8 %1207)
  %1209 = and i8 %1208, 1
  %1210 = icmp eq i8 %1209, 0
  store i1 %1210, i1* %pf
  store volatile i64 13726, i64* @assembly_address
  %1211 = load i1* %zf
  br i1 %1211, label %block_3584, label %block_35a0

block_35a0:                                       ; preds = %block_3596, %block_3589, %block_31d6
  store volatile i64 13728, i64* @assembly_address
  %1212 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %1213 = zext i32 %1212 to i64
  store i64 %1213, i64* %rax
  store volatile i64 13734, i64* @assembly_address
  %1214 = load i64* %rax
  %1215 = trunc i64 %1214 to i32
  %1216 = load i64* %rax
  %1217 = trunc i64 %1216 to i32
  %1218 = and i32 %1215, %1217
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1219 = icmp eq i32 %1218, 0
  store i1 %1219, i1* %zf
  %1220 = icmp slt i32 %1218, 0
  store i1 %1220, i1* %sf
  %1221 = trunc i32 %1218 to i8
  %1222 = call i8 @llvm.ctpop.i8(i8 %1221)
  %1223 = and i8 %1222, 1
  %1224 = icmp eq i8 %1223, 0
  store i1 %1224, i1* %pf
  store volatile i64 13736, i64* @assembly_address
  %1225 = load i1* %zf
  %1226 = icmp eq i1 %1225, false
  br i1 %1226, label %block_31fb, label %block_35ae

block_35ae:                                       ; preds = %block_35a0
  store volatile i64 13742, i64* @assembly_address
  %1227 = load i32* %stack_var_-20
  %1228 = zext i32 %1227 to i64
  store i64 %1228, i64* %rax
  store volatile i64 13745, i64* @assembly_address
  %1229 = load i64* %rax
  %1230 = add i64 %1229, -1
  %1231 = trunc i64 %1230 to i32
  %1232 = zext i32 %1231 to i64
  store i64 %1232, i64* %rdx
  store volatile i64 13748, i64* @assembly_address
  %1233 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1234 = zext i32 %1233 to i64
  store i64 %1234, i64* %rax
  store volatile i64 13754, i64* @assembly_address
  %1235 = load i64* %rax
  %1236 = trunc i64 %1235 to i32
  %1237 = zext i32 %1236 to i64
  store i64 %1237, i64* %rcx
  store volatile i64 13756, i64* @assembly_address
  %1238 = load i64* @global_var_21a430
  store i64 %1238, i64* %rax
  store volatile i64 13763, i64* @assembly_address
  %1239 = load i64* %rcx
  %1240 = load i64* %rax
  %1241 = sub i64 %1239, %1240
  %1242 = and i64 %1239, 15
  %1243 = and i64 %1240, 15
  %1244 = sub i64 %1242, %1243
  %1245 = icmp ugt i64 %1244, 15
  %1246 = icmp ult i64 %1239, %1240
  %1247 = xor i64 %1239, %1240
  %1248 = xor i64 %1239, %1241
  %1249 = and i64 %1247, %1248
  %1250 = icmp slt i64 %1249, 0
  store i1 %1245, i1* %az
  store i1 %1246, i1* %cf
  store i1 %1250, i1* %of
  %1251 = icmp eq i64 %1241, 0
  store i1 %1251, i1* %zf
  %1252 = icmp slt i64 %1241, 0
  store i1 %1252, i1* %sf
  %1253 = trunc i64 %1241 to i8
  %1254 = call i8 @llvm.ctpop.i8(i8 %1253)
  %1255 = and i8 %1254, 1
  %1256 = icmp eq i8 %1255, 0
  store i1 %1256, i1* %pf
  store i64 %1241, i64* %rcx
  store volatile i64 13766, i64* @assembly_address
  %1257 = load i64* %rcx
  store i64 %1257, i64* %rax
  store volatile i64 13769, i64* @assembly_address
  %1258 = load i64* %rax
  store i64 %1258, i64* %rsi
  store volatile i64 13772, i64* @assembly_address
  %1259 = load i64* @global_var_21a430
  store i64 %1259, i64* %rax
  store volatile i64 13779, i64* @assembly_address
  %1260 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1261 = icmp eq i64 %1260, 0
  store i1 %1261, i1* %zf
  %1262 = icmp slt i64 %1260, 0
  store i1 %1262, i1* %sf
  %1263 = trunc i64 %1260 to i8
  %1264 = call i8 @llvm.ctpop.i8(i8 %1263)
  %1265 = and i8 %1264, 1
  %1266 = icmp eq i8 %1265, 0
  store i1 %1266, i1* %pf
  store volatile i64 13782, i64* @assembly_address
  %1267 = load i1* %sf
  br i1 %1267, label %block_35ed, label %block_35d8

block_35d8:                                       ; preds = %block_35ae
  store volatile i64 13784, i64* @assembly_address
  %1268 = load i64* @global_var_21a430
  store i64 %1268, i64* %rax
  store volatile i64 13791, i64* @assembly_address
  %1269 = load i64* %rax
  %1270 = trunc i64 %1269 to i32
  %1271 = zext i32 %1270 to i64
  store i64 %1271, i64* %rcx
  store volatile i64 13793, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13800, i64* @assembly_address
  %1272 = load i64* %rax
  %1273 = load i64* %rcx
  %1274 = add i64 %1272, %1273
  %1275 = and i64 %1272, 15
  %1276 = and i64 %1273, 15
  %1277 = add i64 %1275, %1276
  %1278 = icmp ugt i64 %1277, 15
  %1279 = icmp ult i64 %1274, %1272
  %1280 = xor i64 %1272, %1274
  %1281 = xor i64 %1273, %1274
  %1282 = and i64 %1280, %1281
  %1283 = icmp slt i64 %1282, 0
  store i1 %1278, i1* %az
  store i1 %1279, i1* %cf
  store i1 %1283, i1* %of
  %1284 = icmp eq i64 %1274, 0
  store i1 %1284, i1* %zf
  %1285 = icmp slt i64 %1274, 0
  store i1 %1285, i1* %sf
  %1286 = trunc i64 %1274 to i8
  %1287 = call i8 @llvm.ctpop.i8(i8 %1286)
  %1288 = and i8 %1287, 1
  %1289 = icmp eq i8 %1288, 0
  store i1 %1289, i1* %pf
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13803, i64* @assembly_address
  br label %block_35f2

block_35ed:                                       ; preds = %block_35ae
  store volatile i64 13805, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_35f2

block_35f2:                                       ; preds = %block_35ed, %block_35d8
  store volatile i64 13810, i64* @assembly_address
  store i64 1, i64* %rcx
  store volatile i64 13815, i64* @assembly_address
  %1290 = load i64* %rax
  store i64 %1290, i64* %rdi
  store volatile i64 13818, i64* @assembly_address
  %1291 = load i64* %rdi
  %1292 = load i64* %rsi
  %1293 = load i64* %rdx
  %1294 = load i64* %rcx
  %1295 = trunc i64 %1292 to i32
  %1296 = call i64 @flush_block(i64 %1291, i32 %1295, i64 %1293, i64 %1294)
  store i64 %1296, i64* %rax
  store i64 %1296, i64* %rax
  store volatile i64 13823, i64* @assembly_address
  %1297 = load i64* %stack_var_-8
  store i64 %1297, i64* %rbp
  %1298 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1298, i64* %rsp
  store volatile i64 13824, i64* @assembly_address
  %1299 = load i64* %rax
  %1300 = load i64* %rax
  ret i64 %1300
}

define i32 @deflate(%z_stream_s* %strm, i32 %flush) {
block_3601:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %flush to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint %z_stream_s* %strm to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-40 = alloca i32
  %2 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i64
  store volatile i64 13825, i64* @assembly_address
  %6 = load i64* %rbp
  store i64 %6, i64* %stack_var_-8
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rsp
  store volatile i64 13826, i64* @assembly_address
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rbp
  store volatile i64 13829, i64* @assembly_address
  %9 = load i64* %rbx
  store i64 %9, i64* %stack_var_-16
  %10 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %10, i64* %rsp
  store volatile i64 13830, i64* @assembly_address
  %11 = load i64* %rsp
  %12 = sub i64 %11, 24
  %13 = and i64 %11, 15
  %14 = sub i64 %13, 8
  %15 = icmp ugt i64 %14, 15
  %16 = icmp ult i64 %11, 24
  %17 = xor i64 %11, 24
  %18 = xor i64 %11, %12
  %19 = and i64 %17, %18
  %20 = icmp slt i64 %19, 0
  store i1 %15, i1* %az
  store i1 %16, i1* %cf
  store i1 %20, i1* %of
  %21 = icmp eq i64 %12, 0
  store i1 %21, i1* %zf
  %22 = icmp slt i64 %12, 0
  store i1 %22, i1* %sf
  %23 = trunc i64 %12 to i8
  %24 = call i8 @llvm.ctpop.i8(i8 %23)
  %25 = and i8 %24, 1
  %26 = icmp eq i8 %25, 0
  store i1 %26, i1* %pf
  %27 = ptrtoint i32* %stack_var_-40 to i64
  store i64 %27, i64* %rsp
  store volatile i64 13834, i64* @assembly_address
  %28 = sext i32 0 to i64
  %29 = trunc i64 %28 to i32
  store i32 %29, i32* %stack_var_-40
  store volatile i64 13841, i64* @assembly_address
  store i32 0, i32* %stack_var_-36
  store volatile i64 13848, i64* @assembly_address
  store i64 2, i64* %rbx
  store volatile i64 13853, i64* @assembly_address
  %30 = load i32* bitcast (i64* @global_var_2165a8 to i32*)
  %31 = zext i32 %30 to i64
  store i64 %31, i64* %rax
  store volatile i64 13859, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i32
  %34 = trunc i64 %32 to i32
  store i32 %34, i32* %4
  store i32 3, i32* %3
  %35 = sub i32 %33, 3
  %36 = and i32 %33, 15
  %37 = sub i32 %36, 3
  %38 = icmp ugt i32 %37, 15
  %39 = icmp ult i32 %33, 3
  %40 = xor i32 %33, 3
  %41 = xor i32 %33, %35
  %42 = and i32 %40, %41
  %43 = icmp slt i32 %42, 0
  store i1 %38, i1* %az
  store i1 %39, i1* %cf
  store i1 %43, i1* %of
  %44 = icmp eq i32 %35, 0
  store i1 %44, i1* %zf
  %45 = icmp slt i32 %35, 0
  store i1 %45, i1* %sf
  %46 = trunc i32 %35 to i8
  %47 = call i8 @llvm.ctpop.i8(i8 %46)
  %48 = and i8 %47, 1
  %49 = icmp eq i8 %48, 0
  store i1 %49, i1* %pf
  store volatile i64 13862, i64* @assembly_address
  %50 = load i32* %4
  %51 = sext i32 %50 to i64
  %52 = load i32* %3
  %53 = trunc i64 %51 to i32
  %54 = icmp sgt i32 %53, %52
  br i1 %54, label %block_3b6e, label %block_362c

block_362c:                                       ; preds = %block_3601
  store volatile i64 13868, i64* @assembly_address
  %55 = load i64* %rdi
  %56 = inttoptr i64 %55 to %z_stream_s*
  %57 = load i64* %rsi
  %58 = trunc i64 %57 to i32
  %59 = call i64 @deflate_fast(%z_stream_s* %56, i32 %58)
  store i64 %59, i64* %rax
  store i64 %59, i64* %rax
  store i64 %59, i64* %rax
  store volatile i64 13873, i64* @assembly_address
  br label %block_3bf8

block_3636:                                       ; preds = %block_3b6e
  store volatile i64 13878, i64* @assembly_address
  %60 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %61 = zext i32 %60 to i64
  store i64 %61, i64* %rax
  store volatile i64 13884, i64* @assembly_address
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i32
  %64 = load i1* %of
  %65 = shl i32 %63, 5
  %66 = icmp eq i32 %65, 0
  store i1 %66, i1* %zf
  %67 = icmp slt i32 %65, 0
  store i1 %67, i1* %sf
  %68 = trunc i32 %65 to i8
  %69 = call i8 @llvm.ctpop.i8(i8 %68)
  %70 = and i8 %69, 1
  %71 = icmp eq i8 %70, 0
  store i1 %71, i1* %pf
  %72 = zext i32 %65 to i64
  store i64 %72, i64* %rax
  %73 = shl i32 %63, 4
  %74 = lshr i32 %73, 31
  %75 = trunc i32 %74 to i1
  store i1 %75, i1* %cf
  %76 = lshr i32 %65, 31
  %77 = icmp ne i32 %76, %74
  %78 = select i1 false, i1 %77, i1 %64
  store i1 %78, i1* %of
  store volatile i64 13887, i64* @assembly_address
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  %81 = zext i32 %80 to i64
  store i64 %81, i64* %rdx
  store volatile i64 13889, i64* @assembly_address
  %82 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %83 = zext i32 %82 to i64
  store i64 %83, i64* %rax
  store volatile i64 13895, i64* @assembly_address
  %84 = load i64* %rax
  %85 = trunc i64 %84 to i32
  %86 = add i32 %85, 2
  %87 = and i32 %85, 15
  %88 = add i32 %87, 2
  %89 = icmp ugt i32 %88, 15
  %90 = icmp ult i32 %86, %85
  %91 = xor i32 %85, %86
  %92 = xor i32 2, %86
  %93 = and i32 %91, %92
  %94 = icmp slt i32 %93, 0
  store i1 %89, i1* %az
  store i1 %90, i1* %cf
  store i1 %94, i1* %of
  %95 = icmp eq i32 %86, 0
  store i1 %95, i1* %zf
  %96 = icmp slt i32 %86, 0
  store i1 %96, i1* %sf
  %97 = trunc i32 %86 to i8
  %98 = call i8 @llvm.ctpop.i8(i8 %97)
  %99 = and i8 %98, 1
  %100 = icmp eq i8 %99, 0
  store i1 %100, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a42a to i64), i64* %rax
  store volatile i64 13898, i64* @assembly_address
  %101 = load i64* %rax
  %102 = trunc i64 %101 to i32
  %103 = zext i32 %102 to i64
  store i64 %103, i64* %rcx
  store volatile i64 13900, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 13907, i64* @assembly_address
  %104 = load i64* %rcx
  %105 = load i64* %rax
  %106 = mul i64 %105, 1
  %107 = add i64 %104, %106
  %108 = inttoptr i64 %107 to i8*
  %109 = load i8* %108
  %110 = zext i8 %109 to i64
  store i64 %110, i64* %rax
  store volatile i64 13911, i64* @assembly_address
  %111 = load i64* %rax
  %112 = trunc i64 %111 to i8
  %113 = zext i8 %112 to i64
  store i64 %113, i64* %rax
  store volatile i64 13914, i64* @assembly_address
  %114 = load i64* %rax
  %115 = trunc i64 %114 to i32
  %116 = load i64* %rdx
  %117 = trunc i64 %116 to i32
  %118 = xor i32 %115, %117
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %119 = icmp eq i32 %118, 0
  store i1 %119, i1* %zf
  %120 = icmp slt i32 %118, 0
  store i1 %120, i1* %sf
  %121 = trunc i32 %118 to i8
  %122 = call i8 @llvm.ctpop.i8(i8 %121)
  %123 = and i8 %122, 1
  %124 = icmp eq i8 %123, 0
  store i1 %124, i1* %pf
  %125 = zext i32 %118 to i64
  store i64 %125, i64* %rax
  store volatile i64 13916, i64* @assembly_address
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i32
  %128 = and i32 %127, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %129 = icmp eq i32 %128, 0
  store i1 %129, i1* %zf
  %130 = icmp slt i32 %128, 0
  store i1 %130, i1* %sf
  %131 = trunc i32 %128 to i8
  %132 = call i8 @llvm.ctpop.i8(i8 %131)
  %133 = and i8 %132, 1
  %134 = icmp eq i8 %133, 0
  store i1 %134, i1* %pf
  %135 = zext i32 %128 to i64
  store i64 %135, i64* %rax
  store volatile i64 13921, i64* @assembly_address
  %136 = load i64* %rax
  %137 = trunc i64 %136 to i32
  store i32 %137, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 13927, i64* @assembly_address
  %138 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %139 = zext i32 %138 to i64
  store i64 %139, i64* %rax
  store volatile i64 13933, i64* @assembly_address
  %140 = load i64* %rax
  %141 = trunc i64 %140 to i32
  %142 = zext i32 %141 to i64
  store i64 %142, i64* %rax
  store volatile i64 13935, i64* @assembly_address
  %143 = load i64* %rax
  %144 = add i64 %143, 32768
  %145 = and i64 %143, 15
  %146 = icmp ugt i64 %145, 15
  %147 = icmp ult i64 %144, %143
  %148 = xor i64 %143, %144
  %149 = xor i64 32768, %144
  %150 = and i64 %148, %149
  %151 = icmp slt i64 %150, 0
  store i1 %146, i1* %az
  store i1 %147, i1* %cf
  store i1 %151, i1* %of
  %152 = icmp eq i64 %144, 0
  store i1 %152, i1* %zf
  %153 = icmp slt i64 %144, 0
  store i1 %153, i1* %sf
  %154 = trunc i64 %144 to i8
  %155 = call i8 @llvm.ctpop.i8(i8 %154)
  %156 = and i8 %155, 1
  %157 = icmp eq i8 %156, 0
  store i1 %157, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 13941, i64* @assembly_address
  %158 = load i64* %rax
  %159 = load i64* %rax
  %160 = mul i64 %159, 1
  %161 = add i64 %158, %160
  store i64 %161, i64* %rdx
  store volatile i64 13945, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 13952, i64* @assembly_address
  %162 = load i64* %rax
  %163 = load i64* %rdx
  %164 = add i64 %162, %163
  %165 = and i64 %162, 15
  %166 = and i64 %163, 15
  %167 = add i64 %165, %166
  %168 = icmp ugt i64 %167, 15
  %169 = icmp ult i64 %164, %162
  %170 = xor i64 %162, %164
  %171 = xor i64 %163, %164
  %172 = and i64 %170, %171
  %173 = icmp slt i64 %172, 0
  store i1 %168, i1* %az
  store i1 %169, i1* %cf
  store i1 %173, i1* %of
  %174 = icmp eq i64 %164, 0
  store i1 %174, i1* %zf
  %175 = icmp slt i64 %164, 0
  store i1 %175, i1* %sf
  %176 = trunc i64 %164 to i8
  %177 = call i8 @llvm.ctpop.i8(i8 %176)
  %178 = and i8 %177, 1
  %179 = icmp eq i8 %178, 0
  store i1 %179, i1* %pf
  store i64 %164, i64* %rax
  store volatile i64 13955, i64* @assembly_address
  %180 = load i64* %rax
  %181 = inttoptr i64 %180 to i16*
  %182 = load i16* %181
  %183 = zext i16 %182 to i64
  store i64 %183, i64* %rax
  store volatile i64 13958, i64* @assembly_address
  %184 = load i64* %rax
  %185 = trunc i64 %184 to i16
  %186 = zext i16 %185 to i64
  store i64 %186, i64* %rax
  store volatile i64 13961, i64* @assembly_address
  %187 = load i64* %rax
  %188 = trunc i64 %187 to i32
  store i32 %188, i32* %stack_var_-32
  store volatile i64 13964, i64* @assembly_address
  %189 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %190 = zext i32 %189 to i64
  store i64 %190, i64* %rax
  store volatile i64 13970, i64* @assembly_address
  %191 = load i64* %rax
  %192 = trunc i64 %191 to i32
  %193 = and i32 %192, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %194 = icmp eq i32 %193, 0
  store i1 %194, i1* %zf
  %195 = icmp slt i32 %193, 0
  store i1 %195, i1* %sf
  %196 = trunc i32 %193 to i8
  %197 = call i8 @llvm.ctpop.i8(i8 %196)
  %198 = and i8 %197, 1
  %199 = icmp eq i8 %198, 0
  store i1 %199, i1* %pf
  %200 = zext i32 %193 to i64
  store i64 %200, i64* %rax
  store volatile i64 13975, i64* @assembly_address
  %201 = load i64* %rax
  %202 = trunc i64 %201 to i32
  %203 = zext i32 %202 to i64
  store i64 %203, i64* %rdx
  store volatile i64 13977, i64* @assembly_address
  %204 = load i32* %stack_var_-32
  %205 = zext i32 %204 to i64
  store i64 %205, i64* %rax
  store volatile i64 13980, i64* @assembly_address
  %206 = load i64* %rax
  %207 = trunc i64 %206 to i32
  %208 = zext i32 %207 to i64
  store i64 %208, i64* %rcx
  store volatile i64 13982, i64* @assembly_address
  %209 = load i64* %rdx
  %210 = trunc i64 %209 to i32
  %211 = zext i32 %210 to i64
  store i64 %211, i64* %rax
  store volatile i64 13984, i64* @assembly_address
  %212 = load i64* %rax
  %213 = load i64* %rax
  %214 = mul i64 %213, 1
  %215 = add i64 %212, %214
  store i64 %215, i64* %rdx
  store volatile i64 13988, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 13995, i64* @assembly_address
  %216 = load i64* %rcx
  %217 = trunc i64 %216 to i16
  %218 = load i64* %rdx
  %219 = load i64* %rax
  %220 = mul i64 %219, 1
  %221 = add i64 %218, %220
  %222 = inttoptr i64 %221 to i16*
  store i16 %217, i16* %222
  store volatile i64 13999, i64* @assembly_address
  %223 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %224 = zext i32 %223 to i64
  store i64 %224, i64* %rdx
  store volatile i64 14005, i64* @assembly_address
  %225 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %226 = zext i32 %225 to i64
  store i64 %226, i64* %rax
  store volatile i64 14011, i64* @assembly_address
  %227 = load i64* %rax
  %228 = trunc i64 %227 to i32
  %229 = zext i32 %228 to i64
  store i64 %229, i64* %rax
  store volatile i64 14013, i64* @assembly_address
  %230 = load i64* %rax
  %231 = add i64 %230, 32768
  %232 = and i64 %230, 15
  %233 = icmp ugt i64 %232, 15
  %234 = icmp ult i64 %231, %230
  %235 = xor i64 %230, %231
  %236 = xor i64 32768, %231
  %237 = and i64 %235, %236
  %238 = icmp slt i64 %237, 0
  store i1 %233, i1* %az
  store i1 %234, i1* %cf
  store i1 %238, i1* %of
  %239 = icmp eq i64 %231, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i64 %231, 0
  store i1 %240, i1* %sf
  %241 = trunc i64 %231 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 14019, i64* @assembly_address
  %245 = load i64* %rax
  %246 = load i64* %rax
  %247 = mul i64 %246, 1
  %248 = add i64 %245, %247
  store i64 %248, i64* %rcx
  store volatile i64 14023, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 14030, i64* @assembly_address
  %249 = load i64* %rax
  %250 = load i64* %rcx
  %251 = add i64 %249, %250
  %252 = and i64 %249, 15
  %253 = and i64 %250, 15
  %254 = add i64 %252, %253
  %255 = icmp ugt i64 %254, 15
  %256 = icmp ult i64 %251, %249
  %257 = xor i64 %249, %251
  %258 = xor i64 %250, %251
  %259 = and i64 %257, %258
  %260 = icmp slt i64 %259, 0
  store i1 %255, i1* %az
  store i1 %256, i1* %cf
  store i1 %260, i1* %of
  %261 = icmp eq i64 %251, 0
  store i1 %261, i1* %zf
  %262 = icmp slt i64 %251, 0
  store i1 %262, i1* %sf
  %263 = trunc i64 %251 to i8
  %264 = call i8 @llvm.ctpop.i8(i8 %263)
  %265 = and i8 %264, 1
  %266 = icmp eq i8 %265, 0
  store i1 %266, i1* %pf
  store i64 %251, i64* %rax
  store volatile i64 14033, i64* @assembly_address
  %267 = load i64* %rdx
  %268 = trunc i64 %267 to i16
  %269 = load i64* %rax
  %270 = inttoptr i64 %269 to i16*
  store i16 %268, i16* %270
  store volatile i64 14036, i64* @assembly_address
  %271 = load i64* %rbx
  %272 = trunc i64 %271 to i32
  store i32 %272, i32* bitcast (i64* @global_var_21a440 to i32*)
  store volatile i64 14042, i64* @assembly_address
  %273 = load i32* bitcast (i64* @global_var_21a444 to i32*)
  %274 = zext i32 %273 to i64
  store i64 %274, i64* %rax
  store volatile i64 14048, i64* @assembly_address
  %275 = load i64* %rax
  %276 = trunc i64 %275 to i32
  store i32 %276, i32* %stack_var_-28
  store volatile i64 14051, i64* @assembly_address
  store i64 2, i64* %rbx
  store volatile i64 14056, i64* @assembly_address
  %277 = load i32* %stack_var_-32
  %278 = and i32 %277, 15
  %279 = icmp ugt i32 %278, 15
  %280 = icmp ult i32 %277, 0
  %281 = xor i32 %277, 0
  %282 = and i32 %281, 0
  %283 = icmp slt i32 %282, 0
  store i1 %279, i1* %az
  store i1 %280, i1* %cf
  store i1 %283, i1* %of
  %284 = icmp eq i32 %277, 0
  store i1 %284, i1* %zf
  %285 = icmp slt i32 %277, 0
  store i1 %285, i1* %sf
  %286 = trunc i32 %277 to i8
  %287 = call i8 @llvm.ctpop.i8(i8 %286)
  %288 = and i8 %287, 1
  %289 = icmp eq i8 %288, 0
  store i1 %289, i1* %pf
  store volatile i64 14060, i64* @assembly_address
  %290 = load i1* %zf
  br i1 %290, label %block_3763, label %block_36ee

block_36ee:                                       ; preds = %block_3636
  store volatile i64 14062, i64* @assembly_address
  %291 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %292 = zext i32 %291 to i64
  store i64 %292, i64* %rdx
  store volatile i64 14068, i64* @assembly_address
  %293 = load i32* bitcast (i64* @global_var_2165a4 to i32*)
  %294 = zext i32 %293 to i64
  store i64 %294, i64* %rax
  store volatile i64 14074, i64* @assembly_address
  %295 = load i64* %rdx
  %296 = trunc i64 %295 to i32
  %297 = load i64* %rax
  %298 = trunc i64 %297 to i32
  %299 = sub i32 %296, %298
  %300 = and i32 %296, 15
  %301 = and i32 %298, 15
  %302 = sub i32 %300, %301
  %303 = icmp ugt i32 %302, 15
  %304 = icmp ult i32 %296, %298
  %305 = xor i32 %296, %298
  %306 = xor i32 %296, %299
  %307 = and i32 %305, %306
  %308 = icmp slt i32 %307, 0
  store i1 %303, i1* %az
  store i1 %304, i1* %cf
  store i1 %308, i1* %of
  %309 = icmp eq i32 %299, 0
  store i1 %309, i1* %zf
  %310 = icmp slt i32 %299, 0
  store i1 %310, i1* %sf
  %311 = trunc i32 %299 to i8
  %312 = call i8 @llvm.ctpop.i8(i8 %311)
  %313 = and i8 %312, 1
  %314 = icmp eq i8 %313, 0
  store i1 %314, i1* %pf
  store volatile i64 14076, i64* @assembly_address
  %315 = load i1* %cf
  %316 = icmp eq i1 %315, false
  br i1 %316, label %block_3763, label %block_36fe

block_36fe:                                       ; preds = %block_36ee
  store volatile i64 14078, i64* @assembly_address
  %317 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %318 = zext i32 %317 to i64
  store i64 %318, i64* %rax
  store volatile i64 14084, i64* @assembly_address
  %319 = load i64* %rax
  %320 = trunc i64 %319 to i32
  %321 = load i32* %stack_var_-32
  %322 = sub i32 %320, %321
  %323 = and i32 %320, 15
  %324 = and i32 %321, 15
  %325 = sub i32 %323, %324
  %326 = icmp ugt i32 %325, 15
  %327 = icmp ult i32 %320, %321
  %328 = xor i32 %320, %321
  %329 = xor i32 %320, %322
  %330 = and i32 %328, %329
  %331 = icmp slt i32 %330, 0
  store i1 %326, i1* %az
  store i1 %327, i1* %cf
  store i1 %331, i1* %of
  %332 = icmp eq i32 %322, 0
  store i1 %332, i1* %zf
  %333 = icmp slt i32 %322, 0
  store i1 %333, i1* %sf
  %334 = trunc i32 %322 to i8
  %335 = call i8 @llvm.ctpop.i8(i8 %334)
  %336 = and i8 %335, 1
  %337 = icmp eq i8 %336, 0
  store i1 %337, i1* %pf
  %338 = zext i32 %322 to i64
  store i64 %338, i64* %rax
  store volatile i64 14087, i64* @assembly_address
  %339 = load i64* %rax
  %340 = trunc i64 %339 to i32
  %341 = sub i32 %340, 32506
  %342 = and i32 %340, 15
  %343 = sub i32 %342, 10
  %344 = icmp ugt i32 %343, 15
  %345 = icmp ult i32 %340, 32506
  %346 = xor i32 %340, 32506
  %347 = xor i32 %340, %341
  %348 = and i32 %346, %347
  %349 = icmp slt i32 %348, 0
  store i1 %344, i1* %az
  store i1 %345, i1* %cf
  store i1 %349, i1* %of
  %350 = icmp eq i32 %341, 0
  store i1 %350, i1* %zf
  %351 = icmp slt i32 %341, 0
  store i1 %351, i1* %sf
  %352 = trunc i32 %341 to i8
  %353 = call i8 @llvm.ctpop.i8(i8 %352)
  %354 = and i8 %353, 1
  %355 = icmp eq i8 %354, 0
  store i1 %355, i1* %pf
  store volatile i64 14092, i64* @assembly_address
  %356 = load i1* %cf
  %357 = load i1* %zf
  %358 = or i1 %356, %357
  %359 = icmp ne i1 %358, true
  br i1 %359, label %block_3763, label %block_370e

block_370e:                                       ; preds = %block_36fe
  store volatile i64 14094, i64* @assembly_address
  %360 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %361 = zext i32 %360 to i64
  store i64 %361, i64* %rax
  store volatile i64 14100, i64* @assembly_address
  %362 = load i64* %rax
  %363 = trunc i64 %362 to i32
  %364 = zext i32 %363 to i64
  store i64 %364, i64* %rdx
  store volatile i64 14102, i64* @assembly_address
  %365 = load i64* @global_var_216020
  store i64 %365, i64* %rax
  store volatile i64 14109, i64* @assembly_address
  %366 = load i64* %rax
  %367 = sub i64 %366, 262
  %368 = and i64 %366, 15
  %369 = sub i64 %368, 6
  %370 = icmp ugt i64 %369, 15
  %371 = icmp ult i64 %366, 262
  %372 = xor i64 %366, 262
  %373 = xor i64 %366, %367
  %374 = and i64 %372, %373
  %375 = icmp slt i64 %374, 0
  store i1 %370, i1* %az
  store i1 %371, i1* %cf
  store i1 %375, i1* %of
  %376 = icmp eq i64 %367, 0
  store i1 %376, i1* %zf
  %377 = icmp slt i64 %367, 0
  store i1 %377, i1* %sf
  %378 = trunc i64 %367 to i8
  %379 = call i8 @llvm.ctpop.i8(i8 %378)
  %380 = and i8 %379, 1
  %381 = icmp eq i8 %380, 0
  store i1 %381, i1* %pf
  store i64 %367, i64* %rax
  store volatile i64 14115, i64* @assembly_address
  %382 = load i64* %rdx
  %383 = load i64* %rax
  %384 = sub i64 %382, %383
  %385 = and i64 %382, 15
  %386 = and i64 %383, 15
  %387 = sub i64 %385, %386
  %388 = icmp ugt i64 %387, 15
  %389 = icmp ult i64 %382, %383
  %390 = xor i64 %382, %383
  %391 = xor i64 %382, %384
  %392 = and i64 %390, %391
  %393 = icmp slt i64 %392, 0
  store i1 %388, i1* %az
  store i1 %389, i1* %cf
  store i1 %393, i1* %of
  %394 = icmp eq i64 %384, 0
  store i1 %394, i1* %zf
  %395 = icmp slt i64 %384, 0
  store i1 %395, i1* %sf
  %396 = trunc i64 %384 to i8
  %397 = call i8 @llvm.ctpop.i8(i8 %396)
  %398 = and i8 %397, 1
  %399 = icmp eq i8 %398, 0
  store i1 %399, i1* %pf
  store volatile i64 14118, i64* @assembly_address
  %400 = load i1* %cf
  %401 = load i1* %zf
  %402 = or i1 %400, %401
  %403 = icmp ne i1 %402, true
  br i1 %403, label %block_3763, label %block_3728

block_3728:                                       ; preds = %block_370e
  store volatile i64 14120, i64* @assembly_address
  %404 = load i32* %stack_var_-32
  %405 = zext i32 %404 to i64
  store i64 %405, i64* %rax
  store volatile i64 14123, i64* @assembly_address
  %406 = load i64* %rax
  %407 = trunc i64 %406 to i32
  %408 = zext i32 %407 to i64
  store i64 %408, i64* %rdi
  store volatile i64 14125, i64* @assembly_address
  %409 = load i64* %rdi
  %410 = trunc i64 %409 to i32
  %411 = call i64 @longest_match(i32 %410)
  store i64 %411, i64* %rax
  store i64 %411, i64* %rax
  store volatile i64 14130, i64* @assembly_address
  %412 = load i64* %rax
  %413 = trunc i64 %412 to i32
  %414 = zext i32 %413 to i64
  store i64 %414, i64* %rbx
  store volatile i64 14132, i64* @assembly_address
  %415 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %416 = zext i32 %415 to i64
  store i64 %416, i64* %rax
  store volatile i64 14138, i64* @assembly_address
  %417 = load i64* %rbx
  %418 = trunc i64 %417 to i32
  %419 = load i64* %rax
  %420 = trunc i64 %419 to i32
  %421 = sub i32 %418, %420
  %422 = and i32 %418, 15
  %423 = and i32 %420, 15
  %424 = sub i32 %422, %423
  %425 = icmp ugt i32 %424, 15
  %426 = icmp ult i32 %418, %420
  %427 = xor i32 %418, %420
  %428 = xor i32 %418, %421
  %429 = and i32 %427, %428
  %430 = icmp slt i32 %429, 0
  store i1 %425, i1* %az
  store i1 %426, i1* %cf
  store i1 %430, i1* %of
  %431 = icmp eq i32 %421, 0
  store i1 %431, i1* %zf
  %432 = icmp slt i32 %421, 0
  store i1 %432, i1* %sf
  %433 = trunc i32 %421 to i8
  %434 = call i8 @llvm.ctpop.i8(i8 %433)
  %435 = and i8 %434, 1
  %436 = icmp eq i8 %435, 0
  store i1 %436, i1* %pf
  store volatile i64 14140, i64* @assembly_address
  %437 = load i1* %cf
  %438 = load i1* %zf
  %439 = or i1 %437, %438
  br i1 %439, label %block_3744, label %block_373e

block_373e:                                       ; preds = %block_3728
  store volatile i64 14142, i64* @assembly_address
  %440 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %441 = zext i32 %440 to i64
  store i64 %441, i64* %rbx
  br label %block_3744

block_3744:                                       ; preds = %block_373e, %block_3728
  store volatile i64 14148, i64* @assembly_address
  %442 = load i64* %rbx
  %443 = trunc i64 %442 to i32
  %444 = sub i32 %443, 3
  %445 = and i32 %443, 15
  %446 = sub i32 %445, 3
  %447 = icmp ugt i32 %446, 15
  %448 = icmp ult i32 %443, 3
  %449 = xor i32 %443, 3
  %450 = xor i32 %443, %444
  %451 = and i32 %449, %450
  %452 = icmp slt i32 %451, 0
  store i1 %447, i1* %az
  store i1 %448, i1* %cf
  store i1 %452, i1* %of
  %453 = icmp eq i32 %444, 0
  store i1 %453, i1* %zf
  %454 = icmp slt i32 %444, 0
  store i1 %454, i1* %sf
  %455 = trunc i32 %444 to i8
  %456 = call i8 @llvm.ctpop.i8(i8 %455)
  %457 = and i8 %456, 1
  %458 = icmp eq i8 %457, 0
  store i1 %458, i1* %pf
  store volatile i64 14151, i64* @assembly_address
  %459 = load i1* %zf
  %460 = icmp eq i1 %459, false
  br i1 %460, label %block_3763, label %block_3749

block_3749:                                       ; preds = %block_3744
  store volatile i64 14153, i64* @assembly_address
  %461 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %462 = zext i32 %461 to i64
  store i64 %462, i64* %rdx
  store volatile i64 14159, i64* @assembly_address
  %463 = load i32* bitcast (i64* @global_var_21a444 to i32*)
  %464 = zext i32 %463 to i64
  store i64 %464, i64* %rax
  store volatile i64 14165, i64* @assembly_address
  %465 = load i64* %rdx
  %466 = trunc i64 %465 to i32
  %467 = load i64* %rax
  %468 = trunc i64 %467 to i32
  %469 = sub i32 %466, %468
  %470 = and i32 %466, 15
  %471 = and i32 %468, 15
  %472 = sub i32 %470, %471
  %473 = icmp ugt i32 %472, 15
  %474 = icmp ult i32 %466, %468
  %475 = xor i32 %466, %468
  %476 = xor i32 %466, %469
  %477 = and i32 %475, %476
  %478 = icmp slt i32 %477, 0
  store i1 %473, i1* %az
  store i1 %474, i1* %cf
  store i1 %478, i1* %of
  %479 = icmp eq i32 %469, 0
  store i1 %479, i1* %zf
  %480 = icmp slt i32 %469, 0
  store i1 %480, i1* %sf
  %481 = trunc i32 %469 to i8
  %482 = call i8 @llvm.ctpop.i8(i8 %481)
  %483 = and i8 %482, 1
  %484 = icmp eq i8 %483, 0
  store i1 %484, i1* %pf
  %485 = zext i32 %469 to i64
  store i64 %485, i64* %rdx
  store volatile i64 14167, i64* @assembly_address
  %486 = load i64* %rdx
  %487 = trunc i64 %486 to i32
  %488 = zext i32 %487 to i64
  store i64 %488, i64* %rax
  store volatile i64 14169, i64* @assembly_address
  %489 = load i64* %rax
  %490 = trunc i64 %489 to i32
  %491 = sub i32 %490, 4096
  %492 = and i32 %490, 15
  %493 = icmp ugt i32 %492, 15
  %494 = icmp ult i32 %490, 4096
  %495 = xor i32 %490, 4096
  %496 = xor i32 %490, %491
  %497 = and i32 %495, %496
  %498 = icmp slt i32 %497, 0
  store i1 %493, i1* %az
  store i1 %494, i1* %cf
  store i1 %498, i1* %of
  %499 = icmp eq i32 %491, 0
  store i1 %499, i1* %zf
  %500 = icmp slt i32 %491, 0
  store i1 %500, i1* %sf
  %501 = trunc i32 %491 to i8
  %502 = call i8 @llvm.ctpop.i8(i8 %501)
  %503 = and i8 %502, 1
  %504 = icmp eq i8 %503, 0
  store i1 %504, i1* %pf
  store volatile i64 14174, i64* @assembly_address
  %505 = load i1* %cf
  %506 = load i1* %zf
  %507 = or i1 %505, %506
  br i1 %507, label %block_3763, label %block_3760

block_3760:                                       ; preds = %block_3749
  store volatile i64 14176, i64* @assembly_address
  %508 = load i64* %rbx
  %509 = trunc i64 %508 to i32
  %510 = sub i32 %509, 1
  %511 = and i32 %509, 15
  %512 = sub i32 %511, 1
  %513 = icmp ugt i32 %512, 15
  %514 = icmp ult i32 %509, 1
  %515 = xor i32 %509, 1
  %516 = xor i32 %509, %510
  %517 = and i32 %515, %516
  %518 = icmp slt i32 %517, 0
  store i1 %513, i1* %az
  store i1 %514, i1* %cf
  store i1 %518, i1* %of
  %519 = icmp eq i32 %510, 0
  store i1 %519, i1* %zf
  %520 = icmp slt i32 %510, 0
  store i1 %520, i1* %sf
  %521 = trunc i32 %510 to i8
  %522 = call i8 @llvm.ctpop.i8(i8 %521)
  %523 = and i8 %522, 1
  %524 = icmp eq i8 %523, 0
  store i1 %524, i1* %pf
  %525 = zext i32 %510 to i64
  store i64 %525, i64* %rbx
  br label %block_3763

block_3763:                                       ; preds = %block_3760, %block_3749, %block_3744, %block_370e, %block_36fe, %block_36ee, %block_3636
  store volatile i64 14179, i64* @assembly_address
  %526 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %527 = zext i32 %526 to i64
  store i64 %527, i64* %rax
  store volatile i64 14185, i64* @assembly_address
  %528 = load i64* %rax
  %529 = trunc i64 %528 to i32
  %530 = sub i32 %529, 2
  %531 = and i32 %529, 15
  %532 = sub i32 %531, 2
  %533 = icmp ugt i32 %532, 15
  %534 = icmp ult i32 %529, 2
  %535 = xor i32 %529, 2
  %536 = xor i32 %529, %530
  %537 = and i32 %535, %536
  %538 = icmp slt i32 %537, 0
  store i1 %533, i1* %az
  store i1 %534, i1* %cf
  store i1 %538, i1* %of
  %539 = icmp eq i32 %530, 0
  store i1 %539, i1* %zf
  %540 = icmp slt i32 %530, 0
  store i1 %540, i1* %sf
  %541 = trunc i32 %530 to i8
  %542 = call i8 @llvm.ctpop.i8(i8 %541)
  %543 = and i8 %542, 1
  %544 = icmp eq i8 %543, 0
  store i1 %544, i1* %pf
  store volatile i64 14188, i64* @assembly_address
  %545 = load i1* %cf
  %546 = load i1* %zf
  %547 = or i1 %545, %546
  br i1 %547, label %block_3972, label %block_3772

block_3772:                                       ; preds = %block_3763
  store volatile i64 14194, i64* @assembly_address
  %548 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %549 = zext i32 %548 to i64
  store i64 %549, i64* %rax
  store volatile i64 14200, i64* @assembly_address
  %550 = load i64* %rbx
  %551 = trunc i64 %550 to i32
  %552 = load i64* %rax
  %553 = trunc i64 %552 to i32
  %554 = sub i32 %551, %553
  %555 = and i32 %551, 15
  %556 = and i32 %553, 15
  %557 = sub i32 %555, %556
  %558 = icmp ugt i32 %557, 15
  %559 = icmp ult i32 %551, %553
  %560 = xor i32 %551, %553
  %561 = xor i32 %551, %554
  %562 = and i32 %560, %561
  %563 = icmp slt i32 %562, 0
  store i1 %558, i1* %az
  store i1 %559, i1* %cf
  store i1 %563, i1* %of
  %564 = icmp eq i32 %554, 0
  store i1 %564, i1* %zf
  %565 = icmp slt i32 %554, 0
  store i1 %565, i1* %sf
  %566 = trunc i32 %554 to i8
  %567 = call i8 @llvm.ctpop.i8(i8 %566)
  %568 = and i8 %567, 1
  %569 = icmp eq i8 %568, 0
  store i1 %569, i1* %pf
  store volatile i64 14202, i64* @assembly_address
  %570 = load i1* %cf
  %571 = load i1* %zf
  %572 = or i1 %570, %571
  %573 = icmp ne i1 %572, true
  br i1 %573, label %block_3972, label %block_3780

block_3780:                                       ; preds = %block_3772
  store volatile i64 14208, i64* @assembly_address
  %574 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %575 = zext i32 %574 to i64
  store i64 %575, i64* %rax
  store volatile i64 14214, i64* @assembly_address
  %576 = load i64* %rax
  %577 = trunc i64 %576 to i32
  %578 = sub i32 %577, 3
  %579 = and i32 %577, 15
  %580 = sub i32 %579, 3
  %581 = icmp ugt i32 %580, 15
  %582 = icmp ult i32 %577, 3
  %583 = xor i32 %577, 3
  %584 = xor i32 %577, %578
  %585 = and i32 %583, %584
  %586 = icmp slt i32 %585, 0
  store i1 %581, i1* %az
  store i1 %582, i1* %cf
  store i1 %586, i1* %of
  %587 = icmp eq i32 %578, 0
  store i1 %587, i1* %zf
  %588 = icmp slt i32 %578, 0
  store i1 %588, i1* %sf
  %589 = trunc i32 %578 to i8
  %590 = call i8 @llvm.ctpop.i8(i8 %589)
  %591 = and i8 %590, 1
  %592 = icmp eq i8 %591, 0
  store i1 %592, i1* %pf
  %593 = zext i32 %578 to i64
  store i64 %593, i64* %rax
  store volatile i64 14217, i64* @assembly_address
  %594 = load i64* %rax
  %595 = trunc i64 %594 to i32
  %596 = zext i32 %595 to i64
  store i64 %596, i64* %rdx
  store volatile i64 14219, i64* @assembly_address
  %597 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %598 = zext i32 %597 to i64
  store i64 %598, i64* %rax
  store volatile i64 14225, i64* @assembly_address
  %599 = load i64* %rax
  %600 = trunc i64 %599 to i32
  %601 = load i32* %stack_var_-28
  %602 = sub i32 %600, %601
  %603 = and i32 %600, 15
  %604 = and i32 %601, 15
  %605 = sub i32 %603, %604
  %606 = icmp ugt i32 %605, 15
  %607 = icmp ult i32 %600, %601
  %608 = xor i32 %600, %601
  %609 = xor i32 %600, %602
  %610 = and i32 %608, %609
  %611 = icmp slt i32 %610, 0
  store i1 %606, i1* %az
  store i1 %607, i1* %cf
  store i1 %611, i1* %of
  %612 = icmp eq i32 %602, 0
  store i1 %612, i1* %zf
  %613 = icmp slt i32 %602, 0
  store i1 %613, i1* %sf
  %614 = trunc i32 %602 to i8
  %615 = call i8 @llvm.ctpop.i8(i8 %614)
  %616 = and i8 %615, 1
  %617 = icmp eq i8 %616, 0
  store i1 %617, i1* %pf
  %618 = zext i32 %602 to i64
  store i64 %618, i64* %rax
  store volatile i64 14228, i64* @assembly_address
  %619 = load i64* %rax
  %620 = trunc i64 %619 to i32
  %621 = sub i32 %620, 1
  %622 = and i32 %620, 15
  %623 = sub i32 %622, 1
  %624 = icmp ugt i32 %623, 15
  %625 = icmp ult i32 %620, 1
  %626 = xor i32 %620, 1
  %627 = xor i32 %620, %621
  %628 = and i32 %626, %627
  %629 = icmp slt i32 %628, 0
  store i1 %624, i1* %az
  store i1 %625, i1* %cf
  store i1 %629, i1* %of
  %630 = icmp eq i32 %621, 0
  store i1 %630, i1* %zf
  %631 = icmp slt i32 %621, 0
  store i1 %631, i1* %sf
  %632 = trunc i32 %621 to i8
  %633 = call i8 @llvm.ctpop.i8(i8 %632)
  %634 = and i8 %633, 1
  %635 = icmp eq i8 %634, 0
  store i1 %635, i1* %pf
  %636 = zext i32 %621 to i64
  store i64 %636, i64* %rax
  store volatile i64 14231, i64* @assembly_address
  %637 = load i64* %rdx
  %638 = trunc i64 %637 to i32
  %639 = zext i32 %638 to i64
  store i64 %639, i64* %rsi
  store volatile i64 14233, i64* @assembly_address
  %640 = load i64* %rax
  %641 = trunc i64 %640 to i32
  %642 = zext i32 %641 to i64
  store i64 %642, i64* %rdi
  store volatile i64 14235, i64* @assembly_address
  %643 = load i64* %rdi
  %644 = load i64* %rsi
  %645 = trunc i64 %643 to i32
  %646 = call i64 @ct_tally(i32 %645, i64 %644)
  store i64 %646, i64* %rax
  store i64 %646, i64* %rax
  store volatile i64 14240, i64* @assembly_address
  %647 = load i64* %rax
  %648 = trunc i64 %647 to i32
  %649 = sext i32 %648 to i64
  %650 = trunc i64 %649 to i32
  store i32 %650, i32* %stack_var_-40
  store volatile i64 14243, i64* @assembly_address
  %651 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %652 = zext i32 %651 to i64
  store i64 %652, i64* %rdx
  store volatile i64 14249, i64* @assembly_address
  %653 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %654 = zext i32 %653 to i64
  store i64 %654, i64* %rax
  store volatile i64 14255, i64* @assembly_address
  %655 = load i64* %rdx
  %656 = trunc i64 %655 to i32
  %657 = load i64* %rax
  %658 = trunc i64 %657 to i32
  %659 = sub i32 %656, %658
  %660 = and i32 %656, 15
  %661 = and i32 %658, 15
  %662 = sub i32 %660, %661
  %663 = icmp ugt i32 %662, 15
  %664 = icmp ult i32 %656, %658
  %665 = xor i32 %656, %658
  %666 = xor i32 %656, %659
  %667 = and i32 %665, %666
  %668 = icmp slt i32 %667, 0
  store i1 %663, i1* %az
  store i1 %664, i1* %cf
  store i1 %668, i1* %of
  %669 = icmp eq i32 %659, 0
  store i1 %669, i1* %zf
  %670 = icmp slt i32 %659, 0
  store i1 %670, i1* %sf
  %671 = trunc i32 %659 to i8
  %672 = call i8 @llvm.ctpop.i8(i8 %671)
  %673 = and i8 %672, 1
  %674 = icmp eq i8 %673, 0
  store i1 %674, i1* %pf
  %675 = zext i32 %659 to i64
  store i64 %675, i64* %rdx
  store volatile i64 14257, i64* @assembly_address
  %676 = load i64* %rdx
  %677 = trunc i64 %676 to i32
  %678 = zext i32 %677 to i64
  store i64 %678, i64* %rax
  store volatile i64 14259, i64* @assembly_address
  %679 = load i64* %rax
  %680 = trunc i64 %679 to i32
  %681 = add i32 %680, 1
  %682 = and i32 %680, 15
  %683 = add i32 %682, 1
  %684 = icmp ugt i32 %683, 15
  %685 = icmp ult i32 %681, %680
  %686 = xor i32 %680, %681
  %687 = xor i32 1, %681
  %688 = and i32 %686, %687
  %689 = icmp slt i32 %688, 0
  store i1 %684, i1* %az
  store i1 %685, i1* %cf
  store i1 %689, i1* %of
  %690 = icmp eq i32 %681, 0
  store i1 %690, i1* %zf
  %691 = icmp slt i32 %681, 0
  store i1 %691, i1* %sf
  %692 = trunc i32 %681 to i8
  %693 = call i8 @llvm.ctpop.i8(i8 %692)
  %694 = and i8 %693, 1
  %695 = icmp eq i8 %694, 0
  store i1 %695, i1* %pf
  %696 = zext i32 %681 to i64
  store i64 %696, i64* %rax
  store volatile i64 14262, i64* @assembly_address
  %697 = load i64* %rax
  %698 = trunc i64 %697 to i32
  store i32 %698, i32* bitcast (i64* @global_var_2165a0 to i32*)
  store volatile i64 14268, i64* @assembly_address
  %699 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %700 = zext i32 %699 to i64
  store i64 %700, i64* %rax
  store volatile i64 14274, i64* @assembly_address
  %701 = load i64* %rax
  %702 = trunc i64 %701 to i32
  %703 = sub i32 %702, 2
  %704 = and i32 %702, 15
  %705 = sub i32 %704, 2
  %706 = icmp ugt i32 %705, 15
  %707 = icmp ult i32 %702, 2
  %708 = xor i32 %702, 2
  %709 = xor i32 %702, %703
  %710 = and i32 %708, %709
  %711 = icmp slt i32 %710, 0
  store i1 %706, i1* %az
  store i1 %707, i1* %cf
  store i1 %711, i1* %of
  %712 = icmp eq i32 %703, 0
  store i1 %712, i1* %zf
  %713 = icmp slt i32 %703, 0
  store i1 %713, i1* %sf
  %714 = trunc i32 %703 to i8
  %715 = call i8 @llvm.ctpop.i8(i8 %714)
  %716 = and i8 %715, 1
  %717 = icmp eq i8 %716, 0
  store i1 %717, i1* %pf
  %718 = zext i32 %703 to i64
  store i64 %718, i64* %rax
  store volatile i64 14277, i64* @assembly_address
  %719 = load i64* %rax
  %720 = trunc i64 %719 to i32
  store i32 %720, i32* bitcast (i64* @global_var_21a440 to i32*)
  store volatile i64 14283, i64* @assembly_address
  %721 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %722 = zext i32 %721 to i64
  store i64 %722, i64* %rax
  store volatile i64 14289, i64* @assembly_address
  %723 = load i64* %rax
  %724 = trunc i64 %723 to i32
  %725 = load i64* %rax
  %726 = trunc i64 %725 to i32
  %727 = and i32 %724, %726
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %728 = icmp eq i32 %727, 0
  store i1 %728, i1* %zf
  %729 = icmp slt i32 %727, 0
  store i1 %729, i1* %sf
  %730 = trunc i32 %727 to i8
  %731 = call i8 @llvm.ctpop.i8(i8 %730)
  %732 = and i8 %731, 1
  %733 = icmp eq i8 %732, 0
  store i1 %733, i1* %pf
  store volatile i64 14291, i64* @assembly_address
  %734 = load i1* %zf
  br i1 %734, label %block_37ed, label %block_37d5

block_37d5:                                       ; preds = %block_3780
  store volatile i64 14293, i64* @assembly_address
  %735 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %736 = zext i32 %735 to i64
  store i64 %736, i64* %rax
  store volatile i64 14299, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a441 to i64), i64* %rdx
  store volatile i64 14302, i64* @assembly_address
  %737 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %738 = zext i32 %737 to i64
  store i64 %738, i64* %rax
  store volatile i64 14308, i64* @assembly_address
  %739 = load i64* %rdx
  %740 = trunc i64 %739 to i32
  %741 = zext i32 %740 to i64
  store i64 %741, i64* %rsi
  store volatile i64 14310, i64* @assembly_address
  %742 = load i64* %rax
  %743 = trunc i64 %742 to i32
  %744 = zext i32 %743 to i64
  store i64 %744, i64* %rdi
  store volatile i64 14312, i64* @assembly_address
  %745 = load i64* %rdi
  %746 = load i64* %rsi
  %747 = trunc i64 %745 to i32
  %748 = call i64 @rsync_roll(i32 %747, i64 %746)
  store i64 %748, i64* %rax
  store i64 %748, i64* %rax
  br label %block_37ed

block_37ed:                                       ; preds = %block_37ed, %block_37d5, %block_3780
  store volatile i64 14317, i64* @assembly_address
  %749 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %750 = zext i32 %749 to i64
  store i64 %750, i64* %rax
  store volatile i64 14323, i64* @assembly_address
  %751 = load i64* %rax
  %752 = trunc i64 %751 to i32
  %753 = add i32 %752, 1
  %754 = and i32 %752, 15
  %755 = add i32 %754, 1
  %756 = icmp ugt i32 %755, 15
  %757 = icmp ult i32 %753, %752
  %758 = xor i32 %752, %753
  %759 = xor i32 1, %753
  %760 = and i32 %758, %759
  %761 = icmp slt i32 %760, 0
  store i1 %756, i1* %az
  store i1 %757, i1* %cf
  store i1 %761, i1* %of
  %762 = icmp eq i32 %753, 0
  store i1 %762, i1* %zf
  %763 = icmp slt i32 %753, 0
  store i1 %763, i1* %sf
  %764 = trunc i32 %753 to i8
  %765 = call i8 @llvm.ctpop.i8(i8 %764)
  %766 = and i8 %765, 1
  %767 = icmp eq i8 %766, 0
  store i1 %767, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 14326, i64* @assembly_address
  %768 = load i64* %rax
  %769 = trunc i64 %768 to i32
  store i32 %769, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 14332, i64* @assembly_address
  %770 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %771 = zext i32 %770 to i64
  store i64 %771, i64* %rax
  store volatile i64 14338, i64* @assembly_address
  %772 = load i64* %rax
  %773 = trunc i64 %772 to i32
  %774 = load i1* %of
  %775 = shl i32 %773, 5
  %776 = icmp eq i32 %775, 0
  store i1 %776, i1* %zf
  %777 = icmp slt i32 %775, 0
  store i1 %777, i1* %sf
  %778 = trunc i32 %775 to i8
  %779 = call i8 @llvm.ctpop.i8(i8 %778)
  %780 = and i8 %779, 1
  %781 = icmp eq i8 %780, 0
  store i1 %781, i1* %pf
  %782 = zext i32 %775 to i64
  store i64 %782, i64* %rax
  %783 = shl i32 %773, 4
  %784 = lshr i32 %783, 31
  %785 = trunc i32 %784 to i1
  store i1 %785, i1* %cf
  %786 = lshr i32 %775, 31
  %787 = icmp ne i32 %786, %784
  %788 = select i1 false, i1 %787, i1 %774
  store i1 %788, i1* %of
  store volatile i64 14341, i64* @assembly_address
  %789 = load i64* %rax
  %790 = trunc i64 %789 to i32
  %791 = zext i32 %790 to i64
  store i64 %791, i64* %rdx
  store volatile i64 14343, i64* @assembly_address
  %792 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %793 = zext i32 %792 to i64
  store i64 %793, i64* %rax
  store volatile i64 14349, i64* @assembly_address
  %794 = load i64* %rax
  %795 = trunc i64 %794 to i32
  %796 = add i32 %795, 2
  %797 = and i32 %795, 15
  %798 = add i32 %797, 2
  %799 = icmp ugt i32 %798, 15
  %800 = icmp ult i32 %796, %795
  %801 = xor i32 %795, %796
  %802 = xor i32 2, %796
  %803 = and i32 %801, %802
  %804 = icmp slt i32 %803, 0
  store i1 %799, i1* %az
  store i1 %800, i1* %cf
  store i1 %804, i1* %of
  %805 = icmp eq i32 %796, 0
  store i1 %805, i1* %zf
  %806 = icmp slt i32 %796, 0
  store i1 %806, i1* %sf
  %807 = trunc i32 %796 to i8
  %808 = call i8 @llvm.ctpop.i8(i8 %807)
  %809 = and i8 %808, 1
  %810 = icmp eq i8 %809, 0
  store i1 %810, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a42a to i64), i64* %rax
  store volatile i64 14352, i64* @assembly_address
  %811 = load i64* %rax
  %812 = trunc i64 %811 to i32
  %813 = zext i32 %812 to i64
  store i64 %813, i64* %rcx
  store volatile i64 14354, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 14361, i64* @assembly_address
  %814 = load i64* %rcx
  %815 = load i64* %rax
  %816 = mul i64 %815, 1
  %817 = add i64 %814, %816
  %818 = inttoptr i64 %817 to i8*
  %819 = load i8* %818
  %820 = zext i8 %819 to i64
  store i64 %820, i64* %rax
  store volatile i64 14365, i64* @assembly_address
  %821 = load i64* %rax
  %822 = trunc i64 %821 to i8
  %823 = zext i8 %822 to i64
  store i64 %823, i64* %rax
  store volatile i64 14368, i64* @assembly_address
  %824 = load i64* %rax
  %825 = trunc i64 %824 to i32
  %826 = load i64* %rdx
  %827 = trunc i64 %826 to i32
  %828 = xor i32 %825, %827
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %829 = icmp eq i32 %828, 0
  store i1 %829, i1* %zf
  %830 = icmp slt i32 %828, 0
  store i1 %830, i1* %sf
  %831 = trunc i32 %828 to i8
  %832 = call i8 @llvm.ctpop.i8(i8 %831)
  %833 = and i8 %832, 1
  %834 = icmp eq i8 %833, 0
  store i1 %834, i1* %pf
  %835 = zext i32 %828 to i64
  store i64 %835, i64* %rax
  store volatile i64 14370, i64* @assembly_address
  %836 = load i64* %rax
  %837 = trunc i64 %836 to i32
  %838 = and i32 %837, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %839 = icmp eq i32 %838, 0
  store i1 %839, i1* %zf
  %840 = icmp slt i32 %838, 0
  store i1 %840, i1* %sf
  %841 = trunc i32 %838 to i8
  %842 = call i8 @llvm.ctpop.i8(i8 %841)
  %843 = and i8 %842, 1
  %844 = icmp eq i8 %843, 0
  store i1 %844, i1* %pf
  %845 = zext i32 %838 to i64
  store i64 %845, i64* %rax
  store volatile i64 14375, i64* @assembly_address
  %846 = load i64* %rax
  %847 = trunc i64 %846 to i32
  store i32 %847, i32* bitcast (i64* @global_var_216598 to i32*)
  store volatile i64 14381, i64* @assembly_address
  %848 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %849 = zext i32 %848 to i64
  store i64 %849, i64* %rax
  store volatile i64 14387, i64* @assembly_address
  %850 = load i64* %rax
  %851 = trunc i64 %850 to i32
  %852 = zext i32 %851 to i64
  store i64 %852, i64* %rax
  store volatile i64 14389, i64* @assembly_address
  %853 = load i64* %rax
  %854 = add i64 %853, 32768
  %855 = and i64 %853, 15
  %856 = icmp ugt i64 %855, 15
  %857 = icmp ult i64 %854, %853
  %858 = xor i64 %853, %854
  %859 = xor i64 32768, %854
  %860 = and i64 %858, %859
  %861 = icmp slt i64 %860, 0
  store i1 %856, i1* %az
  store i1 %857, i1* %cf
  store i1 %861, i1* %of
  %862 = icmp eq i64 %854, 0
  store i1 %862, i1* %zf
  %863 = icmp slt i64 %854, 0
  store i1 %863, i1* %sf
  %864 = trunc i64 %854 to i8
  %865 = call i8 @llvm.ctpop.i8(i8 %864)
  %866 = and i8 %865, 1
  %867 = icmp eq i8 %866, 0
  store i1 %867, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 14395, i64* @assembly_address
  %868 = load i64* %rax
  %869 = load i64* %rax
  %870 = mul i64 %869, 1
  %871 = add i64 %868, %870
  store i64 %871, i64* %rdx
  store volatile i64 14399, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 14406, i64* @assembly_address
  %872 = load i64* %rax
  %873 = load i64* %rdx
  %874 = add i64 %872, %873
  %875 = and i64 %872, 15
  %876 = and i64 %873, 15
  %877 = add i64 %875, %876
  %878 = icmp ugt i64 %877, 15
  %879 = icmp ult i64 %874, %872
  %880 = xor i64 %872, %874
  %881 = xor i64 %873, %874
  %882 = and i64 %880, %881
  %883 = icmp slt i64 %882, 0
  store i1 %878, i1* %az
  store i1 %879, i1* %cf
  store i1 %883, i1* %of
  %884 = icmp eq i64 %874, 0
  store i1 %884, i1* %zf
  %885 = icmp slt i64 %874, 0
  store i1 %885, i1* %sf
  %886 = trunc i64 %874 to i8
  %887 = call i8 @llvm.ctpop.i8(i8 %886)
  %888 = and i8 %887, 1
  %889 = icmp eq i8 %888, 0
  store i1 %889, i1* %pf
  store i64 %874, i64* %rax
  store volatile i64 14409, i64* @assembly_address
  %890 = load i64* %rax
  %891 = inttoptr i64 %890 to i16*
  %892 = load i16* %891
  %893 = zext i16 %892 to i64
  store i64 %893, i64* %rax
  store volatile i64 14412, i64* @assembly_address
  %894 = load i64* %rax
  %895 = trunc i64 %894 to i16
  %896 = zext i16 %895 to i64
  store i64 %896, i64* %rax
  store volatile i64 14415, i64* @assembly_address
  %897 = load i64* %rax
  %898 = trunc i64 %897 to i32
  store i32 %898, i32* %stack_var_-32
  store volatile i64 14418, i64* @assembly_address
  %899 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %900 = zext i32 %899 to i64
  store i64 %900, i64* %rax
  store volatile i64 14424, i64* @assembly_address
  %901 = load i64* %rax
  %902 = trunc i64 %901 to i32
  %903 = and i32 %902, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %904 = icmp eq i32 %903, 0
  store i1 %904, i1* %zf
  %905 = icmp slt i32 %903, 0
  store i1 %905, i1* %sf
  %906 = trunc i32 %903 to i8
  %907 = call i8 @llvm.ctpop.i8(i8 %906)
  %908 = and i8 %907, 1
  %909 = icmp eq i8 %908, 0
  store i1 %909, i1* %pf
  %910 = zext i32 %903 to i64
  store i64 %910, i64* %rax
  store volatile i64 14429, i64* @assembly_address
  %911 = load i64* %rax
  %912 = trunc i64 %911 to i32
  %913 = zext i32 %912 to i64
  store i64 %913, i64* %rdx
  store volatile i64 14431, i64* @assembly_address
  %914 = load i32* %stack_var_-32
  %915 = zext i32 %914 to i64
  store i64 %915, i64* %rax
  store volatile i64 14434, i64* @assembly_address
  %916 = load i64* %rax
  %917 = trunc i64 %916 to i32
  %918 = zext i32 %917 to i64
  store i64 %918, i64* %rcx
  store volatile i64 14436, i64* @assembly_address
  %919 = load i64* %rdx
  %920 = trunc i64 %919 to i32
  %921 = zext i32 %920 to i64
  store i64 %921, i64* %rax
  store volatile i64 14438, i64* @assembly_address
  %922 = load i64* %rax
  %923 = load i64* %rax
  %924 = mul i64 %923, 1
  %925 = add i64 %922, %924
  store i64 %925, i64* %rdx
  store volatile i64 14442, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 14449, i64* @assembly_address
  %926 = load i64* %rcx
  %927 = trunc i64 %926 to i16
  %928 = load i64* %rdx
  %929 = load i64* %rax
  %930 = mul i64 %929, 1
  %931 = add i64 %928, %930
  %932 = inttoptr i64 %931 to i16*
  store i16 %927, i16* %932
  store volatile i64 14453, i64* @assembly_address
  %933 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %934 = zext i32 %933 to i64
  store i64 %934, i64* %rdx
  store volatile i64 14459, i64* @assembly_address
  %935 = load i32* bitcast (i64* @global_var_216598 to i32*)
  %936 = zext i32 %935 to i64
  store i64 %936, i64* %rax
  store volatile i64 14465, i64* @assembly_address
  %937 = load i64* %rax
  %938 = trunc i64 %937 to i32
  %939 = zext i32 %938 to i64
  store i64 %939, i64* %rax
  store volatile i64 14467, i64* @assembly_address
  %940 = load i64* %rax
  %941 = add i64 %940, 32768
  %942 = and i64 %940, 15
  %943 = icmp ugt i64 %942, 15
  %944 = icmp ult i64 %941, %940
  %945 = xor i64 %940, %941
  %946 = xor i64 32768, %941
  %947 = and i64 %945, %946
  %948 = icmp slt i64 %947, 0
  store i1 %943, i1* %az
  store i1 %944, i1* %cf
  store i1 %948, i1* %of
  %949 = icmp eq i64 %941, 0
  store i1 %949, i1* %zf
  %950 = icmp slt i64 %941, 0
  store i1 %950, i1* %sf
  %951 = trunc i64 %941 to i8
  %952 = call i8 @llvm.ctpop.i8(i8 %951)
  %953 = and i8 %952, 1
  %954 = icmp eq i8 %953, 0
  store i1 %954, i1* %pf
  store i64 ptrtoint (i64* @global_var_21e598 to i64), i64* %rax
  store volatile i64 14473, i64* @assembly_address
  %955 = load i64* %rax
  %956 = load i64* %rax
  %957 = mul i64 %956, 1
  %958 = add i64 %955, %957
  store i64 %958, i64* %rcx
  store volatile i64 14477, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 14484, i64* @assembly_address
  %959 = load i64* %rax
  %960 = load i64* %rcx
  %961 = add i64 %959, %960
  %962 = and i64 %959, 15
  %963 = and i64 %960, 15
  %964 = add i64 %962, %963
  %965 = icmp ugt i64 %964, 15
  %966 = icmp ult i64 %961, %959
  %967 = xor i64 %959, %961
  %968 = xor i64 %960, %961
  %969 = and i64 %967, %968
  %970 = icmp slt i64 %969, 0
  store i1 %965, i1* %az
  store i1 %966, i1* %cf
  store i1 %970, i1* %of
  %971 = icmp eq i64 %961, 0
  store i1 %971, i1* %zf
  %972 = icmp slt i64 %961, 0
  store i1 %972, i1* %sf
  %973 = trunc i64 %961 to i8
  %974 = call i8 @llvm.ctpop.i8(i8 %973)
  %975 = and i8 %974, 1
  %976 = icmp eq i8 %975, 0
  store i1 %976, i1* %pf
  store i64 %961, i64* %rax
  store volatile i64 14487, i64* @assembly_address
  %977 = load i64* %rdx
  %978 = trunc i64 %977 to i16
  %979 = load i64* %rax
  %980 = inttoptr i64 %979 to i16*
  store i16 %978, i16* %980
  store volatile i64 14490, i64* @assembly_address
  %981 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %982 = zext i32 %981 to i64
  store i64 %982, i64* %rax
  store volatile i64 14496, i64* @assembly_address
  %983 = load i64* %rax
  %984 = trunc i64 %983 to i32
  %985 = sub i32 %984, 1
  %986 = and i32 %984, 15
  %987 = sub i32 %986, 1
  %988 = icmp ugt i32 %987, 15
  %989 = icmp ult i32 %984, 1
  %990 = xor i32 %984, 1
  %991 = xor i32 %984, %985
  %992 = and i32 %990, %991
  %993 = icmp slt i32 %992, 0
  store i1 %988, i1* %az
  store i1 %989, i1* %cf
  store i1 %993, i1* %of
  %994 = icmp eq i32 %985, 0
  store i1 %994, i1* %zf
  %995 = icmp slt i32 %985, 0
  store i1 %995, i1* %sf
  %996 = trunc i32 %985 to i8
  %997 = call i8 @llvm.ctpop.i8(i8 %996)
  %998 = and i8 %997, 1
  %999 = icmp eq i8 %998, 0
  store i1 %999, i1* %pf
  %1000 = zext i32 %985 to i64
  store i64 %1000, i64* %rax
  store volatile i64 14499, i64* @assembly_address
  %1001 = load i64* %rax
  %1002 = trunc i64 %1001 to i32
  store i32 %1002, i32* bitcast (i64* @global_var_21a440 to i32*)
  store volatile i64 14505, i64* @assembly_address
  %1003 = load i32* bitcast (i64* @global_var_21a440 to i32*)
  %1004 = zext i32 %1003 to i64
  store i64 %1004, i64* %rax
  store volatile i64 14511, i64* @assembly_address
  %1005 = load i64* %rax
  %1006 = trunc i64 %1005 to i32
  %1007 = load i64* %rax
  %1008 = trunc i64 %1007 to i32
  %1009 = and i32 %1006, %1008
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1010 = icmp eq i32 %1009, 0
  store i1 %1010, i1* %zf
  %1011 = icmp slt i32 %1009, 0
  store i1 %1011, i1* %sf
  %1012 = trunc i32 %1009 to i8
  %1013 = call i8 @llvm.ctpop.i8(i8 %1012)
  %1014 = and i8 %1013, 1
  %1015 = icmp eq i8 %1014, 0
  store i1 %1015, i1* %pf
  store volatile i64 14513, i64* @assembly_address
  %1016 = load i1* %zf
  %1017 = icmp eq i1 %1016, false
  br i1 %1017, label %block_37ed, label %block_38b7

block_38b7:                                       ; preds = %block_37ed
  store volatile i64 14519, i64* @assembly_address
  store i32 0, i32* %stack_var_-36
  store volatile i64 14526, i64* @assembly_address
  store i64 2, i64* %rbx
  store volatile i64 14531, i64* @assembly_address
  %1018 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1019 = zext i32 %1018 to i64
  store i64 %1019, i64* %rax
  store volatile i64 14537, i64* @assembly_address
  %1020 = load i64* %rax
  %1021 = trunc i64 %1020 to i32
  %1022 = add i32 %1021, 1
  %1023 = and i32 %1021, 15
  %1024 = add i32 %1023, 1
  %1025 = icmp ugt i32 %1024, 15
  %1026 = icmp ult i32 %1022, %1021
  %1027 = xor i32 %1021, %1022
  %1028 = xor i32 1, %1022
  %1029 = and i32 %1027, %1028
  %1030 = icmp slt i32 %1029, 0
  store i1 %1025, i1* %az
  store i1 %1026, i1* %cf
  store i1 %1030, i1* %of
  %1031 = icmp eq i32 %1022, 0
  store i1 %1031, i1* %zf
  %1032 = icmp slt i32 %1022, 0
  store i1 %1032, i1* %sf
  %1033 = trunc i32 %1022 to i8
  %1034 = call i8 @llvm.ctpop.i8(i8 %1033)
  %1035 = and i8 %1034, 1
  %1036 = icmp eq i8 %1035, 0
  store i1 %1036, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 14540, i64* @assembly_address
  %1037 = load i64* %rax
  %1038 = trunc i64 %1037 to i32
  store i32 %1038, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 14546, i64* @assembly_address
  %1039 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %1040 = zext i32 %1039 to i64
  store i64 %1040, i64* %rax
  store volatile i64 14552, i64* @assembly_address
  %1041 = load i64* %rax
  %1042 = trunc i64 %1041 to i32
  %1043 = load i64* %rax
  %1044 = trunc i64 %1043 to i32
  %1045 = and i32 %1042, %1044
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1046 = icmp eq i32 %1045, 0
  store i1 %1046, i1* %zf
  %1047 = icmp slt i32 %1045, 0
  store i1 %1047, i1* %sf
  %1048 = trunc i32 %1045 to i8
  %1049 = call i8 @llvm.ctpop.i8(i8 %1048)
  %1050 = and i8 %1049, 1
  %1051 = icmp eq i8 %1050, 0
  store i1 %1051, i1* %pf
  store volatile i64 14554, i64* @assembly_address
  %1052 = load i1* %zf
  br i1 %1052, label %block_3903, label %block_38dc

block_38dc:                                       ; preds = %block_38b7
  store volatile i64 14556, i64* @assembly_address
  %1053 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1054 = zext i32 %1053 to i64
  store i64 %1054, i64* %rax
  store volatile i64 14562, i64* @assembly_address
  %1055 = load i64* %rax
  %1056 = trunc i64 %1055 to i32
  %1057 = zext i32 %1056 to i64
  store i64 %1057, i64* %rdx
  store volatile i64 14564, i64* @assembly_address
  %1058 = load i64* @global_var_2165b8
  store i64 %1058, i64* %rax
  store volatile i64 14571, i64* @assembly_address
  %1059 = load i64* %rdx
  %1060 = load i64* %rax
  %1061 = sub i64 %1059, %1060
  %1062 = and i64 %1059, 15
  %1063 = and i64 %1060, 15
  %1064 = sub i64 %1062, %1063
  %1065 = icmp ugt i64 %1064, 15
  %1066 = icmp ult i64 %1059, %1060
  %1067 = xor i64 %1059, %1060
  %1068 = xor i64 %1059, %1061
  %1069 = and i64 %1067, %1068
  %1070 = icmp slt i64 %1069, 0
  store i1 %1065, i1* %az
  store i1 %1066, i1* %cf
  store i1 %1070, i1* %of
  %1071 = icmp eq i64 %1061, 0
  store i1 %1071, i1* %zf
  %1072 = icmp slt i64 %1061, 0
  store i1 %1072, i1* %sf
  %1073 = trunc i64 %1061 to i8
  %1074 = call i8 @llvm.ctpop.i8(i8 %1073)
  %1075 = and i8 %1074, 1
  %1076 = icmp eq i8 %1075, 0
  store i1 %1076, i1* %pf
  store volatile i64 14574, i64* @assembly_address
  %1077 = load i1* %cf
  %1078 = load i1* %zf
  %1079 = or i1 %1077, %1078
  br i1 %1079, label %block_3903, label %block_38f0

block_38f0:                                       ; preds = %block_38dc
  store volatile i64 14576, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 14581, i64* @assembly_address
  %1080 = load i64* %rax
  store i64 %1080, i64* @global_var_2165b8
  store volatile i64 14588, i64* @assembly_address
  %1081 = sext i32 2 to i64
  %1082 = trunc i64 %1081 to i32
  store i32 %1082, i32* %stack_var_-40
  br label %block_3903

block_3903:                                       ; preds = %block_38f0, %block_38dc, %block_38b7
  store volatile i64 14595, i64* @assembly_address
  %1083 = load i32* %stack_var_-40
  %1084 = sext i32 %1083 to i64
  %1085 = trunc i64 %1084 to i32
  %1086 = and i32 %1085, 15
  %1087 = icmp ugt i32 %1086, 15
  %1088 = icmp ult i32 %1085, 0
  %1089 = xor i32 %1085, 0
  %1090 = and i32 %1089, 0
  %1091 = icmp slt i32 %1090, 0
  store i1 %1087, i1* %az
  store i1 %1088, i1* %cf
  store i1 %1091, i1* %of
  %1092 = icmp eq i32 %1085, 0
  store i1 %1092, i1* %zf
  %1093 = icmp slt i32 %1085, 0
  store i1 %1093, i1* %sf
  %1094 = trunc i32 %1085 to i8
  %1095 = call i8 @llvm.ctpop.i8(i8 %1094)
  %1096 = and i8 %1095, 1
  %1097 = icmp eq i8 %1096, 0
  store i1 %1097, i1* %pf
  store volatile i64 14599, i64* @assembly_address
  %1098 = load i1* %zf
  br i1 %1098, label %block_3b50, label %block_390d

block_390d:                                       ; preds = %block_3903
  store volatile i64 14605, i64* @assembly_address
  %1099 = load i32* %stack_var_-40
  %1100 = sext i32 %1099 to i64
  %1101 = trunc i64 %1100 to i32
  %1102 = zext i32 %1101 to i64
  store i64 %1102, i64* %rax
  store volatile i64 14608, i64* @assembly_address
  %1103 = load i64* %rax
  %1104 = add i64 %1103, -1
  %1105 = trunc i64 %1104 to i32
  %1106 = zext i32 %1105 to i64
  store i64 %1106, i64* %rdx
  store volatile i64 14611, i64* @assembly_address
  %1107 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1108 = zext i32 %1107 to i64
  store i64 %1108, i64* %rax
  store volatile i64 14617, i64* @assembly_address
  %1109 = load i64* %rax
  %1110 = trunc i64 %1109 to i32
  %1111 = zext i32 %1110 to i64
  store i64 %1111, i64* %rcx
  store volatile i64 14619, i64* @assembly_address
  %1112 = load i64* @global_var_21a430
  store i64 %1112, i64* %rax
  store volatile i64 14626, i64* @assembly_address
  %1113 = load i64* %rcx
  %1114 = load i64* %rax
  %1115 = sub i64 %1113, %1114
  %1116 = and i64 %1113, 15
  %1117 = and i64 %1114, 15
  %1118 = sub i64 %1116, %1117
  %1119 = icmp ugt i64 %1118, 15
  %1120 = icmp ult i64 %1113, %1114
  %1121 = xor i64 %1113, %1114
  %1122 = xor i64 %1113, %1115
  %1123 = and i64 %1121, %1122
  %1124 = icmp slt i64 %1123, 0
  store i1 %1119, i1* %az
  store i1 %1120, i1* %cf
  store i1 %1124, i1* %of
  %1125 = icmp eq i64 %1115, 0
  store i1 %1125, i1* %zf
  %1126 = icmp slt i64 %1115, 0
  store i1 %1126, i1* %sf
  %1127 = trunc i64 %1115 to i8
  %1128 = call i8 @llvm.ctpop.i8(i8 %1127)
  %1129 = and i8 %1128, 1
  %1130 = icmp eq i8 %1129, 0
  store i1 %1130, i1* %pf
  store i64 %1115, i64* %rcx
  store volatile i64 14629, i64* @assembly_address
  %1131 = load i64* %rcx
  store i64 %1131, i64* %rax
  store volatile i64 14632, i64* @assembly_address
  %1132 = load i64* %rax
  store i64 %1132, i64* %rsi
  store volatile i64 14635, i64* @assembly_address
  %1133 = load i64* @global_var_21a430
  store i64 %1133, i64* %rax
  store volatile i64 14642, i64* @assembly_address
  %1134 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1135 = icmp eq i64 %1134, 0
  store i1 %1135, i1* %zf
  %1136 = icmp slt i64 %1134, 0
  store i1 %1136, i1* %sf
  %1137 = trunc i64 %1134 to i8
  %1138 = call i8 @llvm.ctpop.i8(i8 %1137)
  %1139 = and i8 %1138, 1
  %1140 = icmp eq i8 %1139, 0
  store i1 %1140, i1* %pf
  store volatile i64 14645, i64* @assembly_address
  %1141 = load i1* %sf
  br i1 %1141, label %block_394c, label %block_3937

block_3937:                                       ; preds = %block_390d
  store volatile i64 14647, i64* @assembly_address
  %1142 = load i64* @global_var_21a430
  store i64 %1142, i64* %rax
  store volatile i64 14654, i64* @assembly_address
  %1143 = load i64* %rax
  %1144 = trunc i64 %1143 to i32
  %1145 = zext i32 %1144 to i64
  store i64 %1145, i64* %rcx
  store volatile i64 14656, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 14663, i64* @assembly_address
  %1146 = load i64* %rax
  %1147 = load i64* %rcx
  %1148 = add i64 %1146, %1147
  %1149 = and i64 %1146, 15
  %1150 = and i64 %1147, 15
  %1151 = add i64 %1149, %1150
  %1152 = icmp ugt i64 %1151, 15
  %1153 = icmp ult i64 %1148, %1146
  %1154 = xor i64 %1146, %1148
  %1155 = xor i64 %1147, %1148
  %1156 = and i64 %1154, %1155
  %1157 = icmp slt i64 %1156, 0
  store i1 %1152, i1* %az
  store i1 %1153, i1* %cf
  store i1 %1157, i1* %of
  %1158 = icmp eq i64 %1148, 0
  store i1 %1158, i1* %zf
  %1159 = icmp slt i64 %1148, 0
  store i1 %1159, i1* %sf
  %1160 = trunc i64 %1148 to i8
  %1161 = call i8 @llvm.ctpop.i8(i8 %1160)
  %1162 = and i8 %1161, 1
  %1163 = icmp eq i8 %1162, 0
  store i1 %1163, i1* %pf
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 14666, i64* @assembly_address
  br label %block_3951

block_394c:                                       ; preds = %block_390d
  store volatile i64 14668, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_3951

block_3951:                                       ; preds = %block_394c, %block_3937
  store volatile i64 14673, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 14678, i64* @assembly_address
  %1164 = load i64* %rax
  store i64 %1164, i64* %rdi
  store volatile i64 14681, i64* @assembly_address
  %1165 = load i64* %rdi
  %1166 = load i64* %rsi
  %1167 = load i64* %rdx
  %1168 = load i64* %rcx
  %1169 = trunc i64 %1166 to i32
  %1170 = call i64 @flush_block(i64 %1165, i32 %1169, i64 %1167, i64 %1168)
  store i64 %1170, i64* %rax
  store i64 %1170, i64* %rax
  store volatile i64 14686, i64* @assembly_address
  %1171 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1172 = zext i32 %1171 to i64
  store i64 %1172, i64* %rax
  store volatile i64 14692, i64* @assembly_address
  %1173 = load i64* %rax
  %1174 = trunc i64 %1173 to i32
  %1175 = zext i32 %1174 to i64
  store i64 %1175, i64* %rax
  store volatile i64 14694, i64* @assembly_address
  %1176 = load i64* %rax
  store i64 %1176, i64* @global_var_21a430
  store volatile i64 14701, i64* @assembly_address
  br label %block_3b50

block_3972:                                       ; preds = %block_3772, %block_3763
  store volatile i64 14706, i64* @assembly_address
  %1177 = load i32* %stack_var_-36
  %1178 = and i32 %1177, 15
  %1179 = icmp ugt i32 %1178, 15
  %1180 = icmp ult i32 %1177, 0
  %1181 = xor i32 %1177, 0
  %1182 = and i32 %1181, 0
  %1183 = icmp slt i32 %1182, 0
  store i1 %1179, i1* %az
  store i1 %1180, i1* %cf
  store i1 %1183, i1* %of
  %1184 = icmp eq i32 %1177, 0
  store i1 %1184, i1* %zf
  %1185 = icmp slt i32 %1177, 0
  store i1 %1185, i1* %sf
  %1186 = trunc i32 %1177 to i8
  %1187 = call i8 @llvm.ctpop.i8(i8 %1186)
  %1188 = and i8 %1187, 1
  %1189 = icmp eq i8 %1188, 0
  store i1 %1189, i1* %pf
  store volatile i64 14710, i64* @assembly_address
  %1190 = load i1* %zf
  br i1 %1190, label %block_3a7a, label %block_397c

block_397c:                                       ; preds = %block_3972
  store volatile i64 14716, i64* @assembly_address
  %1191 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1192 = zext i32 %1191 to i64
  store i64 %1192, i64* %rax
  store volatile i64 14722, i64* @assembly_address
  %1193 = load i64* %rax
  %1194 = trunc i64 %1193 to i32
  %1195 = sub i32 %1194, 1
  %1196 = and i32 %1194, 15
  %1197 = sub i32 %1196, 1
  %1198 = icmp ugt i32 %1197, 15
  %1199 = icmp ult i32 %1194, 1
  %1200 = xor i32 %1194, 1
  %1201 = xor i32 %1194, %1195
  %1202 = and i32 %1200, %1201
  %1203 = icmp slt i32 %1202, 0
  store i1 %1198, i1* %az
  store i1 %1199, i1* %cf
  store i1 %1203, i1* %of
  %1204 = icmp eq i32 %1195, 0
  store i1 %1204, i1* %zf
  %1205 = icmp slt i32 %1195, 0
  store i1 %1205, i1* %sf
  %1206 = trunc i32 %1195 to i8
  %1207 = call i8 @llvm.ctpop.i8(i8 %1206)
  %1208 = and i8 %1207, 1
  %1209 = icmp eq i8 %1208, 0
  store i1 %1209, i1* %pf
  %1210 = zext i32 %1195 to i64
  store i64 %1210, i64* %rax
  store volatile i64 14725, i64* @assembly_address
  %1211 = load i64* %rax
  %1212 = trunc i64 %1211 to i32
  %1213 = zext i32 %1212 to i64
  store i64 %1213, i64* %rdx
  store volatile i64 14727, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 14734, i64* @assembly_address
  %1214 = load i64* %rdx
  %1215 = load i64* %rax
  %1216 = mul i64 %1215, 1
  %1217 = add i64 %1214, %1216
  %1218 = inttoptr i64 %1217 to i8*
  %1219 = load i8* %1218
  %1220 = zext i8 %1219 to i64
  store i64 %1220, i64* %rax
  store volatile i64 14738, i64* @assembly_address
  %1221 = load i64* %rax
  %1222 = trunc i64 %1221 to i8
  %1223 = zext i8 %1222 to i64
  store i64 %1223, i64* %rax
  store volatile i64 14741, i64* @assembly_address
  %1224 = load i64* %rax
  %1225 = trunc i64 %1224 to i32
  %1226 = zext i32 %1225 to i64
  store i64 %1226, i64* %rsi
  store volatile i64 14743, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 14748, i64* @assembly_address
  %1227 = load i64* %rdi
  %1228 = load i64* %rsi
  %1229 = trunc i64 %1227 to i32
  %1230 = call i64 @ct_tally(i32 %1229, i64 %1228)
  store i64 %1230, i64* %rax
  store i64 %1230, i64* %rax
  store volatile i64 14753, i64* @assembly_address
  %1231 = load i64* %rax
  %1232 = trunc i64 %1231 to i32
  %1233 = sext i32 %1232 to i64
  %1234 = trunc i64 %1233 to i32
  store i32 %1234, i32* %stack_var_-40
  store volatile i64 14756, i64* @assembly_address
  %1235 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %1236 = zext i32 %1235 to i64
  store i64 %1236, i64* %rax
  store volatile i64 14762, i64* @assembly_address
  %1237 = load i64* %rax
  %1238 = trunc i64 %1237 to i32
  %1239 = load i64* %rax
  %1240 = trunc i64 %1239 to i32
  %1241 = and i32 %1238, %1240
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1242 = icmp eq i32 %1241, 0
  store i1 %1242, i1* %zf
  %1243 = icmp slt i32 %1241, 0
  store i1 %1243, i1* %sf
  %1244 = trunc i32 %1241 to i8
  %1245 = call i8 @llvm.ctpop.i8(i8 %1244)
  %1246 = and i8 %1245, 1
  %1247 = icmp eq i8 %1246, 0
  store i1 %1247, i1* %pf
  store volatile i64 14764, i64* @assembly_address
  %1248 = load i1* %zf
  br i1 %1248, label %block_39d5, label %block_39ae

block_39ae:                                       ; preds = %block_397c
  store volatile i64 14766, i64* @assembly_address
  %1249 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1250 = zext i32 %1249 to i64
  store i64 %1250, i64* %rax
  store volatile i64 14772, i64* @assembly_address
  %1251 = load i64* %rax
  %1252 = trunc i64 %1251 to i32
  %1253 = zext i32 %1252 to i64
  store i64 %1253, i64* %rdx
  store volatile i64 14774, i64* @assembly_address
  %1254 = load i64* @global_var_2165b8
  store i64 %1254, i64* %rax
  store volatile i64 14781, i64* @assembly_address
  %1255 = load i64* %rdx
  %1256 = load i64* %rax
  %1257 = sub i64 %1255, %1256
  %1258 = and i64 %1255, 15
  %1259 = and i64 %1256, 15
  %1260 = sub i64 %1258, %1259
  %1261 = icmp ugt i64 %1260, 15
  %1262 = icmp ult i64 %1255, %1256
  %1263 = xor i64 %1255, %1256
  %1264 = xor i64 %1255, %1257
  %1265 = and i64 %1263, %1264
  %1266 = icmp slt i64 %1265, 0
  store i1 %1261, i1* %az
  store i1 %1262, i1* %cf
  store i1 %1266, i1* %of
  %1267 = icmp eq i64 %1257, 0
  store i1 %1267, i1* %zf
  %1268 = icmp slt i64 %1257, 0
  store i1 %1268, i1* %sf
  %1269 = trunc i64 %1257 to i8
  %1270 = call i8 @llvm.ctpop.i8(i8 %1269)
  %1271 = and i8 %1270, 1
  %1272 = icmp eq i8 %1271, 0
  store i1 %1272, i1* %pf
  store volatile i64 14784, i64* @assembly_address
  %1273 = load i1* %cf
  %1274 = load i1* %zf
  %1275 = or i1 %1273, %1274
  br i1 %1275, label %block_39d5, label %block_39c2

block_39c2:                                       ; preds = %block_39ae
  store volatile i64 14786, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 14791, i64* @assembly_address
  %1276 = load i64* %rax
  store i64 %1276, i64* @global_var_2165b8
  store volatile i64 14798, i64* @assembly_address
  %1277 = sext i32 2 to i64
  %1278 = trunc i64 %1277 to i32
  store i32 %1278, i32* %stack_var_-40
  br label %block_39d5

block_39d5:                                       ; preds = %block_39c2, %block_39ae, %block_397c
  store volatile i64 14805, i64* @assembly_address
  %1279 = load i32* %stack_var_-40
  %1280 = sext i32 %1279 to i64
  %1281 = trunc i64 %1280 to i32
  %1282 = and i32 %1281, 15
  %1283 = icmp ugt i32 %1282, 15
  %1284 = icmp ult i32 %1281, 0
  %1285 = xor i32 %1281, 0
  %1286 = and i32 %1285, 0
  %1287 = icmp slt i32 %1286, 0
  store i1 %1283, i1* %az
  store i1 %1284, i1* %cf
  store i1 %1287, i1* %of
  %1288 = icmp eq i32 %1281, 0
  store i1 %1288, i1* %zf
  %1289 = icmp slt i32 %1281, 0
  store i1 %1289, i1* %sf
  %1290 = trunc i32 %1281 to i8
  %1291 = call i8 @llvm.ctpop.i8(i8 %1290)
  %1292 = and i8 %1291, 1
  %1293 = icmp eq i8 %1292, 0
  store i1 %1293, i1* %pf
  store volatile i64 14809, i64* @assembly_address
  %1294 = load i1* %zf
  br i1 %1294, label %block_3a3b, label %block_39db

block_39db:                                       ; preds = %block_39d5
  store volatile i64 14811, i64* @assembly_address
  %1295 = load i32* %stack_var_-40
  %1296 = sext i32 %1295 to i64
  %1297 = trunc i64 %1296 to i32
  %1298 = zext i32 %1297 to i64
  store i64 %1298, i64* %rax
  store volatile i64 14814, i64* @assembly_address
  %1299 = load i64* %rax
  %1300 = add i64 %1299, -1
  %1301 = trunc i64 %1300 to i32
  %1302 = zext i32 %1301 to i64
  store i64 %1302, i64* %rdx
  store volatile i64 14817, i64* @assembly_address
  %1303 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1304 = zext i32 %1303 to i64
  store i64 %1304, i64* %rax
  store volatile i64 14823, i64* @assembly_address
  %1305 = load i64* %rax
  %1306 = trunc i64 %1305 to i32
  %1307 = zext i32 %1306 to i64
  store i64 %1307, i64* %rcx
  store volatile i64 14825, i64* @assembly_address
  %1308 = load i64* @global_var_21a430
  store i64 %1308, i64* %rax
  store volatile i64 14832, i64* @assembly_address
  %1309 = load i64* %rcx
  %1310 = load i64* %rax
  %1311 = sub i64 %1309, %1310
  %1312 = and i64 %1309, 15
  %1313 = and i64 %1310, 15
  %1314 = sub i64 %1312, %1313
  %1315 = icmp ugt i64 %1314, 15
  %1316 = icmp ult i64 %1309, %1310
  %1317 = xor i64 %1309, %1310
  %1318 = xor i64 %1309, %1311
  %1319 = and i64 %1317, %1318
  %1320 = icmp slt i64 %1319, 0
  store i1 %1315, i1* %az
  store i1 %1316, i1* %cf
  store i1 %1320, i1* %of
  %1321 = icmp eq i64 %1311, 0
  store i1 %1321, i1* %zf
  %1322 = icmp slt i64 %1311, 0
  store i1 %1322, i1* %sf
  %1323 = trunc i64 %1311 to i8
  %1324 = call i8 @llvm.ctpop.i8(i8 %1323)
  %1325 = and i8 %1324, 1
  %1326 = icmp eq i8 %1325, 0
  store i1 %1326, i1* %pf
  store i64 %1311, i64* %rcx
  store volatile i64 14835, i64* @assembly_address
  %1327 = load i64* %rcx
  store i64 %1327, i64* %rax
  store volatile i64 14838, i64* @assembly_address
  %1328 = load i64* %rax
  store i64 %1328, i64* %rsi
  store volatile i64 14841, i64* @assembly_address
  %1329 = load i64* @global_var_21a430
  store i64 %1329, i64* %rax
  store volatile i64 14848, i64* @assembly_address
  %1330 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1331 = icmp eq i64 %1330, 0
  store i1 %1331, i1* %zf
  %1332 = icmp slt i64 %1330, 0
  store i1 %1332, i1* %sf
  %1333 = trunc i64 %1330 to i8
  %1334 = call i8 @llvm.ctpop.i8(i8 %1333)
  %1335 = and i8 %1334, 1
  %1336 = icmp eq i8 %1335, 0
  store i1 %1336, i1* %pf
  store volatile i64 14851, i64* @assembly_address
  %1337 = load i1* %sf
  br i1 %1337, label %block_3a1a, label %block_3a05

block_3a05:                                       ; preds = %block_39db
  store volatile i64 14853, i64* @assembly_address
  %1338 = load i64* @global_var_21a430
  store i64 %1338, i64* %rax
  store volatile i64 14860, i64* @assembly_address
  %1339 = load i64* %rax
  %1340 = trunc i64 %1339 to i32
  %1341 = zext i32 %1340 to i64
  store i64 %1341, i64* %rcx
  store volatile i64 14862, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 14869, i64* @assembly_address
  %1342 = load i64* %rax
  %1343 = load i64* %rcx
  %1344 = add i64 %1342, %1343
  %1345 = and i64 %1342, 15
  %1346 = and i64 %1343, 15
  %1347 = add i64 %1345, %1346
  %1348 = icmp ugt i64 %1347, 15
  %1349 = icmp ult i64 %1344, %1342
  %1350 = xor i64 %1342, %1344
  %1351 = xor i64 %1343, %1344
  %1352 = and i64 %1350, %1351
  %1353 = icmp slt i64 %1352, 0
  store i1 %1348, i1* %az
  store i1 %1349, i1* %cf
  store i1 %1353, i1* %of
  %1354 = icmp eq i64 %1344, 0
  store i1 %1354, i1* %zf
  %1355 = icmp slt i64 %1344, 0
  store i1 %1355, i1* %sf
  %1356 = trunc i64 %1344 to i8
  %1357 = call i8 @llvm.ctpop.i8(i8 %1356)
  %1358 = and i8 %1357, 1
  %1359 = icmp eq i8 %1358, 0
  store i1 %1359, i1* %pf
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 14872, i64* @assembly_address
  br label %block_3a1f

block_3a1a:                                       ; preds = %block_39db
  store volatile i64 14874, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_3a1f

block_3a1f:                                       ; preds = %block_3a1a, %block_3a05
  store volatile i64 14879, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 14884, i64* @assembly_address
  %1360 = load i64* %rax
  store i64 %1360, i64* %rdi
  store volatile i64 14887, i64* @assembly_address
  %1361 = load i64* %rdi
  %1362 = load i64* %rsi
  %1363 = load i64* %rdx
  %1364 = load i64* %rcx
  %1365 = trunc i64 %1362 to i32
  %1366 = call i64 @flush_block(i64 %1361, i32 %1365, i64 %1363, i64 %1364)
  store i64 %1366, i64* %rax
  store i64 %1366, i64* %rax
  store volatile i64 14892, i64* @assembly_address
  %1367 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1368 = zext i32 %1367 to i64
  store i64 %1368, i64* %rax
  store volatile i64 14898, i64* @assembly_address
  %1369 = load i64* %rax
  %1370 = trunc i64 %1369 to i32
  %1371 = zext i32 %1370 to i64
  store i64 %1371, i64* %rax
  store volatile i64 14900, i64* @assembly_address
  %1372 = load i64* %rax
  store i64 %1372, i64* @global_var_21a430
  br label %block_3a3b

block_3a3b:                                       ; preds = %block_3a1f, %block_39d5
  store volatile i64 14907, i64* @assembly_address
  %1373 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %1374 = zext i32 %1373 to i64
  store i64 %1374, i64* %rax
  store volatile i64 14913, i64* @assembly_address
  %1375 = load i64* %rax
  %1376 = trunc i64 %1375 to i32
  %1377 = load i64* %rax
  %1378 = trunc i64 %1377 to i32
  %1379 = and i32 %1376, %1378
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1380 = icmp eq i32 %1379, 0
  store i1 %1380, i1* %zf
  %1381 = icmp slt i32 %1379, 0
  store i1 %1381, i1* %sf
  %1382 = trunc i32 %1379 to i8
  %1383 = call i8 @llvm.ctpop.i8(i8 %1382)
  %1384 = and i8 %1383, 1
  %1385 = icmp eq i8 %1384, 0
  store i1 %1385, i1* %pf
  store volatile i64 14915, i64* @assembly_address
  %1386 = load i1* %zf
  br i1 %1386, label %block_3a57, label %block_3a45

block_3a45:                                       ; preds = %block_3a3b
  store volatile i64 14917, i64* @assembly_address
  %1387 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1388 = zext i32 %1387 to i64
  store i64 %1388, i64* %rax
  store volatile i64 14923, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 14928, i64* @assembly_address
  %1389 = load i64* %rax
  %1390 = trunc i64 %1389 to i32
  %1391 = zext i32 %1390 to i64
  store i64 %1391, i64* %rdi
  store volatile i64 14930, i64* @assembly_address
  %1392 = load i64* %rdi
  %1393 = load i64* %rsi
  %1394 = trunc i64 %1392 to i32
  %1395 = call i64 @rsync_roll(i32 %1394, i64 %1393)
  store i64 %1395, i64* %rax
  store i64 %1395, i64* %rax
  br label %block_3a57

block_3a57:                                       ; preds = %block_3a45, %block_3a3b
  store volatile i64 14935, i64* @assembly_address
  %1396 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1397 = zext i32 %1396 to i64
  store i64 %1397, i64* %rax
  store volatile i64 14941, i64* @assembly_address
  %1398 = load i64* %rax
  %1399 = trunc i64 %1398 to i32
  %1400 = add i32 %1399, 1
  %1401 = and i32 %1399, 15
  %1402 = add i32 %1401, 1
  %1403 = icmp ugt i32 %1402, 15
  %1404 = icmp ult i32 %1400, %1399
  %1405 = xor i32 %1399, %1400
  %1406 = xor i32 1, %1400
  %1407 = and i32 %1405, %1406
  %1408 = icmp slt i32 %1407, 0
  store i1 %1403, i1* %az
  store i1 %1404, i1* %cf
  store i1 %1408, i1* %of
  %1409 = icmp eq i32 %1400, 0
  store i1 %1409, i1* %zf
  %1410 = icmp slt i32 %1400, 0
  store i1 %1410, i1* %sf
  %1411 = trunc i32 %1400 to i8
  %1412 = call i8 @llvm.ctpop.i8(i8 %1411)
  %1413 = and i8 %1412, 1
  %1414 = icmp eq i8 %1413, 0
  store i1 %1414, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 14944, i64* @assembly_address
  %1415 = load i64* %rax
  %1416 = trunc i64 %1415 to i32
  store i32 %1416, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 14950, i64* @assembly_address
  %1417 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %1418 = zext i32 %1417 to i64
  store i64 %1418, i64* %rax
  store volatile i64 14956, i64* @assembly_address
  %1419 = load i64* %rax
  %1420 = trunc i64 %1419 to i32
  %1421 = sub i32 %1420, 1
  %1422 = and i32 %1420, 15
  %1423 = sub i32 %1422, 1
  %1424 = icmp ugt i32 %1423, 15
  %1425 = icmp ult i32 %1420, 1
  %1426 = xor i32 %1420, 1
  %1427 = xor i32 %1420, %1421
  %1428 = and i32 %1426, %1427
  %1429 = icmp slt i32 %1428, 0
  store i1 %1424, i1* %az
  store i1 %1425, i1* %cf
  store i1 %1429, i1* %of
  %1430 = icmp eq i32 %1421, 0
  store i1 %1430, i1* %zf
  %1431 = icmp slt i32 %1421, 0
  store i1 %1431, i1* %sf
  %1432 = trunc i32 %1421 to i8
  %1433 = call i8 @llvm.ctpop.i8(i8 %1432)
  %1434 = and i8 %1433, 1
  %1435 = icmp eq i8 %1434, 0
  store i1 %1435, i1* %pf
  %1436 = zext i32 %1421 to i64
  store i64 %1436, i64* %rax
  store volatile i64 14959, i64* @assembly_address
  %1437 = load i64* %rax
  %1438 = trunc i64 %1437 to i32
  store i32 %1438, i32* bitcast (i64* @global_var_2165a0 to i32*)
  store volatile i64 14965, i64* @assembly_address
  br label %block_3b57

block_3a7a:                                       ; preds = %block_3972
  store volatile i64 14970, i64* @assembly_address
  %1439 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %1440 = zext i32 %1439 to i64
  store i64 %1440, i64* %rax
  store volatile i64 14976, i64* @assembly_address
  %1441 = load i64* %rax
  %1442 = trunc i64 %1441 to i32
  %1443 = load i64* %rax
  %1444 = trunc i64 %1443 to i32
  %1445 = and i32 %1442, %1444
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1446 = icmp eq i32 %1445, 0
  store i1 %1446, i1* %zf
  %1447 = icmp slt i32 %1445, 0
  store i1 %1447, i1* %sf
  %1448 = trunc i32 %1445 to i8
  %1449 = call i8 @llvm.ctpop.i8(i8 %1448)
  %1450 = and i8 %1449, 1
  %1451 = icmp eq i8 %1450, 0
  store i1 %1451, i1* %pf
  store volatile i64 14978, i64* @assembly_address
  %1452 = load i1* %zf
  br i1 %1452, label %block_3b0f, label %block_3a88

block_3a88:                                       ; preds = %block_3a7a
  store volatile i64 14984, i64* @assembly_address
  %1453 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1454 = zext i32 %1453 to i64
  store i64 %1454, i64* %rax
  store volatile i64 14990, i64* @assembly_address
  %1455 = load i64* %rax
  %1456 = trunc i64 %1455 to i32
  %1457 = zext i32 %1456 to i64
  store i64 %1457, i64* %rdx
  store volatile i64 14992, i64* @assembly_address
  %1458 = load i64* @global_var_2165b8
  store i64 %1458, i64* %rax
  store volatile i64 14999, i64* @assembly_address
  %1459 = load i64* %rdx
  %1460 = load i64* %rax
  %1461 = sub i64 %1459, %1460
  %1462 = and i64 %1459, 15
  %1463 = and i64 %1460, 15
  %1464 = sub i64 %1462, %1463
  %1465 = icmp ugt i64 %1464, 15
  %1466 = icmp ult i64 %1459, %1460
  %1467 = xor i64 %1459, %1460
  %1468 = xor i64 %1459, %1461
  %1469 = and i64 %1467, %1468
  %1470 = icmp slt i64 %1469, 0
  store i1 %1465, i1* %az
  store i1 %1466, i1* %cf
  store i1 %1470, i1* %of
  %1471 = icmp eq i64 %1461, 0
  store i1 %1471, i1* %zf
  %1472 = icmp slt i64 %1461, 0
  store i1 %1472, i1* %sf
  %1473 = trunc i64 %1461 to i8
  %1474 = call i8 @llvm.ctpop.i8(i8 %1473)
  %1475 = and i8 %1474, 1
  %1476 = icmp eq i8 %1475, 0
  store i1 %1476, i1* %pf
  store volatile i64 15002, i64* @assembly_address
  %1477 = load i1* %cf
  %1478 = load i1* %zf
  %1479 = or i1 %1477, %1478
  br i1 %1479, label %block_3b0f, label %block_3a9c

block_3a9c:                                       ; preds = %block_3a88
  store volatile i64 15004, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 15009, i64* @assembly_address
  %1480 = load i64* %rax
  store i64 %1480, i64* @global_var_2165b8
  store volatile i64 15016, i64* @assembly_address
  %1481 = sext i32 2 to i64
  %1482 = trunc i64 %1481 to i32
  store i32 %1482, i32* %stack_var_-40
  store volatile i64 15023, i64* @assembly_address
  %1483 = load i32* %stack_var_-40
  %1484 = sext i32 %1483 to i64
  %1485 = trunc i64 %1484 to i32
  %1486 = zext i32 %1485 to i64
  store i64 %1486, i64* %rax
  store volatile i64 15026, i64* @assembly_address
  %1487 = load i64* %rax
  %1488 = add i64 %1487, -1
  %1489 = trunc i64 %1488 to i32
  %1490 = zext i32 %1489 to i64
  store i64 %1490, i64* %rdx
  store volatile i64 15029, i64* @assembly_address
  %1491 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1492 = zext i32 %1491 to i64
  store i64 %1492, i64* %rax
  store volatile i64 15035, i64* @assembly_address
  %1493 = load i64* %rax
  %1494 = trunc i64 %1493 to i32
  %1495 = zext i32 %1494 to i64
  store i64 %1495, i64* %rcx
  store volatile i64 15037, i64* @assembly_address
  %1496 = load i64* @global_var_21a430
  store i64 %1496, i64* %rax
  store volatile i64 15044, i64* @assembly_address
  %1497 = load i64* %rcx
  %1498 = load i64* %rax
  %1499 = sub i64 %1497, %1498
  %1500 = and i64 %1497, 15
  %1501 = and i64 %1498, 15
  %1502 = sub i64 %1500, %1501
  %1503 = icmp ugt i64 %1502, 15
  %1504 = icmp ult i64 %1497, %1498
  %1505 = xor i64 %1497, %1498
  %1506 = xor i64 %1497, %1499
  %1507 = and i64 %1505, %1506
  %1508 = icmp slt i64 %1507, 0
  store i1 %1503, i1* %az
  store i1 %1504, i1* %cf
  store i1 %1508, i1* %of
  %1509 = icmp eq i64 %1499, 0
  store i1 %1509, i1* %zf
  %1510 = icmp slt i64 %1499, 0
  store i1 %1510, i1* %sf
  %1511 = trunc i64 %1499 to i8
  %1512 = call i8 @llvm.ctpop.i8(i8 %1511)
  %1513 = and i8 %1512, 1
  %1514 = icmp eq i8 %1513, 0
  store i1 %1514, i1* %pf
  store i64 %1499, i64* %rcx
  store volatile i64 15047, i64* @assembly_address
  %1515 = load i64* %rcx
  store i64 %1515, i64* %rax
  store volatile i64 15050, i64* @assembly_address
  %1516 = load i64* %rax
  store i64 %1516, i64* %rsi
  store volatile i64 15053, i64* @assembly_address
  %1517 = load i64* @global_var_21a430
  store i64 %1517, i64* %rax
  store volatile i64 15060, i64* @assembly_address
  %1518 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1519 = icmp eq i64 %1518, 0
  store i1 %1519, i1* %zf
  %1520 = icmp slt i64 %1518, 0
  store i1 %1520, i1* %sf
  %1521 = trunc i64 %1518 to i8
  %1522 = call i8 @llvm.ctpop.i8(i8 %1521)
  %1523 = and i8 %1522, 1
  %1524 = icmp eq i8 %1523, 0
  store i1 %1524, i1* %pf
  store volatile i64 15063, i64* @assembly_address
  %1525 = load i1* %sf
  br i1 %1525, label %block_3aee, label %block_3ad9

block_3ad9:                                       ; preds = %block_3a9c
  store volatile i64 15065, i64* @assembly_address
  %1526 = load i64* @global_var_21a430
  store i64 %1526, i64* %rax
  store volatile i64 15072, i64* @assembly_address
  %1527 = load i64* %rax
  %1528 = trunc i64 %1527 to i32
  %1529 = zext i32 %1528 to i64
  store i64 %1529, i64* %rcx
  store volatile i64 15074, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 15081, i64* @assembly_address
  %1530 = load i64* %rax
  %1531 = load i64* %rcx
  %1532 = add i64 %1530, %1531
  %1533 = and i64 %1530, 15
  %1534 = and i64 %1531, 15
  %1535 = add i64 %1533, %1534
  %1536 = icmp ugt i64 %1535, 15
  %1537 = icmp ult i64 %1532, %1530
  %1538 = xor i64 %1530, %1532
  %1539 = xor i64 %1531, %1532
  %1540 = and i64 %1538, %1539
  %1541 = icmp slt i64 %1540, 0
  store i1 %1536, i1* %az
  store i1 %1537, i1* %cf
  store i1 %1541, i1* %of
  %1542 = icmp eq i64 %1532, 0
  store i1 %1542, i1* %zf
  %1543 = icmp slt i64 %1532, 0
  store i1 %1543, i1* %sf
  %1544 = trunc i64 %1532 to i8
  %1545 = call i8 @llvm.ctpop.i8(i8 %1544)
  %1546 = and i8 %1545, 1
  %1547 = icmp eq i8 %1546, 0
  store i1 %1547, i1* %pf
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 15084, i64* @assembly_address
  br label %block_3af3

block_3aee:                                       ; preds = %block_3a9c
  store volatile i64 15086, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_3af3

block_3af3:                                       ; preds = %block_3aee, %block_3ad9
  store volatile i64 15091, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 15096, i64* @assembly_address
  %1548 = load i64* %rax
  store i64 %1548, i64* %rdi
  store volatile i64 15099, i64* @assembly_address
  %1549 = load i64* %rdi
  %1550 = load i64* %rsi
  %1551 = load i64* %rdx
  %1552 = load i64* %rcx
  %1553 = trunc i64 %1550 to i32
  %1554 = call i64 @flush_block(i64 %1549, i32 %1553, i64 %1551, i64 %1552)
  store i64 %1554, i64* %rax
  store i64 %1554, i64* %rax
  store volatile i64 15104, i64* @assembly_address
  %1555 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1556 = zext i32 %1555 to i64
  store i64 %1556, i64* %rax
  store volatile i64 15110, i64* @assembly_address
  %1557 = load i64* %rax
  %1558 = trunc i64 %1557 to i32
  %1559 = zext i32 %1558 to i64
  store i64 %1559, i64* %rax
  store volatile i64 15112, i64* @assembly_address
  %1560 = load i64* %rax
  store i64 %1560, i64* @global_var_21a430
  br label %block_3b0f

block_3b0f:                                       ; preds = %block_3af3, %block_3a88, %block_3a7a
  store volatile i64 15119, i64* @assembly_address
  store i32 1, i32* %stack_var_-36
  store volatile i64 15126, i64* @assembly_address
  %1561 = load i32* bitcast (i64* @global_var_2165f4 to i32*)
  %1562 = zext i32 %1561 to i64
  store i64 %1562, i64* %rax
  store volatile i64 15132, i64* @assembly_address
  %1563 = load i64* %rax
  %1564 = trunc i64 %1563 to i32
  %1565 = load i64* %rax
  %1566 = trunc i64 %1565 to i32
  %1567 = and i32 %1564, %1566
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1568 = icmp eq i32 %1567, 0
  store i1 %1568, i1* %zf
  %1569 = icmp slt i32 %1567, 0
  store i1 %1569, i1* %sf
  %1570 = trunc i32 %1567 to i8
  %1571 = call i8 @llvm.ctpop.i8(i8 %1570)
  %1572 = and i8 %1571, 1
  %1573 = icmp eq i8 %1572, 0
  store i1 %1573, i1* %pf
  store volatile i64 15134, i64* @assembly_address
  %1574 = load i1* %zf
  br i1 %1574, label %block_3b32, label %block_3b20

block_3b20:                                       ; preds = %block_3b0f
  store volatile i64 15136, i64* @assembly_address
  %1575 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1576 = zext i32 %1575 to i64
  store i64 %1576, i64* %rax
  store volatile i64 15142, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 15147, i64* @assembly_address
  %1577 = load i64* %rax
  %1578 = trunc i64 %1577 to i32
  %1579 = zext i32 %1578 to i64
  store i64 %1579, i64* %rdi
  store volatile i64 15149, i64* @assembly_address
  %1580 = load i64* %rdi
  %1581 = load i64* %rsi
  %1582 = trunc i64 %1580 to i32
  %1583 = call i64 @rsync_roll(i32 %1582, i64 %1581)
  store i64 %1583, i64* %rax
  store i64 %1583, i64* %rax
  br label %block_3b32

block_3b32:                                       ; preds = %block_3b20, %block_3b0f
  store volatile i64 15154, i64* @assembly_address
  %1584 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1585 = zext i32 %1584 to i64
  store i64 %1585, i64* %rax
  store volatile i64 15160, i64* @assembly_address
  %1586 = load i64* %rax
  %1587 = trunc i64 %1586 to i32
  %1588 = add i32 %1587, 1
  %1589 = and i32 %1587, 15
  %1590 = add i32 %1589, 1
  %1591 = icmp ugt i32 %1590, 15
  %1592 = icmp ult i32 %1588, %1587
  %1593 = xor i32 %1587, %1588
  %1594 = xor i32 1, %1588
  %1595 = and i32 %1593, %1594
  %1596 = icmp slt i32 %1595, 0
  store i1 %1591, i1* %az
  store i1 %1592, i1* %cf
  store i1 %1596, i1* %of
  %1597 = icmp eq i32 %1588, 0
  store i1 %1597, i1* %zf
  %1598 = icmp slt i32 %1588, 0
  store i1 %1598, i1* %sf
  %1599 = trunc i32 %1588 to i8
  %1600 = call i8 @llvm.ctpop.i8(i8 %1599)
  %1601 = and i8 %1600, 1
  %1602 = icmp eq i8 %1601, 0
  store i1 %1602, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a429 to i64), i64* %rax
  store volatile i64 15163, i64* @assembly_address
  %1603 = load i64* %rax
  %1604 = trunc i64 %1603 to i32
  store i32 %1604, i32* bitcast (i64* @global_var_21a428 to i32*)
  store volatile i64 15169, i64* @assembly_address
  %1605 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %1606 = zext i32 %1605 to i64
  store i64 %1606, i64* %rax
  store volatile i64 15175, i64* @assembly_address
  %1607 = load i64* %rax
  %1608 = trunc i64 %1607 to i32
  %1609 = sub i32 %1608, 1
  %1610 = and i32 %1608, 15
  %1611 = sub i32 %1610, 1
  %1612 = icmp ugt i32 %1611, 15
  %1613 = icmp ult i32 %1608, 1
  %1614 = xor i32 %1608, 1
  %1615 = xor i32 %1608, %1609
  %1616 = and i32 %1614, %1615
  %1617 = icmp slt i32 %1616, 0
  store i1 %1612, i1* %az
  store i1 %1613, i1* %cf
  store i1 %1617, i1* %of
  %1618 = icmp eq i32 %1609, 0
  store i1 %1618, i1* %zf
  %1619 = icmp slt i32 %1609, 0
  store i1 %1619, i1* %sf
  %1620 = trunc i32 %1609 to i8
  %1621 = call i8 @llvm.ctpop.i8(i8 %1620)
  %1622 = and i8 %1621, 1
  %1623 = icmp eq i8 %1622, 0
  store i1 %1623, i1* %pf
  %1624 = zext i32 %1609 to i64
  store i64 %1624, i64* %rax
  store volatile i64 15178, i64* @assembly_address
  %1625 = load i64* %rax
  %1626 = trunc i64 %1625 to i32
  store i32 %1626, i32* bitcast (i64* @global_var_2165a0 to i32*)
  br label %block_3b50

block_3b50:                                       ; preds = %block_3b32, %block_3951, %block_3903
  store volatile i64 15184, i64* @assembly_address
  br label %block_3b57

block_3b52:                                       ; preds = %block_3b64
  store volatile i64 15186, i64* @assembly_address
  %1627 = call i64 @fill_window()
  store i64 %1627, i64* %rax
  store i64 %1627, i64* %rax
  store i64 %1627, i64* %rax
  br label %block_3b57

block_3b57:                                       ; preds = %block_3b52, %block_3b50, %block_3a57
  store volatile i64 15191, i64* @assembly_address
  %1628 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %1629 = zext i32 %1628 to i64
  store i64 %1629, i64* %rax
  store volatile i64 15197, i64* @assembly_address
  %1630 = load i64* %rax
  %1631 = trunc i64 %1630 to i32
  %1632 = sub i32 %1631, 261
  %1633 = and i32 %1631, 15
  %1634 = sub i32 %1633, 5
  %1635 = icmp ugt i32 %1634, 15
  %1636 = icmp ult i32 %1631, 261
  %1637 = xor i32 %1631, 261
  %1638 = xor i32 %1631, %1632
  %1639 = and i32 %1637, %1638
  %1640 = icmp slt i32 %1639, 0
  store i1 %1635, i1* %az
  store i1 %1636, i1* %cf
  store i1 %1640, i1* %of
  %1641 = icmp eq i32 %1632, 0
  store i1 %1641, i1* %zf
  %1642 = icmp slt i32 %1632, 0
  store i1 %1642, i1* %sf
  %1643 = trunc i32 %1632 to i8
  %1644 = call i8 @llvm.ctpop.i8(i8 %1643)
  %1645 = and i8 %1644, 1
  %1646 = icmp eq i8 %1645, 0
  store i1 %1646, i1* %pf
  store volatile i64 15202, i64* @assembly_address
  %1647 = load i1* %cf
  %1648 = load i1* %zf
  %1649 = or i1 %1647, %1648
  %1650 = icmp ne i1 %1649, true
  br i1 %1650, label %block_3b6e, label %block_3b64

block_3b64:                                       ; preds = %block_3b57
  store volatile i64 15204, i64* @assembly_address
  %1651 = load i32* bitcast (i64* @global_var_21659c to i32*)
  %1652 = zext i32 %1651 to i64
  store i64 %1652, i64* %rax
  store volatile i64 15210, i64* @assembly_address
  %1653 = load i64* %rax
  %1654 = trunc i64 %1653 to i32
  %1655 = load i64* %rax
  %1656 = trunc i64 %1655 to i32
  %1657 = and i32 %1654, %1656
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1658 = icmp eq i32 %1657, 0
  store i1 %1658, i1* %zf
  %1659 = icmp slt i32 %1657, 0
  store i1 %1659, i1* %sf
  %1660 = trunc i32 %1657 to i8
  %1661 = call i8 @llvm.ctpop.i8(i8 %1660)
  %1662 = and i8 %1661, 1
  %1663 = icmp eq i8 %1662, 0
  store i1 %1663, i1* %pf
  store volatile i64 15212, i64* @assembly_address
  %1664 = load i1* %zf
  br i1 %1664, label %block_3b52, label %block_3b6e

block_3b6e:                                       ; preds = %block_3b64, %block_3b57, %block_3601
  store volatile i64 15214, i64* @assembly_address
  %1665 = load i32* bitcast (i64* @global_var_2165a0 to i32*)
  %1666 = zext i32 %1665 to i64
  store i64 %1666, i64* %rax
  store volatile i64 15220, i64* @assembly_address
  %1667 = load i64* %rax
  %1668 = trunc i64 %1667 to i32
  %1669 = load i64* %rax
  %1670 = trunc i64 %1669 to i32
  %1671 = and i32 %1668, %1670
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1672 = icmp eq i32 %1671, 0
  store i1 %1672, i1* %zf
  %1673 = icmp slt i32 %1671, 0
  store i1 %1673, i1* %sf
  %1674 = trunc i32 %1671 to i8
  %1675 = call i8 @llvm.ctpop.i8(i8 %1674)
  %1676 = and i8 %1675, 1
  %1677 = icmp eq i8 %1676, 0
  store i1 %1677, i1* %pf
  store volatile i64 15222, i64* @assembly_address
  %1678 = load i1* %zf
  %1679 = icmp eq i1 %1678, false
  br i1 %1679, label %block_3636, label %block_3b7c

block_3b7c:                                       ; preds = %block_3b6e
  store volatile i64 15228, i64* @assembly_address
  %1680 = load i32* %stack_var_-36
  %1681 = and i32 %1680, 15
  %1682 = icmp ugt i32 %1681, 15
  %1683 = icmp ult i32 %1680, 0
  %1684 = xor i32 %1680, 0
  %1685 = and i32 %1684, 0
  %1686 = icmp slt i32 %1685, 0
  store i1 %1682, i1* %az
  store i1 %1683, i1* %cf
  store i1 %1686, i1* %of
  %1687 = icmp eq i32 %1680, 0
  store i1 %1687, i1* %zf
  %1688 = icmp slt i32 %1680, 0
  store i1 %1688, i1* %sf
  %1689 = trunc i32 %1680 to i8
  %1690 = call i8 @llvm.ctpop.i8(i8 %1689)
  %1691 = and i8 %1690, 1
  %1692 = icmp eq i8 %1691, 0
  store i1 %1692, i1* %pf
  store volatile i64 15232, i64* @assembly_address
  %1693 = load i1* %zf
  br i1 %1693, label %block_3ba7, label %block_3b82

block_3b82:                                       ; preds = %block_3b7c
  store volatile i64 15234, i64* @assembly_address
  %1694 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1695 = zext i32 %1694 to i64
  store i64 %1695, i64* %rax
  store volatile i64 15240, i64* @assembly_address
  %1696 = load i64* %rax
  %1697 = trunc i64 %1696 to i32
  %1698 = sub i32 %1697, 1
  %1699 = and i32 %1697, 15
  %1700 = sub i32 %1699, 1
  %1701 = icmp ugt i32 %1700, 15
  %1702 = icmp ult i32 %1697, 1
  %1703 = xor i32 %1697, 1
  %1704 = xor i32 %1697, %1698
  %1705 = and i32 %1703, %1704
  %1706 = icmp slt i32 %1705, 0
  store i1 %1701, i1* %az
  store i1 %1702, i1* %cf
  store i1 %1706, i1* %of
  %1707 = icmp eq i32 %1698, 0
  store i1 %1707, i1* %zf
  %1708 = icmp slt i32 %1698, 0
  store i1 %1708, i1* %sf
  %1709 = trunc i32 %1698 to i8
  %1710 = call i8 @llvm.ctpop.i8(i8 %1709)
  %1711 = and i8 %1710, 1
  %1712 = icmp eq i8 %1711, 0
  store i1 %1712, i1* %pf
  %1713 = zext i32 %1698 to i64
  store i64 %1713, i64* %rax
  store volatile i64 15243, i64* @assembly_address
  %1714 = load i64* %rax
  %1715 = trunc i64 %1714 to i32
  %1716 = zext i32 %1715 to i64
  store i64 %1716, i64* %rdx
  store volatile i64 15245, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 15252, i64* @assembly_address
  %1717 = load i64* %rdx
  %1718 = load i64* %rax
  %1719 = mul i64 %1718, 1
  %1720 = add i64 %1717, %1719
  %1721 = inttoptr i64 %1720 to i8*
  %1722 = load i8* %1721
  %1723 = zext i8 %1722 to i64
  store i64 %1723, i64* %rax
  store volatile i64 15256, i64* @assembly_address
  %1724 = load i64* %rax
  %1725 = trunc i64 %1724 to i8
  %1726 = zext i8 %1725 to i64
  store i64 %1726, i64* %rax
  store volatile i64 15259, i64* @assembly_address
  %1727 = load i64* %rax
  %1728 = trunc i64 %1727 to i32
  %1729 = zext i32 %1728 to i64
  store i64 %1729, i64* %rsi
  store volatile i64 15261, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 15266, i64* @assembly_address
  %1730 = load i64* %rdi
  %1731 = load i64* %rsi
  %1732 = trunc i64 %1730 to i32
  %1733 = call i64 @ct_tally(i32 %1732, i64 %1731)
  store i64 %1733, i64* %rax
  store i64 %1733, i64* %rax
  br label %block_3ba7

block_3ba7:                                       ; preds = %block_3b82, %block_3b7c
  store volatile i64 15271, i64* @assembly_address
  %1734 = load i32* %stack_var_-40
  %1735 = sext i32 %1734 to i64
  %1736 = trunc i64 %1735 to i32
  %1737 = zext i32 %1736 to i64
  store i64 %1737, i64* %rax
  store volatile i64 15274, i64* @assembly_address
  %1738 = load i64* %rax
  %1739 = add i64 %1738, -1
  %1740 = trunc i64 %1739 to i32
  %1741 = zext i32 %1740 to i64
  store i64 %1741, i64* %rdx
  store volatile i64 15277, i64* @assembly_address
  %1742 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %1743 = zext i32 %1742 to i64
  store i64 %1743, i64* %rax
  store volatile i64 15283, i64* @assembly_address
  %1744 = load i64* %rax
  %1745 = trunc i64 %1744 to i32
  %1746 = zext i32 %1745 to i64
  store i64 %1746, i64* %rcx
  store volatile i64 15285, i64* @assembly_address
  %1747 = load i64* @global_var_21a430
  store i64 %1747, i64* %rax
  store volatile i64 15292, i64* @assembly_address
  %1748 = load i64* %rcx
  %1749 = load i64* %rax
  %1750 = sub i64 %1748, %1749
  %1751 = and i64 %1748, 15
  %1752 = and i64 %1749, 15
  %1753 = sub i64 %1751, %1752
  %1754 = icmp ugt i64 %1753, 15
  %1755 = icmp ult i64 %1748, %1749
  %1756 = xor i64 %1748, %1749
  %1757 = xor i64 %1748, %1750
  %1758 = and i64 %1756, %1757
  %1759 = icmp slt i64 %1758, 0
  store i1 %1754, i1* %az
  store i1 %1755, i1* %cf
  store i1 %1759, i1* %of
  %1760 = icmp eq i64 %1750, 0
  store i1 %1760, i1* %zf
  %1761 = icmp slt i64 %1750, 0
  store i1 %1761, i1* %sf
  %1762 = trunc i64 %1750 to i8
  %1763 = call i8 @llvm.ctpop.i8(i8 %1762)
  %1764 = and i8 %1763, 1
  %1765 = icmp eq i8 %1764, 0
  store i1 %1765, i1* %pf
  store i64 %1750, i64* %rcx
  store volatile i64 15295, i64* @assembly_address
  %1766 = load i64* %rcx
  store i64 %1766, i64* %rax
  store volatile i64 15298, i64* @assembly_address
  %1767 = load i64* %rax
  store i64 %1767, i64* %rsi
  store volatile i64 15301, i64* @assembly_address
  %1768 = load i64* @global_var_21a430
  store i64 %1768, i64* %rax
  store volatile i64 15308, i64* @assembly_address
  %1769 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1770 = icmp eq i64 %1769, 0
  store i1 %1770, i1* %zf
  %1771 = icmp slt i64 %1769, 0
  store i1 %1771, i1* %sf
  %1772 = trunc i64 %1769 to i8
  %1773 = call i8 @llvm.ctpop.i8(i8 %1772)
  %1774 = and i8 %1773, 1
  %1775 = icmp eq i8 %1774, 0
  store i1 %1775, i1* %pf
  store volatile i64 15311, i64* @assembly_address
  %1776 = load i1* %sf
  br i1 %1776, label %block_3be6, label %block_3bd1

block_3bd1:                                       ; preds = %block_3ba7
  store volatile i64 15313, i64* @assembly_address
  %1777 = load i64* @global_var_21a430
  store i64 %1777, i64* %rax
  store volatile i64 15320, i64* @assembly_address
  %1778 = load i64* %rax
  %1779 = trunc i64 %1778 to i32
  %1780 = zext i32 %1779 to i64
  store i64 %1780, i64* %rcx
  store volatile i64 15322, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 15329, i64* @assembly_address
  %1781 = load i64* %rax
  %1782 = load i64* %rcx
  %1783 = add i64 %1781, %1782
  %1784 = and i64 %1781, 15
  %1785 = and i64 %1782, 15
  %1786 = add i64 %1784, %1785
  %1787 = icmp ugt i64 %1786, 15
  %1788 = icmp ult i64 %1783, %1781
  %1789 = xor i64 %1781, %1783
  %1790 = xor i64 %1782, %1783
  %1791 = and i64 %1789, %1790
  %1792 = icmp slt i64 %1791, 0
  store i1 %1787, i1* %az
  store i1 %1788, i1* %cf
  store i1 %1792, i1* %of
  %1793 = icmp eq i64 %1783, 0
  store i1 %1793, i1* %zf
  %1794 = icmp slt i64 %1783, 0
  store i1 %1794, i1* %sf
  %1795 = trunc i64 %1783 to i8
  %1796 = call i8 @llvm.ctpop.i8(i8 %1795)
  %1797 = and i8 %1796, 1
  %1798 = icmp eq i8 %1797, 0
  store i1 %1798, i1* %pf
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 15332, i64* @assembly_address
  br label %block_3beb

block_3be6:                                       ; preds = %block_3ba7
  store volatile i64 15334, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_3beb

block_3beb:                                       ; preds = %block_3be6, %block_3bd1
  store volatile i64 15339, i64* @assembly_address
  store i64 1, i64* %rcx
  store volatile i64 15344, i64* @assembly_address
  %1799 = load i64* %rax
  store i64 %1799, i64* %rdi
  store volatile i64 15347, i64* @assembly_address
  %1800 = load i64* %rdi
  %1801 = load i64* %rsi
  %1802 = load i64* %rdx
  %1803 = load i64* %rcx
  %1804 = trunc i64 %1801 to i32
  %1805 = call i64 @flush_block(i64 %1800, i32 %1804, i64 %1802, i64 %1803)
  store i64 %1805, i64* %rax
  store i64 %1805, i64* %rax
  br label %block_3bf8

block_3bf8:                                       ; preds = %block_3beb, %block_362c
  store volatile i64 15352, i64* @assembly_address
  %1806 = load i64* %rsp
  %1807 = add i64 %1806, 24
  %1808 = and i64 %1806, 15
  %1809 = add i64 %1808, 8
  %1810 = icmp ugt i64 %1809, 15
  %1811 = icmp ult i64 %1807, %1806
  %1812 = xor i64 %1806, %1807
  %1813 = xor i64 24, %1807
  %1814 = and i64 %1812, %1813
  %1815 = icmp slt i64 %1814, 0
  store i1 %1810, i1* %az
  store i1 %1811, i1* %cf
  store i1 %1815, i1* %of
  %1816 = icmp eq i64 %1807, 0
  store i1 %1816, i1* %zf
  %1817 = icmp slt i64 %1807, 0
  store i1 %1817, i1* %sf
  %1818 = trunc i64 %1807 to i8
  %1819 = call i8 @llvm.ctpop.i8(i8 %1818)
  %1820 = and i8 %1819, 1
  %1821 = icmp eq i8 %1820, 0
  store i1 %1821, i1* %pf
  %1822 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %1822, i64* %rsp
  store volatile i64 15356, i64* @assembly_address
  %1823 = load i64* %stack_var_-16
  store i64 %1823, i64* %rbx
  %1824 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1824, i64* %rsp
  store volatile i64 15357, i64* @assembly_address
  %1825 = load i64* %stack_var_-8
  store i64 %1825, i64* %rbp
  %1826 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1826, i64* %rsp
  store volatile i64 15358, i64* @assembly_address
  %1827 = load i64* %rax
  %1828 = trunc i64 %1827 to i32
  ret i32 %1828
}

define i64 @try_help() {
block_3bff:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 15359, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 15360, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 15363, i64* @assembly_address
  %3 = load i64* @global_var_25f4c8
  store i64 %3, i64* %rdx
  store volatile i64 15370, i64* @assembly_address
  %4 = load i64* @global_var_216580
  store i64 %4, i64* %rax
  store volatile i64 15377, i64* @assembly_address
  store i64 ptrtoint ([39 x i8]* @global_var_10fc8 to i64), i64* %rsi
  store volatile i64 15384, i64* @assembly_address
  %5 = load i64* %rax
  store i64 %5, i64* %rdi
  store volatile i64 15387, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 15392, i64* @assembly_address
  %6 = load i64* %rdi
  %7 = inttoptr i64 %6 to %_IO_FILE*
  %8 = load i64* %rsi
  %9 = inttoptr i64 %8 to i8*
  %10 = load i64* %rdx
  %11 = inttoptr i64 %10 to i8*
  %12 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %7, i8* %9, i8* %11)
  %13 = sext i32 %12 to i64
  store i64 %13, i64* %rax
  %14 = sext i32 %12 to i64
  store i64 %14, i64* %rax
  store volatile i64 15397, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 15402, i64* @assembly_address
  %15 = load i64* %rdi
  %16 = trunc i64 %15 to i32
  %17 = call i64 @do_exit(i32 %16)
  store i64 %17, i64* %rax
  store i64 %17, i64* %rax
  %18 = load i64* %rax
  %19 = load i64* %rax
  ret i64 %19
}

define i64 @help() {
block_3c2f:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8**
  %0 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 15407, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 15408, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 15411, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 15415, i64* @assembly_address
  store i64 ptrtoint ([27 x i8*]* @global_var_215a20 to i64), i64* %rax
  store volatile i64 15422, i64* @assembly_address
  %20 = load i64* %rax
  %21 = inttoptr i64 %20 to i8**
  store i8** %21, i8*** %stack_var_-16
  store volatile i64 15426, i64* @assembly_address
  %22 = load i64* @global_var_25f4c8
  store i64 %22, i64* %rax
  store volatile i64 15433, i64* @assembly_address
  %23 = load i64* %rax
  store i64 %23, i64* %rsi
  store volatile i64 15436, i64* @assembly_address
  store i64 ptrtoint ([33 x i8]* @global_var_10ff0 to i64), i64* %rdi
  store volatile i64 15443, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 15448, i64* @assembly_address
  %24 = load i64* %rdi
  %25 = inttoptr i64 %24 to i8*
  %26 = load i64* %rsi
  %27 = inttoptr i64 %26 to i8*
  %28 = call i32 (i8*, ...)* @printf(i8* %25, i8* %27)
  %29 = sext i32 %28 to i64
  store i64 %29, i64* %rax
  %30 = sext i32 %28 to i64
  store i64 %30, i64* %rax
  store volatile i64 15453, i64* @assembly_address
  br label %block_3c76

block_3c5f:                                       ; preds = %block_3c76
  store volatile i64 15455, i64* @assembly_address
  %31 = load i8*** %stack_var_-16
  %32 = ptrtoint i8** %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 15459, i64* @assembly_address
  %33 = load i64* %rax
  %34 = add i64 %33, 8
  store i64 %34, i64* %rdx
  store volatile i64 15463, i64* @assembly_address
  %35 = load i64* %rdx
  %36 = inttoptr i64 %35 to i8**
  store i8** %36, i8*** %stack_var_-16
  store volatile i64 15467, i64* @assembly_address
  %37 = load i64* %rax
  %38 = inttoptr i64 %37 to i64*
  %39 = load i64* %38
  store i64 %39, i64* %rax
  store volatile i64 15470, i64* @assembly_address
  %40 = load i64* %rax
  store i64 %40, i64* %rdi
  store volatile i64 15473, i64* @assembly_address
  %41 = load i64* %rdi
  %42 = inttoptr i64 %41 to i8*
  %43 = call i32 @puts(i8* %42)
  %44 = sext i32 %43 to i64
  store i64 %44, i64* %rax
  %45 = sext i32 %43 to i64
  store i64 %45, i64* %rax
  br label %block_3c76

block_3c76:                                       ; preds = %block_3c5f, %block_3c2f
  store volatile i64 15478, i64* @assembly_address
  %46 = load i8*** %stack_var_-16
  %47 = ptrtoint i8** %46 to i64
  store i64 %47, i64* %rax
  store volatile i64 15482, i64* @assembly_address
  %48 = load i64* %rax
  %49 = inttoptr i64 %48 to i64*
  %50 = load i64* %49
  store i64 %50, i64* %rax
  store volatile i64 15485, i64* @assembly_address
  %51 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %52 = icmp eq i64 %51, 0
  store i1 %52, i1* %zf
  %53 = icmp slt i64 %51, 0
  store i1 %53, i1* %sf
  %54 = trunc i64 %51 to i8
  %55 = call i8 @llvm.ctpop.i8(i8 %54)
  %56 = and i8 %55, 1
  %57 = icmp eq i8 %56, 0
  store i1 %57, i1* %pf
  store volatile i64 15488, i64* @assembly_address
  %58 = load i1* %zf
  %59 = icmp eq i1 %58, false
  br i1 %59, label %block_3c5f, label %block_3c82

block_3c82:                                       ; preds = %block_3c76
  store volatile i64 15490, i64* @assembly_address
  store volatile i64 15491, i64* @assembly_address
  %60 = load i64* %stack_var_-8
  store i64 %60, i64* %rbp
  %61 = ptrtoint i64* %stack_var_0 to i64
  store i64 %61, i64* %rsp
  store volatile i64 15492, i64* @assembly_address
  %62 = load i64* %rax
  %63 = load i64* %rax
  ret i64 %63
}

define i64 @license() {
block_3c85:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8**
  %0 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 15493, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 15494, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 15497, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 15501, i64* @assembly_address
  store i64 ptrtoint ([5 x i8*]* @global_var_215680 to i64), i64* %rax
  store volatile i64 15508, i64* @assembly_address
  %20 = load i64* %rax
  %21 = inttoptr i64 %20 to i8**
  store i8** %21, i8*** %stack_var_-16
  store volatile i64 15512, i64* @assembly_address
  %22 = load [5 x i8]** @global_var_216558
  %23 = ptrtoint [5 x i8]* %22 to i64
  store i64 %23, i64* %rdx
  store volatile i64 15519, i64* @assembly_address
  %24 = load i64* @global_var_25f4c8
  store i64 %24, i64* %rax
  store volatile i64 15526, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rsi
  store volatile i64 15529, i64* @assembly_address
  store i64 ptrtoint ([7 x i8]* @global_var_11011 to i64), i64* %rdi
  store volatile i64 15536, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 15541, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to i8*
  %28 = load i64* %rsi
  %29 = inttoptr i64 %28 to i8*
  %30 = load i64* %rdx
  %31 = inttoptr i64 %30 to i8*
  %32 = call i32 (i8*, ...)* @printf(i8* %27, i8* %29, i8* %31)
  %33 = sext i32 %32 to i64
  store i64 %33, i64* %rax
  %34 = sext i32 %32 to i64
  store i64 %34, i64* %rax
  store volatile i64 15546, i64* @assembly_address
  br label %block_3cd3

block_3cbc:                                       ; preds = %block_3cd3
  store volatile i64 15548, i64* @assembly_address
  %35 = load i8*** %stack_var_-16
  %36 = ptrtoint i8** %35 to i64
  store i64 %36, i64* %rax
  store volatile i64 15552, i64* @assembly_address
  %37 = load i64* %rax
  %38 = add i64 %37, 8
  store i64 %38, i64* %rdx
  store volatile i64 15556, i64* @assembly_address
  %39 = load i64* %rdx
  %40 = inttoptr i64 %39 to i8**
  store i8** %40, i8*** %stack_var_-16
  store volatile i64 15560, i64* @assembly_address
  %41 = load i64* %rax
  %42 = inttoptr i64 %41 to i64*
  %43 = load i64* %42
  store i64 %43, i64* %rax
  store volatile i64 15563, i64* @assembly_address
  %44 = load i64* %rax
  store i64 %44, i64* %rdi
  store volatile i64 15566, i64* @assembly_address
  %45 = load i64* %rdi
  %46 = inttoptr i64 %45 to i8*
  %47 = call i32 @puts(i8* %46)
  %48 = sext i32 %47 to i64
  store i64 %48, i64* %rax
  %49 = sext i32 %47 to i64
  store i64 %49, i64* %rax
  br label %block_3cd3

block_3cd3:                                       ; preds = %block_3cbc, %block_3c85
  store volatile i64 15571, i64* @assembly_address
  %50 = load i8*** %stack_var_-16
  %51 = ptrtoint i8** %50 to i64
  store i64 %51, i64* %rax
  store volatile i64 15575, i64* @assembly_address
  %52 = load i64* %rax
  %53 = inttoptr i64 %52 to i64*
  %54 = load i64* %53
  store i64 %54, i64* %rax
  store volatile i64 15578, i64* @assembly_address
  %55 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %56 = icmp eq i64 %55, 0
  store i1 %56, i1* %zf
  %57 = icmp slt i64 %55, 0
  store i1 %57, i1* %sf
  %58 = trunc i64 %55 to i8
  %59 = call i8 @llvm.ctpop.i8(i8 %58)
  %60 = and i8 %59, 1
  %61 = icmp eq i8 %60, 0
  store i1 %61, i1* %pf
  store volatile i64 15581, i64* @assembly_address
  %62 = load i1* %zf
  %63 = icmp eq i1 %62, false
  br i1 %63, label %block_3cbc, label %block_3cdf

block_3cdf:                                       ; preds = %block_3cd3
  store volatile i64 15583, i64* @assembly_address
  store volatile i64 15584, i64* @assembly_address
  %64 = load i64* %stack_var_-8
  store i64 %64, i64* %rbp
  %65 = ptrtoint i64* %stack_var_0 to i64
  store i64 %65, i64* %rsp
  store volatile i64 15585, i64* @assembly_address
  %66 = load i64* %rax
  %67 = load i64* %rax
  ret i64 %67
}

define i64 @version() {
block_3ce2:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 15586, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 15587, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 15590, i64* @assembly_address
  %3 = call i64 @license()
  store i64 %3, i64* %rax
  store i64 %3, i64* %rax
  store i64 %3, i64* %rax
  store volatile i64 15595, i64* @assembly_address
  store i64 10, i64* %rdi
  store volatile i64 15600, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = trunc i64 %4 to i32
  %6 = call i32 @putchar(i32 %5)
  %7 = sext i32 %6 to i64
  store i64 %7, i64* %rax
  %8 = sext i32 %6 to i64
  store i64 %8, i64* %rax
  store volatile i64 15605, i64* @assembly_address
  store i64 ptrtoint ([29 x i8]* @global_var_11018 to i64), i64* %rdi
  store volatile i64 15612, i64* @assembly_address
  %9 = load i64* %rdi
  %10 = inttoptr i64 %9 to i8*
  %11 = call i32 @puts(i8* %10)
  %12 = sext i32 %11 to i64
  store i64 %12, i64* %rax
  %13 = sext i32 %11 to i64
  store i64 %13, i64* %rax
  store volatile i64 15617, i64* @assembly_address
  store volatile i64 15618, i64* @assembly_address
  %14 = load i64* %stack_var_-8
  store i64 %14, i64* %rbp
  %15 = ptrtoint i64* %stack_var_0 to i64
  store i64 %15, i64* %rsp
  store volatile i64 15619, i64* @assembly_address
  %16 = load i64* %rax
  %17 = load i64* %rax
  ret i64 %17
}

define i64 @progerror(i8* %arg1) {
block_3d04:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-32 = alloca i8*
  %1 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 15620, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 15621, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 15624, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 15628, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to i8*
  store i8* %22, i8** %stack_var_-32
  store volatile i64 15632, i64* @assembly_address
  %23 = call i32* @__errno_location()
  %24 = ptrtoint i32* %23 to i64
  store i64 %24, i64* %rax
  %25 = ptrtoint i32* %23 to i64
  store i64 %25, i64* %rax
  %26 = ptrtoint i32* %23 to i64
  store i64 %26, i64* %rax
  store volatile i64 15637, i64* @assembly_address
  %27 = load i64* %rax
  %28 = inttoptr i64 %27 to i32*
  %29 = load i32* %28
  %30 = zext i32 %29 to i64
  store i64 %30, i64* %rax
  store volatile i64 15639, i64* @assembly_address
  %31 = load i64* %rax
  %32 = trunc i64 %31 to i32
  store i32 %32, i32* %stack_var_-12
  store volatile i64 15642, i64* @assembly_address
  %33 = load i64* @global_var_25f4c8
  store i64 %33, i64* %rdx
  store volatile i64 15649, i64* @assembly_address
  %34 = load i64* @global_var_216580
  store i64 %34, i64* %rax
  store volatile i64 15656, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11035 to i64), i64* %rsi
  store volatile i64 15663, i64* @assembly_address
  %35 = load i64* %rax
  store i64 %35, i64* %rdi
  store volatile i64 15666, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 15671, i64* @assembly_address
  %36 = load i64* %rdi
  %37 = inttoptr i64 %36 to %_IO_FILE*
  %38 = load i64* %rsi
  %39 = inttoptr i64 %38 to i8*
  %40 = load i64* %rdx
  %41 = inttoptr i64 %40 to i8*
  %42 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %37, i8* %39, i8* %41)
  %43 = sext i32 %42 to i64
  store i64 %43, i64* %rax
  %44 = sext i32 %42 to i64
  store i64 %44, i64* %rax
  store volatile i64 15676, i64* @assembly_address
  %45 = call i32* @__errno_location()
  %46 = ptrtoint i32* %45 to i64
  store i64 %46, i64* %rax
  %47 = ptrtoint i32* %45 to i64
  store i64 %47, i64* %rax
  %48 = ptrtoint i32* %45 to i64
  store i64 %48, i64* %rax
  store volatile i64 15681, i64* @assembly_address
  %49 = load i64* %rax
  store i64 %49, i64* %rdx
  store volatile i64 15684, i64* @assembly_address
  %50 = load i32* %stack_var_-12
  %51 = zext i32 %50 to i64
  store i64 %51, i64* %rax
  store volatile i64 15687, i64* @assembly_address
  %52 = load i64* %rax
  %53 = trunc i64 %52 to i32
  %54 = load i64* %rdx
  %55 = inttoptr i64 %54 to i32*
  store i32 %53, i32* %55
  store volatile i64 15689, i64* @assembly_address
  %56 = load i8** %stack_var_-32
  %57 = ptrtoint i8* %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 15693, i64* @assembly_address
  %58 = load i64* %rax
  store i64 %58, i64* %rdi
  store volatile i64 15696, i64* @assembly_address
  %59 = load i64* %rdi
  %60 = inttoptr i64 %59 to i8*
  call void @perror(i8* %60)
  store volatile i64 15701, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 15711, i64* @assembly_address
  store volatile i64 15712, i64* @assembly_address
  %61 = load i64* %stack_var_-8
  store i64 %61, i64* %rbp
  %62 = ptrtoint i64* %stack_var_0 to i64
  store i64 %62, i64* %rsp
  store volatile i64 15713, i64* @assembly_address
  %63 = load i64* %rax
  ret i64 %63
}

define i64 @main(i32 %argc, i8** %argv) {
block_3d62:
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8** %argv to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %argc to i64
  store i64 %1, i64* %rdi
  %stack_var_-44 = alloca i32
  %stack_var_-48 = alloca i32
  %stack_var_-52 = alloca i32
  %stack_var_-32 = alloca i8**
  %2 = alloca i64
  %stack_var_-56 = alloca i32
  %3 = alloca i64
  %stack_var_-40 = alloca i8**
  %4 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-60 = alloca i32
  %stack_var_-72 = alloca i8**
  %5 = alloca i64
  %stack_var_-8 = alloca i64
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i32
  %9 = alloca i64
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i32
  %13 = alloca i8
  %14 = alloca i8
  %15 = alloca i64
  %16 = alloca i8
  %17 = alloca i8
  %18 = alloca i64
  store volatile i64 15714, i64* @assembly_address
  %19 = load i64* %rbp
  store i64 %19, i64* %stack_var_-8
  %20 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %20, i64* %rsp
  store volatile i64 15715, i64* @assembly_address
  %21 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %21, i64* %rbp
  store volatile i64 15718, i64* @assembly_address
  %22 = load i64* %rsp
  %23 = sub i64 %22, 64
  %24 = and i64 %22, 15
  %25 = icmp ugt i64 %24, 15
  %26 = icmp ult i64 %22, 64
  %27 = xor i64 %22, 64
  %28 = xor i64 %22, %23
  %29 = and i64 %27, %28
  %30 = icmp slt i64 %29, 0
  store i1 %25, i1* %az
  store i1 %26, i1* %cf
  store i1 %30, i1* %of
  %31 = icmp eq i64 %23, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i64 %23, 0
  store i1 %32, i1* %sf
  %33 = trunc i64 %23 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  %37 = ptrtoint i8*** %stack_var_-72 to i64
  store i64 %37, i64* %rsp
  store volatile i64 15722, i64* @assembly_address
  %38 = load i64* %rdi
  %39 = trunc i64 %38 to i32
  store i32 %39, i32* %stack_var_-60
  store volatile i64 15725, i64* @assembly_address
  %40 = load i64* %rsi
  %41 = inttoptr i64 %40 to i8**
  store i8** %41, i8*** %stack_var_-72
  store volatile i64 15729, i64* @assembly_address
  %42 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %42, i64* %rax
  store volatile i64 15738, i64* @assembly_address
  %43 = load i64* %rax
  store i64 %43, i64* %stack_var_-16
  store volatile i64 15742, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %44 = icmp eq i32 0, 0
  store i1 %44, i1* %zf
  %45 = icmp slt i32 0, 0
  store i1 %45, i1* %sf
  %46 = trunc i32 0 to i8
  %47 = call i8 @llvm.ctpop.i8(i8 %46)
  %48 = and i8 %47, 1
  %49 = icmp eq i8 %48, 0
  store i1 %49, i1* %pf
  %50 = zext i32 0 to i64
  store i64 %50, i64* %rax
  store volatile i64 15744, i64* @assembly_address
  %51 = load i8*** %stack_var_-72
  %52 = ptrtoint i8** %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 15748, i64* @assembly_address
  %53 = load i64* %rax
  %54 = inttoptr i64 %53 to i64*
  %55 = load i64* %54
  store i64 %55, i64* %rax
  store volatile i64 15751, i64* @assembly_address
  %56 = load i64* %rax
  store i64 %56, i64* %rdi
  store volatile i64 15754, i64* @assembly_address
  %57 = load i64* %rdi
  %58 = inttoptr i64 %57 to i64*
  %59 = call i64 @gzip_base_name(i64* %58)
  store i64 %59, i64* %rax
  store i64 %59, i64* %rax
  store volatile i64 15759, i64* @assembly_address
  %60 = load i64* %rax
  store i64 %60, i64* @global_var_25f4c8
  store volatile i64 15766, i64* @assembly_address
  %61 = load i64* @global_var_25f4c8
  store i64 %61, i64* %rax
  store volatile i64 15773, i64* @assembly_address
  %62 = load i64* %rax
  store i64 %62, i64* %rdi
  store volatile i64 15776, i64* @assembly_address
  %63 = load i64* %rdi
  %64 = inttoptr i64 %63 to i8*
  %65 = call i32 @strlen(i8* %64)
  %66 = sext i32 %65 to i64
  store i64 %66, i64* %rax
  %67 = sext i32 %65 to i64
  store i64 %67, i64* %rax
  store volatile i64 15781, i64* @assembly_address
  %68 = load i64* %rax
  store i64 %68, i64* %stack_var_-24
  store volatile i64 15785, i64* @assembly_address
  %69 = load i64* %stack_var_-24
  %70 = sub i64 %69, 4
  %71 = and i64 %69, 15
  %72 = sub i64 %71, 4
  %73 = icmp ugt i64 %72, 15
  %74 = icmp ult i64 %69, 4
  %75 = xor i64 %69, 4
  %76 = xor i64 %69, %70
  %77 = and i64 %75, %76
  %78 = icmp slt i64 %77, 0
  store i1 %73, i1* %az
  store i1 %74, i1* %cf
  store i1 %78, i1* %of
  %79 = icmp eq i64 %70, 0
  store i1 %79, i1* %zf
  %80 = icmp slt i64 %70, 0
  store i1 %80, i1* %sf
  %81 = trunc i64 %70 to i8
  %82 = call i8 @llvm.ctpop.i8(i8 %81)
  %83 = and i8 %82, 1
  %84 = icmp eq i8 %83, 0
  store i1 %84, i1* %pf
  store volatile i64 15790, i64* @assembly_address
  %85 = load i1* %cf
  %86 = load i1* %zf
  %87 = or i1 %85, %86
  br i1 %87, label %block_3dea, label %block_3db0

block_3db0:                                       ; preds = %block_3d62
  store volatile i64 15792, i64* @assembly_address
  %88 = load i64* @global_var_25f4c8
  store i64 %88, i64* %rax
  store volatile i64 15799, i64* @assembly_address
  %89 = load i64* %stack_var_-24
  store i64 %89, i64* %rdx
  store volatile i64 15803, i64* @assembly_address
  %90 = load i64* %rdx
  %91 = sub i64 %90, 4
  %92 = and i64 %90, 15
  %93 = sub i64 %92, 4
  %94 = icmp ugt i64 %93, 15
  %95 = icmp ult i64 %90, 4
  %96 = xor i64 %90, 4
  %97 = xor i64 %90, %91
  %98 = and i64 %96, %97
  %99 = icmp slt i64 %98, 0
  store i1 %94, i1* %az
  store i1 %95, i1* %cf
  store i1 %99, i1* %of
  %100 = icmp eq i64 %91, 0
  store i1 %100, i1* %zf
  %101 = icmp slt i64 %91, 0
  store i1 %101, i1* %sf
  %102 = trunc i64 %91 to i8
  %103 = call i8 @llvm.ctpop.i8(i8 %102)
  %104 = and i8 %103, 1
  %105 = icmp eq i8 %104, 0
  store i1 %105, i1* %pf
  store i64 %91, i64* %rdx
  store volatile i64 15807, i64* @assembly_address
  %106 = load i64* %rax
  %107 = load i64* %rdx
  %108 = add i64 %106, %107
  %109 = and i64 %106, 15
  %110 = and i64 %107, 15
  %111 = add i64 %109, %110
  %112 = icmp ugt i64 %111, 15
  %113 = icmp ult i64 %108, %106
  %114 = xor i64 %106, %108
  %115 = xor i64 %107, %108
  %116 = and i64 %114, %115
  %117 = icmp slt i64 %116, 0
  store i1 %112, i1* %az
  store i1 %113, i1* %cf
  store i1 %117, i1* %of
  %118 = icmp eq i64 %108, 0
  store i1 %118, i1* %zf
  %119 = icmp slt i64 %108, 0
  store i1 %119, i1* %sf
  %120 = trunc i64 %108 to i8
  %121 = call i8 @llvm.ctpop.i8(i8 %120)
  %122 = and i8 %121, 1
  %123 = icmp eq i8 %122, 0
  store i1 %123, i1* %pf
  store i64 %108, i64* %rax
  store volatile i64 15810, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_1103a to i64), i64* %rsi
  store volatile i64 15817, i64* @assembly_address
  %124 = load i64* %rax
  store i64 %124, i64* %rdi
  store volatile i64 15820, i64* @assembly_address
  %125 = load i64* %rdi
  %126 = inttoptr i64 %125 to i8*
  %127 = load i64* %rsi
  %128 = inttoptr i64 %127 to i8*
  %129 = call i32 @strcmp(i8* %126, i8* %128)
  %130 = sext i32 %129 to i64
  store i64 %130, i64* %rax
  %131 = sext i32 %129 to i64
  store i64 %131, i64* %rax
  store volatile i64 15825, i64* @assembly_address
  %132 = load i64* %rax
  %133 = trunc i64 %132 to i32
  %134 = load i64* %rax
  %135 = trunc i64 %134 to i32
  %136 = and i32 %133, %135
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %137 = icmp eq i32 %136, 0
  store i1 %137, i1* %zf
  %138 = icmp slt i32 %136, 0
  store i1 %138, i1* %sf
  %139 = trunc i32 %136 to i8
  %140 = call i8 @llvm.ctpop.i8(i8 %139)
  %141 = and i8 %140, 1
  %142 = icmp eq i8 %141, 0
  store i1 %142, i1* %pf
  store volatile i64 15827, i64* @assembly_address
  %143 = load i1* %zf
  %144 = icmp eq i1 %143, false
  br i1 %144, label %block_3dea, label %block_3dd5

block_3dd5:                                       ; preds = %block_3db0
  store volatile i64 15829, i64* @assembly_address
  %145 = load i64* @global_var_25f4c8
  store i64 %145, i64* %rax
  store volatile i64 15836, i64* @assembly_address
  %146 = load i64* %stack_var_-24
  store i64 %146, i64* %rdx
  store volatile i64 15840, i64* @assembly_address
  %147 = load i64* %rdx
  %148 = sub i64 %147, 4
  %149 = and i64 %147, 15
  %150 = sub i64 %149, 4
  %151 = icmp ugt i64 %150, 15
  %152 = icmp ult i64 %147, 4
  %153 = xor i64 %147, 4
  %154 = xor i64 %147, %148
  %155 = and i64 %153, %154
  %156 = icmp slt i64 %155, 0
  store i1 %151, i1* %az
  store i1 %152, i1* %cf
  store i1 %156, i1* %of
  %157 = icmp eq i64 %148, 0
  store i1 %157, i1* %zf
  %158 = icmp slt i64 %148, 0
  store i1 %158, i1* %sf
  %159 = trunc i64 %148 to i8
  %160 = call i8 @llvm.ctpop.i8(i8 %159)
  %161 = and i8 %160, 1
  %162 = icmp eq i8 %161, 0
  store i1 %162, i1* %pf
  store i64 %148, i64* %rdx
  store volatile i64 15844, i64* @assembly_address
  %163 = load i64* %rax
  %164 = load i64* %rdx
  %165 = add i64 %163, %164
  %166 = and i64 %163, 15
  %167 = and i64 %164, 15
  %168 = add i64 %166, %167
  %169 = icmp ugt i64 %168, 15
  %170 = icmp ult i64 %165, %163
  %171 = xor i64 %163, %165
  %172 = xor i64 %164, %165
  %173 = and i64 %171, %172
  %174 = icmp slt i64 %173, 0
  store i1 %169, i1* %az
  store i1 %170, i1* %cf
  store i1 %174, i1* %of
  %175 = icmp eq i64 %165, 0
  store i1 %175, i1* %zf
  %176 = icmp slt i64 %165, 0
  store i1 %176, i1* %sf
  %177 = trunc i64 %165 to i8
  %178 = call i8 @llvm.ctpop.i8(i8 %177)
  %179 = and i8 %178, 1
  %180 = icmp eq i8 %179, 0
  store i1 %180, i1* %pf
  store i64 %165, i64* %rax
  store volatile i64 15847, i64* @assembly_address
  %181 = load i64* %rax
  %182 = inttoptr i64 %181 to i8*
  store i8 0, i8* %182
  br label %block_3dea

block_3dea:                                       ; preds = %block_3dd5, %block_3db0, %block_3d62
  store volatile i64 15850, i64* @assembly_address
  %183 = load i8*** %stack_var_-72
  %184 = ptrtoint i8** %183 to i64
  store i64 %184, i64* %rax
  store volatile i64 15854, i64* @assembly_address
  %185 = load i64* %rax
  %186 = inttoptr i64 %185 to i8**
  store i8** %186, i8*** %stack_var_-40
  store volatile i64 15858, i64* @assembly_address
  %187 = ptrtoint i8*** %stack_var_-40 to i64
  store i64 %187, i64* %rcx
  store volatile i64 15862, i64* @assembly_address
  %188 = ptrtoint i32* %stack_var_-56 to i64
  store i64 %188, i64* %rax
  store volatile i64 15866, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_1103f to i64), i64* %rdx
  store volatile i64 15873, i64* @assembly_address
  %189 = ptrtoint i8*** %stack_var_-40 to i64
  store i64 %189, i64* %rsi
  store volatile i64 15876, i64* @assembly_address
  %190 = ptrtoint i32* %stack_var_-56 to i64
  store i64 %190, i64* %rdi
  store volatile i64 15879, i64* @assembly_address
  %191 = load i64* %rdi
  %192 = inttoptr i64 %191 to i64*
  %193 = load i64* %rsi
  %194 = inttoptr i64 %193 to i64*
  %195 = load i64* %rdx
  %196 = inttoptr i64 %195 to i8*
  %197 = bitcast i64* %192 to i32*
  %198 = call i64 @add_envopt(i32* %197, i64* %194, i8* %196)
  store i64 %198, i64* %rax
  store i64 %198, i64* %rax
  store volatile i64 15884, i64* @assembly_address
  %199 = load i64* %rax
  store i64 %199, i64* @global_var_216628
  store volatile i64 15891, i64* @assembly_address
  %200 = load i64* @global_var_216628
  store i64 %200, i64* %rax
  store volatile i64 15898, i64* @assembly_address
  %201 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %202 = icmp eq i64 %201, 0
  store i1 %202, i1* %zf
  %203 = icmp slt i64 %201, 0
  store i1 %203, i1* %sf
  %204 = trunc i64 %201 to i8
  %205 = call i8 @llvm.ctpop.i8(i8 %204)
  %206 = and i8 %205, 1
  %207 = icmp eq i8 %206, 0
  store i1 %207, i1* %pf
  store volatile i64 15901, i64* @assembly_address
  %208 = load i1* %zf
  br i1 %208, label %block_3e25, label %block_3e1f

block_3e1f:                                       ; preds = %block_3dea
  store volatile i64 15903, i64* @assembly_address
  %209 = load i8*** %stack_var_-40
  %210 = ptrtoint i8** %209 to i64
  store i64 %210, i64* %rax
  store volatile i64 15907, i64* @assembly_address
  br label %block_3e2a

block_3e25:                                       ; preds = %block_3dea
  store volatile i64 15909, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_3e2a

block_3e2a:                                       ; preds = %block_3e25, %block_3e1f
  store volatile i64 15914, i64* @assembly_address
  %211 = load i64* %rax
  %212 = inttoptr i64 %211 to i8**
  store i8** %212, i8*** %stack_var_-32
  store volatile i64 15918, i64* @assembly_address
  store i64 ptrtoint ([4 x i8]* @global_var_11044 to i64), i64* %rax
  store volatile i64 15925, i64* @assembly_address
  %213 = load i64* %rax
  store i64 %213, i64* @global_var_216630
  store volatile i64 15932, i64* @assembly_address
  %214 = load i64* @global_var_216630
  store i64 %214, i64* %rax
  store volatile i64 15939, i64* @assembly_address
  %215 = load i64* %rax
  store i64 %215, i64* %rdi
  store volatile i64 15942, i64* @assembly_address
  %216 = load i64* %rdi
  %217 = inttoptr i64 %216 to i8*
  %218 = call i32 @strlen(i8* %217)
  %219 = sext i32 %218 to i64
  store i64 %219, i64* %rax
  %220 = sext i32 %218 to i64
  store i64 %220, i64* %rax
  store volatile i64 15947, i64* @assembly_address
  %221 = load i64* %rax
  store i64 %221, i64* @global_var_216638
  br label %block_3e52

block_3e52:                                       ; preds = %block_4348, %block_3e2a
  store volatile i64 15954, i64* @assembly_address
  store i32 -1, i32* %stack_var_-52
  store volatile i64 15961, i64* @assembly_address
  %222 = load i8*** %stack_var_-32
  %223 = ptrtoint i8** %222 to i64
  %224 = and i64 %223, 15
  %225 = icmp ugt i64 %224, 15
  %226 = icmp ult i64 %223, 0
  %227 = xor i64 %223, 0
  %228 = and i64 %227, 0
  %229 = icmp slt i64 %228, 0
  store i1 %225, i1* %az
  store i1 %226, i1* %cf
  store i1 %229, i1* %of
  %230 = icmp eq i64 %223, 0
  store i1 %230, i1* %zf
  %231 = icmp slt i64 %223, 0
  store i1 %231, i1* %sf
  %232 = trunc i64 %223 to i8
  %233 = call i8 @llvm.ctpop.i8(i8 %232)
  %234 = and i8 %233, 1
  %235 = icmp eq i8 %234, 0
  store i1 %235, i1* %pf
  store volatile i64 15966, i64* @assembly_address
  %236 = load i1* %zf
  br i1 %236, label %block_3f9b, label %block_3e64

block_3e64:                                       ; preds = %block_3e52
  store volatile i64 15972, i64* @assembly_address
  %237 = load i32* bitcast (i64* @global_var_216568 to i32*)
  %238 = zext i32 %237 to i64
  store i64 %238, i64* %rax
  store volatile i64 15978, i64* @assembly_address
  %239 = load i64* %rax
  %240 = trunc i64 %239 to i32
  %241 = sext i32 %240 to i64
  store i64 %241, i64* %rax
  store volatile i64 15980, i64* @assembly_address
  %242 = load i64* %rax
  %243 = mul i64 %242, 8
  store i64 %243, i64* %rdx
  store volatile i64 15988, i64* @assembly_address
  %244 = load i8*** %stack_var_-32
  %245 = ptrtoint i8** %244 to i64
  store i64 %245, i64* %rax
  store volatile i64 15992, i64* @assembly_address
  %246 = load i64* %rax
  %247 = load i64* %rdx
  %248 = add i64 %246, %247
  %249 = and i64 %246, 15
  %250 = and i64 %247, 15
  %251 = add i64 %249, %250
  %252 = icmp ugt i64 %251, 15
  %253 = icmp ult i64 %248, %246
  %254 = xor i64 %246, %248
  %255 = xor i64 %247, %248
  %256 = and i64 %254, %255
  %257 = icmp slt i64 %256, 0
  store i1 %252, i1* %az
  store i1 %253, i1* %cf
  store i1 %257, i1* %of
  %258 = icmp eq i64 %248, 0
  store i1 %258, i1* %zf
  %259 = icmp slt i64 %248, 0
  store i1 %259, i1* %sf
  %260 = trunc i64 %248 to i8
  %261 = call i8 @llvm.ctpop.i8(i8 %260)
  %262 = and i8 %261, 1
  %263 = icmp eq i8 %262, 0
  store i1 %263, i1* %pf
  store i64 %248, i64* %rax
  store volatile i64 15995, i64* @assembly_address
  %264 = load i64* %rax
  %265 = inttoptr i64 %264 to i64*
  %266 = load i64* %265
  store i64 %266, i64* %rax
  store volatile i64 15998, i64* @assembly_address
  %267 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %268 = icmp eq i64 %267, 0
  store i1 %268, i1* %zf
  %269 = icmp slt i64 %267, 0
  store i1 %269, i1* %sf
  %270 = trunc i64 %267 to i8
  %271 = call i8 @llvm.ctpop.i8(i8 %270)
  %272 = and i8 %271, 1
  %273 = icmp eq i8 %272, 0
  store i1 %273, i1* %pf
  store volatile i64 16001, i64* @assembly_address
  %274 = load i1* %zf
  br i1 %274, label %block_3ebc, label %block_3e83

block_3e83:                                       ; preds = %block_3e64
  store volatile i64 16003, i64* @assembly_address
  %275 = load i32* bitcast (i64* @global_var_216568 to i32*)
  %276 = zext i32 %275 to i64
  store i64 %276, i64* %rax
  store volatile i64 16009, i64* @assembly_address
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i32
  %279 = sext i32 %278 to i64
  store i64 %279, i64* %rax
  store volatile i64 16011, i64* @assembly_address
  %280 = load i64* %rax
  %281 = mul i64 %280, 8
  store i64 %281, i64* %rdx
  store volatile i64 16019, i64* @assembly_address
  %282 = load i8*** %stack_var_-32
  %283 = ptrtoint i8** %282 to i64
  store i64 %283, i64* %rax
  store volatile i64 16023, i64* @assembly_address
  %284 = load i64* %rax
  %285 = load i64* %rdx
  %286 = add i64 %284, %285
  %287 = and i64 %284, 15
  %288 = and i64 %285, 15
  %289 = add i64 %287, %288
  %290 = icmp ugt i64 %289, 15
  %291 = icmp ult i64 %286, %284
  %292 = xor i64 %284, %286
  %293 = xor i64 %285, %286
  %294 = and i64 %292, %293
  %295 = icmp slt i64 %294, 0
  store i1 %290, i1* %az
  store i1 %291, i1* %cf
  store i1 %295, i1* %of
  %296 = icmp eq i64 %286, 0
  store i1 %296, i1* %zf
  %297 = icmp slt i64 %286, 0
  store i1 %297, i1* %sf
  %298 = trunc i64 %286 to i8
  %299 = call i8 @llvm.ctpop.i8(i8 %298)
  %300 = and i8 %299, 1
  %301 = icmp eq i8 %300, 0
  store i1 %301, i1* %pf
  store i64 %286, i64* %rax
  store volatile i64 16026, i64* @assembly_address
  %302 = load i64* %rax
  %303 = inttoptr i64 %302 to i64*
  %304 = load i64* %303
  store i64 %304, i64* %rax
  store volatile i64 16029, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_11048 to i64), i64* %rsi
  store volatile i64 16036, i64* @assembly_address
  %305 = load i64* %rax
  store i64 %305, i64* %rdi
  store volatile i64 16039, i64* @assembly_address
  %306 = load i64* %rdi
  %307 = inttoptr i64 %306 to i8*
  %308 = load i64* %rsi
  %309 = inttoptr i64 %308 to i8*
  %310 = call i32 @strcmp(i8* %307, i8* %309)
  %311 = sext i32 %310 to i64
  store i64 %311, i64* %rax
  %312 = sext i32 %310 to i64
  store i64 %312, i64* %rax
  store volatile i64 16044, i64* @assembly_address
  %313 = load i64* %rax
  %314 = trunc i64 %313 to i32
  %315 = load i64* %rax
  %316 = trunc i64 %315 to i32
  %317 = and i32 %314, %316
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %318 = icmp eq i32 %317, 0
  store i1 %318, i1* %zf
  %319 = icmp slt i32 %317, 0
  store i1 %319, i1* %sf
  %320 = trunc i32 %317 to i8
  %321 = call i8 @llvm.ctpop.i8(i8 %320)
  %322 = and i8 %321, 1
  %323 = icmp eq i8 %322, 0
  store i1 %323, i1* %pf
  store volatile i64 16046, i64* @assembly_address
  %324 = load i1* %zf
  %325 = icmp eq i1 %324, false
  br i1 %325, label %block_3ebc, label %block_3eb0

block_3eb0:                                       ; preds = %block_3e83
  store volatile i64 16048, i64* @assembly_address
  store i32 176, i32* %stack_var_-48
  store volatile i64 16055, i64* @assembly_address
  br label %block_3f9b

block_3ebc:                                       ; preds = %block_3e83, %block_3e64
  store volatile i64 16060, i64* @assembly_address
  %326 = load i32* %stack_var_-56
  %327 = sext i32 %326 to i64
  %328 = trunc i64 %327 to i32
  %329 = zext i32 %328 to i64
  store i64 %329, i64* %rax
  store volatile i64 16063, i64* @assembly_address
  %330 = ptrtoint i32* %stack_var_-52 to i64
  store i64 %330, i64* %rdx
  store volatile i64 16067, i64* @assembly_address
  %331 = load i8*** %stack_var_-32
  %332 = ptrtoint i8** %331 to i64
  store i64 %332, i64* %rsi
  store volatile i64 16071, i64* @assembly_address
  %333 = ptrtoint i32* %stack_var_-52 to i64
  store i64 %333, i64* %r8
  store volatile i64 16074, i64* @assembly_address
  store i64 ptrtoint ([6 x i8]** @global_var_2156c0 to i64), i64* %rcx
  store volatile i64 16081, i64* @assembly_address
  store i64 ptrtoint ([34 x i8]* @global_var_10ee0 to i64), i64* %rdx
  store volatile i64 16088, i64* @assembly_address
  %334 = load i64* %rax
  %335 = trunc i64 %334 to i32
  %336 = zext i32 %335 to i64
  store i64 %336, i64* %rdi
  store volatile i64 16090, i64* @assembly_address
  %337 = load i64* %rdi
  %338 = trunc i64 %337 to i32
  %339 = load i64* %rsi
  %340 = inttoptr i64 %339 to i8**
  %341 = load i64* %rdx
  %342 = inttoptr i64 %341 to i8*
  %343 = load i64* %rcx
  %344 = inttoptr i64 %343 to %option*
  %345 = load i64* %r8
  %346 = inttoptr i64 %345 to i32*
  %347 = call i32 @getopt_long(i32 %338, i8** %340, i8* %342, %option* %344, i32* %346)
  %348 = sext i32 %347 to i64
  store i64 %348, i64* %rax
  %349 = sext i32 %347 to i64
  store i64 %349, i64* %rax
  store volatile i64 16095, i64* @assembly_address
  %350 = load i64* %rax
  %351 = trunc i64 %350 to i32
  store i32 %351, i32* %stack_var_-48
  store volatile i64 16098, i64* @assembly_address
  %352 = load i32* %stack_var_-48
  %353 = and i32 %352, 15
  %354 = icmp ugt i32 %353, 15
  %355 = icmp ult i32 %352, 0
  %356 = xor i32 %352, 0
  %357 = and i32 %356, 0
  %358 = icmp slt i32 %357, 0
  store i1 %354, i1* %az
  store i1 %355, i1* %cf
  store i1 %358, i1* %of
  %359 = icmp eq i32 %352, 0
  store i1 %359, i1* %zf
  %360 = icmp slt i32 %352, 0
  store i1 %360, i1* %sf
  %361 = trunc i32 %352 to i8
  %362 = call i8 @llvm.ctpop.i8(i8 %361)
  %363 = and i8 %362, 1
  %364 = icmp eq i8 %363, 0
  store i1 %364, i1* %pf
  store volatile i64 16102, i64* @assembly_address
  %365 = load i1* %sf
  br i1 %365, label %block_3ef4, label %block_3ee8

block_3ee8:                                       ; preds = %block_3ebc
  store volatile i64 16104, i64* @assembly_address
  %366 = load i32* %stack_var_-48
  %367 = add i32 %366, 131
  %368 = and i32 %366, 15
  %369 = add i32 %368, 3
  %370 = icmp ugt i32 %369, 15
  %371 = icmp ult i32 %367, %366
  %372 = xor i32 %366, %367
  %373 = xor i32 131, %367
  %374 = and i32 %372, %373
  %375 = icmp slt i32 %374, 0
  store i1 %370, i1* %az
  store i1 %371, i1* %cf
  store i1 %375, i1* %of
  %376 = icmp eq i32 %367, 0
  store i1 %376, i1* %zf
  %377 = icmp slt i32 %367, 0
  store i1 %377, i1* %sf
  %378 = trunc i32 %367 to i8
  %379 = call i8 @llvm.ctpop.i8(i8 %378)
  %380 = and i8 %379, 1
  %381 = icmp eq i8 %380, 0
  store i1 %381, i1* %pf
  store i32 %367, i32* %stack_var_-48
  store volatile i64 16111, i64* @assembly_address
  br label %block_3f9b

block_3ef4:                                       ; preds = %block_3ebc
  store volatile i64 16116, i64* @assembly_address
  %382 = load i32* bitcast (i64* @global_var_216568 to i32*)
  %383 = zext i32 %382 to i64
  store i64 %383, i64* %rdx
  store volatile i64 16122, i64* @assembly_address
  %384 = load i32* %stack_var_-56
  %385 = sext i32 %384 to i64
  %386 = trunc i64 %385 to i32
  %387 = zext i32 %386 to i64
  store i64 %387, i64* %rax
  store volatile i64 16125, i64* @assembly_address
  %388 = load i64* %rdx
  %389 = trunc i64 %388 to i32
  %390 = load i64* %rax
  %391 = trunc i64 %390 to i32
  %392 = sub i32 %389, %391
  %393 = and i32 %389, 15
  %394 = and i32 %391, 15
  %395 = sub i32 %393, %394
  %396 = icmp ugt i32 %395, 15
  %397 = icmp ult i32 %389, %391
  %398 = xor i32 %389, %391
  %399 = xor i32 %389, %392
  %400 = and i32 %398, %399
  %401 = icmp slt i32 %400, 0
  store i1 %396, i1* %az
  store i1 %397, i1* %cf
  store i1 %401, i1* %of
  %402 = icmp eq i32 %392, 0
  store i1 %402, i1* %zf
  %403 = icmp slt i32 %392, 0
  store i1 %403, i1* %sf
  %404 = trunc i32 %392 to i8
  %405 = call i8 @llvm.ctpop.i8(i8 %404)
  %406 = and i8 %405, 1
  %407 = icmp eq i8 %406, 0
  store i1 %407, i1* %pf
  store volatile i64 16127, i64* @assembly_address
  %408 = load i1* %zf
  br i1 %408, label %block_3f42, label %block_3f01

block_3f01:                                       ; preds = %block_3ef4
  store volatile i64 16129, i64* @assembly_address
  %409 = load i32* bitcast (i64* @global_var_216568 to i32*)
  %410 = zext i32 %409 to i64
  store i64 %410, i64* %rax
  store volatile i64 16135, i64* @assembly_address
  %411 = load i64* %rax
  %412 = trunc i64 %411 to i32
  %413 = sext i32 %412 to i64
  store i64 %413, i64* %rax
  store volatile i64 16137, i64* @assembly_address
  %414 = load i64* %rax
  %415 = mul i64 %414, 8
  store i64 %415, i64* %rdx
  store volatile i64 16145, i64* @assembly_address
  %416 = load i8*** %stack_var_-32
  %417 = ptrtoint i8** %416 to i64
  store i64 %417, i64* %rax
  store volatile i64 16149, i64* @assembly_address
  %418 = load i64* %rax
  %419 = load i64* %rdx
  %420 = add i64 %418, %419
  %421 = and i64 %418, 15
  %422 = and i64 %419, 15
  %423 = add i64 %421, %422
  %424 = icmp ugt i64 %423, 15
  %425 = icmp ult i64 %420, %418
  %426 = xor i64 %418, %420
  %427 = xor i64 %419, %420
  %428 = and i64 %426, %427
  %429 = icmp slt i64 %428, 0
  store i1 %424, i1* %az
  store i1 %425, i1* %cf
  store i1 %429, i1* %of
  %430 = icmp eq i64 %420, 0
  store i1 %430, i1* %zf
  %431 = icmp slt i64 %420, 0
  store i1 %431, i1* %sf
  %432 = trunc i64 %420 to i8
  %433 = call i8 @llvm.ctpop.i8(i8 %432)
  %434 = and i8 %433, 1
  %435 = icmp eq i8 %434, 0
  store i1 %435, i1* %pf
  store i64 %420, i64* %rax
  store volatile i64 16152, i64* @assembly_address
  %436 = load i64* %rax
  %437 = inttoptr i64 %436 to i64*
  %438 = load i64* %437
  store i64 %438, i64* %rcx
  store volatile i64 16155, i64* @assembly_address
  %439 = load i64* @global_var_25f4c8
  store i64 %439, i64* %rdx
  store volatile i64 16162, i64* @assembly_address
  %440 = load i64* @global_var_216580
  store i64 %440, i64* %rax
  store volatile i64 16169, i64* @assembly_address
  store i64 ptrtoint ([49 x i8]* @global_var_11050 to i64), i64* %rsi
  store volatile i64 16176, i64* @assembly_address
  %441 = load i64* %rax
  store i64 %441, i64* %rdi
  store volatile i64 16179, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 16184, i64* @assembly_address
  %442 = load i64* %rdi
  %443 = inttoptr i64 %442 to %_IO_FILE*
  %444 = load i64* %rsi
  %445 = inttoptr i64 %444 to i8*
  %446 = load i64* %rdx
  %447 = inttoptr i64 %446 to i8*
  %448 = load i64* %rcx
  %449 = inttoptr i64 %448 to i8*
  %450 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %443, i8* %445, i8* %447, i8* %449)
  %451 = sext i32 %450 to i64
  store i64 %451, i64* %rax
  %452 = sext i32 %450 to i64
  store i64 %452, i64* %rax
  store volatile i64 16189, i64* @assembly_address
  %453 = call i64 @try_help()
  store i64 %453, i64* %rax
  store i64 %453, i64* %rax
  store i64 %453, i64* %rax
  unreachable

block_3f42:                                       ; preds = %block_3ef4
  store volatile i64 16194, i64* @assembly_address
  %454 = load i32* %stack_var_-56
  %455 = sext i32 %454 to i64
  %456 = trunc i64 %455 to i32
  %457 = zext i32 %456 to i64
  store i64 %457, i64* %rax
  store volatile i64 16197, i64* @assembly_address
  %458 = load i64* %rax
  %459 = trunc i64 %458 to i32
  %460 = sub i32 %459, 1
  %461 = and i32 %459, 15
  %462 = sub i32 %461, 1
  %463 = icmp ugt i32 %462, 15
  %464 = icmp ult i32 %459, 1
  %465 = xor i32 %459, 1
  %466 = xor i32 %459, %460
  %467 = and i32 %465, %466
  %468 = icmp slt i32 %467, 0
  store i1 %463, i1* %az
  store i1 %464, i1* %cf
  store i1 %468, i1* %of
  %469 = icmp eq i32 %460, 0
  store i1 %469, i1* %zf
  %470 = icmp slt i32 %460, 0
  store i1 %470, i1* %sf
  %471 = trunc i32 %460 to i8
  %472 = call i8 @llvm.ctpop.i8(i8 %471)
  %473 = and i8 %472, 1
  %474 = icmp eq i8 %473, 0
  store i1 %474, i1* %pf
  store volatile i64 16200, i64* @assembly_address
  %475 = load i1* %zf
  br i1 %475, label %block_3f76, label %block_3f4a

block_3f4a:                                       ; preds = %block_3f42
  store volatile i64 16202, i64* @assembly_address
  %476 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %477 = zext i32 %476 to i64
  store i64 %477, i64* %rax
  store volatile i64 16208, i64* @assembly_address
  %478 = load i64* %rax
  %479 = trunc i64 %478 to i32
  %480 = load i64* %rax
  %481 = trunc i64 %480 to i32
  %482 = and i32 %479, %481
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %483 = icmp eq i32 %482, 0
  store i1 %483, i1* %zf
  %484 = icmp slt i32 %482, 0
  store i1 %484, i1* %sf
  %485 = trunc i32 %482 to i8
  %486 = call i8 @llvm.ctpop.i8(i8 %485)
  %487 = and i8 %486, 1
  %488 = icmp eq i8 %487, 0
  store i1 %488, i1* %pf
  store volatile i64 16210, i64* @assembly_address
  %489 = load i1* %zf
  %490 = icmp eq i1 %489, false
  br i1 %490, label %block_3f76, label %block_3f54

block_3f54:                                       ; preds = %block_3f4a
  store volatile i64 16212, i64* @assembly_address
  %491 = load i64* @global_var_25f4c8
  store i64 %491, i64* %rdx
  store volatile i64 16219, i64* @assembly_address
  %492 = load i64* @global_var_216580
  store i64 %492, i64* %rax
  store volatile i64 16226, i64* @assembly_address
  store i64 ptrtoint ([78 x i8]* @global_var_11088 to i64), i64* %rsi
  store volatile i64 16233, i64* @assembly_address
  %493 = load i64* %rax
  store i64 %493, i64* %rdi
  store volatile i64 16236, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 16241, i64* @assembly_address
  %494 = load i64* %rdi
  %495 = inttoptr i64 %494 to %_IO_FILE*
  %496 = load i64* %rsi
  %497 = inttoptr i64 %496 to i8*
  %498 = load i64* %rdx
  %499 = inttoptr i64 %498 to i8*
  %500 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %495, i8* %497, i8* %499)
  %501 = sext i32 %500 to i64
  store i64 %501, i64* %rax
  %502 = sext i32 %500 to i64
  store i64 %502, i64* %rax
  br label %block_3f76

block_3f76:                                       ; preds = %block_3f54, %block_3f4a, %block_3f42
  store volatile i64 16246, i64* @assembly_address
  %503 = load i8*** %stack_var_-32
  %504 = ptrtoint i8** %503 to i64
  store i64 %504, i64* %rax
  store volatile i64 16250, i64* @assembly_address
  %505 = load i64* %rax
  store i64 %505, i64* %rdi
  store volatile i64 16253, i64* @assembly_address
  %506 = load i64* %rdi
  %507 = inttoptr i64 %506 to i64*
  call void @free(i64* %507)
  store volatile i64 16258, i64* @assembly_address
  %508 = inttoptr i64 0 to i8**
  store i8** %508, i8*** %stack_var_-32
  store volatile i64 16266, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216568 to i32*)
  store volatile i64 16276, i64* @assembly_address
  store i32 -1, i32* %stack_var_-52
  br label %block_3f9b

block_3f9b:                                       ; preds = %block_3f76, %block_3ee8, %block_3eb0, %block_3e52
  store volatile i64 16283, i64* @assembly_address
  %509 = load i8*** %stack_var_-32
  %510 = ptrtoint i8** %509 to i64
  %511 = and i64 %510, 15
  %512 = icmp ugt i64 %511, 15
  %513 = icmp ult i64 %510, 0
  %514 = xor i64 %510, 0
  %515 = and i64 %514, 0
  %516 = icmp slt i64 %515, 0
  store i1 %512, i1* %az
  store i1 %513, i1* %cf
  store i1 %516, i1* %of
  %517 = icmp eq i64 %510, 0
  store i1 %517, i1* %zf
  %518 = icmp slt i64 %510, 0
  store i1 %518, i1* %sf
  %519 = trunc i64 %510 to i8
  %520 = call i8 @llvm.ctpop.i8(i8 %519)
  %521 = and i8 %520, 1
  %522 = icmp eq i8 %521, 0
  store i1 %522, i1* %pf
  store volatile i64 16288, i64* @assembly_address
  %523 = load i1* %zf
  %524 = icmp eq i1 %523, false
  br i1 %524, label %block_3fc8, label %block_3fa2

block_3fa2:                                       ; preds = %block_3f9b
  store volatile i64 16290, i64* @assembly_address
  %525 = ptrtoint i32* %stack_var_-52 to i64
  store i64 %525, i64* %rdx
  store volatile i64 16294, i64* @assembly_address
  %526 = load i8*** %stack_var_-72
  %527 = ptrtoint i8** %526 to i64
  store i64 %527, i64* %rsi
  store volatile i64 16298, i64* @assembly_address
  %528 = load i32* %stack_var_-60
  %529 = zext i32 %528 to i64
  store i64 %529, i64* %rax
  store volatile i64 16301, i64* @assembly_address
  %530 = ptrtoint i32* %stack_var_-52 to i64
  store i64 %530, i64* %r8
  store volatile i64 16304, i64* @assembly_address
  store i64 ptrtoint ([6 x i8]** @global_var_2156c0 to i64), i64* %rcx
  store volatile i64 16311, i64* @assembly_address
  store i64 ptrtoint ([34 x i8]* @global_var_10ee0 to i64), i64* %rdx
  store volatile i64 16318, i64* @assembly_address
  %531 = load i64* %rax
  %532 = trunc i64 %531 to i32
  %533 = zext i32 %532 to i64
  store i64 %533, i64* %rdi
  store volatile i64 16320, i64* @assembly_address
  %534 = load i64* %rdi
  %535 = trunc i64 %534 to i32
  %536 = load i64* %rsi
  %537 = inttoptr i64 %536 to i8**
  %538 = load i64* %rdx
  %539 = inttoptr i64 %538 to i8*
  %540 = load i64* %rcx
  %541 = inttoptr i64 %540 to %option*
  %542 = load i64* %r8
  %543 = inttoptr i64 %542 to i32*
  %544 = call i32 @getopt_long(i32 %535, i8** %537, i8* %539, %option* %541, i32* %543)
  %545 = sext i32 %544 to i64
  store i64 %545, i64* %rax
  %546 = sext i32 %544 to i64
  store i64 %546, i64* %rax
  store volatile i64 16325, i64* @assembly_address
  %547 = load i64* %rax
  %548 = trunc i64 %547 to i32
  store i32 %548, i32* %stack_var_-48
  br label %block_3fc8

block_3fc8:                                       ; preds = %block_3fa2, %block_3f9b
  store volatile i64 16328, i64* @assembly_address
  %549 = load i32* %stack_var_-48
  %550 = and i32 %549, 15
  %551 = icmp ugt i32 %550, 15
  %552 = icmp ult i32 %549, 0
  %553 = xor i32 %549, 0
  %554 = and i32 %553, 0
  %555 = icmp slt i32 %554, 0
  store i1 %551, i1* %az
  store i1 %552, i1* %cf
  store i1 %555, i1* %of
  %556 = icmp eq i32 %549, 0
  store i1 %556, i1* %zf
  %557 = icmp slt i32 %549, 0
  store i1 %557, i1* %sf
  %558 = trunc i32 %549 to i8
  %559 = call i8 @llvm.ctpop.i8(i8 %558)
  %560 = and i8 %559, 1
  %561 = icmp eq i8 %560, 0
  store i1 %561, i1* %pf
  store volatile i64 16332, i64* @assembly_address
  %562 = load i1* %sf
  %563 = icmp eq i1 %562, false
  br i1 %563, label %block_3fe1, label %block_3fce

block_3fce:                                       ; preds = %block_3fc8
  store volatile i64 16334, i64* @assembly_address
  %564 = load i32* bitcast (i64* @global_var_216094 to i32*)
  %565 = zext i32 %564 to i64
  store i64 %565, i64* %rax
  store volatile i64 16340, i64* @assembly_address
  %566 = load i64* %rax
  %567 = trunc i64 %566 to i32
  %568 = load i64* %rax
  %569 = trunc i64 %568 to i32
  %570 = and i32 %567, %569
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %571 = icmp eq i32 %570, 0
  store i1 %571, i1* %zf
  %572 = icmp slt i32 %570, 0
  store i1 %572, i1* %sf
  %573 = trunc i32 %570 to i8
  %574 = call i8 @llvm.ctpop.i8(i8 %573)
  %575 = and i8 %574, 1
  %576 = icmp eq i8 %575, 0
  store i1 %576, i1* %pf
  store volatile i64 16342, i64* @assembly_address
  %577 = load i1* %sf
  br i1 %577, label %block_434d, label %block_3fdc

block_3fdc:                                       ; preds = %block_3fce
  store volatile i64 16348, i64* @assembly_address
  br label %block_4359

block_3fe1:                                       ; preds = %block_3fc8
  store volatile i64 16353, i64* @assembly_address
  %578 = load i32* %stack_var_-48
  %579 = zext i32 %578 to i64
  store i64 %579, i64* %rax
  store volatile i64 16356, i64* @assembly_address
  %580 = load i64* %rax
  %581 = trunc i64 %580 to i32
  %582 = sub i32 %581, 49
  %583 = and i32 %581, 15
  %584 = sub i32 %583, 1
  %585 = icmp ugt i32 %584, 15
  %586 = icmp ult i32 %581, 49
  %587 = xor i32 %581, 49
  %588 = xor i32 %581, %582
  %589 = and i32 %587, %588
  %590 = icmp slt i32 %589, 0
  store i1 %585, i1* %az
  store i1 %586, i1* %cf
  store i1 %590, i1* %of
  %591 = icmp eq i32 %582, 0
  store i1 %591, i1* %zf
  %592 = icmp slt i32 %582, 0
  store i1 %592, i1* %sf
  %593 = trunc i32 %582 to i8
  %594 = call i8 @llvm.ctpop.i8(i8 %593)
  %595 = and i8 %594, 1
  %596 = icmp eq i8 %595, 0
  store i1 %596, i1* %pf
  %597 = zext i32 %582 to i64
  store i64 %597, i64* %rax
  store volatile i64 16359, i64* @assembly_address
  %598 = load i64* %rax
  %599 = trunc i64 %598 to i32
  %600 = sub i32 %599, 211
  %601 = and i32 %599, 15
  %602 = sub i32 %601, 3
  %603 = icmp ugt i32 %602, 15
  %604 = icmp ult i32 %599, 211
  %605 = xor i32 %599, 211
  %606 = xor i32 %599, %600
  %607 = and i32 %605, %606
  %608 = icmp slt i32 %607, 0
  store i1 %603, i1* %az
  store i1 %604, i1* %cf
  store i1 %608, i1* %of
  %609 = icmp eq i32 %600, 0
  store i1 %609, i1* %zf
  %610 = icmp slt i32 %600, 0
  store i1 %610, i1* %sf
  %611 = trunc i32 %600 to i8
  %612 = call i8 @llvm.ctpop.i8(i8 %611)
  %613 = and i8 %612, 1
  %614 = icmp eq i8 %613, 0
  store i1 %614, i1* %pf
  store volatile i64 16364, i64* @assembly_address
  br i1 false, label %block_4288, label %block_3ff2

block_3ff2:                                       ; preds = %block_3fe1
  store volatile i64 16370, i64* @assembly_address
  %615 = load i64* %rax
  %616 = trunc i64 %615 to i32
  %617 = zext i32 %616 to i64
  store i64 %617, i64* %rax
  store volatile i64 16372, i64* @assembly_address
  %618 = load i64* %rax
  %619 = mul i64 %618, 4
  store i64 %619, i64* %rdx
  store volatile i64 16380, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_111ac to i64), i64* %rax
  store volatile i64 16387, i64* @assembly_address
  %620 = load i64* %rdx
  %621 = load i64* %rax
  %622 = mul i64 %621, 1
  %623 = add i64 %620, %622
  %624 = inttoptr i64 %623 to i32*
  %625 = load i32* %624
  %626 = zext i32 %625 to i64
  store i64 %626, i64* %rax
  store volatile i64 16390, i64* @assembly_address
  %627 = load i64* %rax
  %628 = trunc i64 %627 to i32
  %629 = sext i32 %628 to i64
  store i64 %629, i64* %rdx
  store volatile i64 16393, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_111ac to i64), i64* %rax
  store volatile i64 16400, i64* @assembly_address
  %630 = load i64* %rax
  %631 = load i64* %rdx
  %632 = add i64 %630, %631
  %633 = and i64 %630, 15
  %634 = and i64 %631, 15
  %635 = add i64 %633, %634
  %636 = icmp ugt i64 %635, 15
  %637 = icmp ult i64 %632, %630
  %638 = xor i64 %630, %632
  %639 = xor i64 %631, %632
  %640 = and i64 %638, %639
  %641 = icmp slt i64 %640, 0
  store i1 %636, i1* %az
  store i1 %637, i1* %cf
  store i1 %641, i1* %of
  %642 = icmp eq i64 %632, 0
  store i1 %642, i1* %zf
  %643 = icmp slt i64 %632, 0
  store i1 %643, i1* %sf
  %644 = trunc i64 %632 to i8
  %645 = call i8 @llvm.ctpop.i8(i8 %644)
  %646 = and i8 %645, 1
  %647 = icmp eq i8 %646, 0
  store i1 %647, i1* %pf
  store i64 %632, i64* %rax
  store volatile i64 16403, i64* @assembly_address
  switch i64 %618, label %block_4288 [
    i64 0, label %block_4277
    i64 1, label %block_4277
    i64 2, label %block_4277
    i64 3, label %block_4277
    i64 4, label %block_4277
    i64 5, label %block_4277
    i64 6, label %block_4277
    i64 7, label %block_4277
    i64 8, label %block_4277
    i64 23, label %block_40d5
    i64 27, label %block_411a
    i64 28, label %block_4138
    i64 29, label %block_4162
    i64 34, label %block_41c0
    i64 37, label %block_423a
    i64 41, label %block_4249
    i64 48, label %block_4015
    i64 49, label %block_4024
    i64 50, label %block_40a3
    i64 51, label %block_40b2
    i64 53, label %block_40c1
    i64 55, label %block_40d5
    i64 58, label %block_40e4
    i64 59, label %block_40f3
    i64 60, label %block_4129
    i64 61, label %block_4147
    i64 64, label %block_4189
    i64 65, label %block_41a2
    i64 67, label %block_41f5
    i64 69, label %block_421c
    i64 79, label %block_417d
    i64 80, label %block_41b1
    i64 81, label %block_41e9
    i64 131, label %block_4270
    i64 132, label %block_4270
    i64 133, label %block_4270
    i64 134, label %block_4270
    i64 135, label %block_4270
    i64 136, label %block_4270
    i64 137, label %block_4270
    i64 138, label %block_4270
    i64 139, label %block_4270
    i64 160, label %block_4162
    i64 192, label %block_4147
    i64 195, label %block_4189
    i64 200, label %block_421c
    i64 211, label %block_41b1
  ]

block_4015:                                       ; preds = %block_3ff2
  store volatile i64 16405, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165fc to i32*)
  store volatile i64 16415, i64* @assembly_address
  br label %block_4348

block_4024:                                       ; preds = %block_3ff2
  store volatile i64 16420, i64* @assembly_address
  %648 = load i64* @global_var_216570
  store i64 %648, i64* %rax
  store volatile i64 16427, i64* @assembly_address
  %649 = load i64* %rax
  store i64 %649, i64* %rdi
  store volatile i64 16430, i64* @assembly_address
  %650 = load i64* %rdi
  %651 = inttoptr i64 %650 to i8*
  %652 = call i32 @atoi(i8* %651)
  %653 = sext i32 %652 to i64
  store i64 %653, i64* %rax
  %654 = sext i32 %652 to i64
  store i64 %654, i64* %rax
  store volatile i64 16435, i64* @assembly_address
  %655 = load i64* %rax
  %656 = trunc i64 %655 to i32
  store i32 %656, i32* bitcast (i64* @global_var_216098 to i32*)
  store volatile i64 16441, i64* @assembly_address
  br label %block_4090

block_403b:                                       ; preds = %block_4090
  store volatile i64 16443, i64* @assembly_address
  %657 = load i64* @global_var_216570
  store i64 %657, i64* %rax
  store volatile i64 16450, i64* @assembly_address
  %658 = load i64* %rax
  %659 = inttoptr i64 %658 to i8*
  %660 = load i8* %659
  %661 = zext i8 %660 to i64
  store i64 %661, i64* %rax
  store volatile i64 16453, i64* @assembly_address
  %662 = load i64* %rax
  %663 = trunc i64 %662 to i8
  %664 = trunc i64 %662 to i8
  store i8 %664, i8* %17
  store i8 47, i8* %16
  %665 = sub i8 %663, 47
  %666 = and i8 %663, 15
  %667 = sub i8 %666, 15
  %668 = icmp ugt i8 %667, 15
  %669 = icmp ult i8 %663, 47
  %670 = xor i8 %663, 47
  %671 = xor i8 %663, %665
  %672 = and i8 %670, %671
  %673 = icmp slt i8 %672, 0
  store i1 %668, i1* %az
  store i1 %669, i1* %cf
  store i1 %673, i1* %of
  %674 = icmp eq i8 %665, 0
  store i1 %674, i1* %zf
  %675 = icmp slt i8 %665, 0
  store i1 %675, i1* %sf
  %676 = call i8 @llvm.ctpop.i8(i8 %665)
  %677 = and i8 %676, 1
  %678 = icmp eq i8 %677, 0
  store i1 %678, i1* %pf
  store volatile i64 16455, i64* @assembly_address
  %679 = load i8* %17
  %680 = sext i8 %679 to i64
  %681 = load i8* %16
  %682 = trunc i64 %680 to i8
  %683 = icmp sle i8 %682, %681
  br i1 %683, label %block_4057, label %block_4049

block_4049:                                       ; preds = %block_403b
  store volatile i64 16457, i64* @assembly_address
  %684 = load i64* @global_var_216570
  store i64 %684, i64* %rax
  store volatile i64 16464, i64* @assembly_address
  %685 = load i64* %rax
  %686 = inttoptr i64 %685 to i8*
  %687 = load i8* %686
  %688 = zext i8 %687 to i64
  store i64 %688, i64* %rax
  store volatile i64 16467, i64* @assembly_address
  %689 = load i64* %rax
  %690 = trunc i64 %689 to i8
  %691 = trunc i64 %689 to i8
  store i8 %691, i8* %14
  store i8 57, i8* %13
  %692 = sub i8 %690, 57
  %693 = and i8 %690, 15
  %694 = sub i8 %693, 9
  %695 = icmp ugt i8 %694, 15
  %696 = icmp ult i8 %690, 57
  %697 = xor i8 %690, 57
  %698 = xor i8 %690, %692
  %699 = and i8 %697, %698
  %700 = icmp slt i8 %699, 0
  store i1 %695, i1* %az
  store i1 %696, i1* %cf
  store i1 %700, i1* %of
  %701 = icmp eq i8 %692, 0
  store i1 %701, i1* %zf
  %702 = icmp slt i8 %692, 0
  store i1 %702, i1* %sf
  %703 = call i8 @llvm.ctpop.i8(i8 %692)
  %704 = and i8 %703, 1
  %705 = icmp eq i8 %704, 0
  store i1 %705, i1* %pf
  store volatile i64 16469, i64* @assembly_address
  %706 = load i8* %14
  %707 = sext i8 %706 to i64
  %708 = load i8* %13
  %709 = trunc i64 %707 to i8
  %710 = icmp sle i8 %709, %708
  br i1 %710, label %block_407e, label %block_4057

block_4057:                                       ; preds = %block_4049, %block_403b
  store volatile i64 16471, i64* @assembly_address
  %711 = load i64* @global_var_25f4c8
  store i64 %711, i64* %rdx
  store volatile i64 16478, i64* @assembly_address
  %712 = load i64* @global_var_216580
  store i64 %712, i64* %rax
  store volatile i64 16485, i64* @assembly_address
  store i64 ptrtoint ([34 x i8]* @global_var_110d8 to i64), i64* %rsi
  store volatile i64 16492, i64* @assembly_address
  %713 = load i64* %rax
  store i64 %713, i64* %rdi
  store volatile i64 16495, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 16500, i64* @assembly_address
  %714 = load i64* %rdi
  %715 = inttoptr i64 %714 to %_IO_FILE*
  %716 = load i64* %rsi
  %717 = inttoptr i64 %716 to i8*
  %718 = load i64* %rdx
  %719 = inttoptr i64 %718 to i8*
  %720 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %715, i8* %717, i8* %719)
  %721 = sext i32 %720 to i64
  store i64 %721, i64* %rax
  %722 = sext i32 %720 to i64
  store i64 %722, i64* %rax
  store volatile i64 16505, i64* @assembly_address
  %723 = call i64 @try_help()
  store i64 %723, i64* %rax
  store i64 %723, i64* %rax
  store i64 %723, i64* %rax
  unreachable

block_407e:                                       ; preds = %block_4049
  store volatile i64 16510, i64* @assembly_address
  %724 = load i64* @global_var_216570
  store i64 %724, i64* %rax
  store volatile i64 16517, i64* @assembly_address
  %725 = load i64* %rax
  %726 = add i64 %725, 1
  %727 = and i64 %725, 15
  %728 = add i64 %727, 1
  %729 = icmp ugt i64 %728, 15
  %730 = icmp ult i64 %726, %725
  %731 = xor i64 %725, %726
  %732 = xor i64 1, %726
  %733 = and i64 %731, %732
  %734 = icmp slt i64 %733, 0
  store i1 %729, i1* %az
  store i1 %730, i1* %cf
  store i1 %734, i1* %of
  %735 = icmp eq i64 %726, 0
  store i1 %735, i1* %zf
  %736 = icmp slt i64 %726, 0
  store i1 %736, i1* %sf
  %737 = trunc i64 %726 to i8
  %738 = call i8 @llvm.ctpop.i8(i8 %737)
  %739 = and i8 %738, 1
  %740 = icmp eq i8 %739, 0
  store i1 %740, i1* %pf
  store i64 %726, i64* %rax
  store volatile i64 16521, i64* @assembly_address
  %741 = load i64* %rax
  store i64 %741, i64* @global_var_216570
  br label %block_4090

block_4090:                                       ; preds = %block_407e, %block_4024
  store volatile i64 16528, i64* @assembly_address
  %742 = load i64* @global_var_216570
  store i64 %742, i64* %rax
  store volatile i64 16535, i64* @assembly_address
  %743 = load i64* %rax
  %744 = inttoptr i64 %743 to i8*
  %745 = load i8* %744
  %746 = zext i8 %745 to i64
  store i64 %746, i64* %rax
  store volatile i64 16538, i64* @assembly_address
  %747 = load i64* %rax
  %748 = trunc i64 %747 to i8
  %749 = load i64* %rax
  %750 = trunc i64 %749 to i8
  %751 = and i8 %748, %750
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %752 = icmp eq i8 %751, 0
  store i1 %752, i1* %zf
  %753 = icmp slt i8 %751, 0
  store i1 %753, i1* %sf
  %754 = call i8 @llvm.ctpop.i8(i8 %751)
  %755 = and i8 %754, 1
  %756 = icmp eq i8 %755, 0
  store i1 %756, i1* %pf
  store volatile i64 16540, i64* @assembly_address
  %757 = load i1* %zf
  %758 = icmp eq i1 %757, false
  br i1 %758, label %block_403b, label %block_409e

block_409e:                                       ; preds = %block_4090
  store volatile i64 16542, i64* @assembly_address
  br label %block_4348

block_40a3:                                       ; preds = %block_3ff2
  store volatile i64 16547, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165e0 to i32*)
  store volatile i64 16557, i64* @assembly_address
  br label %block_4348

block_40b2:                                       ; preds = %block_3ff2
  store volatile i64 16562, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216600 to i32*)
  store volatile i64 16572, i64* @assembly_address
  br label %block_4348

block_40c1:                                       ; preds = %block_3ff2
  store volatile i64 16577, i64* @assembly_address
  %759 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %760 = zext i32 %759 to i64
  store i64 %760, i64* %rax
  store volatile i64 16583, i64* @assembly_address
  %761 = load i64* %rax
  %762 = trunc i64 %761 to i32
  %763 = add i32 %762, 1
  %764 = and i32 %762, 15
  %765 = add i32 %764, 1
  %766 = icmp ugt i32 %765, 15
  %767 = icmp ult i32 %763, %762
  %768 = xor i32 %762, %763
  %769 = xor i32 1, %763
  %770 = and i32 %768, %769
  %771 = icmp slt i32 %770, 0
  store i1 %766, i1* %az
  store i1 %767, i1* %cf
  store i1 %771, i1* %of
  %772 = icmp eq i32 %763, 0
  store i1 %772, i1* %zf
  %773 = icmp slt i32 %763, 0
  store i1 %773, i1* %sf
  %774 = trunc i32 %763 to i8
  %775 = call i8 @llvm.ctpop.i8(i8 %774)
  %776 = and i8 %775, 1
  %777 = icmp eq i8 %776, 0
  store i1 %777, i1* %pf
  store i64 ptrtoint (i64* @global_var_216605 to i64), i64* %rax
  store volatile i64 16586, i64* @assembly_address
  %778 = load i64* %rax
  %779 = trunc i64 %778 to i32
  store i32 %779, i32* bitcast (i64* @global_var_216604 to i32*)
  store volatile i64 16592, i64* @assembly_address
  br label %block_4348

block_40d5:                                       ; preds = %block_3ff2, %block_3ff2
  store volatile i64 16597, i64* @assembly_address
  %780 = call i64 @help()
  store i64 %780, i64* %rax
  store i64 %780, i64* %rax
  store i64 %780, i64* %rax
  store volatile i64 16602, i64* @assembly_address
  %781 = call i64 @finish_out()
  store i64 %781, i64* %rax
  store i64 %781, i64* %rax
  store i64 %781, i64* %rax
  unreachable

block_40df:                                       ; No predecessors!
  store volatile i64 16607, i64* @assembly_address
  br label %block_4348

block_40e4:                                       ; preds = %block_3ff2
  store volatile i64 16612, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216608 to i32*)
  store volatile i64 16622, i64* @assembly_address
  br label %block_4348

block_40f3:                                       ; preds = %block_3ff2
  store volatile i64 16627, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165e0 to i32*)
  store volatile i64 16637, i64* @assembly_address
  %782 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %783 = zext i32 %782 to i64
  store i64 %783, i64* %rax
  store volatile i64 16643, i64* @assembly_address
  %784 = load i64* %rax
  %785 = trunc i64 %784 to i32
  store i32 %785, i32* bitcast (i64* @global_var_216600 to i32*)
  store volatile i64 16649, i64* @assembly_address
  %786 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %787 = zext i32 %786 to i64
  store i64 %787, i64* %rax
  store volatile i64 16655, i64* @assembly_address
  %788 = load i64* %rax
  %789 = trunc i64 %788 to i32
  store i32 %789, i32* bitcast (i64* @global_var_216610 to i32*)
  store volatile i64 16661, i64* @assembly_address
  br label %block_4348

block_411a:                                       ; preds = %block_3ff2
  store volatile i64 16666, i64* @assembly_address
  %790 = call i64 @license()
  store i64 %790, i64* %rax
  store i64 %790, i64* %rax
  store i64 %790, i64* %rax
  store volatile i64 16671, i64* @assembly_address
  %791 = call i64 @finish_out()
  store i64 %791, i64* %rax
  store i64 %791, i64* %rax
  store i64 %791, i64* %rax
  unreachable

block_4124:                                       ; No predecessors!
  store volatile i64 16676, i64* @assembly_address
  br label %block_4348

block_4129:                                       ; preds = %block_3ff2
  store volatile i64 16681, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216094 to i32*)
  store volatile i64 16691, i64* @assembly_address
  br label %block_4348

block_4138:                                       ; preds = %block_3ff2
  store volatile i64 16696, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216094 to i32*)
  store volatile i64 16706, i64* @assembly_address
  br label %block_4348

block_4147:                                       ; preds = %block_3ff2, %block_3ff2
  store volatile i64 16711, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216094 to i32*)
  store volatile i64 16721, i64* @assembly_address
  %792 = load i32* bitcast (i64* @global_var_216094 to i32*)
  %793 = zext i32 %792 to i64
  store i64 %793, i64* %rax
  store volatile i64 16727, i64* @assembly_address
  %794 = load i64* %rax
  %795 = trunc i64 %794 to i32
  store i32 %795, i32* bitcast (i64* @global_var_216090 to i32*)
  store volatile i64 16733, i64* @assembly_address
  br label %block_4348

block_4162:                                       ; preds = %block_3ff2, %block_3ff2
  store volatile i64 16738, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216094 to i32*)
  store volatile i64 16748, i64* @assembly_address
  %796 = load i32* bitcast (i64* @global_var_216094 to i32*)
  %797 = zext i32 %796 to i64
  store i64 %797, i64* %rax
  store volatile i64 16754, i64* @assembly_address
  %798 = load i64* %rax
  %799 = trunc i64 %798 to i32
  store i32 %799, i32* bitcast (i64* @global_var_216090 to i32*)
  store volatile i64 16760, i64* @assembly_address
  br label %block_4348

block_417d:                                       ; preds = %block_3ff2
  store volatile i64 16765, i64* @assembly_address
  store i8 1, i8* bitcast (i64* @global_var_2165f8 to i8*)
  store volatile i64 16772, i64* @assembly_address
  br label %block_4348

block_4189:                                       ; preds = %block_3ff2, %block_3ff2
  store volatile i64 16777, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165e8 to i32*)
  store volatile i64 16787, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_2165e4 to i32*)
  store volatile i64 16797, i64* @assembly_address
  br label %block_4348

block_41a2:                                       ; preds = %block_3ff2
  store volatile i64 16802, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21660c to i32*)
  store volatile i64 16812, i64* @assembly_address
  br label %block_4348

block_41b1:                                       ; preds = %block_3ff2, %block_3ff2
  store volatile i64 16817, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f4 to i32*)
  store volatile i64 16827, i64* @assembly_address
  br label %block_4348

block_41c0:                                       ; preds = %block_3ff2
  store volatile i64 16832, i64* @assembly_address
  %800 = load i64* @global_var_216570
  store i64 %800, i64* %rax
  store volatile i64 16839, i64* @assembly_address
  %801 = load i64* %rax
  store i64 %801, i64* %rdi
  store volatile i64 16842, i64* @assembly_address
  %802 = load i64* %rdi
  %803 = inttoptr i64 %802 to i8*
  %804 = call i32 @strlen(i8* %803)
  %805 = sext i32 %804 to i64
  store i64 %805, i64* %rax
  %806 = sext i32 %804 to i64
  store i64 %806, i64* %rax
  store volatile i64 16847, i64* @assembly_address
  %807 = load i64* %rax
  store i64 %807, i64* @global_var_216638
  store volatile i64 16854, i64* @assembly_address
  %808 = load i64* @global_var_216570
  store i64 %808, i64* %rax
  store volatile i64 16861, i64* @assembly_address
  %809 = load i64* %rax
  store i64 %809, i64* @global_var_216630
  store volatile i64 16868, i64* @assembly_address
  br label %block_4348

block_41e9:                                       ; preds = %block_3ff2
  store volatile i64 16873, i64* @assembly_address
  store i8 1, i8* bitcast (i64* @global_var_2165f9 to i8*)
  store volatile i64 16880, i64* @assembly_address
  br label %block_4348

block_41f5:                                       ; preds = %block_3ff2
  store volatile i64 16885, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165e0 to i32*)
  store volatile i64 16895, i64* @assembly_address
  %810 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %811 = zext i32 %810 to i64
  store i64 %811, i64* %rax
  store volatile i64 16901, i64* @assembly_address
  %812 = load i64* %rax
  %813 = trunc i64 %812 to i32
  store i32 %813, i32* bitcast (i64* @global_var_216600 to i32*)
  store volatile i64 16907, i64* @assembly_address
  %814 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %815 = zext i32 %814 to i64
  store i64 %815, i64* %rax
  store volatile i64 16913, i64* @assembly_address
  %816 = load i64* %rax
  %817 = trunc i64 %816 to i32
  store i32 %817, i32* bitcast (i64* @global_var_2165ec to i32*)
  store volatile i64 16919, i64* @assembly_address
  br label %block_4348

block_421c:                                       ; preds = %block_3ff2, %block_3ff2
  store volatile i64 16924, i64* @assembly_address
  %818 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %819 = zext i32 %818 to i64
  store i64 %819, i64* %rax
  store volatile i64 16930, i64* @assembly_address
  %820 = load i64* %rax
  %821 = trunc i64 %820 to i32
  %822 = add i32 %821, 1
  %823 = and i32 %821, 15
  %824 = add i32 %823, 1
  %825 = icmp ugt i32 %824, 15
  %826 = icmp ult i32 %822, %821
  %827 = xor i32 %821, %822
  %828 = xor i32 1, %822
  %829 = and i32 %827, %828
  %830 = icmp slt i32 %829, 0
  store i1 %825, i1* %az
  store i1 %826, i1* %cf
  store i1 %830, i1* %of
  %831 = icmp eq i32 %822, 0
  store i1 %831, i1* %zf
  %832 = icmp slt i32 %822, 0
  store i1 %832, i1* %sf
  %833 = trunc i32 %822 to i8
  %834 = call i8 @llvm.ctpop.i8(i8 %833)
  %835 = and i8 %834, 1
  %836 = icmp eq i8 %835, 0
  store i1 %836, i1* %pf
  store i64 ptrtoint (i64* @global_var_2165e5 to i64), i64* %rax
  store volatile i64 16933, i64* @assembly_address
  %837 = load i64* %rax
  %838 = trunc i64 %837 to i32
  store i32 %838, i32* bitcast (i64* @global_var_2165e4 to i32*)
  store volatile i64 16939, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_2165e8 to i32*)
  store volatile i64 16949, i64* @assembly_address
  br label %block_4348

block_423a:                                       ; preds = %block_3ff2
  store volatile i64 16954, i64* @assembly_address
  %839 = call i64 @version()
  store i64 %839, i64* %rax
  store i64 %839, i64* %rax
  store i64 %839, i64* %rax
  store volatile i64 16959, i64* @assembly_address
  %840 = call i64 @finish_out()
  store i64 %840, i64* %rax
  store i64 %840, i64* %rax
  store i64 %840, i64* %rax
  unreachable

block_4244:                                       ; No predecessors!
  store volatile i64 16964, i64* @assembly_address
  br label %block_4348

block_4249:                                       ; preds = %block_3ff2
  store volatile i64 16969, i64* @assembly_address
  %841 = load i64* @global_var_25f4c8
  store i64 %841, i64* %rdx
  store volatile i64 16976, i64* @assembly_address
  %842 = load i64* @global_var_216580
  store i64 %842, i64* %rax
  store volatile i64 16983, i64* @assembly_address
  store i64 ptrtoint ([38 x i8]* @global_var_11100 to i64), i64* %rsi
  store volatile i64 16990, i64* @assembly_address
  %843 = load i64* %rax
  store i64 %843, i64* %rdi
  store volatile i64 16993, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 16998, i64* @assembly_address
  %844 = load i64* %rdi
  %845 = inttoptr i64 %844 to %_IO_FILE*
  %846 = load i64* %rsi
  %847 = inttoptr i64 %846 to i8*
  %848 = load i64* %rdx
  %849 = inttoptr i64 %848 to i8*
  %850 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %845, i8* %847, i8* %849)
  %851 = sext i32 %850 to i64
  store i64 %851, i64* %rax
  %852 = sext i32 %850 to i64
  store i64 %852, i64* %rax
  store volatile i64 17003, i64* @assembly_address
  %853 = call i64 @try_help()
  store i64 %853, i64* %rax
  store i64 %853, i64* %rax
  store i64 %853, i64* %rax
  unreachable

block_4270:                                       ; preds = %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2
  store volatile i64 17008, i64* @assembly_address
  %854 = load i32* %stack_var_-48
  %855 = sub i32 %854, 131
  %856 = and i32 %854, 15
  %857 = sub i32 %856, 3
  %858 = icmp ugt i32 %857, 15
  %859 = icmp ult i32 %854, 131
  %860 = xor i32 %854, 131
  %861 = xor i32 %854, %855
  %862 = and i32 %860, %861
  %863 = icmp slt i32 %862, 0
  store i1 %858, i1* %az
  store i1 %859, i1* %cf
  store i1 %863, i1* %of
  %864 = icmp eq i32 %855, 0
  store i1 %864, i1* %zf
  %865 = icmp slt i32 %855, 0
  store i1 %865, i1* %sf
  %866 = trunc i32 %855 to i8
  %867 = call i8 @llvm.ctpop.i8(i8 %866)
  %868 = and i8 %867, 1
  %869 = icmp eq i8 %868, 0
  store i1 %869, i1* %pf
  store i32 %855, i32* %stack_var_-48
  br label %block_4277

block_4277:                                       ; preds = %block_4270, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2, %block_3ff2
  store volatile i64 17015, i64* @assembly_address
  %870 = load i32* %stack_var_-48
  %871 = zext i32 %870 to i64
  store i64 %871, i64* %rax
  store volatile i64 17018, i64* @assembly_address
  %872 = load i64* %rax
  %873 = trunc i64 %872 to i32
  %874 = sub i32 %873, 48
  %875 = and i32 %873, 15
  %876 = icmp ugt i32 %875, 15
  %877 = icmp ult i32 %873, 48
  %878 = xor i32 %873, 48
  %879 = xor i32 %873, %874
  %880 = and i32 %878, %879
  %881 = icmp slt i32 %880, 0
  store i1 %876, i1* %az
  store i1 %877, i1* %cf
  store i1 %881, i1* %of
  %882 = icmp eq i32 %874, 0
  store i1 %882, i1* %zf
  %883 = icmp slt i32 %874, 0
  store i1 %883, i1* %sf
  %884 = trunc i32 %874 to i8
  %885 = call i8 @llvm.ctpop.i8(i8 %884)
  %886 = and i8 %885, 1
  %887 = icmp eq i8 %886, 0
  store i1 %887, i1* %pf
  %888 = zext i32 %874 to i64
  store i64 %888, i64* %rax
  store volatile i64 17021, i64* @assembly_address
  %889 = load i64* %rax
  %890 = trunc i64 %889 to i32
  store i32 %890, i32* bitcast (i64* @global_var_2160a0 to i32*)
  store volatile i64 17027, i64* @assembly_address
  br label %block_4348

block_4288:                                       ; preds = %block_3ff2, %block_3fe1
  store volatile i64 17032, i64* @assembly_address
  %891 = load i32* %stack_var_-48
  store i32 %891, i32* %12
  store i32 130, i32* %11
  %892 = sub i32 %891, 130
  %893 = and i32 %891, 15
  %894 = sub i32 %893, 2
  %895 = icmp ugt i32 %894, 15
  %896 = icmp ult i32 %891, 130
  %897 = xor i32 %891, 130
  %898 = xor i32 %891, %892
  %899 = and i32 %897, %898
  %900 = icmp slt i32 %899, 0
  store i1 %895, i1* %az
  store i1 %896, i1* %cf
  store i1 %900, i1* %of
  %901 = icmp eq i32 %892, 0
  store i1 %901, i1* %zf
  %902 = icmp slt i32 %892, 0
  store i1 %902, i1* %sf
  %903 = trunc i32 %892 to i8
  %904 = call i8 @llvm.ctpop.i8(i8 %903)
  %905 = and i8 %904, 1
  %906 = icmp eq i8 %905, 0
  store i1 %906, i1* %pf
  store volatile i64 17039, i64* @assembly_address
  %907 = load i32* %12
  %908 = load i32* %11
  %909 = icmp sle i32 %907, %908
  br i1 %909, label %block_4343, label %block_4295

block_4295:                                       ; preds = %block_4288
  store volatile i64 17045, i64* @assembly_address
  %910 = load i32* %stack_var_-48
  %911 = sub i32 %910, 194
  %912 = and i32 %910, 15
  %913 = sub i32 %912, 2
  %914 = icmp ugt i32 %913, 15
  %915 = icmp ult i32 %910, 194
  %916 = xor i32 %910, 194
  %917 = xor i32 %910, %911
  %918 = and i32 %916, %917
  %919 = icmp slt i32 %918, 0
  store i1 %914, i1* %az
  store i1 %915, i1* %cf
  store i1 %919, i1* %of
  %920 = icmp eq i32 %911, 0
  store i1 %920, i1* %zf
  %921 = icmp slt i32 %911, 0
  store i1 %921, i1* %sf
  %922 = trunc i32 %911 to i8
  %923 = call i8 @llvm.ctpop.i8(i8 %922)
  %924 = and i8 %923, 1
  %925 = icmp eq i8 %924, 0
  store i1 %925, i1* %pf
  store volatile i64 17052, i64* @assembly_address
  %926 = load i1* %zf
  br i1 %926, label %block_4343, label %block_42a2

block_42a2:                                       ; preds = %block_4295
  store volatile i64 17058, i64* @assembly_address
  %927 = load i64* @global_var_25f4c8
  store i64 %927, i64* %rdx
  store volatile i64 17065, i64* @assembly_address
  %928 = load i64* @global_var_216580
  store i64 %928, i64* %rax
  store volatile i64 17072, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11035 to i64), i64* %rsi
  store volatile i64 17079, i64* @assembly_address
  %929 = load i64* %rax
  store i64 %929, i64* %rdi
  store volatile i64 17082, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 17087, i64* @assembly_address
  %930 = load i64* %rdi
  %931 = inttoptr i64 %930 to %_IO_FILE*
  %932 = load i64* %rsi
  %933 = inttoptr i64 %932 to i8*
  %934 = load i64* %rdx
  %935 = inttoptr i64 %934 to i8*
  %936 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %931, i8* %933, i8* %935)
  %937 = sext i32 %936 to i64
  store i64 %937, i64* %rax
  %938 = sext i32 %936 to i64
  store i64 %938, i64* %rax
  store volatile i64 17092, i64* @assembly_address
  %939 = load i32* %stack_var_-52
  %940 = zext i32 %939 to i64
  store i64 %940, i64* %rax
  store volatile i64 17095, i64* @assembly_address
  %941 = load i64* %rax
  %942 = trunc i64 %941 to i32
  %943 = load i64* %rax
  %944 = trunc i64 %943 to i32
  %945 = and i32 %942, %944
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %946 = icmp eq i32 %945, 0
  store i1 %946, i1* %zf
  %947 = icmp slt i32 %945, 0
  store i1 %947, i1* %sf
  %948 = trunc i32 %945 to i8
  %949 = call i8 @llvm.ctpop.i8(i8 %948)
  %950 = and i8 %949, 1
  %951 = icmp eq i8 %950, 0
  store i1 %951, i1* %pf
  store volatile i64 17097, i64* @assembly_address
  %952 = load i1* %sf
  %953 = icmp eq i1 %952, false
  br i1 %953, label %block_42f1, label %block_42cb

block_42cb:                                       ; preds = %block_42a2
  store volatile i64 17099, i64* @assembly_address
  %954 = load i32* %stack_var_-48
  %955 = zext i32 %954 to i64
  store i64 %955, i64* %rax
  store volatile i64 17102, i64* @assembly_address
  %956 = load i64* %rax
  %957 = add i64 %956, -131
  %958 = trunc i64 %957 to i32
  %959 = zext i32 %958 to i64
  store i64 %959, i64* %rdx
  store volatile i64 17108, i64* @assembly_address
  %960 = load i64* @global_var_216580
  store i64 %960, i64* %rax
  store volatile i64 17115, i64* @assembly_address
  store i64 ptrtoint ([6 x i8]* @global_var_11126 to i64), i64* %rsi
  store volatile i64 17122, i64* @assembly_address
  %961 = load i64* %rax
  store i64 %961, i64* %rdi
  store volatile i64 17125, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 17130, i64* @assembly_address
  %962 = load i64* %rdi
  %963 = inttoptr i64 %962 to %_IO_FILE*
  %964 = load i64* %rsi
  %965 = inttoptr i64 %964 to i8*
  %966 = load i64* %rdx
  %967 = trunc i64 %966 to i8
  %968 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %963, i8* %965, i8 %967)
  %969 = sext i32 %968 to i64
  store i64 %969, i64* %rax
  %970 = sext i32 %968 to i64
  store i64 %970, i64* %rax
  store volatile i64 17135, i64* @assembly_address
  br label %block_4323

block_42f1:                                       ; preds = %block_42a2
  store volatile i64 17137, i64* @assembly_address
  %971 = load i32* %stack_var_-52
  %972 = zext i32 %971 to i64
  store i64 %972, i64* %rax
  store volatile i64 17140, i64* @assembly_address
  %973 = load i64* %rax
  %974 = trunc i64 %973 to i32
  %975 = sext i32 %974 to i64
  store i64 %975, i64* %rax
  store volatile i64 17142, i64* @assembly_address
  %976 = load i64* %rax
  %977 = load i1* %of
  %978 = shl i64 %976, 5
  %979 = icmp eq i64 %978, 0
  store i1 %979, i1* %zf
  %980 = icmp slt i64 %978, 0
  store i1 %980, i1* %sf
  %981 = trunc i64 %978 to i8
  %982 = call i8 @llvm.ctpop.i8(i8 %981)
  %983 = and i8 %982, 1
  %984 = icmp eq i8 %983, 0
  store i1 %984, i1* %pf
  store i64 %978, i64* %rax
  %985 = shl i64 %976, 4
  %986 = lshr i64 %985, 63
  %987 = trunc i64 %986 to i1
  store i1 %987, i1* %cf
  %988 = lshr i64 %978, 63
  %989 = icmp ne i64 %988, %986
  %990 = select i1 false, i1 %989, i1 %977
  store i1 %990, i1* %of
  store volatile i64 17146, i64* @assembly_address
  %991 = load i64* %rax
  store i64 %991, i64* %rdx
  store volatile i64 17149, i64* @assembly_address
  store i64 ptrtoint ([6 x i8]** @global_var_2156c0 to i64), i64* %rax
  store volatile i64 17156, i64* @assembly_address
  %992 = load i64* %rdx
  %993 = load i64* %rax
  %994 = mul i64 %993, 1
  %995 = add i64 %992, %994
  %996 = inttoptr i64 %995 to i64*
  %997 = load i64* %996
  store i64 %997, i64* %rdx
  store volatile i64 17160, i64* @assembly_address
  %998 = load i64* @global_var_216580
  store i64 %998, i64* %rax
  store volatile i64 17167, i64* @assembly_address
  store i64 ptrtoint ([7 x i8]* @global_var_1112c to i64), i64* %rsi
  store volatile i64 17174, i64* @assembly_address
  %999 = load i64* %rax
  store i64 %999, i64* %rdi
  store volatile i64 17177, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 17182, i64* @assembly_address
  %1000 = load i64* %rdi
  %1001 = inttoptr i64 %1000 to %_IO_FILE*
  %1002 = load i64* %rsi
  %1003 = inttoptr i64 %1002 to i8*
  %1004 = load i64* %rdx
  %1005 = inttoptr i64 %1004 to i8*
  %1006 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1001, i8* %1003, i8* %1005)
  %1007 = sext i32 %1006 to i64
  store i64 %1007, i64* %rax
  %1008 = sext i32 %1006 to i64
  store i64 %1008, i64* %rax
  br label %block_4323

block_4323:                                       ; preds = %block_42f1, %block_42cb
  store volatile i64 17187, i64* @assembly_address
  %1009 = load i64* @global_var_216580
  store i64 %1009, i64* %rax
  store volatile i64 17194, i64* @assembly_address
  %1010 = load i64* %rax
  store i64 %1010, i64* %rcx
  store volatile i64 17197, i64* @assembly_address
  store i64 46, i64* %rdx
  store volatile i64 17202, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 17207, i64* @assembly_address
  store i64 ptrtoint ([47 x i8]* @global_var_11138 to i64), i64* %rdi
  store volatile i64 17214, i64* @assembly_address
  %1011 = load i64* %rdi
  %1012 = inttoptr i64 %1011 to i64*
  %1013 = load i64* %rsi
  %1014 = trunc i64 %1013 to i32
  %1015 = load i64* %rdx
  %1016 = trunc i64 %1015 to i32
  %1017 = load i64* %rcx
  %1018 = inttoptr i64 %1017 to %_IO_FILE*
  %1019 = call i32 @fwrite(i64* %1012, i32 %1014, i32 %1016, %_IO_FILE* %1018)
  %1020 = sext i32 %1019 to i64
  store i64 %1020, i64* %rax
  %1021 = sext i32 %1019 to i64
  store i64 %1021, i64* %rax
  br label %block_4343

block_4343:                                       ; preds = %block_4323, %block_4295, %block_4288
  store volatile i64 17219, i64* @assembly_address
  %1022 = call i64 @try_help()
  store i64 %1022, i64* %rax
  store i64 %1022, i64* %rax
  store i64 %1022, i64* %rax
  unreachable

block_4348:                                       ; preds = %block_4277, %block_4244, %block_421c, %block_41f5, %block_41e9, %block_41c0, %block_41b1, %block_41a2, %block_4189, %block_417d, %block_4162, %block_4147, %block_4138, %block_4129, %block_4124, %block_40f3, %block_40e4, %block_40df, %block_40c1, %block_40b2, %block_40a3, %block_409e, %block_4015
  store volatile i64 17224, i64* @assembly_address
  br label %block_3e52

block_434d:                                       ; preds = %block_3fce
  store volatile i64 17229, i64* @assembly_address
  %1023 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %1024 = zext i32 %1023 to i64
  store i64 %1024, i64* %rax
  store volatile i64 17235, i64* @assembly_address
  %1025 = load i64* %rax
  %1026 = trunc i64 %1025 to i32
  store i32 %1026, i32* bitcast (i64* @global_var_216094 to i32*)
  br label %block_4359

block_4359:                                       ; preds = %block_434d, %block_3fdc
  store volatile i64 17241, i64* @assembly_address
  %1027 = load i32* bitcast (i64* @global_var_216090 to i32*)
  %1028 = zext i32 %1027 to i64
  store i64 %1028, i64* %rax
  store volatile i64 17247, i64* @assembly_address
  %1029 = load i64* %rax
  %1030 = trunc i64 %1029 to i32
  %1031 = load i64* %rax
  %1032 = trunc i64 %1031 to i32
  %1033 = and i32 %1030, %1032
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1034 = icmp eq i32 %1033, 0
  store i1 %1034, i1* %zf
  %1035 = icmp slt i32 %1033, 0
  store i1 %1035, i1* %sf
  %1036 = trunc i32 %1033 to i8
  %1037 = call i8 @llvm.ctpop.i8(i8 %1036)
  %1038 = and i8 %1037, 1
  %1039 = icmp eq i8 %1038, 0
  store i1 %1039, i1* %pf
  store volatile i64 17249, i64* @assembly_address
  %1040 = load i1* %sf
  %1041 = icmp eq i1 %1040, false
  br i1 %1041, label %block_436f, label %block_4363

block_4363:                                       ; preds = %block_4359
  store volatile i64 17251, i64* @assembly_address
  %1042 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %1043 = zext i32 %1042 to i64
  store i64 %1043, i64* %rax
  store volatile i64 17257, i64* @assembly_address
  %1044 = load i64* %rax
  %1045 = trunc i64 %1044 to i32
  store i32 %1045, i32* bitcast (i64* @global_var_216090 to i32*)
  br label %block_436f

block_436f:                                       ; preds = %block_4363, %block_4359
  store volatile i64 17263, i64* @assembly_address
  %1046 = load i32* bitcast (i64* @global_var_216568 to i32*)
  %1047 = zext i32 %1046 to i64
  store i64 %1047, i64* %rax
  store volatile i64 17269, i64* @assembly_address
  %1048 = load i32* %stack_var_-60
  %1049 = zext i32 %1048 to i64
  store i64 %1049, i64* %rdx
  store volatile i64 17272, i64* @assembly_address
  %1050 = load i64* %rdx
  %1051 = trunc i64 %1050 to i32
  %1052 = load i64* %rax
  %1053 = trunc i64 %1052 to i32
  %1054 = sub i32 %1051, %1053
  %1055 = and i32 %1051, 15
  %1056 = and i32 %1053, 15
  %1057 = sub i32 %1055, %1056
  %1058 = icmp ugt i32 %1057, 15
  %1059 = icmp ult i32 %1051, %1053
  %1060 = xor i32 %1051, %1053
  %1061 = xor i32 %1051, %1054
  %1062 = and i32 %1060, %1061
  %1063 = icmp slt i32 %1062, 0
  store i1 %1058, i1* %az
  store i1 %1059, i1* %cf
  store i1 %1063, i1* %of
  %1064 = icmp eq i32 %1054, 0
  store i1 %1064, i1* %zf
  %1065 = icmp slt i32 %1054, 0
  store i1 %1065, i1* %sf
  %1066 = trunc i32 %1054 to i8
  %1067 = call i8 @llvm.ctpop.i8(i8 %1066)
  %1068 = and i8 %1067, 1
  %1069 = icmp eq i8 %1068, 0
  store i1 %1069, i1* %pf
  %1070 = zext i32 %1054 to i64
  store i64 %1070, i64* %rdx
  store volatile i64 17274, i64* @assembly_address
  %1071 = load i64* %rdx
  %1072 = trunc i64 %1071 to i32
  %1073 = zext i32 %1072 to i64
  store i64 %1073, i64* %rax
  store volatile i64 17276, i64* @assembly_address
  %1074 = load i64* %rax
  %1075 = trunc i64 %1074 to i32
  store i32 %1075, i32* %stack_var_-44
  store volatile i64 17279, i64* @assembly_address
  %1076 = load i32* bitcast (i64* @global_var_2165fc to i32*)
  %1077 = zext i32 %1076 to i64
  store i64 %1077, i64* %rax
  store volatile i64 17285, i64* @assembly_address
  %1078 = load i64* %rax
  %1079 = trunc i64 %1078 to i32
  %1080 = load i64* %rax
  %1081 = trunc i64 %1080 to i32
  %1082 = and i32 %1079, %1081
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1083 = icmp eq i32 %1082, 0
  store i1 %1083, i1* %zf
  %1084 = icmp slt i32 %1082, 0
  store i1 %1084, i1* %sf
  %1085 = trunc i32 %1082 to i8
  %1086 = call i8 @llvm.ctpop.i8(i8 %1085)
  %1087 = and i8 %1086, 1
  %1088 = icmp eq i8 %1087, 0
  store i1 %1088, i1* %pf
  store volatile i64 17287, i64* @assembly_address
  %1089 = load i1* %zf
  br i1 %1089, label %block_43b5, label %block_4389

block_4389:                                       ; preds = %block_436f
  store volatile i64 17289, i64* @assembly_address
  %1090 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %1091 = zext i32 %1090 to i64
  store i64 %1091, i64* %rax
  store volatile i64 17295, i64* @assembly_address
  %1092 = load i64* %rax
  %1093 = trunc i64 %1092 to i32
  %1094 = load i64* %rax
  %1095 = trunc i64 %1094 to i32
  %1096 = and i32 %1093, %1095
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1097 = icmp eq i32 %1096, 0
  store i1 %1097, i1* %zf
  %1098 = icmp slt i32 %1096, 0
  store i1 %1098, i1* %sf
  %1099 = trunc i32 %1096 to i8
  %1100 = call i8 @llvm.ctpop.i8(i8 %1099)
  %1101 = and i8 %1100, 1
  %1102 = icmp eq i8 %1101, 0
  store i1 %1102, i1* %pf
  store volatile i64 17297, i64* @assembly_address
  %1103 = load i1* %zf
  %1104 = icmp eq i1 %1103, false
  br i1 %1104, label %block_43b5, label %block_4393

block_4393:                                       ; preds = %block_4389
  store volatile i64 17299, i64* @assembly_address
  %1105 = load i64* @global_var_25f4c8
  store i64 %1105, i64* %rdx
  store volatile i64 17306, i64* @assembly_address
  %1106 = load i64* @global_var_216580
  store i64 %1106, i64* %rax
  store volatile i64 17313, i64* @assembly_address
  store i64 ptrtoint ([43 x i8]* @global_var_11168 to i64), i64* %rsi
  store volatile i64 17320, i64* @assembly_address
  %1107 = load i64* %rax
  store i64 %1107, i64* %rdi
  store volatile i64 17323, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 17328, i64* @assembly_address
  %1108 = load i64* %rdi
  %1109 = inttoptr i64 %1108 to %_IO_FILE*
  %1110 = load i64* %rsi
  %1111 = inttoptr i64 %1110 to i8*
  %1112 = load i64* %rdx
  %1113 = inttoptr i64 %1112 to i8*
  %1114 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1109, i8* %1111, i8* %1113)
  %1115 = sext i32 %1114 to i64
  store i64 %1115, i64* %rax
  %1116 = sext i32 %1114 to i64
  store i64 %1116, i64* %rax
  br label %block_43b5

block_43b5:                                       ; preds = %block_4393, %block_4389, %block_436f
  store volatile i64 17333, i64* @assembly_address
  %1117 = load i64* @global_var_216638
  store i64 %1117, i64* %rax
  store volatile i64 17340, i64* @assembly_address
  %1118 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1119 = icmp eq i64 %1118, 0
  store i1 %1119, i1* %zf
  %1120 = icmp slt i64 %1118, 0
  store i1 %1120, i1* %sf
  %1121 = trunc i64 %1118 to i8
  %1122 = call i8 @llvm.ctpop.i8(i8 %1121)
  %1123 = and i8 %1122, 1
  %1124 = icmp eq i8 %1123, 0
  store i1 %1124, i1* %pf
  store volatile i64 17343, i64* @assembly_address
  %1125 = load i1* %zf
  br i1 %1125, label %block_43ce, label %block_43c1

block_43c1:                                       ; preds = %block_43b5
  store volatile i64 17345, i64* @assembly_address
  %1126 = load i64* @global_var_216638
  store i64 %1126, i64* %rax
  store volatile i64 17352, i64* @assembly_address
  %1127 = load i64* %rax
  %1128 = sub i64 %1127, 30
  %1129 = and i64 %1127, 15
  %1130 = sub i64 %1129, 14
  %1131 = icmp ugt i64 %1130, 15
  %1132 = icmp ult i64 %1127, 30
  %1133 = xor i64 %1127, 30
  %1134 = xor i64 %1127, %1128
  %1135 = and i64 %1133, %1134
  %1136 = icmp slt i64 %1135, 0
  store i1 %1131, i1* %az
  store i1 %1132, i1* %cf
  store i1 %1136, i1* %of
  %1137 = icmp eq i64 %1128, 0
  store i1 %1137, i1* %zf
  %1138 = icmp slt i64 %1128, 0
  store i1 %1138, i1* %sf
  %1139 = trunc i64 %1128 to i8
  %1140 = call i8 @llvm.ctpop.i8(i8 %1139)
  %1141 = and i8 %1140, 1
  %1142 = icmp eq i8 %1141, 0
  store i1 %1142, i1* %pf
  store volatile i64 17356, i64* @assembly_address
  %1143 = load i1* %cf
  %1144 = load i1* %zf
  %1145 = or i1 %1143, %1144
  br i1 %1145, label %block_4401, label %block_43ce

block_43ce:                                       ; preds = %block_43c1, %block_43b5
  store volatile i64 17358, i64* @assembly_address
  %1146 = load i64* @global_var_216630
  store i64 %1146, i64* %rcx
  store volatile i64 17365, i64* @assembly_address
  %1147 = load i64* @global_var_25f4c8
  store i64 %1147, i64* %rdx
  store volatile i64 17372, i64* @assembly_address
  %1148 = load i64* @global_var_216580
  store i64 %1148, i64* %rax
  store volatile i64 17379, i64* @assembly_address
  store i64 ptrtoint ([25 x i8]* @global_var_11193 to i64), i64* %rsi
  store volatile i64 17386, i64* @assembly_address
  %1149 = load i64* %rax
  store i64 %1149, i64* %rdi
  store volatile i64 17389, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 17394, i64* @assembly_address
  %1150 = load i64* %rdi
  %1151 = inttoptr i64 %1150 to %_IO_FILE*
  %1152 = load i64* %rsi
  %1153 = inttoptr i64 %1152 to i8*
  %1154 = load i64* %rdx
  %1155 = inttoptr i64 %1154 to i8*
  %1156 = load i64* %rcx
  %1157 = inttoptr i64 %1156 to i8*
  %1158 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1151, i8* %1153, i8* %1155, i8* %1157)
  %1159 = sext i32 %1158 to i64
  store i64 %1159, i64* %rax
  %1160 = sext i32 %1158 to i64
  store i64 %1160, i64* %rax
  store volatile i64 17399, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 17404, i64* @assembly_address
  %1161 = load i64* %rdi
  %1162 = trunc i64 %1161 to i32
  %1163 = call i64 @do_exit(i32 %1162)
  store i64 %1163, i64* %rax
  store i64 %1163, i64* %rax
  unreachable

block_4401:                                       ; preds = %block_43c1
  store volatile i64 17409, i64* @assembly_address
  %1164 = load i32* bitcast (i64* @global_var_216614 to i32*)
  %1165 = zext i32 %1164 to i64
  store i64 %1165, i64* %rax
  store volatile i64 17415, i64* @assembly_address
  %1166 = load i64* %rax
  %1167 = trunc i64 %1166 to i32
  %1168 = load i64* %rax
  %1169 = trunc i64 %1168 to i32
  %1170 = and i32 %1167, %1169
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1171 = icmp eq i32 %1170, 0
  store i1 %1171, i1* %zf
  %1172 = icmp slt i32 %1170, 0
  store i1 %1172, i1* %sf
  %1173 = trunc i32 %1170 to i8
  %1174 = call i8 @llvm.ctpop.i8(i8 %1173)
  %1175 = and i8 %1174, 1
  %1176 = icmp eq i8 %1175, 0
  store i1 %1176, i1* %pf
  store volatile i64 17417, i64* @assembly_address
  %1177 = load i1* %zf
  br i1 %1177, label %block_4423, label %block_440b

block_440b:                                       ; preds = %block_4401
  store volatile i64 17419, i64* @assembly_address
  %1178 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %1179 = zext i32 %1178 to i64
  store i64 %1179, i64* %rax
  store volatile i64 17425, i64* @assembly_address
  %1180 = load i64* %rax
  %1181 = trunc i64 %1180 to i32
  %1182 = load i64* %rax
  %1183 = trunc i64 %1182 to i32
  %1184 = and i32 %1181, %1183
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1185 = icmp eq i32 %1184, 0
  store i1 %1185, i1* %zf
  %1186 = icmp slt i32 %1184, 0
  store i1 %1186, i1* %sf
  %1187 = trunc i32 %1184 to i8
  %1188 = call i8 @llvm.ctpop.i8(i8 %1187)
  %1189 = and i8 %1188, 1
  %1190 = icmp eq i8 %1189, 0
  store i1 %1190, i1* %pf
  store volatile i64 17427, i64* @assembly_address
  %1191 = load i1* %zf
  %1192 = icmp eq i1 %1191, false
  br i1 %1192, label %block_4423, label %block_4415

block_4415:                                       ; preds = %block_440b
  store volatile i64 17429, i64* @assembly_address
  store i64 38699, i64* %rax
  store volatile i64 17436, i64* @assembly_address
  %1193 = load i64* %rax
  store i64 %1193, i64* @global_var_2160d0
  br label %block_4423

block_4423:                                       ; preds = %block_4415, %block_440b, %block_4401
  store volatile i64 17443, i64* @assembly_address
  %1194 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %1195 = zext i32 %1194 to i64
  store i64 %1195, i64* %rax
  store volatile i64 17449, i64* @assembly_address
  %1196 = load i64* %rax
  %1197 = trunc i64 %1196 to i32
  %1198 = load i64* %rax
  %1199 = trunc i64 %1198 to i32
  %1200 = and i32 %1197, %1199
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1201 = icmp eq i32 %1200, 0
  store i1 %1201, i1* %zf
  %1202 = icmp slt i32 %1200, 0
  store i1 %1202, i1* %sf
  %1203 = trunc i32 %1200 to i8
  %1204 = call i8 @llvm.ctpop.i8(i8 %1203)
  %1205 = and i8 %1204, 1
  %1206 = icmp eq i8 %1205, 0
  store i1 %1206, i1* %pf
  store volatile i64 17451, i64* @assembly_address
  %1207 = load i1* %zf
  br i1 %1207, label %block_4434, label %block_442d

block_442d:                                       ; preds = %block_4423
  store volatile i64 17453, i64* @assembly_address
  store i64 13, i64* %rax
  store volatile i64 17458, i64* @assembly_address
  br label %block_4439

block_4434:                                       ; preds = %block_4423
  store volatile i64 17460, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_4439

block_4439:                                       ; preds = %block_4434, %block_442d
  store volatile i64 17465, i64* @assembly_address
  %1208 = load i64* %rax
  %1209 = trunc i64 %1208 to i32
  store i32 %1209, i32* bitcast (i64* @global_var_2166c0 to i32*)
  store volatile i64 17471, i64* @assembly_address
  %1210 = call i64 @install_signal_handlers()
  store i64 %1210, i64* %rax
  store i64 %1210, i64* %rax
  store i64 %1210, i64* %rax
  store volatile i64 17476, i64* @assembly_address
  %1211 = load i32* %stack_var_-44
  %1212 = and i32 %1211, 15
  %1213 = icmp ugt i32 %1212, 15
  %1214 = icmp ult i32 %1211, 0
  %1215 = xor i32 %1211, 0
  %1216 = and i32 %1215, 0
  %1217 = icmp slt i32 %1216, 0
  store i1 %1213, i1* %az
  store i1 %1214, i1* %cf
  store i1 %1217, i1* %of
  %1218 = icmp eq i32 %1211, 0
  store i1 %1218, i1* %zf
  %1219 = icmp slt i32 %1211, 0
  store i1 %1219, i1* %sf
  %1220 = trunc i32 %1211 to i8
  %1221 = call i8 @llvm.ctpop.i8(i8 %1220)
  %1222 = and i8 %1221, 1
  %1223 = icmp eq i8 %1222, 0
  store i1 %1223, i1* %pf
  store volatile i64 17480, i64* @assembly_address
  %1224 = load i1* %zf
  br i1 %1224, label %block_44aa, label %block_444a

block_444a:                                       ; preds = %block_4439
  store volatile i64 17482, i64* @assembly_address
  %1225 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1226 = zext i32 %1225 to i64
  store i64 %1226, i64* %rax
  store volatile i64 17488, i64* @assembly_address
  %1227 = load i64* %rax
  %1228 = trunc i64 %1227 to i32
  %1229 = load i64* %rax
  %1230 = trunc i64 %1229 to i32
  %1231 = and i32 %1228, %1230
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1232 = icmp eq i32 %1231, 0
  store i1 %1232, i1* %zf
  %1233 = icmp slt i32 %1231, 0
  store i1 %1233, i1* %sf
  %1234 = trunc i32 %1231 to i8
  %1235 = call i8 @llvm.ctpop.i8(i8 %1234)
  %1236 = and i8 %1235, 1
  %1237 = icmp eq i8 %1236, 0
  store i1 %1237, i1* %pf
  store volatile i64 17490, i64* @assembly_address
  %1238 = load i1* %zf
  br i1 %1238, label %block_449d, label %block_4454

block_4454:                                       ; preds = %block_444a
  store volatile i64 17492, i64* @assembly_address
  %1239 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %1240 = zext i32 %1239 to i64
  store i64 %1240, i64* %rax
  store volatile i64 17498, i64* @assembly_address
  %1241 = load i64* %rax
  %1242 = trunc i64 %1241 to i32
  %1243 = load i64* %rax
  %1244 = trunc i64 %1243 to i32
  %1245 = and i32 %1242, %1244
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1246 = icmp eq i32 %1245, 0
  store i1 %1246, i1* %zf
  %1247 = icmp slt i32 %1245, 0
  store i1 %1247, i1* %sf
  %1248 = trunc i32 %1245 to i8
  %1249 = call i8 @llvm.ctpop.i8(i8 %1248)
  %1250 = and i8 %1249, 1
  %1251 = icmp eq i8 %1250, 0
  store i1 %1251, i1* %pf
  store volatile i64 17500, i64* @assembly_address
  %1252 = load i1* %zf
  %1253 = icmp eq i1 %1252, false
  br i1 %1253, label %block_449d, label %block_445e

block_445e:                                       ; preds = %block_4454
  store volatile i64 17502, i64* @assembly_address
  %1254 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %1255 = zext i32 %1254 to i64
  store i64 %1255, i64* %rax
  store volatile i64 17508, i64* @assembly_address
  %1256 = load i64* %rax
  %1257 = trunc i64 %1256 to i32
  %1258 = load i64* %rax
  %1259 = trunc i64 %1258 to i32
  %1260 = and i32 %1257, %1259
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1261 = icmp eq i32 %1260, 0
  store i1 %1261, i1* %zf
  %1262 = icmp slt i32 %1260, 0
  store i1 %1262, i1* %sf
  %1263 = trunc i32 %1260 to i8
  %1264 = call i8 @llvm.ctpop.i8(i8 %1263)
  %1265 = and i8 %1264, 1
  %1266 = icmp eq i8 %1265, 0
  store i1 %1266, i1* %pf
  store volatile i64 17510, i64* @assembly_address
  %1267 = load i1* %zf
  %1268 = icmp eq i1 %1267, false
  br i1 %1268, label %block_449d, label %block_4468

block_4468:                                       ; preds = %block_445e
  store volatile i64 17512, i64* @assembly_address
  %1269 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %1270 = zext i32 %1269 to i64
  store i64 %1270, i64* %rax
  store volatile i64 17518, i64* @assembly_address
  %1271 = load i64* %rax
  %1272 = trunc i64 %1271 to i32
  %1273 = load i64* %rax
  %1274 = trunc i64 %1273 to i32
  %1275 = and i32 %1272, %1274
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1276 = icmp eq i32 %1275, 0
  store i1 %1276, i1* %zf
  %1277 = icmp slt i32 %1275, 0
  store i1 %1277, i1* %sf
  %1278 = trunc i32 %1275 to i8
  %1279 = call i8 @llvm.ctpop.i8(i8 %1278)
  %1280 = and i8 %1279, 1
  %1281 = icmp eq i8 %1280, 0
  store i1 %1281, i1* %pf
  store volatile i64 17520, i64* @assembly_address
  br label %block_449d

block_4472:                                       ; preds = %block_449d
  store volatile i64 17522, i64* @assembly_address
  %1282 = load i32* bitcast (i64* @global_var_216568 to i32*)
  %1283 = zext i32 %1282 to i64
  store i64 %1283, i64* %rax
  store volatile i64 17528, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216569 to i64), i64* %rdx
  store volatile i64 17531, i64* @assembly_address
  %1284 = load i64* %rdx
  %1285 = trunc i64 %1284 to i32
  store i32 %1285, i32* bitcast (i64* @global_var_216568 to i32*)
  store volatile i64 17537, i64* @assembly_address
  %1286 = load i64* %rax
  %1287 = trunc i64 %1286 to i32
  %1288 = sext i32 %1287 to i64
  store i64 %1288, i64* %rax
  store volatile i64 17539, i64* @assembly_address
  %1289 = load i64* %rax
  %1290 = mul i64 %1289, 8
  store i64 %1290, i64* %rdx
  store volatile i64 17547, i64* @assembly_address
  %1291 = load i8*** %stack_var_-72
  %1292 = ptrtoint i8** %1291 to i64
  store i64 %1292, i64* %rax
  store volatile i64 17551, i64* @assembly_address
  %1293 = load i64* %rax
  %1294 = load i64* %rdx
  %1295 = add i64 %1293, %1294
  %1296 = and i64 %1293, 15
  %1297 = and i64 %1294, 15
  %1298 = add i64 %1296, %1297
  %1299 = icmp ugt i64 %1298, 15
  %1300 = icmp ult i64 %1295, %1293
  %1301 = xor i64 %1293, %1295
  %1302 = xor i64 %1294, %1295
  %1303 = and i64 %1301, %1302
  %1304 = icmp slt i64 %1303, 0
  store i1 %1299, i1* %az
  store i1 %1300, i1* %cf
  store i1 %1304, i1* %of
  %1305 = icmp eq i64 %1295, 0
  store i1 %1305, i1* %zf
  %1306 = icmp slt i64 %1295, 0
  store i1 %1306, i1* %sf
  %1307 = trunc i64 %1295 to i8
  %1308 = call i8 @llvm.ctpop.i8(i8 %1307)
  %1309 = and i8 %1308, 1
  %1310 = icmp eq i8 %1309, 0
  store i1 %1310, i1* %pf
  store i64 %1295, i64* %rax
  store volatile i64 17554, i64* @assembly_address
  %1311 = load i64* %rax
  %1312 = inttoptr i64 %1311 to i64*
  %1313 = load i64* %1312
  store i64 %1313, i64* %rax
  store volatile i64 17557, i64* @assembly_address
  %1314 = load i64* %rax
  store i64 %1314, i64* %rdi
  store volatile i64 17560, i64* @assembly_address
  %1315 = load i64* %rdi
  %1316 = inttoptr i64 %1315 to i8*
  %1317 = call i64 @treat_file(i8* %1316)
  store i64 %1317, i64* %rax
  store i64 %1317, i64* %rax
  br label %block_449d

block_449d:                                       ; preds = %block_4472, %block_4468, %block_445e, %block_4454, %block_444a
  store volatile i64 17565, i64* @assembly_address
  %1318 = load i32* bitcast (i64* @global_var_216568 to i32*)
  %1319 = zext i32 %1318 to i64
  store i64 %1319, i64* %rax
  store volatile i64 17571, i64* @assembly_address
  %1320 = load i32* %stack_var_-60
  %1321 = load i64* %rax
  %1322 = trunc i64 %1321 to i32
  store i32 %1320, i32* %10
  %1323 = trunc i64 %1321 to i32
  store i32 %1323, i32* %8
  %1324 = sub i32 %1320, %1322
  %1325 = and i32 %1320, 15
  %1326 = and i32 %1322, 15
  %1327 = sub i32 %1325, %1326
  %1328 = icmp ugt i32 %1327, 15
  %1329 = icmp ult i32 %1320, %1322
  %1330 = xor i32 %1320, %1322
  %1331 = xor i32 %1320, %1324
  %1332 = and i32 %1330, %1331
  %1333 = icmp slt i32 %1332, 0
  store i1 %1328, i1* %az
  store i1 %1329, i1* %cf
  store i1 %1333, i1* %of
  %1334 = icmp eq i32 %1324, 0
  store i1 %1334, i1* %zf
  %1335 = icmp slt i32 %1324, 0
  store i1 %1335, i1* %sf
  %1336 = trunc i32 %1324 to i8
  %1337 = call i8 @llvm.ctpop.i8(i8 %1336)
  %1338 = and i8 %1337, 1
  %1339 = icmp eq i8 %1338, 0
  store i1 %1339, i1* %pf
  store volatile i64 17574, i64* @assembly_address
  %1340 = load i32* %10
  %1341 = load i32* %8
  %1342 = sext i32 %1341 to i64
  %1343 = sext i32 %1340 to i64
  %1344 = icmp sgt i64 %1343, %1342
  br i1 %1344, label %block_4472, label %block_44a8

block_44a8:                                       ; preds = %block_449d
  store volatile i64 17576, i64* @assembly_address
  br label %block_44af

block_44aa:                                       ; preds = %block_4439
  store volatile i64 17578, i64* @assembly_address
  %1345 = load i64* %rdi
  %1346 = load i64* %rsi
  %1347 = load i64* %rdx
  %1348 = call i64 @treat_stdin(i64 %1345, i64 %1346, i64 %1347)
  store i64 %1348, i64* %rax
  store i64 %1348, i64* %rax
  store i64 %1348, i64* %rax
  br label %block_44af

block_44af:                                       ; preds = %block_44aa, %block_44a8
  store volatile i64 17583, i64* @assembly_address
  %1349 = load i8* bitcast (i64* @global_var_216ae0 to i8*)
  %1350 = zext i8 %1349 to i64
  store i64 %1350, i64* %rax
  store volatile i64 17590, i64* @assembly_address
  %1351 = load i64* %rax
  %1352 = trunc i64 %1351 to i8
  %1353 = load i64* %rax
  %1354 = trunc i64 %1353 to i8
  %1355 = and i8 %1352, %1354
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1356 = icmp eq i8 %1355, 0
  store i1 %1356, i1* %zf
  %1357 = icmp slt i8 %1355, 0
  store i1 %1357, i1* %sf
  %1358 = call i8 @llvm.ctpop.i8(i8 %1355)
  %1359 = and i8 %1358, 1
  %1360 = icmp eq i8 %1359, 0
  store i1 %1360, i1* %pf
  store volatile i64 17592, i64* @assembly_address
  %1361 = load i1* %zf
  br i1 %1361, label %block_44e0, label %block_44ba

block_44ba:                                       ; preds = %block_44af
  store volatile i64 17594, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 17599, i64* @assembly_address
  %1362 = load i64* %rdi
  %1363 = trunc i64 %1362 to i32
  %1364 = call i32 @close(i32 %1363)
  %1365 = sext i32 %1364 to i64
  store i64 %1365, i64* %rax
  %1366 = sext i32 %1364 to i64
  store i64 %1366, i64* %rax
  store volatile i64 17604, i64* @assembly_address
  %1367 = load i64* %rax
  %1368 = trunc i64 %1367 to i32
  %1369 = load i64* %rax
  %1370 = trunc i64 %1369 to i32
  %1371 = and i32 %1368, %1370
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1372 = icmp eq i32 %1371, 0
  store i1 %1372, i1* %zf
  %1373 = icmp slt i32 %1371, 0
  store i1 %1373, i1* %sf
  %1374 = trunc i32 %1371 to i8
  %1375 = call i8 @llvm.ctpop.i8(i8 %1374)
  %1376 = and i8 %1375, 1
  %1377 = icmp eq i8 %1376, 0
  store i1 %1377, i1* %pf
  store volatile i64 17606, i64* @assembly_address
  %1378 = load i1* %zf
  br i1 %1378, label %block_44e0, label %block_44c8

block_44c8:                                       ; preds = %block_44ba
  store volatile i64 17608, i64* @assembly_address
  store i32 1768191091, i32* bitcast (i64* @global_var_21a460 to i32*)
  store volatile i64 17618, i64* @assembly_address
  store i16 110, i16* bitcast (i64* @global_var_21a464 to i16*)
  store volatile i64 17627, i64* @assembly_address
  %1379 = call i64 @read_error()
  store i64 %1379, i64* %rax
  store i64 %1379, i64* %rax
  store i64 %1379, i64* %rax
  unreachable

block_44e0:                                       ; preds = %block_44ba, %block_44af
  store volatile i64 17632, i64* @assembly_address
  %1380 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %1381 = zext i32 %1380 to i64
  store i64 %1381, i64* %rax
  store volatile i64 17638, i64* @assembly_address
  %1382 = load i64* %rax
  %1383 = trunc i64 %1382 to i32
  %1384 = load i64* %rax
  %1385 = trunc i64 %1384 to i32
  %1386 = and i32 %1383, %1385
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1387 = icmp eq i32 %1386, 0
  store i1 %1387, i1* %zf
  %1388 = icmp slt i32 %1386, 0
  store i1 %1388, i1* %sf
  %1389 = trunc i32 %1386 to i8
  %1390 = call i8 @llvm.ctpop.i8(i8 %1389)
  %1391 = and i8 %1390, 1
  %1392 = icmp eq i8 %1391, 0
  store i1 %1392, i1* %pf
  store volatile i64 17640, i64* @assembly_address
  %1393 = load i1* %zf
  br i1 %1393, label %block_4521, label %block_44ea

block_44ea:                                       ; preds = %block_44e0
  store volatile i64 17642, i64* @assembly_address
  %1394 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %1395 = zext i32 %1394 to i64
  store i64 %1395, i64* %rax
  store volatile i64 17648, i64* @assembly_address
  %1396 = load i64* %rax
  %1397 = trunc i64 %1396 to i32
  %1398 = load i64* %rax
  %1399 = trunc i64 %1398 to i32
  %1400 = and i32 %1397, %1399
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1401 = icmp eq i32 %1400, 0
  store i1 %1401, i1* %zf
  %1402 = icmp slt i32 %1400, 0
  store i1 %1402, i1* %sf
  %1403 = trunc i32 %1400 to i8
  %1404 = call i8 @llvm.ctpop.i8(i8 %1403)
  %1405 = and i8 %1404, 1
  %1406 = icmp eq i8 %1405, 0
  store i1 %1406, i1* %pf
  store volatile i64 17650, i64* @assembly_address
  %1407 = load i1* %zf
  %1408 = icmp eq i1 %1407, false
  br i1 %1408, label %block_4509, label %block_44f4

block_44f4:                                       ; preds = %block_44ea
  store volatile i64 17652, i64* @assembly_address
  %1409 = load i32* %stack_var_-44
  store i32 %1409, i32* %7
  store i32 1, i32* %6
  %1410 = sub i32 %1409, 1
  %1411 = and i32 %1409, 15
  %1412 = sub i32 %1411, 1
  %1413 = icmp ugt i32 %1412, 15
  %1414 = icmp ult i32 %1409, 1
  %1415 = xor i32 %1409, 1
  %1416 = xor i32 %1409, %1410
  %1417 = and i32 %1415, %1416
  %1418 = icmp slt i32 %1417, 0
  store i1 %1413, i1* %az
  store i1 %1414, i1* %cf
  store i1 %1418, i1* %of
  %1419 = icmp eq i32 %1410, 0
  store i1 %1419, i1* %zf
  %1420 = icmp slt i32 %1410, 0
  store i1 %1420, i1* %sf
  %1421 = trunc i32 %1410 to i8
  %1422 = call i8 @llvm.ctpop.i8(i8 %1421)
  %1423 = and i8 %1422, 1
  %1424 = icmp eq i8 %1423, 0
  store i1 %1424, i1* %pf
  store volatile i64 17656, i64* @assembly_address
  %1425 = load i32* %7
  %1426 = load i32* %6
  %1427 = icmp sle i32 %1425, %1426
  br i1 %1427, label %block_4509, label %block_44fa

block_44fa:                                       ; preds = %block_44f4
  store volatile i64 17658, i64* @assembly_address
  store i64 4294967295, i64* %rsi
  store volatile i64 17663, i64* @assembly_address
  store i64 4294967295, i64* %rdi
  store volatile i64 17668, i64* @assembly_address
  %1428 = load i64* %rdi
  %1429 = load i64* %rsi
  %1430 = trunc i64 %1428 to i32
  %1431 = call i64 @do_list(i32 %1430, i64 %1429)
  store i64 %1431, i64* %rax
  store i64 %1431, i64* %rax
  br label %block_4509

block_4509:                                       ; preds = %block_44fa, %block_44f4, %block_44ea
  store volatile i64 17673, i64* @assembly_address
  %1432 = load i64* @global_var_216560
  store i64 %1432, i64* %rax
  store volatile i64 17680, i64* @assembly_address
  %1433 = load i64* %rax
  store i64 %1433, i64* %rdi
  store volatile i64 17683, i64* @assembly_address
  %1434 = load i64* %rdi
  %1435 = inttoptr i64 %1434 to %_IO_FILE*
  %1436 = call i64 @rpl_fflush(%_IO_FILE* %1435)
  store i64 %1436, i64* %rax
  store i64 %1436, i64* %rax
  store volatile i64 17688, i64* @assembly_address
  %1437 = load i64* %rax
  %1438 = trunc i64 %1437 to i32
  %1439 = load i64* %rax
  %1440 = trunc i64 %1439 to i32
  %1441 = and i32 %1438, %1440
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1442 = icmp eq i32 %1441, 0
  store i1 %1442, i1* %zf
  %1443 = icmp slt i32 %1441, 0
  store i1 %1443, i1* %sf
  %1444 = trunc i32 %1441 to i8
  %1445 = call i8 @llvm.ctpop.i8(i8 %1444)
  %1446 = and i8 %1445, 1
  %1447 = icmp eq i8 %1446, 0
  store i1 %1447, i1* %pf
  store volatile i64 17690, i64* @assembly_address
  %1448 = load i1* %zf
  br i1 %1448, label %block_4521, label %block_451c

block_451c:                                       ; preds = %block_4509
  store volatile i64 17692, i64* @assembly_address
  %1449 = call i64 @write_error()
  store i64 %1449, i64* %rax
  store i64 %1449, i64* %rax
  store i64 %1449, i64* %rax
  unreachable

block_4521:                                       ; preds = %block_4509, %block_44e0
  store volatile i64 17697, i64* @assembly_address
  %1450 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1451 = zext i32 %1450 to i64
  store i64 %1451, i64* %rax
  store volatile i64 17703, i64* @assembly_address
  %1452 = load i64* %rax
  %1453 = trunc i64 %1452 to i32
  %1454 = load i64* %rax
  %1455 = trunc i64 %1454 to i32
  %1456 = and i32 %1453, %1455
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1457 = icmp eq i32 %1456, 0
  store i1 %1457, i1* %zf
  %1458 = icmp slt i32 %1456, 0
  store i1 %1458, i1* %sf
  %1459 = trunc i32 %1456 to i8
  %1460 = call i8 @llvm.ctpop.i8(i8 %1459)
  %1461 = and i8 %1460, 1
  %1462 = icmp eq i8 %1461, 0
  store i1 %1462, i1* %pf
  store volatile i64 17705, i64* @assembly_address
  %1463 = load i1* %zf
  br i1 %1463, label %block_456f, label %block_452b

block_452b:                                       ; preds = %block_4521
  store volatile i64 17707, i64* @assembly_address
  %1464 = load i8* bitcast (i64* @global_var_2165f9 to i8*)
  %1465 = zext i8 %1464 to i64
  store i64 %1465, i64* %rax
  store volatile i64 17714, i64* @assembly_address
  %1466 = load i64* %rax
  %1467 = trunc i64 %1466 to i8
  %1468 = load i64* %rax
  %1469 = trunc i64 %1468 to i8
  %1470 = and i8 %1467, %1469
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1471 = icmp eq i8 %1470, 0
  store i1 %1471, i1* %zf
  %1472 = icmp slt i8 %1470, 0
  store i1 %1472, i1* %sf
  %1473 = call i8 @llvm.ctpop.i8(i8 %1470)
  %1474 = and i8 %1473, 1
  %1475 = icmp eq i8 %1474, 0
  store i1 %1475, i1* %pf
  store volatile i64 17716, i64* @assembly_address
  %1476 = load i1* %zf
  br i1 %1476, label %block_4550, label %block_4536

block_4536:                                       ; preds = %block_452b
  store volatile i64 17718, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 17723, i64* @assembly_address
  %1477 = load i64* %rdi
  %1478 = trunc i64 %1477 to i32
  %1479 = call i32 @fdatasync(i32 %1478)
  %1480 = sext i32 %1479 to i64
  store i64 %1480, i64* %rax
  %1481 = sext i32 %1479 to i64
  store i64 %1481, i64* %rax
  store volatile i64 17728, i64* @assembly_address
  %1482 = load i64* %rax
  %1483 = trunc i64 %1482 to i32
  %1484 = load i64* %rax
  %1485 = trunc i64 %1484 to i32
  %1486 = and i32 %1483, %1485
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1487 = icmp eq i32 %1486, 0
  store i1 %1487, i1* %zf
  %1488 = icmp slt i32 %1486, 0
  store i1 %1488, i1* %sf
  %1489 = trunc i32 %1486 to i8
  %1490 = call i8 @llvm.ctpop.i8(i8 %1489)
  %1491 = and i8 %1490, 1
  %1492 = icmp eq i8 %1491, 0
  store i1 %1492, i1* %pf
  store volatile i64 17730, i64* @assembly_address
  %1493 = load i1* %zf
  br i1 %1493, label %block_4550, label %block_4544

block_4544:                                       ; preds = %block_4536
  store volatile i64 17732, i64* @assembly_address
  %1494 = call i32* @__errno_location()
  %1495 = ptrtoint i32* %1494 to i64
  store i64 %1495, i64* %rax
  %1496 = ptrtoint i32* %1494 to i64
  store i64 %1496, i64* %rax
  %1497 = ptrtoint i32* %1494 to i64
  store i64 %1497, i64* %rax
  store volatile i64 17737, i64* @assembly_address
  %1498 = load i64* %rax
  %1499 = inttoptr i64 %1498 to i32*
  %1500 = load i32* %1499
  %1501 = zext i32 %1500 to i64
  store i64 %1501, i64* %rax
  store volatile i64 17739, i64* @assembly_address
  %1502 = load i64* %rax
  %1503 = trunc i64 %1502 to i32
  %1504 = sub i32 %1503, 22
  %1505 = and i32 %1503, 15
  %1506 = sub i32 %1505, 6
  %1507 = icmp ugt i32 %1506, 15
  %1508 = icmp ult i32 %1503, 22
  %1509 = xor i32 %1503, 22
  %1510 = xor i32 %1503, %1504
  %1511 = and i32 %1509, %1510
  %1512 = icmp slt i32 %1511, 0
  store i1 %1507, i1* %az
  store i1 %1508, i1* %cf
  store i1 %1512, i1* %of
  %1513 = icmp eq i32 %1504, 0
  store i1 %1513, i1* %zf
  %1514 = icmp slt i32 %1504, 0
  store i1 %1514, i1* %sf
  %1515 = trunc i32 %1504 to i8
  %1516 = call i8 @llvm.ctpop.i8(i8 %1515)
  %1517 = and i8 %1516, 1
  %1518 = icmp eq i8 %1517, 0
  store i1 %1518, i1* %pf
  store volatile i64 17742, i64* @assembly_address
  %1519 = load i1* %zf
  %1520 = icmp eq i1 %1519, false
  br i1 %1520, label %block_455e, label %block_4550

block_4550:                                       ; preds = %block_4544, %block_4536, %block_452b
  store volatile i64 17744, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 17749, i64* @assembly_address
  %1521 = load i64* %rdi
  %1522 = trunc i64 %1521 to i32
  %1523 = call i32 @close(i32 %1522)
  %1524 = sext i32 %1523 to i64
  store i64 %1524, i64* %rax
  %1525 = sext i32 %1523 to i64
  store i64 %1525, i64* %rax
  store volatile i64 17754, i64* @assembly_address
  %1526 = load i64* %rax
  %1527 = trunc i64 %1526 to i32
  %1528 = load i64* %rax
  %1529 = trunc i64 %1528 to i32
  %1530 = and i32 %1527, %1529
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1531 = icmp eq i32 %1530, 0
  store i1 %1531, i1* %zf
  %1532 = icmp slt i32 %1530, 0
  store i1 %1532, i1* %sf
  %1533 = trunc i32 %1530 to i8
  %1534 = call i8 @llvm.ctpop.i8(i8 %1533)
  %1535 = and i8 %1534, 1
  %1536 = icmp eq i8 %1535, 0
  store i1 %1536, i1* %pf
  store volatile i64 17756, i64* @assembly_address
  %1537 = load i1* %zf
  br i1 %1537, label %block_456f, label %block_455e

block_455e:                                       ; preds = %block_4550, %block_4544
  store volatile i64 17758, i64* @assembly_address
  %1538 = call i32* @__errno_location()
  %1539 = ptrtoint i32* %1538 to i64
  store i64 %1539, i64* %rax
  %1540 = ptrtoint i32* %1538 to i64
  store i64 %1540, i64* %rax
  %1541 = ptrtoint i32* %1538 to i64
  store i64 %1541, i64* %rax
  store volatile i64 17763, i64* @assembly_address
  %1542 = load i64* %rax
  %1543 = inttoptr i64 %1542 to i32*
  %1544 = load i32* %1543
  %1545 = zext i32 %1544 to i64
  store i64 %1545, i64* %rax
  store volatile i64 17765, i64* @assembly_address
  %1546 = load i64* %rax
  %1547 = trunc i64 %1546 to i32
  %1548 = sub i32 %1547, 9
  %1549 = and i32 %1547, 15
  %1550 = sub i32 %1549, 9
  %1551 = icmp ugt i32 %1550, 15
  %1552 = icmp ult i32 %1547, 9
  %1553 = xor i32 %1547, 9
  %1554 = xor i32 %1547, %1548
  %1555 = and i32 %1553, %1554
  %1556 = icmp slt i32 %1555, 0
  store i1 %1551, i1* %az
  store i1 %1552, i1* %cf
  store i1 %1556, i1* %of
  %1557 = icmp eq i32 %1548, 0
  store i1 %1557, i1* %zf
  %1558 = icmp slt i32 %1548, 0
  store i1 %1558, i1* %sf
  %1559 = trunc i32 %1548 to i8
  %1560 = call i8 @llvm.ctpop.i8(i8 %1559)
  %1561 = and i8 %1560, 1
  %1562 = icmp eq i8 %1561, 0
  store i1 %1562, i1* %pf
  store volatile i64 17768, i64* @assembly_address
  %1563 = load i1* %zf
  br i1 %1563, label %block_456f, label %block_456a

block_456a:                                       ; preds = %block_455e
  store volatile i64 17770, i64* @assembly_address
  %1564 = call i64 @write_error()
  store i64 %1564, i64* %rax
  store i64 %1564, i64* %rax
  store i64 %1564, i64* %rax
  unreachable

block_456f:                                       ; preds = %block_455e, %block_4550, %block_4521
  store volatile i64 17775, i64* @assembly_address
  %1565 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %1566 = zext i32 %1565 to i64
  store i64 %1566, i64* %rax
  store volatile i64 17781, i64* @assembly_address
  %1567 = load i64* %rax
  %1568 = trunc i64 %1567 to i32
  %1569 = zext i32 %1568 to i64
  store i64 %1569, i64* %rdi
  store volatile i64 17783, i64* @assembly_address
  %1570 = load i64* %rdi
  %1571 = trunc i64 %1570 to i32
  %1572 = call i64 @do_exit(i32 %1571)
  store i64 %1572, i64* %rax
  store i64 %1572, i64* %rax
  %1573 = load i64* %rax
  ret i64 %1573
}

declare i64 @170(i64, i8**)

define i64 @input_eof() {
block_457c:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 17788, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 17789, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 17792, i64* @assembly_address
  %3 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %4 = zext i32 %3 to i64
  store i64 %4, i64* %rax
  store volatile i64 17798, i64* @assembly_address
  %5 = load i64* %rax
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rax
  %8 = trunc i64 %7 to i32
  %9 = and i32 %6, %8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %10 = icmp eq i32 %9, 0
  store i1 %10, i1* %zf
  %11 = icmp slt i32 %9, 0
  store i1 %11, i1* %sf
  %12 = trunc i32 %9 to i8
  %13 = call i8 @llvm.ctpop.i8(i8 %12)
  %14 = and i8 %13, 1
  %15 = icmp eq i8 %14, 0
  store i1 %15, i1* %pf
  store volatile i64 17800, i64* @assembly_address
  %16 = load i1* %zf
  br i1 %16, label %block_4594, label %block_458a

block_458a:                                       ; preds = %block_457c
  store volatile i64 17802, i64* @assembly_address
  %17 = load i32* bitcast (i64* @global_var_21661c to i32*)
  %18 = zext i32 %17 to i64
  store i64 %18, i64* %rax
  store volatile i64 17808, i64* @assembly_address
  %19 = load i64* %rax
  %20 = trunc i64 %19 to i32
  %21 = load i64* %rax
  %22 = trunc i64 %21 to i32
  %23 = and i32 %20, %22
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %24 = icmp eq i32 %23, 0
  store i1 %24, i1* %zf
  %25 = icmp slt i32 %23, 0
  store i1 %25, i1* %sf
  %26 = trunc i32 %23 to i8
  %27 = call i8 @llvm.ctpop.i8(i8 %26)
  %28 = and i8 %27, 1
  %29 = icmp eq i8 %28, 0
  store i1 %29, i1* %pf
  store volatile i64 17810, i64* @assembly_address
  %30 = load i1* %zf
  br i1 %30, label %block_459b, label %block_4594

block_4594:                                       ; preds = %block_458a, %block_457c
  store volatile i64 17812, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 17817, i64* @assembly_address
  br label %block_45dd

block_459b:                                       ; preds = %block_458a
  store volatile i64 17819, i64* @assembly_address
  %31 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %32 = zext i32 %31 to i64
  store i64 %32, i64* %rdx
  store volatile i64 17825, i64* @assembly_address
  %33 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %34 = zext i32 %33 to i64
  store i64 %34, i64* %rax
  store volatile i64 17831, i64* @assembly_address
  %35 = load i64* %rdx
  %36 = trunc i64 %35 to i32
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i32
  %39 = sub i32 %36, %38
  %40 = and i32 %36, 15
  %41 = and i32 %38, 15
  %42 = sub i32 %40, %41
  %43 = icmp ugt i32 %42, 15
  %44 = icmp ult i32 %36, %38
  %45 = xor i32 %36, %38
  %46 = xor i32 %36, %39
  %47 = and i32 %45, %46
  %48 = icmp slt i32 %47, 0
  store i1 %43, i1* %az
  store i1 %44, i1* %cf
  store i1 %48, i1* %of
  %49 = icmp eq i32 %39, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i32 %39, 0
  store i1 %50, i1* %sf
  %51 = trunc i32 %39 to i8
  %52 = call i8 @llvm.ctpop.i8(i8 %51)
  %53 = and i8 %52, 1
  %54 = icmp eq i8 %53, 0
  store i1 %54, i1* %pf
  store volatile i64 17833, i64* @assembly_address
  %55 = load i1* %zf
  %56 = icmp eq i1 %55, false
  br i1 %56, label %block_45d8, label %block_45ab

block_45ab:                                       ; preds = %block_459b
  store volatile i64 17835, i64* @assembly_address
  %57 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %58 = zext i32 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 17841, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  %61 = sub i32 %60, 32768
  %62 = and i32 %60, 15
  %63 = icmp ugt i32 %62, 15
  %64 = icmp ult i32 %60, 32768
  %65 = xor i32 %60, 32768
  %66 = xor i32 %60, %61
  %67 = and i32 %65, %66
  %68 = icmp slt i32 %67, 0
  store i1 %63, i1* %az
  store i1 %64, i1* %cf
  store i1 %68, i1* %of
  %69 = icmp eq i32 %61, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 %61, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 %61 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  store volatile i64 17846, i64* @assembly_address
  %75 = load i1* %zf
  %76 = icmp eq i1 %75, false
  br i1 %76, label %block_45c7, label %block_45b8

block_45b8:                                       ; preds = %block_45ab
  store volatile i64 17848, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 17853, i64* @assembly_address
  %77 = load i64* %rdi
  %78 = trunc i64 %77 to i32
  %79 = call i64 @fill_inbuf(i32 %78)
  store i64 %79, i64* %rax
  store i64 %79, i64* %rax
  store volatile i64 17858, i64* @assembly_address
  %80 = load i64* %rax
  %81 = trunc i64 %80 to i32
  %82 = sub i32 %81, -1
  %83 = and i32 %81, 15
  %84 = sub i32 %83, 15
  %85 = icmp ugt i32 %84, 15
  %86 = icmp ult i32 %81, -1
  %87 = xor i32 %81, -1
  %88 = xor i32 %81, %82
  %89 = and i32 %87, %88
  %90 = icmp slt i32 %89, 0
  store i1 %85, i1* %az
  store i1 %86, i1* %cf
  store i1 %90, i1* %of
  %91 = icmp eq i32 %82, 0
  store i1 %91, i1* %zf
  %92 = icmp slt i32 %82, 0
  store i1 %92, i1* %sf
  %93 = trunc i32 %82 to i8
  %94 = call i8 @llvm.ctpop.i8(i8 %93)
  %95 = and i8 %94, 1
  %96 = icmp eq i8 %95, 0
  store i1 %96, i1* %pf
  store volatile i64 17861, i64* @assembly_address
  %97 = load i1* %zf
  %98 = icmp eq i1 %97, false
  br i1 %98, label %block_45ce, label %block_45c7

block_45c7:                                       ; preds = %block_45b8, %block_45ab
  store volatile i64 17863, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 17868, i64* @assembly_address
  br label %block_45dd

block_45ce:                                       ; preds = %block_45b8
  store volatile i64 17870, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_24a884 to i32*)
  br label %block_45d8

block_45d8:                                       ; preds = %block_45ce, %block_459b
  store volatile i64 17880, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_45dd

block_45dd:                                       ; preds = %block_45d8, %block_45c7, %block_4594
  store volatile i64 17885, i64* @assembly_address
  %99 = load i64* %stack_var_-8
  store i64 %99, i64* %rbp
  %100 = ptrtoint i64* %stack_var_0 to i64
  store i64 %100, i64* %rsp
  store volatile i64 17886, i64* @assembly_address
  %101 = load i64* %rax
  %102 = load i64* %rax
  ret i64 %102
}

define i64 @get_input_size_and_time(i64 %arg1, i64 %arg2, i64 %arg3) {
block_45df:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 17887, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 17888, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 17891, i64* @assembly_address
  store i64 -1, i64* @global_var_24a890
  store volatile i64 17902, i64* @assembly_address
  store i64 -1, i64* @global_var_25f4d8
  store volatile i64 17913, i64* @assembly_address
  %3 = load i32* bitcast (i64* @global_var_216f18 to i32*)
  %4 = zext i32 %3 to i64
  store i64 %4, i64* %rax
  store volatile i64 17919, i64* @assembly_address
  %5 = load i64* %rax
  %6 = trunc i64 %5 to i32
  %7 = and i32 %6, 61440
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %8 = icmp eq i32 %7, 0
  store i1 %8, i1* %zf
  %9 = icmp slt i32 %7, 0
  store i1 %9, i1* %sf
  %10 = trunc i32 %7 to i8
  %11 = call i8 @llvm.ctpop.i8(i8 %10)
  %12 = and i8 %11, 1
  %13 = icmp eq i8 %12, 0
  store i1 %13, i1* %pf
  %14 = zext i32 %7 to i64
  store i64 %14, i64* %rax
  store volatile i64 17924, i64* @assembly_address
  %15 = load i64* %rax
  %16 = trunc i64 %15 to i32
  %17 = sub i32 %16, 32768
  %18 = and i32 %16, 15
  %19 = icmp ugt i32 %18, 15
  %20 = icmp ult i32 %16, 32768
  %21 = xor i32 %16, 32768
  %22 = xor i32 %16, %17
  %23 = and i32 %21, %22
  %24 = icmp slt i32 %23, 0
  store i1 %19, i1* %az
  store i1 %20, i1* %cf
  store i1 %24, i1* %of
  %25 = icmp eq i32 %17, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i32 %17, 0
  store i1 %26, i1* %sf
  %27 = trunc i32 %17 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  store volatile i64 17929, i64* @assembly_address
  %31 = load i1* %zf
  %32 = icmp eq i1 %31, false
  br i1 %32, label %block_4647, label %block_460b

block_460b:                                       ; preds = %block_45df
  store volatile i64 17931, i64* @assembly_address
  %33 = load i64* @global_var_216f30
  store i64 %33, i64* %rax
  store volatile i64 17938, i64* @assembly_address
  %34 = load i64* %rax
  store i64 %34, i64* @global_var_24a890
  store volatile i64 17945, i64* @assembly_address
  %35 = load i32* bitcast (i64* @global_var_216094 to i32*)
  %36 = zext i32 %35 to i64
  store i64 %36, i64* %rax
  store volatile i64 17951, i64* @assembly_address
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i32
  %39 = load i64* %rax
  %40 = trunc i64 %39 to i32
  %41 = and i32 %38, %40
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %42 = icmp eq i32 %41, 0
  store i1 %42, i1* %zf
  %43 = icmp slt i32 %41, 0
  store i1 %43, i1* %sf
  %44 = trunc i32 %41 to i8
  %45 = call i8 @llvm.ctpop.i8(i8 %44)
  %46 = and i8 %45, 1
  %47 = icmp eq i8 %46, 0
  store i1 %47, i1* %pf
  store volatile i64 17953, i64* @assembly_address
  %48 = load i1* %zf
  br i1 %48, label %block_462d, label %block_4623

block_4623:                                       ; preds = %block_460b
  store volatile i64 17955, i64* @assembly_address
  %49 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %50 = zext i32 %49 to i64
  store i64 %50, i64* %rax
  store volatile i64 17961, i64* @assembly_address
  %51 = load i64* %rax
  %52 = trunc i64 %51 to i32
  %53 = load i64* %rax
  %54 = trunc i64 %53 to i32
  %55 = and i32 %52, %54
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %56 = icmp eq i32 %55, 0
  store i1 %56, i1* %zf
  %57 = icmp slt i32 %55, 0
  store i1 %57, i1* %sf
  %58 = trunc i32 %55 to i8
  %59 = call i8 @llvm.ctpop.i8(i8 %58)
  %60 = and i8 %59, 1
  %61 = icmp eq i8 %60, 0
  store i1 %61, i1* %pf
  store volatile i64 17963, i64* @assembly_address
  %62 = load i1* %zf
  br i1 %62, label %block_4647, label %block_462d

block_462d:                                       ; preds = %block_4623, %block_460b
  store volatile i64 17965, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216f00 to i64), i64* %rdi
  store volatile i64 17972, i64* @assembly_address
  %63 = load i64* %rdi
  %64 = inttoptr i64 %63 to i64*
  %65 = call i64 @get_stat_mtime(i64* %64)
  store i64 %65, i64* %rax
  store i64 %65, i64* %rax
  store volatile i64 17977, i64* @assembly_address
  %66 = load i64* %rax
  store i64 %66, i64* @global_var_25f4d0
  store volatile i64 17984, i64* @assembly_address
  %67 = load i64* %rdx
  store i64 %67, i64* @global_var_25f4d8
  br label %block_4647

block_4647:                                       ; preds = %block_462d, %block_4623, %block_45df
  store volatile i64 17991, i64* @assembly_address
  store volatile i64 17992, i64* @assembly_address
  %68 = load i64* %stack_var_-8
  store i64 %68, i64* %rbp
  %69 = ptrtoint i64* %stack_var_0 to i64
  store i64 %69, i64* %rsp
  store volatile i64 17993, i64* @assembly_address
  %70 = load i64* %rax
  ret i64 %70
}

define i64 @treat_stdin(i64 %arg1, i64 %arg2, i64 %arg3) {
block_464a:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 17994, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 17995, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 17998, i64* @assembly_address
  %3 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %4 = zext i32 %3 to i64
  store i64 %4, i64* %rax
  store volatile i64 18004, i64* @assembly_address
  %5 = load i64* %rax
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rax
  %8 = trunc i64 %7 to i32
  %9 = and i32 %6, %8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %10 = icmp eq i32 %9, 0
  store i1 %10, i1* %zf
  %11 = icmp slt i32 %9, 0
  store i1 %11, i1* %sf
  %12 = trunc i32 %9 to i8
  %13 = call i8 @llvm.ctpop.i8(i8 %12)
  %14 = and i8 %13, 1
  %15 = icmp eq i8 %14, 0
  store i1 %15, i1* %pf
  store volatile i64 18006, i64* @assembly_address
  %16 = load i1* %zf
  %17 = icmp eq i1 %16, false
  br i1 %17, label %block_4708, label %block_465c

block_465c:                                       ; preds = %block_464a
  store volatile i64 18012, i64* @assembly_address
  %18 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %19 = zext i32 %18 to i64
  store i64 %19, i64* %rax
  store volatile i64 18018, i64* @assembly_address
  %20 = load i64* %rax
  %21 = trunc i64 %20 to i32
  %22 = load i64* %rax
  %23 = trunc i64 %22 to i32
  %24 = and i32 %21, %23
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %25 = icmp eq i32 %24, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i32 %24, 0
  store i1 %26, i1* %sf
  %27 = trunc i32 %24 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  store volatile i64 18020, i64* @assembly_address
  %31 = load i1* %zf
  %32 = icmp eq i1 %31, false
  br i1 %32, label %block_4708, label %block_466a

block_466a:                                       ; preds = %block_465c
  store volatile i64 18026, i64* @assembly_address
  %33 = load i8* bitcast (i64* @global_var_2165f8 to i8*)
  %34 = zext i8 %33 to i64
  store i64 %34, i64* %rax
  store volatile i64 18033, i64* @assembly_address
  %35 = load i64* %rax
  %36 = trunc i64 %35 to i8
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i8
  %39 = and i8 %36, %38
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %40 = icmp eq i8 %39, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i8 %39, 0
  store i1 %41, i1* %sf
  %42 = call i8 @llvm.ctpop.i8(i8 %39)
  %43 = and i8 %42, 1
  %44 = icmp eq i8 %43, 0
  store i1 %44, i1* %pf
  store volatile i64 18035, i64* @assembly_address
  %45 = load i1* %zf
  %46 = icmp eq i1 %45, false
  br i1 %46, label %block_468e, label %block_4675

block_4675:                                       ; preds = %block_466a
  store volatile i64 18037, i64* @assembly_address
  %47 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rax
  store volatile i64 18043, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  %51 = load i64* %rax
  %52 = trunc i64 %51 to i32
  %53 = and i32 %50, %52
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %54 = icmp eq i32 %53, 0
  store i1 %54, i1* %zf
  %55 = icmp slt i32 %53, 0
  store i1 %55, i1* %sf
  %56 = trunc i32 %53 to i8
  %57 = call i8 @llvm.ctpop.i8(i8 %56)
  %58 = and i8 %57, 1
  %59 = icmp eq i8 %58, 0
  store i1 %59, i1* %pf
  store volatile i64 18045, i64* @assembly_address
  %60 = load i1* %zf
  %61 = zext i1 %60 to i8
  %62 = zext i8 %61 to i64
  %63 = load i64* %rax
  %64 = and i64 %63, -256
  %65 = or i64 %64, %62
  store i64 %65, i64* %rax
  store volatile i64 18048, i64* @assembly_address
  %66 = load i64* %rax
  %67 = trunc i64 %66 to i8
  %68 = zext i8 %67 to i64
  store i64 %68, i64* %rax
  store volatile i64 18051, i64* @assembly_address
  %69 = load i64* %rax
  %70 = trunc i64 %69 to i32
  %71 = zext i32 %70 to i64
  store i64 %71, i64* %rdi
  store volatile i64 18053, i64* @assembly_address
  %72 = load i64* %rdi
  %73 = trunc i64 %72 to i32
  %74 = call i32 @isatty(i32 %73)
  %75 = sext i32 %74 to i64
  store i64 %75, i64* %rax
  %76 = sext i32 %74 to i64
  store i64 %76, i64* %rax
  store volatile i64 18058, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i32
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  %81 = and i32 %78, %80
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %82 = icmp eq i32 %81, 0
  store i1 %82, i1* %zf
  %83 = icmp slt i32 %81, 0
  store i1 %83, i1* %sf
  %84 = trunc i32 %81 to i8
  %85 = call i8 @llvm.ctpop.i8(i8 %84)
  %86 = and i8 %85, 1
  %87 = icmp eq i8 %86, 0
  store i1 %87, i1* %pf
  store volatile i64 18060, i64* @assembly_address
  %88 = load i1* %zf
  br i1 %88, label %block_4708, label %block_468e

block_468e:                                       ; preds = %block_4675, %block_466a
  store volatile i64 18062, i64* @assembly_address
  %89 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %90 = zext i32 %89 to i64
  store i64 %90, i64* %rax
  store volatile i64 18068, i64* @assembly_address
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i32
  %93 = load i64* %rax
  %94 = trunc i64 %93 to i32
  %95 = and i32 %92, %94
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %96 = icmp eq i32 %95, 0
  store i1 %96, i1* %zf
  %97 = icmp slt i32 %95, 0
  store i1 %97, i1* %sf
  %98 = trunc i32 %95 to i8
  %99 = call i8 @llvm.ctpop.i8(i8 %98)
  %100 = and i8 %99, 1
  %101 = icmp eq i8 %100, 0
  store i1 %101, i1* %pf
  store volatile i64 18070, i64* @assembly_address
  %102 = load i1* %zf
  %103 = icmp eq i1 %102, false
  br i1 %103, label %block_46fe, label %block_4698

block_4698:                                       ; preds = %block_468e
  store volatile i64 18072, i64* @assembly_address
  %104 = load i64* @global_var_25f4c8
  store i64 %104, i64* %rcx
  store volatile i64 18079, i64* @assembly_address
  %105 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %106 = zext i32 %105 to i64
  store i64 %106, i64* %rax
  store volatile i64 18085, i64* @assembly_address
  %107 = load i64* %rax
  %108 = trunc i64 %107 to i32
  %109 = load i64* %rax
  %110 = trunc i64 %109 to i32
  %111 = and i32 %108, %110
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %112 = icmp eq i32 %111, 0
  store i1 %112, i1* %zf
  %113 = icmp slt i32 %111, 0
  store i1 %113, i1* %sf
  %114 = trunc i32 %111 to i8
  %115 = call i8 @llvm.ctpop.i8(i8 %114)
  %116 = and i8 %115, 1
  %117 = icmp eq i8 %116, 0
  store i1 %117, i1* %pf
  store volatile i64 18087, i64* @assembly_address
  %118 = load i1* %zf
  br i1 %118, label %block_46b2, label %block_46a9

block_46a9:                                       ; preds = %block_4698
  store volatile i64 18089, i64* @assembly_address
  store i64 ptrtoint ([3 x i8]* @global_var_114fc to i64), i64* %rdx
  store volatile i64 18096, i64* @assembly_address
  br label %block_46b9

block_46b2:                                       ; preds = %block_4698
  store volatile i64 18098, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_114ff to i64), i64* %rdx
  br label %block_46b9

block_46b9:                                       ; preds = %block_46b2, %block_46a9
  store volatile i64 18105, i64* @assembly_address
  %119 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %120 = zext i32 %119 to i64
  store i64 %120, i64* %rax
  store volatile i64 18111, i64* @assembly_address
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i32
  %123 = load i64* %rax
  %124 = trunc i64 %123 to i32
  %125 = and i32 %122, %124
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %126 = icmp eq i32 %125, 0
  store i1 %126, i1* %zf
  %127 = icmp slt i32 %125, 0
  store i1 %127, i1* %sf
  %128 = trunc i32 %125 to i8
  %129 = call i8 @llvm.ctpop.i8(i8 %128)
  %130 = and i8 %129, 1
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %pf
  store volatile i64 18113, i64* @assembly_address
  %132 = load i1* %zf
  br i1 %132, label %block_46cc, label %block_46c3

block_46c3:                                       ; preds = %block_46b9
  store volatile i64 18115, i64* @assembly_address
  store i64 ptrtoint ([10 x i8]* @global_var_11500 to i64), i64* %rax
  store volatile i64 18122, i64* @assembly_address
  br label %block_46d3

block_46cc:                                       ; preds = %block_46b9
  store volatile i64 18124, i64* @assembly_address
  store i64 ptrtoint ([11 x i8]* @global_var_1150a to i64), i64* %rax
  br label %block_46d3

block_46d3:                                       ; preds = %block_46cc, %block_46c3
  store volatile i64 18131, i64* @assembly_address
  %133 = load i64* @global_var_25f4c8
  store i64 %133, i64* %rsi
  store volatile i64 18138, i64* @assembly_address
  %134 = load i64* @global_var_216580
  store i64 %134, i64* %rdi
  store volatile i64 18145, i64* @assembly_address
  %135 = load i64* %rcx
  store i64 %135, i64* %r9
  store volatile i64 18148, i64* @assembly_address
  %136 = load i64* %rdx
  store i64 %136, i64* %r8
  store volatile i64 18151, i64* @assembly_address
  %137 = load i64* %rax
  store i64 %137, i64* %rcx
  store volatile i64 18154, i64* @assembly_address
  %138 = load i64* %rsi
  store i64 %138, i64* %rdx
  store volatile i64 18157, i64* @assembly_address
  store i64 ptrtoint ([93 x i8]* @global_var_11518 to i64), i64* %rsi
  store volatile i64 18164, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 18169, i64* @assembly_address
  %139 = load i64* %rdi
  %140 = inttoptr i64 %139 to %_IO_FILE*
  %141 = load i64* %rsi
  %142 = inttoptr i64 %141 to i8*
  %143 = load i64* %rdx
  %144 = inttoptr i64 %143 to i8*
  %145 = load i64* %rcx
  %146 = inttoptr i64 %145 to i8*
  %147 = load i64* %r8
  %148 = inttoptr i64 %147 to i8*
  %149 = load i64* %r9
  %150 = inttoptr i64 %149 to i8*
  %151 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %140, i8* %142, i8* %144, i8* %146, i8* %148, i8* %150)
  %152 = sext i32 %151 to i64
  store i64 %152, i64* %rax
  %153 = sext i32 %151 to i64
  store i64 %153, i64* %rax
  br label %block_46fe

block_46fe:                                       ; preds = %block_46d3, %block_468e
  store volatile i64 18174, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 18179, i64* @assembly_address
  %154 = load i64* %rdi
  %155 = trunc i64 %154 to i32
  %156 = call i64 @do_exit(i32 %155)
  store i64 %156, i64* %rax
  store i64 %156, i64* %rax
  unreachable

block_4708:                                       ; preds = %block_4675, %block_465c, %block_464a
  store volatile i64 18184, i64* @assembly_address
  %157 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %158 = zext i32 %157 to i64
  store i64 %158, i64* %rax
  store volatile i64 18190, i64* @assembly_address
  %159 = load i64* %rax
  %160 = trunc i64 %159 to i32
  %161 = load i64* %rax
  %162 = trunc i64 %161 to i32
  %163 = and i32 %160, %162
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %164 = icmp eq i32 %163, 0
  store i1 %164, i1* %zf
  %165 = icmp slt i32 %163, 0
  store i1 %165, i1* %sf
  %166 = trunc i32 %163 to i8
  %167 = call i8 @llvm.ctpop.i8(i8 %166)
  %168 = and i8 %167, 1
  %169 = icmp eq i8 %168, 0
  store i1 %169, i1* %pf
  store volatile i64 18192, i64* @assembly_address
  %170 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %171 = zext i32 %170 to i64
  store i64 %171, i64* %rax
  store volatile i64 18198, i64* @assembly_address
  %172 = load i64* %rax
  %173 = trunc i64 %172 to i32
  %174 = load i64* %rax
  %175 = trunc i64 %174 to i32
  %176 = and i32 %173, %175
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %177 = icmp eq i32 %176, 0
  store i1 %177, i1* %zf
  %178 = icmp slt i32 %176, 0
  store i1 %178, i1* %sf
  %179 = trunc i32 %176 to i8
  %180 = call i8 @llvm.ctpop.i8(i8 %179)
  %181 = and i8 %180, 1
  %182 = icmp eq i8 %181, 0
  store i1 %182, i1* %pf
  store volatile i64 18200, i64* @assembly_address
  %183 = load i1* %zf
  %184 = icmp eq i1 %183, false
  br i1 %184, label %block_472c, label %block_471a

block_471a:                                       ; preds = %block_4708
  store volatile i64 18202, i64* @assembly_address
  %185 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %186 = zext i32 %185 to i64
  store i64 %186, i64* %rax
  store volatile i64 18208, i64* @assembly_address
  %187 = load i64* %rax
  %188 = trunc i64 %187 to i32
  %189 = load i64* %rax
  %190 = trunc i64 %189 to i32
  %191 = and i32 %188, %190
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %192 = icmp eq i32 %191, 0
  store i1 %192, i1* %zf
  %193 = icmp slt i32 %191, 0
  store i1 %193, i1* %sf
  %194 = trunc i32 %191 to i8
  %195 = call i8 @llvm.ctpop.i8(i8 %194)
  %196 = and i8 %195, 1
  %197 = icmp eq i8 %196, 0
  store i1 %197, i1* %pf
  store volatile i64 18210, i64* @assembly_address
  %198 = load i1* %zf
  %199 = icmp eq i1 %198, false
  br i1 %199, label %block_472c, label %block_4724

block_4724:                                       ; preds = %block_471a
  store volatile i64 18212, i64* @assembly_address
  %200 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %201 = zext i32 %200 to i64
  store i64 %201, i64* %rax
  store volatile i64 18218, i64* @assembly_address
  %202 = load i64* %rax
  %203 = trunc i64 %202 to i32
  %204 = load i64* %rax
  %205 = trunc i64 %204 to i32
  %206 = and i32 %203, %205
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %207 = icmp eq i32 %206, 0
  store i1 %207, i1* %zf
  %208 = icmp slt i32 %206, 0
  store i1 %208, i1* %sf
  %209 = trunc i32 %206 to i8
  %210 = call i8 @llvm.ctpop.i8(i8 %209)
  %211 = and i8 %210, 1
  %212 = icmp eq i8 %211, 0
  store i1 %212, i1* %pf
  br label %block_472c

block_472c:                                       ; preds = %block_4724, %block_471a, %block_4708
  store volatile i64 18220, i64* @assembly_address
  store i32 1768191091, i32* bitcast (i64* @global_var_21a460 to i32*)
  store volatile i64 18230, i64* @assembly_address
  store i16 110, i16* bitcast (i64* @global_var_21a464 to i16*)
  store volatile i64 18239, i64* @assembly_address
  store i32 1868854387, i32* bitcast (i64* @global_var_24f0c0 to i32*)
  store volatile i64 18249, i64* @assembly_address
  store i16 29813, i16* bitcast (i64* @global_var_24f0c4 to i16*)
  store volatile i64 18258, i64* @assembly_address
  store i8 0, i8* bitcast (i64* @global_var_24f0c6 to i8*)
  store volatile i64 18265, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216f00 to i64), i64* %rsi
  store volatile i64 18272, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 18277, i64* @assembly_address
  %213 = load i64* %rdi
  %214 = trunc i64 %213 to i32
  %215 = load i64* %rsi
  %216 = inttoptr i64 %215 to %stat*
  %217 = call i32 @fstat(i32 %214, %stat* %216)
  %218 = sext i32 %217 to i64
  store i64 %218, i64* %rax
  %219 = sext i32 %217 to i64
  store i64 %219, i64* %rax
  store volatile i64 18282, i64* @assembly_address
  %220 = load i64* %rax
  %221 = trunc i64 %220 to i32
  %222 = load i64* %rax
  %223 = trunc i64 %222 to i32
  %224 = and i32 %221, %223
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %225 = icmp eq i32 %224, 0
  store i1 %225, i1* %zf
  %226 = icmp slt i32 %224, 0
  store i1 %226, i1* %sf
  %227 = trunc i32 %224 to i8
  %228 = call i8 @llvm.ctpop.i8(i8 %227)
  %229 = and i8 %228, 1
  %230 = icmp eq i8 %229, 0
  store i1 %230, i1* %pf
  store volatile i64 18284, i64* @assembly_address
  %231 = load i1* %zf
  br i1 %231, label %block_4784, label %block_476e

block_476e:                                       ; preds = %block_472c
  store volatile i64 18286, i64* @assembly_address
  store i64 ptrtoint ([15 x i8]* @global_var_11575 to i64), i64* %rdi
  store volatile i64 18293, i64* @assembly_address
  %232 = load i64* %rdi
  %233 = inttoptr i64 %232 to i8*
  %234 = call i64 @progerror(i8* %233)
  store i64 %234, i64* %rax
  store i64 %234, i64* %rax
  store volatile i64 18298, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 18303, i64* @assembly_address
  %235 = load i64* %rdi
  %236 = trunc i64 %235 to i32
  %237 = call i64 @do_exit(i32 %236)
  store i64 %237, i64* %rax
  store i64 %237, i64* %rax
  unreachable

block_4784:                                       ; preds = %block_472c
  store volatile i64 18308, i64* @assembly_address
  %238 = load i64* %rdi
  %239 = load i64* %rsi
  %240 = load i64* %rdx
  %241 = call i64 @get_input_size_and_time(i64 %238, i64 %239, i64 %240)
  store i64 %241, i64* %rax
  store i64 %241, i64* %rax
  store volatile i64 18313, i64* @assembly_address
  %242 = call i64 @clear_bufs()
  store i64 %242, i64* %rax
  store i64 %242, i64* %rax
  store i64 %242, i64* %rax
  store volatile i64 18318, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165e0 to i32*)
  store volatile i64 18328, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216620 to i32*)
  store volatile i64 18338, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_24f0a0 to i32*)
  store volatile i64 18348, i64* @assembly_address
  store i8 1, i8* bitcast (i64* @global_var_216ae0 to i8*)
  store volatile i64 18355, i64* @assembly_address
  %243 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %244 = zext i32 %243 to i64
  store i64 %244, i64* %rax
  store volatile i64 18361, i64* @assembly_address
  %245 = load i64* %rax
  %246 = trunc i64 %245 to i32
  %247 = load i64* %rax
  %248 = trunc i64 %247 to i32
  %249 = and i32 %246, %248
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %250 = icmp eq i32 %249, 0
  store i1 %250, i1* %zf
  %251 = icmp slt i32 %249, 0
  store i1 %251, i1* %sf
  %252 = trunc i32 %249 to i8
  %253 = call i8 @llvm.ctpop.i8(i8 %252)
  %254 = and i8 %253, 1
  %255 = icmp eq i8 %254, 0
  store i1 %255, i1* %pf
  store volatile i64 18363, i64* @assembly_address
  %256 = load i1* %zf
  br i1 %256, label %block_47e7, label %block_47bd

block_47bd:                                       ; preds = %block_4784
  store volatile i64 18365, i64* @assembly_address
  %257 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %258 = zext i32 %257 to i64
  store i64 %258, i64* %rax
  store volatile i64 18371, i64* @assembly_address
  %259 = load i64* %rax
  %260 = trunc i64 %259 to i32
  %261 = zext i32 %260 to i64
  store i64 %261, i64* %rdi
  store volatile i64 18373, i64* @assembly_address
  %262 = load i64* %rdi
  %263 = trunc i64 %262 to i32
  %264 = call i64 @get_method(i32 %263)
  store i64 %264, i64* %rax
  store i64 %264, i64* %rax
  store volatile i64 18378, i64* @assembly_address
  %265 = load i64* %rax
  %266 = trunc i64 %265 to i32
  store i32 %266, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 18384, i64* @assembly_address
  %267 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %268 = zext i32 %267 to i64
  store i64 %268, i64* %rax
  store volatile i64 18390, i64* @assembly_address
  %269 = load i64* %rax
  %270 = trunc i64 %269 to i32
  %271 = load i64* %rax
  %272 = trunc i64 %271 to i32
  %273 = and i32 %270, %272
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %274 = icmp eq i32 %273, 0
  store i1 %274, i1* %zf
  %275 = icmp slt i32 %273, 0
  store i1 %275, i1* %sf
  %276 = trunc i32 %273 to i8
  %277 = call i8 @llvm.ctpop.i8(i8 %276)
  %278 = and i8 %277, 1
  %279 = icmp eq i8 %278, 0
  store i1 %279, i1* %pf
  store volatile i64 18392, i64* @assembly_address
  %280 = load i1* %sf
  %281 = icmp eq i1 %280, false
  br i1 %281, label %block_47e7, label %block_47da

block_47da:                                       ; preds = %block_47bd
  store volatile i64 18394, i64* @assembly_address
  %282 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %283 = zext i32 %282 to i64
  store i64 %283, i64* %rax
  store volatile i64 18400, i64* @assembly_address
  %284 = load i64* %rax
  %285 = trunc i64 %284 to i32
  %286 = zext i32 %285 to i64
  store i64 %286, i64* %rdi
  store volatile i64 18402, i64* @assembly_address
  %287 = load i64* %rdi
  %288 = trunc i64 %287 to i32
  %289 = call i64 @do_exit(i32 %288)
  store i64 %289, i64* %rax
  store i64 %289, i64* %rax
  unreachable

block_47e7:                                       ; preds = %block_47bd, %block_4784
  store volatile i64 18407, i64* @assembly_address
  %290 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %291 = zext i32 %290 to i64
  store i64 %291, i64* %rax
  store volatile i64 18413, i64* @assembly_address
  %292 = load i64* %rax
  %293 = trunc i64 %292 to i32
  %294 = load i64* %rax
  %295 = trunc i64 %294 to i32
  %296 = and i32 %293, %295
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %297 = icmp eq i32 %296, 0
  store i1 %297, i1* %zf
  %298 = icmp slt i32 %296, 0
  store i1 %298, i1* %sf
  %299 = trunc i32 %296 to i8
  %300 = call i8 @llvm.ctpop.i8(i8 %299)
  %301 = and i8 %300, 1
  %302 = icmp eq i8 %301, 0
  store i1 %302, i1* %pf
  store volatile i64 18415, i64* @assembly_address
  %303 = load i1* %zf
  br i1 %303, label %block_480b, label %block_47f1

block_47f1:                                       ; preds = %block_47e7
  store volatile i64 18417, i64* @assembly_address
  %304 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %305 = zext i32 %304 to i64
  store i64 %305, i64* %rdx
  store volatile i64 18423, i64* @assembly_address
  %306 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %307 = zext i32 %306 to i64
  store i64 %307, i64* %rax
  store volatile i64 18429, i64* @assembly_address
  %308 = load i64* %rdx
  %309 = trunc i64 %308 to i32
  %310 = zext i32 %309 to i64
  store i64 %310, i64* %rsi
  store volatile i64 18431, i64* @assembly_address
  %311 = load i64* %rax
  %312 = trunc i64 %311 to i32
  %313 = zext i32 %312 to i64
  store i64 %313, i64* %rdi
  store volatile i64 18433, i64* @assembly_address
  %314 = load i64* %rdi
  %315 = load i64* %rsi
  %316 = trunc i64 %314 to i32
  %317 = call i64 @do_list(i32 %316, i64 %315)
  store i64 %317, i64* %rax
  store i64 %317, i64* %rax
  store volatile i64 18438, i64* @assembly_address
  br label %block_48f3

block_480b:                                       ; preds = %block_4850, %block_47e7
  store volatile i64 18443, i64* @assembly_address
  %318 = load i64* @global_var_2160d0
  store i64 %318, i64* %rax
  store volatile i64 18450, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 18455, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 18460, i64* @assembly_address
  %319 = load i64* %rdi
  %320 = load i64* %rsi
  %321 = trunc i64 %319 to i32
  %322 = call i64 @zip(i32 %321, i64 %320)
  store i64 %322, i64* %rax
  store i64 %322, i64* %rax
  store volatile i64 18462, i64* @assembly_address
  %323 = load i64* %rax
  %324 = trunc i64 %323 to i32
  %325 = load i64* %rax
  %326 = trunc i64 %325 to i32
  %327 = and i32 %324, %326
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %328 = icmp eq i32 %327, 0
  store i1 %328, i1* %zf
  %329 = icmp slt i32 %327, 0
  store i1 %329, i1* %sf
  %330 = trunc i32 %327 to i8
  %331 = call i8 @llvm.ctpop.i8(i8 %330)
  %332 = and i8 %331, 1
  %333 = icmp eq i8 %332, 0
  store i1 %333, i1* %pf
  store volatile i64 18464, i64* @assembly_address
  %334 = load i1* %zf
  %335 = icmp eq i1 %334, false
  br i1 %335, label %block_48ef, label %block_4826

block_4826:                                       ; preds = %block_480b
  store volatile i64 18470, i64* @assembly_address
  %336 = call i64 @input_eof()
  store i64 %336, i64* %rax
  store i64 %336, i64* %rax
  store i64 %336, i64* %rax
  store volatile i64 18475, i64* @assembly_address
  %337 = load i64* %rax
  %338 = trunc i64 %337 to i32
  %339 = load i64* %rax
  %340 = trunc i64 %339 to i32
  %341 = and i32 %338, %340
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %342 = icmp eq i32 %341, 0
  store i1 %342, i1* %zf
  %343 = icmp slt i32 %341, 0
  store i1 %343, i1* %sf
  %344 = trunc i32 %341 to i8
  %345 = call i8 @llvm.ctpop.i8(i8 %344)
  %346 = and i8 %345, 1
  %347 = icmp eq i8 %346, 0
  store i1 %347, i1* %pf
  store volatile i64 18477, i64* @assembly_address
  %348 = load i1* %zf
  %349 = icmp eq i1 %348, false
  br i1 %349, label %block_485d, label %block_482f

block_482f:                                       ; preds = %block_4826
  store volatile i64 18479, i64* @assembly_address
  %350 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %351 = zext i32 %350 to i64
  store i64 %351, i64* %rax
  store volatile i64 18485, i64* @assembly_address
  %352 = load i64* %rax
  %353 = trunc i64 %352 to i32
  %354 = zext i32 %353 to i64
  store i64 %354, i64* %rdi
  store volatile i64 18487, i64* @assembly_address
  %355 = load i64* %rdi
  %356 = trunc i64 %355 to i32
  %357 = call i64 @get_method(i32 %356)
  store i64 %357, i64* %rax
  store i64 %357, i64* %rax
  store volatile i64 18492, i64* @assembly_address
  %358 = load i64* %rax
  %359 = trunc i64 %358 to i32
  store i32 %359, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 18498, i64* @assembly_address
  %360 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %361 = zext i32 %360 to i64
  store i64 %361, i64* %rax
  store volatile i64 18504, i64* @assembly_address
  %362 = load i64* %rax
  %363 = trunc i64 %362 to i32
  %364 = load i64* %rax
  %365 = trunc i64 %364 to i32
  %366 = and i32 %363, %365
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %367 = icmp eq i32 %366, 0
  store i1 %367, i1* %zf
  %368 = icmp slt i32 %366, 0
  store i1 %368, i1* %sf
  %369 = trunc i32 %366 to i8
  %370 = call i8 @llvm.ctpop.i8(i8 %369)
  %371 = and i8 %370, 1
  %372 = icmp eq i8 %371, 0
  store i1 %372, i1* %pf
  store volatile i64 18506, i64* @assembly_address
  %373 = load i1* %sf
  br i1 %373, label %block_48f2, label %block_4850

block_4850:                                       ; preds = %block_482f
  store volatile i64 18512, i64* @assembly_address
  store i64 0, i64* @global_var_25f4c0
  store volatile i64 18523, i64* @assembly_address
  br label %block_480b

block_485d:                                       ; preds = %block_4826
  store volatile i64 18525, i64* @assembly_address
  store volatile i64 18526, i64* @assembly_address
  %374 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %375 = zext i32 %374 to i64
  store i64 %375, i64* %rax
  store volatile i64 18532, i64* @assembly_address
  %376 = load i64* %rax
  %377 = trunc i64 %376 to i32
  %378 = load i64* %rax
  %379 = trunc i64 %378 to i32
  %380 = and i32 %377, %379
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %381 = icmp eq i32 %380, 0
  store i1 %381, i1* %zf
  %382 = icmp slt i32 %380, 0
  store i1 %382, i1* %sf
  %383 = trunc i32 %380 to i8
  %384 = call i8 @llvm.ctpop.i8(i8 %383)
  %385 = and i8 %384, 1
  %386 = icmp eq i8 %385, 0
  store i1 %386, i1* %pf
  store volatile i64 18534, i64* @assembly_address
  %387 = load i1* %zf
  br i1 %387, label %block_48f3, label %block_486c

block_486c:                                       ; preds = %block_485d
  store volatile i64 18540, i64* @assembly_address
  %388 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %389 = zext i32 %388 to i64
  store i64 %389, i64* %rax
  store volatile i64 18546, i64* @assembly_address
  %390 = load i64* %rax
  %391 = trunc i64 %390 to i32
  %392 = load i64* %rax
  %393 = trunc i64 %392 to i32
  %394 = and i32 %391, %393
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %395 = icmp eq i32 %394, 0
  store i1 %395, i1* %zf
  %396 = icmp slt i32 %394, 0
  store i1 %396, i1* %sf
  %397 = trunc i32 %394 to i8
  %398 = call i8 @llvm.ctpop.i8(i8 %397)
  %399 = and i8 %398, 1
  %400 = icmp eq i8 %399, 0
  store i1 %400, i1* %pf
  store volatile i64 18548, i64* @assembly_address
  %401 = load i1* %zf
  br i1 %401, label %block_4898, label %block_4876

block_4876:                                       ; preds = %block_486c
  store volatile i64 18550, i64* @assembly_address
  %402 = load i64* @global_var_216580
  store i64 %402, i64* %rax
  store volatile i64 18557, i64* @assembly_address
  %403 = load i64* %rax
  store i64 %403, i64* %rcx
  store volatile i64 18560, i64* @assembly_address
  store i64 4, i64* %rdx
  store volatile i64 18565, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 18570, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11584 to i64), i64* %rdi
  store volatile i64 18577, i64* @assembly_address
  %404 = load i64* %rdi
  %405 = inttoptr i64 %404 to i64*
  %406 = load i64* %rsi
  %407 = trunc i64 %406 to i32
  %408 = load i64* %rdx
  %409 = trunc i64 %408 to i32
  %410 = load i64* %rcx
  %411 = inttoptr i64 %410 to %_IO_FILE*
  %412 = call i32 @fwrite(i64* %405, i32 %407, i32 %409, %_IO_FILE* %411)
  %413 = sext i32 %412 to i64
  store i64 %413, i64* %rax
  %414 = sext i32 %412 to i64
  store i64 %414, i64* %rax
  store volatile i64 18582, i64* @assembly_address
  br label %block_48f3

block_4898:                                       ; preds = %block_486c
  store volatile i64 18584, i64* @assembly_address
  %415 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %416 = zext i32 %415 to i64
  store i64 %416, i64* %rax
  store volatile i64 18590, i64* @assembly_address
  %417 = load i64* %rax
  %418 = trunc i64 %417 to i32
  %419 = load i64* %rax
  %420 = trunc i64 %419 to i32
  %421 = and i32 %418, %420
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %422 = icmp eq i32 %421, 0
  store i1 %422, i1* %zf
  %423 = icmp slt i32 %421, 0
  store i1 %423, i1* %sf
  %424 = trunc i32 %421 to i8
  %425 = call i8 @llvm.ctpop.i8(i8 %424)
  %426 = and i8 %425, 1
  %427 = icmp eq i8 %426, 0
  store i1 %427, i1* %pf
  store volatile i64 18592, i64* @assembly_address
  %428 = load i1* %zf
  %429 = icmp eq i1 %428, false
  br i1 %429, label %block_48f3, label %block_48a2

block_48a2:                                       ; preds = %block_4898
  store volatile i64 18594, i64* @assembly_address
  %430 = load i64* @global_var_216580
  store i64 %430, i64* %rdx
  store volatile i64 18601, i64* @assembly_address
  %431 = load i64* @global_var_21a860
  store i64 %431, i64* %rax
  store volatile i64 18608, i64* @assembly_address
  %432 = load i64* @global_var_21a860
  store i64 %432, i64* %rcx
  store volatile i64 18615, i64* @assembly_address
  %433 = load i64* @global_var_25f4c0
  store i64 %433, i64* %rdi
  store volatile i64 18622, i64* @assembly_address
  %434 = load i64* @global_var_267540
  store i64 %434, i64* %rsi
  store volatile i64 18629, i64* @assembly_address
  %435 = load i64* %rdi
  %436 = load i64* %rsi
  %437 = sub i64 %435, %436
  %438 = and i64 %435, 15
  %439 = and i64 %436, 15
  %440 = sub i64 %438, %439
  %441 = icmp ugt i64 %440, 15
  %442 = icmp ult i64 %435, %436
  %443 = xor i64 %435, %436
  %444 = xor i64 %435, %437
  %445 = and i64 %443, %444
  %446 = icmp slt i64 %445, 0
  store i1 %441, i1* %az
  store i1 %442, i1* %cf
  store i1 %446, i1* %of
  %447 = icmp eq i64 %437, 0
  store i1 %447, i1* %zf
  %448 = icmp slt i64 %437, 0
  store i1 %448, i1* %sf
  %449 = trunc i64 %437 to i8
  %450 = call i8 @llvm.ctpop.i8(i8 %449)
  %451 = and i8 %450, 1
  %452 = icmp eq i8 %451, 0
  store i1 %452, i1* %pf
  store i64 %437, i64* %rdi
  store volatile i64 18632, i64* @assembly_address
  %453 = load i64* %rdi
  store i64 %453, i64* %rsi
  store volatile i64 18635, i64* @assembly_address
  %454 = load i64* %rcx
  %455 = load i64* %rsi
  %456 = sub i64 %454, %455
  %457 = and i64 %454, 15
  %458 = and i64 %455, 15
  %459 = sub i64 %457, %458
  %460 = icmp ugt i64 %459, 15
  %461 = icmp ult i64 %454, %455
  %462 = xor i64 %454, %455
  %463 = xor i64 %454, %456
  %464 = and i64 %462, %463
  %465 = icmp slt i64 %464, 0
  store i1 %460, i1* %az
  store i1 %461, i1* %cf
  store i1 %465, i1* %of
  %466 = icmp eq i64 %456, 0
  store i1 %466, i1* %zf
  %467 = icmp slt i64 %456, 0
  store i1 %467, i1* %sf
  %468 = trunc i64 %456 to i8
  %469 = call i8 @llvm.ctpop.i8(i8 %468)
  %470 = and i8 %469, 1
  %471 = icmp eq i8 %470, 0
  store i1 %471, i1* %pf
  store i64 %456, i64* %rcx
  store volatile i64 18638, i64* @assembly_address
  %472 = load i64* %rax
  store i64 %472, i64* %rsi
  store volatile i64 18641, i64* @assembly_address
  %473 = load i64* %rcx
  store i64 %473, i64* %rdi
  store volatile i64 18644, i64* @assembly_address
  %474 = load i64* %rdi
  %475 = load i64* %rsi
  %476 = load i64* %rdx
  %477 = inttoptr i64 %476 to %_IO_FILE*
  %478 = call i64 @display_ratio(i64 %474, i64 %475, %_IO_FILE* %477)
  store i64 %478, i64* %rax
  store i64 %478, i64* %rax
  store volatile i64 18649, i64* @assembly_address
  %479 = load i64* @global_var_216580
  store i64 %479, i64* %rax
  store volatile i64 18656, i64* @assembly_address
  %480 = load i64* %rax
  store i64 %480, i64* %rsi
  store volatile i64 18659, i64* @assembly_address
  store i64 10, i64* %rdi
  store volatile i64 18664, i64* @assembly_address
  %481 = load i64* %rdi
  %482 = trunc i64 %481 to i32
  %483 = load i64* %rsi
  %484 = inttoptr i64 %483 to %_IO_FILE*
  %485 = call i32 @fputc(i32 %482, %_IO_FILE* %484)
  %486 = sext i32 %485 to i64
  store i64 %486, i64* %rax
  %487 = sext i32 %485 to i64
  store i64 %487, i64* %rax
  store volatile i64 18669, i64* @assembly_address
  br label %block_48f3

block_48ef:                                       ; preds = %block_480b
  store volatile i64 18671, i64* @assembly_address
  store volatile i64 18672, i64* @assembly_address
  br label %block_48f3

block_48f2:                                       ; preds = %block_482f
  store volatile i64 18674, i64* @assembly_address
  br label %block_48f3

block_48f3:                                       ; preds = %block_48f2, %block_48ef, %block_48a2, %block_4898, %block_4876, %block_485d, %block_47f1
  store volatile i64 18675, i64* @assembly_address
  %488 = load i64* %stack_var_-8
  store i64 %488, i64* %rbp
  %489 = ptrtoint i64* %stack_var_0 to i64
  store i64 %489, i64* %rsp
  store volatile i64 18676, i64* @assembly_address
  %490 = load i64* %rax
  %491 = load i64* %rax
  ret i64 %491
}

define i64 @atdir_eq(i64 %arg1, i32 %arg2) {
block_48f5:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i32
  %1 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 18677, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 18678, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 18681, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 18685, i64* @assembly_address
  %21 = load i64* %rdi
  store i64 %21, i64* %stack_var_-16
  store volatile i64 18689, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-24
  store volatile i64 18693, i64* @assembly_address
  %24 = load i32* %stack_var_-24
  %25 = sext i32 %24 to i64
  %26 = and i64 %25, 15
  %27 = icmp ugt i64 %26, 15
  %28 = icmp ult i64 %25, 0
  %29 = xor i64 %25, 0
  %30 = and i64 %29, 0
  %31 = icmp slt i64 %30, 0
  store i1 %27, i1* %az
  store i1 %28, i1* %cf
  store i1 %31, i1* %of
  %32 = icmp eq i64 %25, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i64 %25, 0
  store i1 %33, i1* %sf
  %34 = trunc i64 %25 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  store volatile i64 18698, i64* @assembly_address
  %38 = load i1* %zf
  %39 = icmp eq i1 %38, false
  br i1 %39, label %block_491f, label %block_490c

block_490c:                                       ; preds = %block_48f5
  store volatile i64 18700, i64* @assembly_address
  store i64 ptrtoint ([3 x i8]* @global_var_11589 to i64), i64* %rax
  store volatile i64 18707, i64* @assembly_address
  %40 = load i64* %rax
  store i64 %40, i64* %stack_var_-16
  store volatile i64 18711, i64* @assembly_address
  %41 = trunc i64 1 to i32
  store i32 %41, i32* %stack_var_-24
  br label %block_491f

block_491f:                                       ; preds = %block_490c, %block_48f5
  store volatile i64 18719, i64* @assembly_address
  %42 = load i32* %stack_var_-24
  %43 = sext i32 %42 to i64
  store i64 %43, i64* %rdx
  store volatile i64 18723, i64* @assembly_address
  %44 = load i64* %stack_var_-16
  store i64 %44, i64* %rax
  store volatile i64 18727, i64* @assembly_address
  %45 = load i64* %rax
  store i64 %45, i64* %rsi
  store volatile i64 18730, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216b00 to i64), i64* %rdi
  store volatile i64 18737, i64* @assembly_address
  %46 = load i64* %rdi
  %47 = inttoptr i64 %46 to i64*
  %48 = load i64* %rsi
  %49 = inttoptr i64 %48 to i64*
  %50 = load i64* %rdx
  %51 = trunc i64 %50 to i32
  %52 = call i32 @memcmp(i64* %47, i64* %49, i32 %51)
  %53 = sext i32 %52 to i64
  store i64 %53, i64* %rax
  %54 = sext i32 %52 to i64
  store i64 %54, i64* %rax
  store volatile i64 18742, i64* @assembly_address
  %55 = load i64* %rax
  %56 = trunc i64 %55 to i32
  %57 = load i64* %rax
  %58 = trunc i64 %57 to i32
  %59 = and i32 %56, %58
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %60 = icmp eq i32 %59, 0
  store i1 %60, i1* %zf
  %61 = icmp slt i32 %59, 0
  store i1 %61, i1* %sf
  %62 = trunc i32 %59 to i8
  %63 = call i8 @llvm.ctpop.i8(i8 %62)
  %64 = and i8 %63, 1
  %65 = icmp eq i8 %64, 0
  store i1 %65, i1* %pf
  store volatile i64 18744, i64* @assembly_address
  %66 = load i1* %zf
  %67 = icmp eq i1 %66, false
  br i1 %67, label %block_4956, label %block_493a

block_493a:                                       ; preds = %block_491f
  store volatile i64 18746, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216b00 to i64), i64* %rdx
  store volatile i64 18753, i64* @assembly_address
  %68 = load i32* %stack_var_-24
  %69 = sext i32 %68 to i64
  store i64 %69, i64* %rax
  store volatile i64 18757, i64* @assembly_address
  %70 = load i64* %rax
  %71 = load i64* %rdx
  %72 = add i64 %70, %71
  %73 = and i64 %70, 15
  %74 = and i64 %71, 15
  %75 = add i64 %73, %74
  %76 = icmp ugt i64 %75, 15
  %77 = icmp ult i64 %72, %70
  %78 = xor i64 %70, %72
  %79 = xor i64 %71, %72
  %80 = and i64 %78, %79
  %81 = icmp slt i64 %80, 0
  store i1 %76, i1* %az
  store i1 %77, i1* %cf
  store i1 %81, i1* %of
  %82 = icmp eq i64 %72, 0
  store i1 %82, i1* %zf
  %83 = icmp slt i64 %72, 0
  store i1 %83, i1* %sf
  %84 = trunc i64 %72 to i8
  %85 = call i8 @llvm.ctpop.i8(i8 %84)
  %86 = and i8 %85, 1
  %87 = icmp eq i8 %86, 0
  store i1 %87, i1* %pf
  store i64 %72, i64* %rax
  store volatile i64 18760, i64* @assembly_address
  %88 = load i64* %rax
  %89 = inttoptr i64 %88 to i8*
  %90 = load i8* %89
  %91 = zext i8 %90 to i64
  store i64 %91, i64* %rax
  store volatile i64 18763, i64* @assembly_address
  %92 = load i64* %rax
  %93 = trunc i64 %92 to i8
  %94 = load i64* %rax
  %95 = trunc i64 %94 to i8
  %96 = and i8 %93, %95
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %97 = icmp eq i8 %96, 0
  store i1 %97, i1* %zf
  %98 = icmp slt i8 %96, 0
  store i1 %98, i1* %sf
  %99 = call i8 @llvm.ctpop.i8(i8 %96)
  %100 = and i8 %99, 1
  %101 = icmp eq i8 %100, 0
  store i1 %101, i1* %pf
  store volatile i64 18765, i64* @assembly_address
  %102 = load i1* %zf
  %103 = icmp eq i1 %102, false
  br i1 %103, label %block_4956, label %block_494f

block_494f:                                       ; preds = %block_493a
  store volatile i64 18767, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 18772, i64* @assembly_address
  br label %block_495b

block_4956:                                       ; preds = %block_493a, %block_491f
  store volatile i64 18774, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_495b

block_495b:                                       ; preds = %block_4956, %block_494f
  store volatile i64 18779, i64* @assembly_address
  %104 = load i64* %rax
  %105 = trunc i64 %104 to i32
  %106 = and i32 %105, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %107 = icmp eq i32 %106, 0
  store i1 %107, i1* %zf
  %108 = icmp slt i32 %106, 0
  store i1 %108, i1* %sf
  %109 = trunc i32 %106 to i8
  %110 = call i8 @llvm.ctpop.i8(i8 %109)
  %111 = and i8 %110, 1
  %112 = icmp eq i8 %111, 0
  store i1 %112, i1* %pf
  %113 = zext i32 %106 to i64
  store i64 %113, i64* %rax
  store volatile i64 18782, i64* @assembly_address
  %114 = load i64* %stack_var_-8
  store i64 %114, i64* %rbp
  %115 = ptrtoint i64* %stack_var_0 to i64
  store i64 %115, i64* %rsp
  store volatile i64 18783, i64* @assembly_address
  %116 = load i64* %rax
  ret i64 %116
}

declare i64 @171(i64, i64)

define i64 @atdir_set(i64* %arg1, i32 %arg2) {
block_4960:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i64* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i32
  %2 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 18784, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 18785, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 18788, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 16
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 16
  %11 = xor i64 %6, 16
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %21, i64* %rsp
  store volatile i64 18792, i64* @assembly_address
  %22 = load i64* %rdi
  store i64 %22, i64* %stack_var_-16
  store volatile i64 18796, i64* @assembly_address
  %23 = load i64* %rsi
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-24
  store volatile i64 18800, i64* @assembly_address
  %25 = load i32* %stack_var_-24
  %26 = sext i32 %25 to i64
  store i64 %26, i64* %rdx
  store volatile i64 18804, i64* @assembly_address
  %27 = load i64* %stack_var_-16
  store i64 %27, i64* %rax
  store volatile i64 18808, i64* @assembly_address
  %28 = load i64* %rdx
  store i64 %28, i64* %rsi
  store volatile i64 18811, i64* @assembly_address
  %29 = load i64* %rax
  store i64 %29, i64* %rdi
  store volatile i64 18814, i64* @assembly_address
  %30 = load i64* %rdi
  %31 = load i64* %rsi
  %32 = trunc i64 %31 to i32
  %33 = call i64 @atdir_eq(i64 %30, i32 %32)
  store i64 %33, i64* %rax
  store i64 %33, i64* %rax
  store volatile i64 18819, i64* @assembly_address
  %34 = load i64* %rax
  %35 = trunc i64 %34 to i32
  %36 = xor i32 %35, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %37 = icmp eq i32 %36, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i32 %36, 0
  store i1 %38, i1* %sf
  %39 = trunc i32 %36 to i8
  %40 = call i8 @llvm.ctpop.i8(i8 %39)
  %41 = and i8 %40, 1
  %42 = icmp eq i8 %41, 0
  store i1 %42, i1* %pf
  %43 = zext i32 %36 to i64
  store i64 %43, i64* %rax
  store volatile i64 18822, i64* @assembly_address
  %44 = load i64* %rax
  %45 = trunc i64 %44 to i8
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i8
  %48 = and i8 %45, %47
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %49 = icmp eq i8 %48, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i8 %48, 0
  store i1 %50, i1* %sf
  %51 = call i8 @llvm.ctpop.i8(i8 %48)
  %52 = and i8 %51, 1
  %53 = icmp eq i8 %52, 0
  store i1 %53, i1* %pf
  store volatile i64 18824, i64* @assembly_address
  %54 = load i1* %zf
  br i1 %54, label %block_49ff, label %block_498a

block_498a:                                       ; preds = %block_4960
  store volatile i64 18826, i64* @assembly_address
  %55 = load i32* bitcast (i64* @global_var_2160a8 to i32*)
  %56 = zext i32 %55 to i64
  store i64 %56, i64* %rax
  store volatile i64 18832, i64* @assembly_address
  %57 = load i64* %rax
  %58 = trunc i64 %57 to i32
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  %61 = and i32 %58, %60
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %62 = icmp eq i32 %61, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i32 %61, 0
  store i1 %63, i1* %sf
  %64 = trunc i32 %61 to i8
  %65 = call i8 @llvm.ctpop.i8(i8 %64)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store volatile i64 18834, i64* @assembly_address
  %68 = load i1* %sf
  br i1 %68, label %block_49a1, label %block_4994

block_4994:                                       ; preds = %block_498a
  store volatile i64 18836, i64* @assembly_address
  %69 = load i32* bitcast (i64* @global_var_2160a8 to i32*)
  %70 = zext i32 %69 to i64
  store i64 %70, i64* %rax
  store volatile i64 18842, i64* @assembly_address
  %71 = load i64* %rax
  %72 = trunc i64 %71 to i32
  %73 = zext i32 %72 to i64
  store i64 %73, i64* %rdi
  store volatile i64 18844, i64* @assembly_address
  %74 = load i64* %rdi
  %75 = trunc i64 %74 to i32
  %76 = call i32 @close(i32 %75)
  %77 = sext i32 %76 to i64
  store i64 %77, i64* %rax
  %78 = sext i32 %76 to i64
  store i64 %78, i64* %rax
  br label %block_49a1

block_49a1:                                       ; preds = %block_4994, %block_498a
  store volatile i64 18849, i64* @assembly_address
  %79 = load i32* %stack_var_-24
  %80 = sext i32 %79 to i64
  %81 = and i64 %80, 15
  %82 = icmp ugt i64 %81, 15
  %83 = icmp ult i64 %80, 0
  %84 = xor i64 %80, 0
  %85 = and i64 %84, 0
  %86 = icmp slt i64 %85, 0
  store i1 %82, i1* %az
  store i1 %83, i1* %cf
  store i1 %86, i1* %of
  %87 = icmp eq i64 %80, 0
  store i1 %87, i1* %zf
  %88 = icmp slt i64 %80, 0
  store i1 %88, i1* %sf
  %89 = trunc i64 %80 to i8
  %90 = call i8 @llvm.ctpop.i8(i8 %89)
  %91 = and i8 %90, 1
  %92 = icmp eq i8 %91, 0
  store i1 %92, i1* %pf
  store volatile i64 18854, i64* @assembly_address
  %93 = load i1* %zf
  %94 = icmp eq i1 %93, false
  br i1 %94, label %block_49bb, label %block_49a8

block_49a8:                                       ; preds = %block_49a1
  store volatile i64 18856, i64* @assembly_address
  store i64 ptrtoint ([3 x i8]* @global_var_11589 to i64), i64* %rax
  store volatile i64 18863, i64* @assembly_address
  %95 = load i64* %rax
  store i64 %95, i64* %stack_var_-16
  store volatile i64 18867, i64* @assembly_address
  %96 = trunc i64 1 to i32
  store i32 %96, i32* %stack_var_-24
  br label %block_49bb

block_49bb:                                       ; preds = %block_49a8, %block_49a1
  store volatile i64 18875, i64* @assembly_address
  %97 = load i32* %stack_var_-24
  %98 = sext i32 %97 to i64
  store i64 %98, i64* %rdx
  store volatile i64 18879, i64* @assembly_address
  %99 = load i64* %stack_var_-16
  store i64 %99, i64* %rax
  store volatile i64 18883, i64* @assembly_address
  %100 = load i64* %rax
  store i64 %100, i64* %rsi
  store volatile i64 18886, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216b00 to i64), i64* %rdi
  store volatile i64 18893, i64* @assembly_address
  %101 = load i64* %rdi
  %102 = inttoptr i64 %101 to i64*
  %103 = load i64* %rsi
  %104 = inttoptr i64 %103 to i64*
  %105 = load i64* %rdx
  %106 = trunc i64 %105 to i32
  %107 = call i64* @memcpy(i64* %102, i64* %104, i32 %106)
  %108 = ptrtoint i64* %107 to i64
  store i64 %108, i64* %rax
  %109 = ptrtoint i64* %107 to i64
  store i64 %109, i64* %rax
  store volatile i64 18898, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216b00 to i64), i64* %rdx
  store volatile i64 18905, i64* @assembly_address
  %110 = load i32* %stack_var_-24
  %111 = sext i32 %110 to i64
  store i64 %111, i64* %rax
  store volatile i64 18909, i64* @assembly_address
  %112 = load i64* %rax
  %113 = load i64* %rdx
  %114 = add i64 %112, %113
  %115 = and i64 %112, 15
  %116 = and i64 %113, 15
  %117 = add i64 %115, %116
  %118 = icmp ugt i64 %117, 15
  %119 = icmp ult i64 %114, %112
  %120 = xor i64 %112, %114
  %121 = xor i64 %113, %114
  %122 = and i64 %120, %121
  %123 = icmp slt i64 %122, 0
  store i1 %118, i1* %az
  store i1 %119, i1* %cf
  store i1 %123, i1* %of
  %124 = icmp eq i64 %114, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i64 %114, 0
  store i1 %125, i1* %sf
  %126 = trunc i64 %114 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  store i64 %114, i64* %rax
  store volatile i64 18912, i64* @assembly_address
  %130 = load i64* %rax
  %131 = inttoptr i64 %130 to i8*
  store i8 0, i8* %131
  store volatile i64 18915, i64* @assembly_address
  store i64 65536, i64* %rsi
  store volatile i64 18920, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216b00 to i64), i64* %rdi
  store volatile i64 18927, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 18932, i64* @assembly_address
  %132 = load i64* %rdi
  %133 = inttoptr i64 %132 to i64*
  %134 = load i64* %rsi
  %135 = load i64* %rdx
  %136 = inttoptr i64 %135 to i64*
  %137 = load i64* %rcx
  %138 = load i64* %r8
  %139 = load i64* %r9
  %140 = bitcast i64* %133 to i8*
  %141 = call i64 @open_safer(i8* %140, i64 %134, i64* %136, i64 %137, i64 %138, i64 %139)
  store i64 %141, i64* %rax
  store i64 %141, i64* %rax
  store volatile i64 18937, i64* @assembly_address
  %142 = load i64* %rax
  %143 = trunc i64 %142 to i32
  store i32 %143, i32* bitcast (i64* @global_var_2160a8 to i32*)
  br label %block_49ff

block_49ff:                                       ; preds = %block_49bb, %block_4960
  store volatile i64 18943, i64* @assembly_address
  %144 = load i32* bitcast (i64* @global_var_2160a8 to i32*)
  %145 = zext i32 %144 to i64
  store i64 %145, i64* %rax
  store volatile i64 18949, i64* @assembly_address
  %146 = load i64* %stack_var_-8
  store i64 %146, i64* %rbp
  %147 = ptrtoint i64* %stack_var_0 to i64
  store i64 %147, i64* %rsp
  store volatile i64 18950, i64* @assembly_address
  %148 = load i64* %rax
  ret i64 %148
}

declare i64 @172(i64*, i64)

define i64 @treat_file(i8* %arg1) {
block_4a07:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-168 = alloca i32
  %stack_var_-172 = alloca i32
  %stack_var_-152 = alloca i64
  %stack_var_-176 = alloca i32
  %stack_var_-160 = alloca i8*
  %1 = alloca i64
  %stack_var_-164 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-192 = alloca i8*
  %2 = alloca i64
  %stack_var_-200 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 18951, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 18952, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 18955, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 192
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 192
  %11 = xor i64 %6, 192
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-200 to i64
  store i64 %21, i64* %rsp
  store volatile i64 18962, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = inttoptr i64 %22 to i8*
  store i8* %23, i8** %stack_var_-192
  store volatile i64 18969, i64* @assembly_address
  %24 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %24, i64* %rax
  store volatile i64 18978, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %stack_var_-16
  store volatile i64 18982, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %26 = icmp eq i32 0, 0
  store i1 %26, i1* %zf
  %27 = icmp slt i32 0, 0
  store i1 %27, i1* %sf
  %28 = trunc i32 0 to i8
  %29 = call i8 @llvm.ctpop.i8(i8 %28)
  %30 = and i8 %29, 1
  %31 = icmp eq i8 %30, 0
  store i1 %31, i1* %pf
  %32 = zext i32 0 to i64
  store i64 %32, i64* %rax
  store volatile i64 18984, i64* @assembly_address
  %33 = load i8** %stack_var_-192
  %34 = ptrtoint i8* %33 to i64
  store i64 %34, i64* %rax
  store volatile i64 18991, i64* @assembly_address
  store i64 ptrtoint ([2 x i8]* @global_var_1158a to i64), i64* %rsi
  store volatile i64 18998, i64* @assembly_address
  %35 = load i64* %rax
  store i64 %35, i64* %rdi
  store volatile i64 19001, i64* @assembly_address
  %36 = load i64* %rdi
  %37 = inttoptr i64 %36 to i8*
  %38 = load i64* %rsi
  %39 = inttoptr i64 %38 to i8*
  %40 = call i32 @strcmp(i8* %37, i8* %39)
  %41 = sext i32 %40 to i64
  store i64 %41, i64* %rax
  %42 = sext i32 %40 to i64
  store i64 %42, i64* %rax
  store volatile i64 19006, i64* @assembly_address
  %43 = load i64* %rax
  %44 = trunc i64 %43 to i32
  %45 = load i64* %rax
  %46 = trunc i64 %45 to i32
  %47 = and i32 %44, %46
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %48 = icmp eq i32 %47, 0
  store i1 %48, i1* %zf
  %49 = icmp slt i32 %47, 0
  store i1 %49, i1* %sf
  %50 = trunc i32 %47 to i8
  %51 = call i8 @llvm.ctpop.i8(i8 %50)
  %52 = and i8 %51, 1
  %53 = icmp eq i8 %52, 0
  store i1 %53, i1* %pf
  store volatile i64 19008, i64* @assembly_address
  %54 = load i1* %zf
  %55 = icmp eq i1 %54, false
  br i1 %55, label %block_4a64, label %block_4a42

block_4a42:                                       ; preds = %block_4a07
  store volatile i64 19010, i64* @assembly_address
  %56 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %57 = zext i32 %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 19016, i64* @assembly_address
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i32
  store i32 %59, i32* %stack_var_-164
  store volatile i64 19022, i64* @assembly_address
  %60 = load i64* %rdi
  %61 = load i64* %rsi
  %62 = load i64* %rdx
  %63 = call i64 @treat_stdin(i64 %60, i64 %61, i64 %62)
  store i64 %63, i64* %rax
  store i64 %63, i64* %rax
  store i64 %63, i64* %rax
  store volatile i64 19027, i64* @assembly_address
  %64 = load i32* %stack_var_-164
  %65 = zext i32 %64 to i64
  store i64 %65, i64* %rax
  store volatile i64 19033, i64* @assembly_address
  %66 = load i64* %rax
  %67 = trunc i64 %66 to i32
  store i32 %67, i32* bitcast (i64* @global_var_2165e0 to i32*)
  store volatile i64 19039, i64* @assembly_address
  br label %block_52bc

block_4a64:                                       ; preds = %block_4a07
  store volatile i64 19044, i64* @assembly_address
  %68 = load i8** %stack_var_-192
  %69 = ptrtoint i8* %68 to i64
  store i64 %69, i64* %rax
  store volatile i64 19051, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216f00 to i64), i64* %rsi
  store volatile i64 19058, i64* @assembly_address
  %70 = load i64* %rax
  store i64 %70, i64* %rdi
  store volatile i64 19061, i64* @assembly_address
  %71 = load i64* %rdi
  %72 = load i64* %rsi
  %73 = inttoptr i64 %72 to i64*
  %74 = inttoptr i64 %71 to i8*
  %75 = call i64 @open_input_file(i8* %74, i64* %73)
  store i64 %75, i64* %rax
  store i64 %75, i64* %rax
  store volatile i64 19066, i64* @assembly_address
  %76 = load i64* %rax
  %77 = trunc i64 %76 to i32
  store i32 %77, i32* bitcast (i64* @global_var_24f0a0 to i32*)
  store volatile i64 19072, i64* @assembly_address
  %78 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %79 = zext i32 %78 to i64
  store i64 %79, i64* %rax
  store volatile i64 19078, i64* @assembly_address
  %80 = load i64* %rax
  %81 = trunc i64 %80 to i32
  %82 = load i64* %rax
  %83 = trunc i64 %82 to i32
  %84 = and i32 %81, %83
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %85 = icmp eq i32 %84, 0
  store i1 %85, i1* %zf
  %86 = icmp slt i32 %84, 0
  store i1 %86, i1* %sf
  %87 = trunc i32 %84 to i8
  %88 = call i8 @llvm.ctpop.i8(i8 %87)
  %89 = and i8 %88, 1
  %90 = icmp eq i8 %89, 0
  store i1 %90, i1* %pf
  store volatile i64 19080, i64* @assembly_address
  %91 = load i1* %sf
  br i1 %91, label %block_52af, label %block_4a8e

block_4a8e:                                       ; preds = %block_4a64
  store volatile i64 19086, i64* @assembly_address
  %92 = load i32* bitcast (i64* @global_var_216f18 to i32*)
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rax
  store volatile i64 19092, i64* @assembly_address
  %94 = load i64* %rax
  %95 = trunc i64 %94 to i32
  %96 = and i32 %95, 61440
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %97 = icmp eq i32 %96, 0
  store i1 %97, i1* %zf
  %98 = icmp slt i32 %96, 0
  store i1 %98, i1* %sf
  %99 = trunc i32 %96 to i8
  %100 = call i8 @llvm.ctpop.i8(i8 %99)
  %101 = and i8 %100, 1
  %102 = icmp eq i8 %101, 0
  store i1 %102, i1* %pf
  %103 = zext i32 %96 to i64
  store i64 %103, i64* %rax
  store volatile i64 19097, i64* @assembly_address
  %104 = load i64* %rax
  %105 = trunc i64 %104 to i32
  %106 = sub i32 %105, 16384
  %107 = and i32 %105, 15
  %108 = icmp ugt i32 %107, 15
  %109 = icmp ult i32 %105, 16384
  %110 = xor i32 %105, 16384
  %111 = xor i32 %105, %106
  %112 = and i32 %110, %111
  %113 = icmp slt i32 %112, 0
  store i1 %108, i1* %az
  store i1 %109, i1* %cf
  store i1 %113, i1* %of
  %114 = icmp eq i32 %106, 0
  store i1 %114, i1* %zf
  %115 = icmp slt i32 %106, 0
  store i1 %115, i1* %sf
  %116 = trunc i32 %106 to i8
  %117 = call i8 @llvm.ctpop.i8(i8 %116)
  %118 = and i8 %117, 1
  %119 = icmp eq i8 %118, 0
  store i1 %119, i1* %pf
  store volatile i64 19102, i64* @assembly_address
  %120 = load i1* %zf
  %121 = icmp eq i1 %120, false
  br i1 %121, label %block_4b27, label %block_4aa4

block_4aa4:                                       ; preds = %block_4a8e
  store volatile i64 19108, i64* @assembly_address
  %122 = load i32* bitcast (i64* @global_var_21660c to i32*)
  %123 = zext i32 %122 to i64
  store i64 %123, i64* %rax
  store volatile i64 19114, i64* @assembly_address
  %124 = load i64* %rax
  %125 = trunc i64 %124 to i32
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i32
  %128 = and i32 %125, %127
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %129 = icmp eq i32 %128, 0
  store i1 %129, i1* %zf
  %130 = icmp slt i32 %128, 0
  store i1 %130, i1* %sf
  %131 = trunc i32 %128 to i8
  %132 = call i8 @llvm.ctpop.i8(i8 %131)
  %133 = and i8 %132, 1
  %134 = icmp eq i8 %133, 0
  store i1 %134, i1* %pf
  store volatile i64 19116, i64* @assembly_address
  %135 = load i1* %zf
  br i1 %135, label %block_4aca, label %block_4aae

block_4aae:                                       ; preds = %block_4aa4
  store volatile i64 19118, i64* @assembly_address
  %136 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %137 = zext i32 %136 to i64
  store i64 %137, i64* %rax
  store volatile i64 19124, i64* @assembly_address
  %138 = load i8** %stack_var_-192
  %139 = ptrtoint i8* %138 to i64
  store i64 %139, i64* %rdx
  store volatile i64 19131, i64* @assembly_address
  %140 = load i64* %rdx
  store i64 %140, i64* %rsi
  store volatile i64 19134, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  %143 = zext i32 %142 to i64
  store i64 %143, i64* %rdi
  store volatile i64 19136, i64* @assembly_address
  %144 = load i64* %rdi
  %145 = load i64* %rsi
  %146 = trunc i64 %144 to i32
  %147 = call i64 @treat_dir(i32 %146, i64 %145)
  store i64 %147, i64* %rax
  store i64 %147, i64* %rax
  store volatile i64 19141, i64* @assembly_address
  br label %block_52bc

block_4aca:                                       ; preds = %block_4aa4
  store volatile i64 19146, i64* @assembly_address
  %148 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %149 = zext i32 %148 to i64
  store i64 %149, i64* %rax
  store volatile i64 19152, i64* @assembly_address
  %150 = load i64* %rax
  %151 = trunc i64 %150 to i32
  %152 = zext i32 %151 to i64
  store i64 %152, i64* %rdi
  store volatile i64 19154, i64* @assembly_address
  %153 = load i64* %rdi
  %154 = trunc i64 %153 to i32
  %155 = call i32 @close(i32 %154)
  %156 = sext i32 %155 to i64
  store i64 %156, i64* %rax
  %157 = sext i32 %155 to i64
  store i64 %157, i64* %rax
  store volatile i64 19159, i64* @assembly_address
  %158 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %159 = zext i32 %158 to i64
  store i64 %159, i64* %rax
  store volatile i64 19165, i64* @assembly_address
  %160 = load i64* %rax
  %161 = trunc i64 %160 to i32
  %162 = load i64* %rax
  %163 = trunc i64 %162 to i32
  %164 = and i32 %161, %163
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %165 = icmp eq i32 %164, 0
  store i1 %165, i1* %zf
  %166 = icmp slt i32 %164, 0
  store i1 %166, i1* %sf
  %167 = trunc i32 %164 to i8
  %168 = call i8 @llvm.ctpop.i8(i8 %167)
  %169 = and i8 %168, 1
  %170 = icmp eq i8 %169, 0
  store i1 %170, i1* %pf
  store volatile i64 19167, i64* @assembly_address
  %171 = load i1* %zf
  %172 = icmp eq i1 %171, false
  br i1 %172, label %block_4b0a, label %block_4ae1

block_4ae1:                                       ; preds = %block_4aca
  store volatile i64 19169, i64* @assembly_address
  %173 = load i64* @global_var_25f4c8
  store i64 %173, i64* %rdx
  store volatile i64 19176, i64* @assembly_address
  %174 = load i64* @global_var_216580
  store i64 %174, i64* %rax
  store volatile i64 19183, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 19190, i64* @assembly_address
  store i64 ptrtoint ([34 x i8]* @global_var_11590 to i64), i64* %rsi
  store volatile i64 19197, i64* @assembly_address
  %175 = load i64* %rax
  store i64 %175, i64* %rdi
  store volatile i64 19200, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 19205, i64* @assembly_address
  %176 = load i64* %rdi
  %177 = inttoptr i64 %176 to %_IO_FILE*
  %178 = load i64* %rsi
  %179 = inttoptr i64 %178 to i8*
  %180 = load i64* %rdx
  %181 = inttoptr i64 %180 to i8*
  %182 = load i64* %rcx
  %183 = inttoptr i64 %182 to i8*
  %184 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %177, i8* %179, i8* %181, i8* %183)
  %185 = sext i32 %184 to i64
  store i64 %185, i64* %rax
  %186 = sext i32 %184 to i64
  store i64 %186, i64* %rax
  br label %block_4b0a

block_4b0a:                                       ; preds = %block_4ae1, %block_4aca
  store volatile i64 19210, i64* @assembly_address
  %187 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %188 = zext i32 %187 to i64
  store i64 %188, i64* %rax
  store volatile i64 19216, i64* @assembly_address
  %189 = load i64* %rax
  %190 = trunc i64 %189 to i32
  %191 = load i64* %rax
  %192 = trunc i64 %191 to i32
  %193 = and i32 %190, %192
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %194 = icmp eq i32 %193, 0
  store i1 %194, i1* %zf
  %195 = icmp slt i32 %193, 0
  store i1 %195, i1* %sf
  %196 = trunc i32 %193 to i8
  %197 = call i8 @llvm.ctpop.i8(i8 %196)
  %198 = and i8 %197, 1
  %199 = icmp eq i8 %198, 0
  store i1 %199, i1* %pf
  store volatile i64 19218, i64* @assembly_address
  %200 = load i1* %zf
  %201 = icmp eq i1 %200, false
  br i1 %201, label %block_52b2, label %block_4b18

block_4b18:                                       ; preds = %block_4b0a
  store volatile i64 19224, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 19234, i64* @assembly_address
  br label %block_52b2

block_4b27:                                       ; preds = %block_4a8e
  store volatile i64 19239, i64* @assembly_address
  %202 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %203 = zext i32 %202 to i64
  store i64 %203, i64* %rax
  store volatile i64 19245, i64* @assembly_address
  %204 = load i64* %rax
  %205 = trunc i64 %204 to i32
  %206 = load i64* %rax
  %207 = trunc i64 %206 to i32
  %208 = and i32 %205, %207
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %209 = icmp eq i32 %208, 0
  store i1 %209, i1* %zf
  %210 = icmp slt i32 %208, 0
  store i1 %210, i1* %sf
  %211 = trunc i32 %208 to i8
  %212 = call i8 @llvm.ctpop.i8(i8 %211)
  %213 = and i8 %212, 1
  %214 = icmp eq i8 %213, 0
  store i1 %214, i1* %pf
  store volatile i64 19247, i64* @assembly_address
  %215 = load i1* %zf
  %216 = icmp eq i1 %215, false
  br i1 %216, label %block_4d7a, label %block_4b35

block_4b35:                                       ; preds = %block_4b27
  store volatile i64 19253, i64* @assembly_address
  %217 = load i32* bitcast (i64* @global_var_216f18 to i32*)
  %218 = zext i32 %217 to i64
  store i64 %218, i64* %rax
  store volatile i64 19259, i64* @assembly_address
  %219 = load i64* %rax
  %220 = trunc i64 %219 to i32
  %221 = and i32 %220, 61440
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %222 = icmp eq i32 %221, 0
  store i1 %222, i1* %zf
  %223 = icmp slt i32 %221, 0
  store i1 %223, i1* %sf
  %224 = trunc i32 %221 to i8
  %225 = call i8 @llvm.ctpop.i8(i8 %224)
  %226 = and i8 %225, 1
  %227 = icmp eq i8 %226, 0
  store i1 %227, i1* %pf
  %228 = zext i32 %221 to i64
  store i64 %228, i64* %rax
  store volatile i64 19264, i64* @assembly_address
  %229 = load i64* %rax
  %230 = trunc i64 %229 to i32
  %231 = sub i32 %230, 32768
  %232 = and i32 %230, 15
  %233 = icmp ugt i32 %232, 15
  %234 = icmp ult i32 %230, 32768
  %235 = xor i32 %230, 32768
  %236 = xor i32 %230, %231
  %237 = and i32 %235, %236
  %238 = icmp slt i32 %237, 0
  store i1 %233, i1* %az
  store i1 %234, i1* %cf
  store i1 %238, i1* %of
  %239 = icmp eq i32 %231, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i32 %231, 0
  store i1 %240, i1* %sf
  %241 = trunc i32 %231 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  store volatile i64 19269, i64* @assembly_address
  %245 = load i1* %zf
  br i1 %245, label %block_4ba0, label %block_4b47

block_4b47:                                       ; preds = %block_4b35
  store volatile i64 19271, i64* @assembly_address
  %246 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %247 = zext i32 %246 to i64
  store i64 %247, i64* %rax
  store volatile i64 19277, i64* @assembly_address
  %248 = load i64* %rax
  %249 = trunc i64 %248 to i32
  %250 = load i64* %rax
  %251 = trunc i64 %250 to i32
  %252 = and i32 %249, %251
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %253 = icmp eq i32 %252, 0
  store i1 %253, i1* %zf
  %254 = icmp slt i32 %252, 0
  store i1 %254, i1* %sf
  %255 = trunc i32 %252 to i8
  %256 = call i8 @llvm.ctpop.i8(i8 %255)
  %257 = and i8 %256, 1
  %258 = icmp eq i8 %257, 0
  store i1 %258, i1* %pf
  store volatile i64 19279, i64* @assembly_address
  %259 = load i1* %zf
  %260 = icmp eq i1 %259, false
  br i1 %260, label %block_4b7a, label %block_4b51

block_4b51:                                       ; preds = %block_4b47
  store volatile i64 19281, i64* @assembly_address
  %261 = load i64* @global_var_25f4c8
  store i64 %261, i64* %rdx
  store volatile i64 19288, i64* @assembly_address
  %262 = load i64* @global_var_216580
  store i64 %262, i64* %rax
  store volatile i64 19295, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 19302, i64* @assembly_address
  store i64 ptrtoint ([55 x i8]* @global_var_115b8 to i64), i64* %rsi
  store volatile i64 19309, i64* @assembly_address
  %263 = load i64* %rax
  store i64 %263, i64* %rdi
  store volatile i64 19312, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 19317, i64* @assembly_address
  %264 = load i64* %rdi
  %265 = inttoptr i64 %264 to %_IO_FILE*
  %266 = load i64* %rsi
  %267 = inttoptr i64 %266 to i8*
  %268 = load i64* %rdx
  %269 = inttoptr i64 %268 to i8*
  %270 = load i64* %rcx
  %271 = inttoptr i64 %270 to i8*
  %272 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %265, i8* %267, i8* %269, i8* %271)
  %273 = sext i32 %272 to i64
  store i64 %273, i64* %rax
  %274 = sext i32 %272 to i64
  store i64 %274, i64* %rax
  br label %block_4b7a

block_4b7a:                                       ; preds = %block_4b51, %block_4b47
  store volatile i64 19322, i64* @assembly_address
  %275 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %276 = zext i32 %275 to i64
  store i64 %276, i64* %rax
  store volatile i64 19328, i64* @assembly_address
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i32
  %279 = load i64* %rax
  %280 = trunc i64 %279 to i32
  %281 = and i32 %278, %280
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %282 = icmp eq i32 %281, 0
  store i1 %282, i1* %zf
  %283 = icmp slt i32 %281, 0
  store i1 %283, i1* %sf
  %284 = trunc i32 %281 to i8
  %285 = call i8 @llvm.ctpop.i8(i8 %284)
  %286 = and i8 %285, 1
  %287 = icmp eq i8 %286, 0
  store i1 %287, i1* %pf
  store volatile i64 19330, i64* @assembly_address
  %288 = load i1* %zf
  %289 = icmp eq i1 %288, false
  br i1 %289, label %block_4b8e, label %block_4b84

block_4b84:                                       ; preds = %block_4b7a
  store volatile i64 19332, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_4b8e

block_4b8e:                                       ; preds = %block_4b84, %block_4b7a
  store volatile i64 19342, i64* @assembly_address
  %290 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %291 = zext i32 %290 to i64
  store i64 %291, i64* %rax
  store volatile i64 19348, i64* @assembly_address
  %292 = load i64* %rax
  %293 = trunc i64 %292 to i32
  %294 = zext i32 %293 to i64
  store i64 %294, i64* %rdi
  store volatile i64 19350, i64* @assembly_address
  %295 = load i64* %rdi
  %296 = trunc i64 %295 to i32
  %297 = call i32 @close(i32 %296)
  %298 = sext i32 %297 to i64
  store i64 %298, i64* %rax
  %299 = sext i32 %297 to i64
  store i64 %299, i64* %rax
  store volatile i64 19355, i64* @assembly_address
  br label %block_52bc

block_4ba0:                                       ; preds = %block_4b35
  store volatile i64 19360, i64* @assembly_address
  %300 = load i32* bitcast (i64* @global_var_216f18 to i32*)
  %301 = zext i32 %300 to i64
  store i64 %301, i64* %rax
  store volatile i64 19366, i64* @assembly_address
  %302 = load i64* %rax
  %303 = trunc i64 %302 to i32
  %304 = and i32 %303, ptrtoint (i64* @global_var_800 to i32)
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %305 = icmp eq i32 %304, 0
  store i1 %305, i1* %zf
  %306 = icmp slt i32 %304, 0
  store i1 %306, i1* %sf
  %307 = trunc i32 %304 to i8
  %308 = call i8 @llvm.ctpop.i8(i8 %307)
  %309 = and i8 %308, 1
  %310 = icmp eq i8 %309, 0
  store i1 %310, i1* %pf
  %311 = zext i32 %304 to i64
  store i64 %311, i64* %rax
  store volatile i64 19371, i64* @assembly_address
  %312 = load i64* %rax
  %313 = trunc i64 %312 to i32
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i32
  %316 = and i32 %313, %315
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %317 = icmp eq i32 %316, 0
  store i1 %317, i1* %zf
  %318 = icmp slt i32 %316, 0
  store i1 %318, i1* %sf
  %319 = trunc i32 %316 to i8
  %320 = call i8 @llvm.ctpop.i8(i8 %319)
  %321 = and i8 %320, 1
  %322 = icmp eq i8 %321, 0
  store i1 %322, i1* %pf
  store volatile i64 19373, i64* @assembly_address
  %323 = load i1* %zf
  br i1 %323, label %block_4c08, label %block_4baf

block_4baf:                                       ; preds = %block_4ba0
  store volatile i64 19375, i64* @assembly_address
  %324 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %325 = zext i32 %324 to i64
  store i64 %325, i64* %rax
  store volatile i64 19381, i64* @assembly_address
  %326 = load i64* %rax
  %327 = trunc i64 %326 to i32
  %328 = load i64* %rax
  %329 = trunc i64 %328 to i32
  %330 = and i32 %327, %329
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %331 = icmp eq i32 %330, 0
  store i1 %331, i1* %zf
  %332 = icmp slt i32 %330, 0
  store i1 %332, i1* %sf
  %333 = trunc i32 %330 to i8
  %334 = call i8 @llvm.ctpop.i8(i8 %333)
  %335 = and i8 %334, 1
  %336 = icmp eq i8 %335, 0
  store i1 %336, i1* %pf
  store volatile i64 19383, i64* @assembly_address
  %337 = load i1* %zf
  %338 = icmp eq i1 %337, false
  br i1 %338, label %block_4be2, label %block_4bb9

block_4bb9:                                       ; preds = %block_4baf
  store volatile i64 19385, i64* @assembly_address
  %339 = load i64* @global_var_25f4c8
  store i64 %339, i64* %rdx
  store volatile i64 19392, i64* @assembly_address
  %340 = load i64* @global_var_216580
  store i64 %340, i64* %rax
  store volatile i64 19399, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 19406, i64* @assembly_address
  store i64 ptrtoint ([46 x i8]* @global_var_115f0 to i64), i64* %rsi
  store volatile i64 19413, i64* @assembly_address
  %341 = load i64* %rax
  store i64 %341, i64* %rdi
  store volatile i64 19416, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 19421, i64* @assembly_address
  %342 = load i64* %rdi
  %343 = inttoptr i64 %342 to %_IO_FILE*
  %344 = load i64* %rsi
  %345 = inttoptr i64 %344 to i8*
  %346 = load i64* %rdx
  %347 = inttoptr i64 %346 to i8*
  %348 = load i64* %rcx
  %349 = inttoptr i64 %348 to i8*
  %350 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %343, i8* %345, i8* %347, i8* %349)
  %351 = sext i32 %350 to i64
  store i64 %351, i64* %rax
  %352 = sext i32 %350 to i64
  store i64 %352, i64* %rax
  br label %block_4be2

block_4be2:                                       ; preds = %block_4bb9, %block_4baf
  store volatile i64 19426, i64* @assembly_address
  %353 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %354 = zext i32 %353 to i64
  store i64 %354, i64* %rax
  store volatile i64 19432, i64* @assembly_address
  %355 = load i64* %rax
  %356 = trunc i64 %355 to i32
  %357 = load i64* %rax
  %358 = trunc i64 %357 to i32
  %359 = and i32 %356, %358
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %360 = icmp eq i32 %359, 0
  store i1 %360, i1* %zf
  %361 = icmp slt i32 %359, 0
  store i1 %361, i1* %sf
  %362 = trunc i32 %359 to i8
  %363 = call i8 @llvm.ctpop.i8(i8 %362)
  %364 = and i8 %363, 1
  %365 = icmp eq i8 %364, 0
  store i1 %365, i1* %pf
  store volatile i64 19434, i64* @assembly_address
  %366 = load i1* %zf
  %367 = icmp eq i1 %366, false
  br i1 %367, label %block_4bf6, label %block_4bec

block_4bec:                                       ; preds = %block_4be2
  store volatile i64 19436, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_4bf6

block_4bf6:                                       ; preds = %block_4bec, %block_4be2
  store volatile i64 19446, i64* @assembly_address
  %368 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %369 = zext i32 %368 to i64
  store i64 %369, i64* %rax
  store volatile i64 19452, i64* @assembly_address
  %370 = load i64* %rax
  %371 = trunc i64 %370 to i32
  %372 = zext i32 %371 to i64
  store i64 %372, i64* %rdi
  store volatile i64 19454, i64* @assembly_address
  %373 = load i64* %rdi
  %374 = trunc i64 %373 to i32
  %375 = call i32 @close(i32 %374)
  %376 = sext i32 %375 to i64
  store i64 %376, i64* %rax
  %377 = sext i32 %375 to i64
  store i64 %377, i64* %rax
  store volatile i64 19459, i64* @assembly_address
  br label %block_52bc

block_4c08:                                       ; preds = %block_4ba0
  store volatile i64 19464, i64* @assembly_address
  %378 = load i32* bitcast (i64* @global_var_216f18 to i32*)
  %379 = zext i32 %378 to i64
  store i64 %379, i64* %rax
  store volatile i64 19470, i64* @assembly_address
  %380 = load i64* %rax
  %381 = trunc i64 %380 to i32
  %382 = and i32 %381, ptrtoint (i64* @global_var_400 to i32)
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %383 = icmp eq i32 %382, 0
  store i1 %383, i1* %zf
  %384 = icmp slt i32 %382, 0
  store i1 %384, i1* %sf
  %385 = trunc i32 %382 to i8
  %386 = call i8 @llvm.ctpop.i8(i8 %385)
  %387 = and i8 %386, 1
  %388 = icmp eq i8 %387, 0
  store i1 %388, i1* %pf
  %389 = zext i32 %382 to i64
  store i64 %389, i64* %rax
  store volatile i64 19475, i64* @assembly_address
  %390 = load i64* %rax
  %391 = trunc i64 %390 to i32
  %392 = load i64* %rax
  %393 = trunc i64 %392 to i32
  %394 = and i32 %391, %393
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %395 = icmp eq i32 %394, 0
  store i1 %395, i1* %zf
  %396 = icmp slt i32 %394, 0
  store i1 %396, i1* %sf
  %397 = trunc i32 %394 to i8
  %398 = call i8 @llvm.ctpop.i8(i8 %397)
  %399 = and i8 %398, 1
  %400 = icmp eq i8 %399, 0
  store i1 %400, i1* %pf
  store volatile i64 19477, i64* @assembly_address
  %401 = load i1* %zf
  br i1 %401, label %block_4c70, label %block_4c17

block_4c17:                                       ; preds = %block_4c08
  store volatile i64 19479, i64* @assembly_address
  %402 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %403 = zext i32 %402 to i64
  store i64 %403, i64* %rax
  store volatile i64 19485, i64* @assembly_address
  %404 = load i64* %rax
  %405 = trunc i64 %404 to i32
  %406 = load i64* %rax
  %407 = trunc i64 %406 to i32
  %408 = and i32 %405, %407
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %409 = icmp eq i32 %408, 0
  store i1 %409, i1* %zf
  %410 = icmp slt i32 %408, 0
  store i1 %410, i1* %sf
  %411 = trunc i32 %408 to i8
  %412 = call i8 @llvm.ctpop.i8(i8 %411)
  %413 = and i8 %412, 1
  %414 = icmp eq i8 %413, 0
  store i1 %414, i1* %pf
  store volatile i64 19487, i64* @assembly_address
  %415 = load i1* %zf
  %416 = icmp eq i1 %415, false
  br i1 %416, label %block_4c4a, label %block_4c21

block_4c21:                                       ; preds = %block_4c17
  store volatile i64 19489, i64* @assembly_address
  %417 = load i64* @global_var_25f4c8
  store i64 %417, i64* %rdx
  store volatile i64 19496, i64* @assembly_address
  %418 = load i64* @global_var_216580
  store i64 %418, i64* %rax
  store volatile i64 19503, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 19510, i64* @assembly_address
  store i64 ptrtoint ([47 x i8]* @global_var_11620 to i64), i64* %rsi
  store volatile i64 19517, i64* @assembly_address
  %419 = load i64* %rax
  store i64 %419, i64* %rdi
  store volatile i64 19520, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 19525, i64* @assembly_address
  %420 = load i64* %rdi
  %421 = inttoptr i64 %420 to %_IO_FILE*
  %422 = load i64* %rsi
  %423 = inttoptr i64 %422 to i8*
  %424 = load i64* %rdx
  %425 = inttoptr i64 %424 to i8*
  %426 = load i64* %rcx
  %427 = inttoptr i64 %426 to i8*
  %428 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %421, i8* %423, i8* %425, i8* %427)
  %429 = sext i32 %428 to i64
  store i64 %429, i64* %rax
  %430 = sext i32 %428 to i64
  store i64 %430, i64* %rax
  br label %block_4c4a

block_4c4a:                                       ; preds = %block_4c21, %block_4c17
  store volatile i64 19530, i64* @assembly_address
  %431 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %432 = zext i32 %431 to i64
  store i64 %432, i64* %rax
  store volatile i64 19536, i64* @assembly_address
  %433 = load i64* %rax
  %434 = trunc i64 %433 to i32
  %435 = load i64* %rax
  %436 = trunc i64 %435 to i32
  %437 = and i32 %434, %436
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %438 = icmp eq i32 %437, 0
  store i1 %438, i1* %zf
  %439 = icmp slt i32 %437, 0
  store i1 %439, i1* %sf
  %440 = trunc i32 %437 to i8
  %441 = call i8 @llvm.ctpop.i8(i8 %440)
  %442 = and i8 %441, 1
  %443 = icmp eq i8 %442, 0
  store i1 %443, i1* %pf
  store volatile i64 19538, i64* @assembly_address
  %444 = load i1* %zf
  %445 = icmp eq i1 %444, false
  br i1 %445, label %block_4c5e, label %block_4c54

block_4c54:                                       ; preds = %block_4c4a
  store volatile i64 19540, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_4c5e

block_4c5e:                                       ; preds = %block_4c54, %block_4c4a
  store volatile i64 19550, i64* @assembly_address
  %446 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %447 = zext i32 %446 to i64
  store i64 %447, i64* %rax
  store volatile i64 19556, i64* @assembly_address
  %448 = load i64* %rax
  %449 = trunc i64 %448 to i32
  %450 = zext i32 %449 to i64
  store i64 %450, i64* %rdi
  store volatile i64 19558, i64* @assembly_address
  %451 = load i64* %rdi
  %452 = trunc i64 %451 to i32
  %453 = call i32 @close(i32 %452)
  %454 = sext i32 %453 to i64
  store i64 %454, i64* %rax
  %455 = sext i32 %453 to i64
  store i64 %455, i64* %rax
  store volatile i64 19563, i64* @assembly_address
  br label %block_52bc

block_4c70:                                       ; preds = %block_4c08
  store volatile i64 19568, i64* @assembly_address
  %456 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %457 = zext i32 %456 to i64
  store i64 %457, i64* %rax
  store volatile i64 19574, i64* @assembly_address
  %458 = load i64* %rax
  %459 = trunc i64 %458 to i32
  %460 = load i64* %rax
  %461 = trunc i64 %460 to i32
  %462 = and i32 %459, %461
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %463 = icmp eq i32 %462, 0
  store i1 %463, i1* %zf
  %464 = icmp slt i32 %462, 0
  store i1 %464, i1* %sf
  %465 = trunc i32 %462 to i8
  %466 = call i8 @llvm.ctpop.i8(i8 %465)
  %467 = and i8 %466, 1
  %468 = icmp eq i8 %467, 0
  store i1 %468, i1* %pf
  store volatile i64 19576, i64* @assembly_address
  %469 = load i1* %zf
  %470 = icmp eq i1 %469, false
  br i1 %470, label %block_4d7a, label %block_4c7e

block_4c7e:                                       ; preds = %block_4c70
  store volatile i64 19582, i64* @assembly_address
  %471 = load i32* bitcast (i64* @global_var_216f18 to i32*)
  %472 = zext i32 %471 to i64
  store i64 %472, i64* %rax
  store volatile i64 19588, i64* @assembly_address
  %473 = load i64* %rax
  %474 = trunc i64 %473 to i32
  %475 = and i32 %474, 512
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %476 = icmp eq i32 %475, 0
  store i1 %476, i1* %zf
  %477 = icmp slt i32 %475, 0
  store i1 %477, i1* %sf
  %478 = trunc i32 %475 to i8
  %479 = call i8 @llvm.ctpop.i8(i8 %478)
  %480 = and i8 %479, 1
  %481 = icmp eq i8 %480, 0
  store i1 %481, i1* %pf
  %482 = zext i32 %475 to i64
  store i64 %482, i64* %rax
  store volatile i64 19593, i64* @assembly_address
  %483 = load i64* %rax
  %484 = trunc i64 %483 to i32
  %485 = load i64* %rax
  %486 = trunc i64 %485 to i32
  %487 = and i32 %484, %486
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %488 = icmp eq i32 %487, 0
  store i1 %488, i1* %zf
  %489 = icmp slt i32 %487, 0
  store i1 %489, i1* %sf
  %490 = trunc i32 %487 to i8
  %491 = call i8 @llvm.ctpop.i8(i8 %490)
  %492 = and i8 %491, 1
  %493 = icmp eq i8 %492, 0
  store i1 %493, i1* %pf
  store volatile i64 19595, i64* @assembly_address
  %494 = load i1* %zf
  br i1 %494, label %block_4ce6, label %block_4c8d

block_4c8d:                                       ; preds = %block_4c7e
  store volatile i64 19597, i64* @assembly_address
  %495 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %496 = zext i32 %495 to i64
  store i64 %496, i64* %rax
  store volatile i64 19603, i64* @assembly_address
  %497 = load i64* %rax
  %498 = trunc i64 %497 to i32
  %499 = load i64* %rax
  %500 = trunc i64 %499 to i32
  %501 = and i32 %498, %500
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %502 = icmp eq i32 %501, 0
  store i1 %502, i1* %zf
  %503 = icmp slt i32 %501, 0
  store i1 %503, i1* %sf
  %504 = trunc i32 %501 to i8
  %505 = call i8 @llvm.ctpop.i8(i8 %504)
  %506 = and i8 %505, 1
  %507 = icmp eq i8 %506, 0
  store i1 %507, i1* %pf
  store volatile i64 19605, i64* @assembly_address
  %508 = load i1* %zf
  %509 = icmp eq i1 %508, false
  br i1 %509, label %block_4cc0, label %block_4c97

block_4c97:                                       ; preds = %block_4c8d
  store volatile i64 19607, i64* @assembly_address
  %510 = load i64* @global_var_25f4c8
  store i64 %510, i64* %rdx
  store volatile i64 19614, i64* @assembly_address
  %511 = load i64* @global_var_216580
  store i64 %511, i64* %rax
  store volatile i64 19621, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 19628, i64* @assembly_address
  store i64 ptrtoint ([46 x i8]* @global_var_11650 to i64), i64* %rsi
  store volatile i64 19635, i64* @assembly_address
  %512 = load i64* %rax
  store i64 %512, i64* %rdi
  store volatile i64 19638, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 19643, i64* @assembly_address
  %513 = load i64* %rdi
  %514 = inttoptr i64 %513 to %_IO_FILE*
  %515 = load i64* %rsi
  %516 = inttoptr i64 %515 to i8*
  %517 = load i64* %rdx
  %518 = inttoptr i64 %517 to i8*
  %519 = load i64* %rcx
  %520 = inttoptr i64 %519 to i8*
  %521 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %514, i8* %516, i8* %518, i8* %520)
  %522 = sext i32 %521 to i64
  store i64 %522, i64* %rax
  %523 = sext i32 %521 to i64
  store i64 %523, i64* %rax
  br label %block_4cc0

block_4cc0:                                       ; preds = %block_4c97, %block_4c8d
  store volatile i64 19648, i64* @assembly_address
  %524 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %525 = zext i32 %524 to i64
  store i64 %525, i64* %rax
  store volatile i64 19654, i64* @assembly_address
  %526 = load i64* %rax
  %527 = trunc i64 %526 to i32
  %528 = load i64* %rax
  %529 = trunc i64 %528 to i32
  %530 = and i32 %527, %529
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %531 = icmp eq i32 %530, 0
  store i1 %531, i1* %zf
  %532 = icmp slt i32 %530, 0
  store i1 %532, i1* %sf
  %533 = trunc i32 %530 to i8
  %534 = call i8 @llvm.ctpop.i8(i8 %533)
  %535 = and i8 %534, 1
  %536 = icmp eq i8 %535, 0
  store i1 %536, i1* %pf
  store volatile i64 19656, i64* @assembly_address
  %537 = load i1* %zf
  %538 = icmp eq i1 %537, false
  br i1 %538, label %block_4cd4, label %block_4cca

block_4cca:                                       ; preds = %block_4cc0
  store volatile i64 19658, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_4cd4

block_4cd4:                                       ; preds = %block_4cca, %block_4cc0
  store volatile i64 19668, i64* @assembly_address
  %539 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %540 = zext i32 %539 to i64
  store i64 %540, i64* %rax
  store volatile i64 19674, i64* @assembly_address
  %541 = load i64* %rax
  %542 = trunc i64 %541 to i32
  %543 = zext i32 %542 to i64
  store i64 %543, i64* %rdi
  store volatile i64 19676, i64* @assembly_address
  %544 = load i64* %rdi
  %545 = trunc i64 %544 to i32
  %546 = call i32 @close(i32 %545)
  %547 = sext i32 %546 to i64
  store i64 %547, i64* %rax
  %548 = sext i32 %546 to i64
  store i64 %548, i64* %rax
  store volatile i64 19681, i64* @assembly_address
  br label %block_52bc

block_4ce6:                                       ; preds = %block_4c7e
  store volatile i64 19686, i64* @assembly_address
  %549 = load i64* @global_var_216f10
  store i64 %549, i64* %rax
  store volatile i64 19693, i64* @assembly_address
  %550 = load i64* %rax
  %551 = sub i64 %550, 1
  %552 = and i64 %550, 15
  %553 = sub i64 %552, 1
  %554 = icmp ugt i64 %553, 15
  %555 = icmp ult i64 %550, 1
  %556 = xor i64 %550, 1
  %557 = xor i64 %550, %551
  %558 = and i64 %556, %557
  %559 = icmp slt i64 %558, 0
  store i1 %554, i1* %az
  store i1 %555, i1* %cf
  store i1 %559, i1* %of
  %560 = icmp eq i64 %551, 0
  store i1 %560, i1* %zf
  %561 = icmp slt i64 %551, 0
  store i1 %561, i1* %sf
  %562 = trunc i64 %551 to i8
  %563 = call i8 @llvm.ctpop.i8(i8 %562)
  %564 = and i8 %563, 1
  %565 = icmp eq i8 %564, 0
  store i1 %565, i1* %pf
  store volatile i64 19697, i64* @assembly_address
  %566 = load i1* %cf
  %567 = load i1* %zf
  %568 = or i1 %566, %567
  br i1 %568, label %block_4d7a, label %block_4cf7

block_4cf7:                                       ; preds = %block_4ce6
  store volatile i64 19703, i64* @assembly_address
  %569 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %570 = zext i32 %569 to i64
  store i64 %570, i64* %rax
  store volatile i64 19709, i64* @assembly_address
  %571 = load i64* %rax
  %572 = trunc i64 %571 to i32
  %573 = load i64* %rax
  %574 = trunc i64 %573 to i32
  %575 = and i32 %572, %574
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %576 = icmp eq i32 %575, 0
  store i1 %576, i1* %zf
  %577 = icmp slt i32 %575, 0
  store i1 %577, i1* %sf
  %578 = trunc i32 %575 to i8
  %579 = call i8 @llvm.ctpop.i8(i8 %578)
  %580 = and i8 %579, 1
  %581 = icmp eq i8 %580, 0
  store i1 %581, i1* %pf
  store volatile i64 19711, i64* @assembly_address
  %582 = load i1* %zf
  %583 = icmp eq i1 %582, false
  br i1 %583, label %block_4d54, label %block_4d01

block_4d01:                                       ; preds = %block_4cf7
  store volatile i64 19713, i64* @assembly_address
  %584 = load i64* @global_var_216f10
  store i64 %584, i64* %rax
  store volatile i64 19720, i64* @assembly_address
  %585 = load i64* %rax
  %586 = sub i64 %585, 2
  %587 = and i64 %585, 15
  %588 = sub i64 %587, 2
  %589 = icmp ugt i64 %588, 15
  %590 = icmp ult i64 %585, 2
  %591 = xor i64 %585, 2
  %592 = xor i64 %585, %586
  %593 = and i64 %591, %592
  %594 = icmp slt i64 %593, 0
  store i1 %589, i1* %az
  store i1 %590, i1* %cf
  store i1 %594, i1* %of
  %595 = icmp eq i64 %586, 0
  store i1 %595, i1* %zf
  %596 = icmp slt i64 %586, 0
  store i1 %596, i1* %sf
  %597 = trunc i64 %586 to i8
  %598 = call i8 @llvm.ctpop.i8(i8 %597)
  %599 = and i8 %598, 1
  %600 = icmp eq i8 %599, 0
  store i1 %600, i1* %pf
  store volatile i64 19724, i64* @assembly_address
  %601 = load i1* %zf
  %602 = icmp eq i1 %601, false
  br i1 %602, label %block_4d15, label %block_4d0e

block_4d0e:                                       ; preds = %block_4d01
  store volatile i64 19726, i64* @assembly_address
  store i64 32, i64* %rsi
  store volatile i64 19731, i64* @assembly_address
  br label %block_4d1a

block_4d15:                                       ; preds = %block_4d01
  store volatile i64 19733, i64* @assembly_address
  store i64 115, i64* %rsi
  br label %block_4d1a

block_4d1a:                                       ; preds = %block_4d15, %block_4d0e
  store volatile i64 19738, i64* @assembly_address
  %603 = load i64* @global_var_216f10
  store i64 %603, i64* %rax
  store volatile i64 19745, i64* @assembly_address
  %604 = load i64* %rax
  %605 = add i64 %604, -1
  store i64 %605, i64* %rcx
  store volatile i64 19749, i64* @assembly_address
  %606 = load i64* @global_var_25f4c8
  store i64 %606, i64* %rdx
  store volatile i64 19756, i64* @assembly_address
  %607 = load i64* @global_var_216580
  store i64 %607, i64* %rax
  store volatile i64 19763, i64* @assembly_address
  %608 = load i64* %rsi
  %609 = trunc i64 %608 to i32
  %610 = zext i32 %609 to i64
  store i64 %610, i64* %r9
  store volatile i64 19766, i64* @assembly_address
  %611 = load i64* %rcx
  store i64 %611, i64* %r8
  store volatile i64 19769, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 19776, i64* @assembly_address
  store i64 ptrtoint ([42 x i8]* @global_var_11680 to i64), i64* %rsi
  store volatile i64 19783, i64* @assembly_address
  %612 = load i64* %rax
  store i64 %612, i64* %rdi
  store volatile i64 19786, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 19791, i64* @assembly_address
  %613 = load i64* %rdi
  %614 = inttoptr i64 %613 to %_IO_FILE*
  %615 = load i64* %rsi
  %616 = inttoptr i64 %615 to i8*
  %617 = load i64* %rdx
  %618 = inttoptr i64 %617 to i8*
  %619 = load i64* %rcx
  %620 = inttoptr i64 %619 to i8*
  %621 = load i64* %r8
  %622 = trunc i64 %621 to i32
  %623 = load i64* %r9
  %624 = trunc i64 %623 to i8
  %625 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %614, i8* %616, i8* %618, i8* %620, i32 %622, i8 %624)
  %626 = sext i32 %625 to i64
  store i64 %626, i64* %rax
  %627 = sext i32 %625 to i64
  store i64 %627, i64* %rax
  br label %block_4d54

block_4d54:                                       ; preds = %block_4d1a, %block_4cf7
  store volatile i64 19796, i64* @assembly_address
  %628 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %629 = zext i32 %628 to i64
  store i64 %629, i64* %rax
  store volatile i64 19802, i64* @assembly_address
  %630 = load i64* %rax
  %631 = trunc i64 %630 to i32
  %632 = load i64* %rax
  %633 = trunc i64 %632 to i32
  %634 = and i32 %631, %633
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %635 = icmp eq i32 %634, 0
  store i1 %635, i1* %zf
  %636 = icmp slt i32 %634, 0
  store i1 %636, i1* %sf
  %637 = trunc i32 %634 to i8
  %638 = call i8 @llvm.ctpop.i8(i8 %637)
  %639 = and i8 %638, 1
  %640 = icmp eq i8 %639, 0
  store i1 %640, i1* %pf
  store volatile i64 19804, i64* @assembly_address
  %641 = load i1* %zf
  %642 = icmp eq i1 %641, false
  br i1 %642, label %block_4d68, label %block_4d5e

block_4d5e:                                       ; preds = %block_4d54
  store volatile i64 19806, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_4d68

block_4d68:                                       ; preds = %block_4d5e, %block_4d54
  store volatile i64 19816, i64* @assembly_address
  %643 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %644 = zext i32 %643 to i64
  store i64 %644, i64* %rax
  store volatile i64 19822, i64* @assembly_address
  %645 = load i64* %rax
  %646 = trunc i64 %645 to i32
  %647 = zext i32 %646 to i64
  store i64 %647, i64* %rdi
  store volatile i64 19824, i64* @assembly_address
  %648 = load i64* %rdi
  %649 = trunc i64 %648 to i32
  %650 = call i32 @close(i32 %649)
  %651 = sext i32 %650 to i64
  store i64 %651, i64* %rax
  %652 = sext i32 %650 to i64
  store i64 %652, i64* %rax
  store volatile i64 19829, i64* @assembly_address
  br label %block_52bc

block_4d7a:                                       ; preds = %block_4ce6, %block_4c70, %block_4b27
  store volatile i64 19834, i64* @assembly_address
  %653 = load i64* %rdi
  %654 = load i64* %rsi
  %655 = load i64* %rdx
  %656 = call i64 @get_input_size_and_time(i64 %653, i64 %654, i64 %655)
  store i64 %656, i64* %rax
  store i64 %656, i64* %rax
  store volatile i64 19839, i64* @assembly_address
  %657 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %658 = zext i32 %657 to i64
  store i64 %658, i64* %rax
  store volatile i64 19845, i64* @assembly_address
  %659 = load i64* %rax
  %660 = trunc i64 %659 to i32
  %661 = load i64* %rax
  %662 = trunc i64 %661 to i32
  %663 = and i32 %660, %662
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %664 = icmp eq i32 %663, 0
  store i1 %664, i1* %zf
  %665 = icmp slt i32 %663, 0
  store i1 %665, i1* %sf
  %666 = trunc i32 %663 to i8
  %667 = call i8 @llvm.ctpop.i8(i8 %666)
  %668 = and i8 %667, 1
  %669 = icmp eq i8 %668, 0
  store i1 %669, i1* %pf
  store volatile i64 19847, i64* @assembly_address
  %670 = load i1* %zf
  br i1 %670, label %block_4db9, label %block_4d89

block_4d89:                                       ; preds = %block_4d7a
  store volatile i64 19849, i64* @assembly_address
  %671 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %672 = zext i32 %671 to i64
  store i64 %672, i64* %rax
  store volatile i64 19855, i64* @assembly_address
  %673 = load i64* %rax
  %674 = trunc i64 %673 to i32
  %675 = load i64* %rax
  %676 = trunc i64 %675 to i32
  %677 = and i32 %674, %676
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %678 = icmp eq i32 %677, 0
  store i1 %678, i1* %zf
  %679 = icmp slt i32 %677, 0
  store i1 %679, i1* %sf
  %680 = trunc i32 %677 to i8
  %681 = call i8 @llvm.ctpop.i8(i8 %680)
  %682 = and i8 %681, 1
  %683 = icmp eq i8 %682, 0
  store i1 %683, i1* %pf
  store volatile i64 19857, i64* @assembly_address
  %684 = load i1* %zf
  %685 = icmp eq i1 %684, false
  br i1 %685, label %block_4db9, label %block_4d93

block_4d93:                                       ; preds = %block_4d89
  store volatile i64 19859, i64* @assembly_address
  %686 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %687 = zext i32 %686 to i64
  store i64 %687, i64* %rax
  store volatile i64 19865, i64* @assembly_address
  %688 = load i64* %rax
  %689 = trunc i64 %688 to i32
  %690 = load i64* %rax
  %691 = trunc i64 %690 to i32
  %692 = and i32 %689, %691
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %693 = icmp eq i32 %692, 0
  store i1 %693, i1* %zf
  %694 = icmp slt i32 %692, 0
  store i1 %694, i1* %sf
  %695 = trunc i32 %692 to i8
  %696 = call i8 @llvm.ctpop.i8(i8 %695)
  %697 = and i8 %696, 1
  %698 = icmp eq i8 %697, 0
  store i1 %698, i1* %pf
  store volatile i64 19867, i64* @assembly_address
  %699 = load i1* %zf
  %700 = icmp eq i1 %699, false
  br i1 %700, label %block_4db9, label %block_4d9d

block_4d9d:                                       ; preds = %block_4d93
  store volatile i64 19869, i64* @assembly_address
  store i32 1868854387, i32* bitcast (i64* @global_var_24f0c0 to i32*)
  store volatile i64 19879, i64* @assembly_address
  store i16 29813, i16* bitcast (i64* @global_var_24f0c4 to i16*)
  store volatile i64 19888, i64* @assembly_address
  store i8 0, i8* bitcast (i64* @global_var_24f0c6 to i8*)
  store volatile i64 19895, i64* @assembly_address
  br label %block_4dd4

block_4db9:                                       ; preds = %block_4d93, %block_4d89, %block_4d7a
  store volatile i64 19897, i64* @assembly_address
  %701 = call i64 @make_ofname()
  store i64 %701, i64* %rax
  store i64 %701, i64* %rax
  store i64 %701, i64* %rax
  store volatile i64 19902, i64* @assembly_address
  %702 = load i64* %rax
  %703 = trunc i64 %702 to i32
  %704 = load i64* %rax
  %705 = trunc i64 %704 to i32
  %706 = and i32 %703, %705
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %707 = icmp eq i32 %706, 0
  store i1 %707, i1* %zf
  %708 = icmp slt i32 %706, 0
  store i1 %708, i1* %sf
  %709 = trunc i32 %706 to i8
  %710 = call i8 @llvm.ctpop.i8(i8 %709)
  %711 = and i8 %710, 1
  %712 = icmp eq i8 %711, 0
  store i1 %712, i1* %pf
  store volatile i64 19904, i64* @assembly_address
  %713 = load i1* %zf
  br i1 %713, label %block_4dd4, label %block_4dc2

block_4dc2:                                       ; preds = %block_4db9
  store volatile i64 19906, i64* @assembly_address
  %714 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %715 = zext i32 %714 to i64
  store i64 %715, i64* %rax
  store volatile i64 19912, i64* @assembly_address
  %716 = load i64* %rax
  %717 = trunc i64 %716 to i32
  %718 = zext i32 %717 to i64
  store i64 %718, i64* %rdi
  store volatile i64 19914, i64* @assembly_address
  %719 = load i64* %rdi
  %720 = trunc i64 %719 to i32
  %721 = call i32 @close(i32 %720)
  %722 = sext i32 %721 to i64
  store i64 %722, i64* %rax
  %723 = sext i32 %721 to i64
  store i64 %723, i64* %rax
  store volatile i64 19919, i64* @assembly_address
  br label %block_52bc

block_4dd4:                                       ; preds = %block_4db9, %block_4d9d
  store volatile i64 19924, i64* @assembly_address
  %724 = call i64 @clear_bufs()
  store i64 %724, i64* %rax
  store i64 %724, i64* %rax
  store i64 %724, i64* %rax
  store volatile i64 19929, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216620 to i32*)
  store volatile i64 19939, i64* @assembly_address
  %725 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %726 = zext i32 %725 to i64
  store i64 %726, i64* %rax
  store volatile i64 19945, i64* @assembly_address
  %727 = load i64* %rax
  %728 = trunc i64 %727 to i32
  %729 = load i64* %rax
  %730 = trunc i64 %729 to i32
  %731 = and i32 %728, %730
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %732 = icmp eq i32 %731, 0
  store i1 %732, i1* %zf
  %733 = icmp slt i32 %731, 0
  store i1 %733, i1* %sf
  %734 = trunc i32 %731 to i8
  %735 = call i8 @llvm.ctpop.i8(i8 %734)
  %736 = and i8 %735, 1
  %737 = icmp eq i8 %736, 0
  store i1 %737, i1* %pf
  store volatile i64 19947, i64* @assembly_address
  %738 = load i1* %zf
  br i1 %738, label %block_4e1c, label %block_4ded

block_4ded:                                       ; preds = %block_4dd4
  store volatile i64 19949, i64* @assembly_address
  %739 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %740 = zext i32 %739 to i64
  store i64 %740, i64* %rax
  store volatile i64 19955, i64* @assembly_address
  %741 = load i64* %rax
  %742 = trunc i64 %741 to i32
  %743 = zext i32 %742 to i64
  store i64 %743, i64* %rdi
  store volatile i64 19957, i64* @assembly_address
  %744 = load i64* %rdi
  %745 = trunc i64 %744 to i32
  %746 = call i64 @get_method(i32 %745)
  store i64 %746, i64* %rax
  store i64 %746, i64* %rax
  store volatile i64 19962, i64* @assembly_address
  %747 = load i64* %rax
  %748 = trunc i64 %747 to i32
  store i32 %748, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 19968, i64* @assembly_address
  %749 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %750 = zext i32 %749 to i64
  store i64 %750, i64* %rax
  store volatile i64 19974, i64* @assembly_address
  %751 = load i64* %rax
  %752 = trunc i64 %751 to i32
  %753 = load i64* %rax
  %754 = trunc i64 %753 to i32
  %755 = and i32 %752, %754
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %756 = icmp eq i32 %755, 0
  store i1 %756, i1* %zf
  %757 = icmp slt i32 %755, 0
  store i1 %757, i1* %sf
  %758 = trunc i32 %755 to i8
  %759 = call i8 @llvm.ctpop.i8(i8 %758)
  %760 = and i8 %759, 1
  %761 = icmp eq i8 %760, 0
  store i1 %761, i1* %pf
  store volatile i64 19976, i64* @assembly_address
  %762 = load i1* %sf
  %763 = icmp eq i1 %762, false
  br i1 %763, label %block_4e1c, label %block_4e0a

block_4e0a:                                       ; preds = %block_4ded
  store volatile i64 19978, i64* @assembly_address
  %764 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %765 = zext i32 %764 to i64
  store i64 %765, i64* %rax
  store volatile i64 19984, i64* @assembly_address
  %766 = load i64* %rax
  %767 = trunc i64 %766 to i32
  %768 = zext i32 %767 to i64
  store i64 %768, i64* %rdi
  store volatile i64 19986, i64* @assembly_address
  %769 = load i64* %rdi
  %770 = trunc i64 %769 to i32
  %771 = call i32 @close(i32 %770)
  %772 = sext i32 %771 to i64
  store i64 %772, i64* %rax
  %773 = sext i32 %771 to i64
  store i64 %773, i64* %rax
  store volatile i64 19991, i64* @assembly_address
  br label %block_52bc

block_4e1c:                                       ; preds = %block_4ded, %block_4dd4
  store volatile i64 19996, i64* @assembly_address
  %774 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %775 = zext i32 %774 to i64
  store i64 %775, i64* %rax
  store volatile i64 20002, i64* @assembly_address
  %776 = load i64* %rax
  %777 = trunc i64 %776 to i32
  %778 = load i64* %rax
  %779 = trunc i64 %778 to i32
  %780 = and i32 %777, %779
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %781 = icmp eq i32 %780, 0
  store i1 %781, i1* %zf
  %782 = icmp slt i32 %780, 0
  store i1 %782, i1* %sf
  %783 = trunc i32 %780 to i8
  %784 = call i8 @llvm.ctpop.i8(i8 %783)
  %785 = and i8 %784, 1
  %786 = icmp eq i8 %785, 0
  store i1 %786, i1* %pf
  store volatile i64 20004, i64* @assembly_address
  %787 = load i1* %zf
  br i1 %787, label %block_4e55, label %block_4e26

block_4e26:                                       ; preds = %block_4e1c
  store volatile i64 20006, i64* @assembly_address
  %788 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %789 = zext i32 %788 to i64
  store i64 %789, i64* %rdx
  store volatile i64 20012, i64* @assembly_address
  %790 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %791 = zext i32 %790 to i64
  store i64 %791, i64* %rax
  store volatile i64 20018, i64* @assembly_address
  %792 = load i64* %rdx
  %793 = trunc i64 %792 to i32
  %794 = zext i32 %793 to i64
  store i64 %794, i64* %rsi
  store volatile i64 20020, i64* @assembly_address
  %795 = load i64* %rax
  %796 = trunc i64 %795 to i32
  %797 = zext i32 %796 to i64
  store i64 %797, i64* %rdi
  store volatile i64 20022, i64* @assembly_address
  %798 = load i64* %rdi
  %799 = load i64* %rsi
  %800 = trunc i64 %798 to i32
  %801 = call i64 @do_list(i32 %800, i64 %799)
  store i64 %801, i64* %rax
  store i64 %801, i64* %rax
  store volatile i64 20027, i64* @assembly_address
  %802 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %803 = zext i32 %802 to i64
  store i64 %803, i64* %rax
  store volatile i64 20033, i64* @assembly_address
  %804 = load i64* %rax
  %805 = trunc i64 %804 to i32
  %806 = zext i32 %805 to i64
  store i64 %806, i64* %rdi
  store volatile i64 20035, i64* @assembly_address
  %807 = load i64* %rdi
  %808 = trunc i64 %807 to i32
  %809 = call i32 @close(i32 %808)
  %810 = sext i32 %809 to i64
  store i64 %810, i64* %rax
  %811 = sext i32 %809 to i64
  store i64 %811, i64* %rax
  store volatile i64 20040, i64* @assembly_address
  %812 = load i64* %rax
  %813 = trunc i64 %812 to i32
  %814 = load i64* %rax
  %815 = trunc i64 %814 to i32
  %816 = and i32 %813, %815
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %817 = icmp eq i32 %816, 0
  store i1 %817, i1* %zf
  %818 = icmp slt i32 %816, 0
  store i1 %818, i1* %sf
  %819 = trunc i32 %816 to i8
  %820 = call i8 @llvm.ctpop.i8(i8 %819)
  %821 = and i8 %820, 1
  %822 = icmp eq i8 %821, 0
  store i1 %822, i1* %pf
  store volatile i64 20042, i64* @assembly_address
  %823 = load i1* %zf
  br i1 %823, label %block_52b5, label %block_4e50

block_4e50:                                       ; preds = %block_4e26
  store volatile i64 20048, i64* @assembly_address
  %824 = call i64 @read_error()
  store i64 %824, i64* %rax
  store i64 %824, i64* %rax
  store i64 %824, i64* %rax
  unreachable

block_4e55:                                       ; preds = %block_4e1c
  store volatile i64 20053, i64* @assembly_address
  %825 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %826 = zext i32 %825 to i64
  store i64 %826, i64* %rax
  store volatile i64 20059, i64* @assembly_address
  %827 = load i64* %rax
  %828 = trunc i64 %827 to i32
  %829 = load i64* %rax
  %830 = trunc i64 %829 to i32
  %831 = and i32 %828, %830
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %832 = icmp eq i32 %831, 0
  store i1 %832, i1* %zf
  %833 = icmp slt i32 %831, 0
  store i1 %833, i1* %sf
  %834 = trunc i32 %831 to i8
  %835 = call i8 @llvm.ctpop.i8(i8 %834)
  %836 = and i8 %835, 1
  %837 = icmp eq i8 %836, 0
  store i1 %837, i1* %pf
  store volatile i64 20061, i64* @assembly_address
  %838 = load i1* %zf
  br i1 %838, label %block_4e6b, label %block_4e5f

block_4e5f:                                       ; preds = %block_4e55
  store volatile i64 20063, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_24a880 to i32*)
  store volatile i64 20073, i64* @assembly_address
  br label %block_4ed0

block_4e6b:                                       ; preds = %block_4e55
  store volatile i64 20075, i64* @assembly_address
  %839 = load i64* %rdi
  %840 = load i64* %rsi
  %841 = load i64* %rdx
  %842 = load i64* %rcx
  %843 = load i64* %r8
  %844 = load i64* %r9
  %845 = call i64 @create_outfile(i64 %839, i64 %840, i64 %841, i64 %842, i64 %843, i64 %844)
  store i64 %845, i64* %rax
  store i64 %845, i64* %rax
  store i64 %845, i64* %rax
  store volatile i64 20080, i64* @assembly_address
  %846 = load i64* %rax
  %847 = trunc i64 %846 to i32
  %848 = load i64* %rax
  %849 = trunc i64 %848 to i32
  %850 = and i32 %847, %849
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %851 = icmp eq i32 %850, 0
  store i1 %851, i1* %zf
  %852 = icmp slt i32 %850, 0
  store i1 %852, i1* %sf
  %853 = trunc i32 %850 to i8
  %854 = call i8 @llvm.ctpop.i8(i8 %853)
  %855 = and i8 %854, 1
  %856 = icmp eq i8 %855, 0
  store i1 %856, i1* %pf
  store volatile i64 20082, i64* @assembly_address
  %857 = load i1* %zf
  %858 = icmp eq i1 %857, false
  br i1 %858, label %block_52b8, label %block_4e78

block_4e78:                                       ; preds = %block_4e6b
  store volatile i64 20088, i64* @assembly_address
  %859 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %860 = zext i32 %859 to i64
  store i64 %860, i64* %rax
  store volatile i64 20094, i64* @assembly_address
  %861 = load i64* %rax
  %862 = trunc i64 %861 to i32
  %863 = load i64* %rax
  %864 = trunc i64 %863 to i32
  %865 = and i32 %862, %864
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %866 = icmp eq i32 %865, 0
  store i1 %866, i1* %zf
  %867 = icmp slt i32 %865, 0
  store i1 %867, i1* %sf
  %868 = trunc i32 %865 to i8
  %869 = call i8 @llvm.ctpop.i8(i8 %868)
  %870 = and i8 %869, 1
  %871 = icmp eq i8 %870, 0
  store i1 %871, i1* %pf
  store volatile i64 20096, i64* @assembly_address
  %872 = load i1* %zf
  %873 = icmp eq i1 %872, false
  br i1 %873, label %block_4ed0, label %block_4e82

block_4e82:                                       ; preds = %block_4e78
  store volatile i64 20098, i64* @assembly_address
  %874 = load i32* bitcast (i64* @global_var_24a888 to i32*)
  %875 = zext i32 %874 to i64
  store i64 %875, i64* %rax
  store volatile i64 20104, i64* @assembly_address
  %876 = load i64* %rax
  %877 = trunc i64 %876 to i32
  %878 = load i64* %rax
  %879 = trunc i64 %878 to i32
  %880 = and i32 %877, %879
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %881 = icmp eq i32 %880, 0
  store i1 %881, i1* %zf
  %882 = icmp slt i32 %880, 0
  store i1 %882, i1* %sf
  %883 = trunc i32 %880 to i8
  %884 = call i8 @llvm.ctpop.i8(i8 %883)
  %885 = and i8 %884, 1
  %886 = icmp eq i8 %885, 0
  store i1 %886, i1* %pf
  store volatile i64 20106, i64* @assembly_address
  %887 = load i1* %zf
  br i1 %887, label %block_4ed0, label %block_4e8c

block_4e8c:                                       ; preds = %block_4e82
  store volatile i64 20108, i64* @assembly_address
  %888 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %889 = zext i32 %888 to i64
  store i64 %889, i64* %rax
  store volatile i64 20114, i64* @assembly_address
  %890 = load i64* %rax
  %891 = trunc i64 %890 to i32
  %892 = load i64* %rax
  %893 = trunc i64 %892 to i32
  %894 = and i32 %891, %893
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %895 = icmp eq i32 %894, 0
  store i1 %895, i1* %zf
  %896 = icmp slt i32 %894, 0
  store i1 %896, i1* %sf
  %897 = trunc i32 %894 to i8
  %898 = call i8 @llvm.ctpop.i8(i8 %897)
  %899 = and i8 %898, 1
  %900 = icmp eq i8 %899, 0
  store i1 %900, i1* %pf
  store volatile i64 20116, i64* @assembly_address
  %901 = load i1* %zf
  %902 = icmp eq i1 %901, false
  br i1 %902, label %block_4ed0, label %block_4e96

block_4e96:                                       ; preds = %block_4e8c
  store volatile i64 20118, i64* @assembly_address
  %903 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %904 = zext i32 %903 to i64
  store i64 %904, i64* %rax
  store volatile i64 20124, i64* @assembly_address
  %905 = load i64* %rax
  %906 = trunc i64 %905 to i32
  %907 = load i64* %rax
  %908 = trunc i64 %907 to i32
  %909 = and i32 %906, %908
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %910 = icmp eq i32 %909, 0
  store i1 %910, i1* %zf
  %911 = icmp slt i32 %909, 0
  store i1 %911, i1* %sf
  %912 = trunc i32 %909 to i8
  %913 = call i8 @llvm.ctpop.i8(i8 %912)
  %914 = and i8 %913, 1
  %915 = icmp eq i8 %914, 0
  store i1 %915, i1* %pf
  store volatile i64 20126, i64* @assembly_address
  %916 = load i1* %zf
  %917 = icmp eq i1 %916, false
  br i1 %917, label %block_4ed0, label %block_4ea0

block_4ea0:                                       ; preds = %block_4e96
  store volatile i64 20128, i64* @assembly_address
  %918 = load i64* @global_var_25f4c8
  store i64 %918, i64* %rdx
  store volatile i64 20135, i64* @assembly_address
  %919 = load i64* @global_var_216580
  store i64 %919, i64* %rax
  store volatile i64 20142, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %r8
  store volatile i64 20149, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 20156, i64* @assembly_address
  store i64 ptrtoint ([25 x i8]* @global_var_116aa to i64), i64* %rsi
  store volatile i64 20163, i64* @assembly_address
  %920 = load i64* %rax
  store i64 %920, i64* %rdi
  store volatile i64 20166, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 20171, i64* @assembly_address
  %921 = load i64* %rdi
  %922 = inttoptr i64 %921 to %_IO_FILE*
  %923 = load i64* %rsi
  %924 = inttoptr i64 %923 to i8*
  %925 = load i64* %rdx
  %926 = inttoptr i64 %925 to i8*
  %927 = load i64* %rcx
  %928 = inttoptr i64 %927 to i8*
  %929 = load i64* %r8
  %930 = inttoptr i64 %929 to i8*
  %931 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %922, i8* %924, i8* %926, i8* %928, i8* %930)
  %932 = sext i32 %931 to i64
  store i64 %932, i64* %rax
  %933 = sext i32 %931 to i64
  store i64 %933, i64* %rax
  br label %block_4ed0

block_4ed0:                                       ; preds = %block_4ea0, %block_4e96, %block_4e8c, %block_4e82, %block_4e78, %block_4e5f
  store volatile i64 20176, i64* @assembly_address
  %934 = load i32* bitcast (i64* @global_var_24a888 to i32*)
  %935 = zext i32 %934 to i64
  store i64 %935, i64* %rax
  store volatile i64 20182, i64* @assembly_address
  %936 = load i64* %rax
  %937 = trunc i64 %936 to i32
  %938 = load i64* %rax
  %939 = trunc i64 %938 to i32
  %940 = and i32 %937, %939
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %941 = icmp eq i32 %940, 0
  store i1 %941, i1* %zf
  %942 = icmp slt i32 %940, 0
  store i1 %942, i1* %sf
  %943 = trunc i32 %940 to i8
  %944 = call i8 @llvm.ctpop.i8(i8 %943)
  %945 = and i8 %944, 1
  %946 = icmp eq i8 %945, 0
  store i1 %946, i1* %pf
  store volatile i64 20184, i64* @assembly_address
  %947 = load i1* %zf
  %948 = icmp eq i1 %947, false
  br i1 %948, label %block_4eee, label %block_4eda

block_4eda:                                       ; preds = %block_4ed0
  store volatile i64 20186, i64* @assembly_address
  %949 = load i32* bitcast (i64* @global_var_216090 to i32*)
  %950 = zext i32 %949 to i64
  store i64 %950, i64* %rax
  store volatile i64 20192, i64* @assembly_address
  %951 = load i64* %rax
  %952 = trunc i64 %951 to i32
  %953 = load i64* %rax
  %954 = trunc i64 %953 to i32
  %955 = and i32 %952, %954
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %956 = icmp eq i32 %955, 0
  store i1 %956, i1* %zf
  %957 = icmp slt i32 %955, 0
  store i1 %957, i1* %sf
  %958 = trunc i32 %955 to i8
  %959 = call i8 @llvm.ctpop.i8(i8 %958)
  %960 = and i8 %959, 1
  %961 = icmp eq i8 %960, 0
  store i1 %961, i1* %pf
  store volatile i64 20194, i64* @assembly_address
  %962 = load i1* %zf
  %963 = zext i1 %962 to i8
  %964 = zext i8 %963 to i64
  %965 = load i64* %rax
  %966 = and i64 %965, -256
  %967 = or i64 %966, %964
  store i64 %967, i64* %rax
  store volatile i64 20197, i64* @assembly_address
  %968 = load i64* %rax
  %969 = trunc i64 %968 to i8
  %970 = zext i8 %969 to i64
  store i64 %970, i64* %rax
  store volatile i64 20200, i64* @assembly_address
  %971 = load i64* %rax
  %972 = trunc i64 %971 to i32
  store i32 %972, i32* bitcast (i64* @global_var_24a888 to i32*)
  br label %block_4eee

block_4eee:                                       ; preds = %block_4eda, %block_4ed0
  store volatile i64 20206, i64* @assembly_address
  %973 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %974 = zext i32 %973 to i64
  store i64 %974, i64* %rax
  store volatile i64 20212, i64* @assembly_address
  %975 = load i64* %rax
  %976 = trunc i64 %975 to i32
  %977 = load i64* %rax
  %978 = trunc i64 %977 to i32
  %979 = and i32 %976, %978
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %980 = icmp eq i32 %979, 0
  store i1 %980, i1* %zf
  %981 = icmp slt i32 %979, 0
  store i1 %981, i1* %sf
  %982 = trunc i32 %979 to i8
  %983 = call i8 @llvm.ctpop.i8(i8 %982)
  %984 = and i8 %983, 1
  %985 = icmp eq i8 %984, 0
  store i1 %985, i1* %pf
  store volatile i64 20214, i64* @assembly_address
  %986 = load i1* %zf
  br i1 %986, label %block_4f1a, label %block_4ef8

block_4ef8:                                       ; preds = %block_4eee
  store volatile i64 20216, i64* @assembly_address
  %987 = load i64* @global_var_216580
  store i64 %987, i64* %rax
  store volatile i64 20223, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdx
  store volatile i64 20230, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_116c3 to i64), i64* %rsi
  store volatile i64 20237, i64* @assembly_address
  %988 = load i64* %rax
  store i64 %988, i64* %rdi
  store volatile i64 20240, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 20245, i64* @assembly_address
  %989 = load i64* %rdi
  %990 = inttoptr i64 %989 to %_IO_FILE*
  %991 = load i64* %rsi
  %992 = inttoptr i64 %991 to i8*
  %993 = load i64* %rdx
  %994 = inttoptr i64 %993 to i8*
  %995 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %990, i8* %992, i8* %994)
  %996 = sext i32 %995 to i64
  store i64 %996, i64* %rax
  %997 = sext i32 %995 to i64
  store i64 %997, i64* %rax
  br label %block_4f1a

block_4f1a:                                       ; preds = %block_4f69, %block_4ef8, %block_4eee
  store volatile i64 20250, i64* @assembly_address
  %998 = load i64* @global_var_2160d0
  store i64 %998, i64* %rax
  store volatile i64 20257, i64* @assembly_address
  %999 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %1000 = zext i32 %999 to i64
  store i64 %1000, i64* %rcx
  store volatile i64 20263, i64* @assembly_address
  %1001 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %1002 = zext i32 %1001 to i64
  store i64 %1002, i64* %rdx
  store volatile i64 20269, i64* @assembly_address
  %1003 = load i64* %rcx
  %1004 = trunc i64 %1003 to i32
  %1005 = zext i32 %1004 to i64
  store i64 %1005, i64* %rsi
  store volatile i64 20271, i64* @assembly_address
  %1006 = load i64* %rdx
  %1007 = trunc i64 %1006 to i32
  %1008 = zext i32 %1007 to i64
  store i64 %1008, i64* %rdi
  store volatile i64 20273, i64* @assembly_address
  %1009 = load i64* %rdi
  %1010 = load i64* %rsi
  %1011 = trunc i64 %1009 to i32
  %1012 = call i64 @zip(i32 %1011, i64 %1010)
  store i64 %1012, i64* %rax
  store i64 %1012, i64* %rax
  store volatile i64 20275, i64* @assembly_address
  %1013 = load i64* %rax
  %1014 = trunc i64 %1013 to i32
  %1015 = load i64* %rax
  %1016 = trunc i64 %1015 to i32
  %1017 = and i32 %1014, %1016
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1018 = icmp eq i32 %1017, 0
  store i1 %1018, i1* %zf
  %1019 = icmp slt i32 %1017, 0
  store i1 %1019, i1* %sf
  %1020 = trunc i32 %1017 to i8
  %1021 = call i8 @llvm.ctpop.i8(i8 %1020)
  %1022 = and i8 %1021, 1
  %1023 = icmp eq i8 %1022, 0
  store i1 %1023, i1* %pf
  store volatile i64 20277, i64* @assembly_address
  %1024 = load i1* %zf
  br i1 %1024, label %block_4f43, label %block_4f37

block_4f37:                                       ; preds = %block_4f1a
  store volatile i64 20279, i64* @assembly_address
  store i32 -1, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 20289, i64* @assembly_address
  br label %block_4f7a

block_4f43:                                       ; preds = %block_4f1a
  store volatile i64 20291, i64* @assembly_address
  %1025 = call i64 @input_eof()
  store i64 %1025, i64* %rax
  store i64 %1025, i64* %rax
  store i64 %1025, i64* %rax
  store volatile i64 20296, i64* @assembly_address
  %1026 = load i64* %rax
  %1027 = trunc i64 %1026 to i32
  %1028 = load i64* %rax
  %1029 = trunc i64 %1028 to i32
  %1030 = and i32 %1027, %1029
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1031 = icmp eq i32 %1030, 0
  store i1 %1031, i1* %zf
  %1032 = icmp slt i32 %1030, 0
  store i1 %1032, i1* %sf
  %1033 = trunc i32 %1030 to i8
  %1034 = call i8 @llvm.ctpop.i8(i8 %1033)
  %1035 = and i8 %1034, 1
  %1036 = icmp eq i8 %1035, 0
  store i1 %1036, i1* %pf
  store volatile i64 20298, i64* @assembly_address
  %1037 = load i1* %zf
  %1038 = icmp eq i1 %1037, false
  br i1 %1038, label %block_4f76, label %block_4f4c

block_4f4c:                                       ; preds = %block_4f43
  store volatile i64 20300, i64* @assembly_address
  %1039 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %1040 = zext i32 %1039 to i64
  store i64 %1040, i64* %rax
  store volatile i64 20306, i64* @assembly_address
  %1041 = load i64* %rax
  %1042 = trunc i64 %1041 to i32
  %1043 = zext i32 %1042 to i64
  store i64 %1043, i64* %rdi
  store volatile i64 20308, i64* @assembly_address
  %1044 = load i64* %rdi
  %1045 = trunc i64 %1044 to i32
  %1046 = call i64 @get_method(i32 %1045)
  store i64 %1046, i64* %rax
  store i64 %1046, i64* %rax
  store volatile i64 20313, i64* @assembly_address
  %1047 = load i64* %rax
  %1048 = trunc i64 %1047 to i32
  store i32 %1048, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 20319, i64* @assembly_address
  %1049 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %1050 = zext i32 %1049 to i64
  store i64 %1050, i64* %rax
  store volatile i64 20325, i64* @assembly_address
  %1051 = load i64* %rax
  %1052 = trunc i64 %1051 to i32
  %1053 = load i64* %rax
  %1054 = trunc i64 %1053 to i32
  %1055 = and i32 %1052, %1054
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1056 = icmp eq i32 %1055, 0
  store i1 %1056, i1* %zf
  %1057 = icmp slt i32 %1055, 0
  store i1 %1057, i1* %sf
  %1058 = trunc i32 %1055 to i8
  %1059 = call i8 @llvm.ctpop.i8(i8 %1058)
  %1060 = and i8 %1059, 1
  %1061 = icmp eq i8 %1060, 0
  store i1 %1061, i1* %pf
  store volatile i64 20327, i64* @assembly_address
  %1062 = load i1* %sf
  br i1 %1062, label %block_4f79, label %block_4f69

block_4f69:                                       ; preds = %block_4f4c
  store volatile i64 20329, i64* @assembly_address
  store i64 0, i64* @global_var_25f4c0
  store volatile i64 20340, i64* @assembly_address
  br label %block_4f1a

block_4f76:                                       ; preds = %block_4f43
  store volatile i64 20342, i64* @assembly_address
  store volatile i64 20343, i64* @assembly_address
  br label %block_4f7a

block_4f79:                                       ; preds = %block_4f4c
  store volatile i64 20345, i64* @assembly_address
  br label %block_4f7a

block_4f7a:                                       ; preds = %block_4f79, %block_4f76, %block_4f37
  store volatile i64 20346, i64* @assembly_address
  %1063 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %1064 = zext i32 %1063 to i64
  store i64 %1064, i64* %rax
  store volatile i64 20352, i64* @assembly_address
  %1065 = load i64* %rax
  %1066 = trunc i64 %1065 to i32
  %1067 = zext i32 %1066 to i64
  store i64 %1067, i64* %rdi
  store volatile i64 20354, i64* @assembly_address
  %1068 = load i64* %rdi
  %1069 = trunc i64 %1068 to i32
  %1070 = call i32 @close(i32 %1069)
  %1071 = sext i32 %1070 to i64
  store i64 %1071, i64* %rax
  %1072 = sext i32 %1070 to i64
  store i64 %1072, i64* %rax
  store volatile i64 20359, i64* @assembly_address
  %1073 = load i64* %rax
  %1074 = trunc i64 %1073 to i32
  %1075 = load i64* %rax
  %1076 = trunc i64 %1075 to i32
  %1077 = and i32 %1074, %1076
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1078 = icmp eq i32 %1077, 0
  store i1 %1078, i1* %zf
  %1079 = icmp slt i32 %1077, 0
  store i1 %1079, i1* %sf
  %1080 = trunc i32 %1077 to i8
  %1081 = call i8 @llvm.ctpop.i8(i8 %1080)
  %1082 = and i8 %1081, 1
  %1083 = icmp eq i8 %1082, 0
  store i1 %1083, i1* %pf
  store volatile i64 20361, i64* @assembly_address
  %1084 = load i1* %zf
  br i1 %1084, label %block_4f90, label %block_4f8b

block_4f8b:                                       ; preds = %block_4f7a
  store volatile i64 20363, i64* @assembly_address
  %1085 = call i64 @read_error()
  store i64 %1085, i64* %rax
  store i64 %1085, i64* %rax
  store i64 %1085, i64* %rax
  unreachable

block_4f90:                                       ; preds = %block_4f7a
  store volatile i64 20368, i64* @assembly_address
  %1086 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1087 = zext i32 %1086 to i64
  store i64 %1087, i64* %rax
  store volatile i64 20374, i64* @assembly_address
  %1088 = load i64* %rax
  %1089 = trunc i64 %1088 to i32
  %1090 = load i64* %rax
  %1091 = trunc i64 %1090 to i32
  %1092 = and i32 %1089, %1091
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1093 = icmp eq i32 %1092, 0
  store i1 %1093, i1* %zf
  %1094 = icmp slt i32 %1092, 0
  store i1 %1094, i1* %sf
  %1095 = trunc i32 %1092 to i8
  %1096 = call i8 @llvm.ctpop.i8(i8 %1095)
  %1097 = and i8 %1096, 1
  %1098 = icmp eq i8 %1097, 0
  store i1 %1098, i1* %pf
  store volatile i64 20376, i64* @assembly_address
  %1099 = load i1* %zf
  %1100 = icmp eq i1 %1099, false
  br i1 %1100, label %block_516d, label %block_4f9e

block_4f9e:                                       ; preds = %block_4f90
  store volatile i64 20382, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216f00 to i64), i64* %rdi
  store volatile i64 20389, i64* @assembly_address
  %1101 = load i64* %rdi
  %1102 = inttoptr i64 %1101 to i64*
  %1103 = load i64* %rsi
  %1104 = load i64* %rdx
  %1105 = call i64 @copy_stat(i64* %1102, i64 %1103, i64 %1104)
  store i64 %1105, i64* %rax
  store i64 %1105, i64* %rax
  store volatile i64 20394, i64* @assembly_address
  %1106 = load i8* bitcast (i64* @global_var_2165f9 to i8*)
  %1107 = zext i8 %1106 to i64
  store i64 %1107, i64* %rax
  store volatile i64 20401, i64* @assembly_address
  %1108 = load i64* %rax
  %1109 = trunc i64 %1108 to i8
  %1110 = load i64* %rax
  %1111 = trunc i64 %1110 to i8
  %1112 = and i8 %1109, %1111
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1113 = icmp eq i8 %1112, 0
  store i1 %1113, i1* %zf
  %1114 = icmp slt i8 %1112, 0
  store i1 %1114, i1* %sf
  %1115 = call i8 @llvm.ctpop.i8(i8 %1112)
  %1116 = and i8 %1115, 1
  %1117 = icmp eq i8 %1116, 0
  store i1 %1117, i1* %pf
  store volatile i64 20403, i64* @assembly_address
  %1118 = load i1* %zf
  br i1 %1118, label %block_4ff9, label %block_4fb5

block_4fb5:                                       ; preds = %block_4f9e
  store volatile i64 20405, i64* @assembly_address
  %1119 = load i32* bitcast (i64* @global_var_2160a8 to i32*)
  %1120 = zext i32 %1119 to i64
  store i64 %1120, i64* %rax
  store volatile i64 20411, i64* @assembly_address
  %1121 = load i64* %rax
  %1122 = trunc i64 %1121 to i32
  %1123 = load i64* %rax
  %1124 = trunc i64 %1123 to i32
  %1125 = and i32 %1122, %1124
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1126 = icmp eq i32 %1125, 0
  store i1 %1126, i1* %zf
  %1127 = icmp slt i32 %1125, 0
  store i1 %1127, i1* %sf
  %1128 = trunc i32 %1125 to i8
  %1129 = call i8 @llvm.ctpop.i8(i8 %1128)
  %1130 = and i8 %1129, 1
  %1131 = icmp eq i8 %1130, 0
  store i1 %1131, i1* %pf
  store volatile i64 20413, i64* @assembly_address
  %1132 = load i1* %sf
  br i1 %1132, label %block_4fdc, label %block_4fbf

block_4fbf:                                       ; preds = %block_4fb5
  store volatile i64 20415, i64* @assembly_address
  %1133 = load i32* bitcast (i64* @global_var_2160a8 to i32*)
  %1134 = zext i32 %1133 to i64
  store i64 %1134, i64* %rax
  store volatile i64 20421, i64* @assembly_address
  %1135 = load i64* %rax
  %1136 = trunc i64 %1135 to i32
  %1137 = zext i32 %1136 to i64
  store i64 %1137, i64* %rdi
  store volatile i64 20423, i64* @assembly_address
  %1138 = load i64* %rdi
  %1139 = trunc i64 %1138 to i32
  %1140 = call i32 @fdatasync(i32 %1139)
  %1141 = sext i32 %1140 to i64
  store i64 %1141, i64* %rax
  %1142 = sext i32 %1140 to i64
  store i64 %1142, i64* %rax
  store volatile i64 20428, i64* @assembly_address
  %1143 = load i64* %rax
  %1144 = trunc i64 %1143 to i32
  %1145 = load i64* %rax
  %1146 = trunc i64 %1145 to i32
  %1147 = and i32 %1144, %1146
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1148 = icmp eq i32 %1147, 0
  store i1 %1148, i1* %zf
  %1149 = icmp slt i32 %1147, 0
  store i1 %1149, i1* %sf
  %1150 = trunc i32 %1147 to i8
  %1151 = call i8 @llvm.ctpop.i8(i8 %1150)
  %1152 = and i8 %1151, 1
  %1153 = icmp eq i8 %1152, 0
  store i1 %1153, i1* %pf
  store volatile i64 20430, i64* @assembly_address
  %1154 = load i1* %zf
  br i1 %1154, label %block_4fdc, label %block_4fd0

block_4fd0:                                       ; preds = %block_4fbf
  store volatile i64 20432, i64* @assembly_address
  %1155 = call i32* @__errno_location()
  %1156 = ptrtoint i32* %1155 to i64
  store i64 %1156, i64* %rax
  %1157 = ptrtoint i32* %1155 to i64
  store i64 %1157, i64* %rax
  %1158 = ptrtoint i32* %1155 to i64
  store i64 %1158, i64* %rax
  store volatile i64 20437, i64* @assembly_address
  %1159 = load i64* %rax
  %1160 = inttoptr i64 %1159 to i32*
  %1161 = load i32* %1160
  %1162 = zext i32 %1161 to i64
  store i64 %1162, i64* %rax
  store volatile i64 20439, i64* @assembly_address
  %1163 = load i64* %rax
  %1164 = trunc i64 %1163 to i32
  %1165 = sub i32 %1164, 22
  %1166 = and i32 %1164, 15
  %1167 = sub i32 %1166, 6
  %1168 = icmp ugt i32 %1167, 15
  %1169 = icmp ult i32 %1164, 22
  %1170 = xor i32 %1164, 22
  %1171 = xor i32 %1164, %1165
  %1172 = and i32 %1170, %1171
  %1173 = icmp slt i32 %1172, 0
  store i1 %1168, i1* %az
  store i1 %1169, i1* %cf
  store i1 %1173, i1* %of
  %1174 = icmp eq i32 %1165, 0
  store i1 %1174, i1* %zf
  %1175 = icmp slt i32 %1165, 0
  store i1 %1175, i1* %sf
  %1176 = trunc i32 %1165 to i8
  %1177 = call i8 @llvm.ctpop.i8(i8 %1176)
  %1178 = and i8 %1177, 1
  %1179 = icmp eq i8 %1178, 0
  store i1 %1179, i1* %pf
  store volatile i64 20442, i64* @assembly_address
  %1180 = load i1* %zf
  %1181 = icmp eq i1 %1180, false
  br i1 %1181, label %block_500a, label %block_4fdc

block_4fdc:                                       ; preds = %block_4fd0, %block_4fbf, %block_4fb5
  store volatile i64 20444, i64* @assembly_address
  %1182 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %1183 = zext i32 %1182 to i64
  store i64 %1183, i64* %rax
  store volatile i64 20450, i64* @assembly_address
  %1184 = load i64* %rax
  %1185 = trunc i64 %1184 to i32
  %1186 = zext i32 %1185 to i64
  store i64 %1186, i64* %rdi
  store volatile i64 20452, i64* @assembly_address
  %1187 = load i64* %rdi
  %1188 = trunc i64 %1187 to i32
  %1189 = call i32 @fsync(i32 %1188)
  %1190 = sext i32 %1189 to i64
  store i64 %1190, i64* %rax
  %1191 = sext i32 %1189 to i64
  store i64 %1191, i64* %rax
  store volatile i64 20457, i64* @assembly_address
  %1192 = load i64* %rax
  %1193 = trunc i64 %1192 to i32
  %1194 = load i64* %rax
  %1195 = trunc i64 %1194 to i32
  %1196 = and i32 %1193, %1195
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1197 = icmp eq i32 %1196, 0
  store i1 %1197, i1* %zf
  %1198 = icmp slt i32 %1196, 0
  store i1 %1198, i1* %sf
  %1199 = trunc i32 %1196 to i8
  %1200 = call i8 @llvm.ctpop.i8(i8 %1199)
  %1201 = and i8 %1200, 1
  %1202 = icmp eq i8 %1201, 0
  store i1 %1202, i1* %pf
  store volatile i64 20459, i64* @assembly_address
  %1203 = load i1* %zf
  br i1 %1203, label %block_4ff9, label %block_4fed

block_4fed:                                       ; preds = %block_4fdc
  store volatile i64 20461, i64* @assembly_address
  %1204 = call i32* @__errno_location()
  %1205 = ptrtoint i32* %1204 to i64
  store i64 %1205, i64* %rax
  %1206 = ptrtoint i32* %1204 to i64
  store i64 %1206, i64* %rax
  %1207 = ptrtoint i32* %1204 to i64
  store i64 %1207, i64* %rax
  store volatile i64 20466, i64* @assembly_address
  %1208 = load i64* %rax
  %1209 = inttoptr i64 %1208 to i32*
  %1210 = load i32* %1209
  %1211 = zext i32 %1210 to i64
  store i64 %1211, i64* %rax
  store volatile i64 20468, i64* @assembly_address
  %1212 = load i64* %rax
  %1213 = trunc i64 %1212 to i32
  %1214 = sub i32 %1213, 22
  %1215 = and i32 %1213, 15
  %1216 = sub i32 %1215, 6
  %1217 = icmp ugt i32 %1216, 15
  %1218 = icmp ult i32 %1213, 22
  %1219 = xor i32 %1213, 22
  %1220 = xor i32 %1213, %1214
  %1221 = and i32 %1219, %1220
  %1222 = icmp slt i32 %1221, 0
  store i1 %1217, i1* %az
  store i1 %1218, i1* %cf
  store i1 %1222, i1* %of
  %1223 = icmp eq i32 %1214, 0
  store i1 %1223, i1* %zf
  %1224 = icmp slt i32 %1214, 0
  store i1 %1224, i1* %sf
  %1225 = trunc i32 %1214 to i8
  %1226 = call i8 @llvm.ctpop.i8(i8 %1225)
  %1227 = and i8 %1226, 1
  %1228 = icmp eq i8 %1227, 0
  store i1 %1228, i1* %pf
  store volatile i64 20471, i64* @assembly_address
  %1229 = load i1* %zf
  %1230 = icmp eq i1 %1229, false
  br i1 %1230, label %block_500a, label %block_4ff9

block_4ff9:                                       ; preds = %block_4fed, %block_4fdc, %block_4f9e
  store volatile i64 20473, i64* @assembly_address
  %1231 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %1232 = zext i32 %1231 to i64
  store i64 %1232, i64* %rax
  store volatile i64 20479, i64* @assembly_address
  %1233 = load i64* %rax
  %1234 = trunc i64 %1233 to i32
  %1235 = zext i32 %1234 to i64
  store i64 %1235, i64* %rdi
  store volatile i64 20481, i64* @assembly_address
  %1236 = load i64* %rdi
  %1237 = trunc i64 %1236 to i32
  %1238 = call i32 @close(i32 %1237)
  %1239 = sext i32 %1238 to i64
  store i64 %1239, i64* %rax
  %1240 = sext i32 %1238 to i64
  store i64 %1240, i64* %rax
  store volatile i64 20486, i64* @assembly_address
  %1241 = load i64* %rax
  %1242 = trunc i64 %1241 to i32
  %1243 = load i64* %rax
  %1244 = trunc i64 %1243 to i32
  %1245 = and i32 %1242, %1244
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1246 = icmp eq i32 %1245, 0
  store i1 %1246, i1* %zf
  %1247 = icmp slt i32 %1245, 0
  store i1 %1247, i1* %sf
  %1248 = trunc i32 %1245 to i8
  %1249 = call i8 @llvm.ctpop.i8(i8 %1248)
  %1250 = and i8 %1249, 1
  %1251 = icmp eq i8 %1250, 0
  store i1 %1251, i1* %pf
  store volatile i64 20488, i64* @assembly_address
  %1252 = load i1* %zf
  br i1 %1252, label %block_500f, label %block_500a

block_500a:                                       ; preds = %block_4ff9, %block_4fed, %block_4fd0
  store volatile i64 20490, i64* @assembly_address
  %1253 = call i64 @write_error()
  store i64 %1253, i64* %rax
  store i64 %1253, i64* %rax
  store i64 %1253, i64* %rax
  unreachable

block_500f:                                       ; preds = %block_4ff9
  store volatile i64 20495, i64* @assembly_address
  %1254 = load i32* bitcast (i64* @global_var_216608 to i32*)
  %1255 = zext i32 %1254 to i64
  store i64 %1255, i64* %rax
  store volatile i64 20501, i64* @assembly_address
  %1256 = load i64* %rax
  %1257 = trunc i64 %1256 to i32
  %1258 = load i64* %rax
  %1259 = trunc i64 %1258 to i32
  %1260 = and i32 %1257, %1259
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1261 = icmp eq i32 %1260, 0
  store i1 %1261, i1* %zf
  %1262 = icmp slt i32 %1260, 0
  store i1 %1262, i1* %sf
  %1263 = trunc i32 %1260 to i8
  %1264 = call i8 @llvm.ctpop.i8(i8 %1263)
  %1265 = and i8 %1264, 1
  %1266 = icmp eq i8 %1265, 0
  store i1 %1266, i1* %pf
  store volatile i64 20503, i64* @assembly_address
  %1267 = load i1* %zf
  %1268 = icmp eq i1 %1267, false
  br i1 %1268, label %block_516d, label %block_501d

block_501d:                                       ; preds = %block_500f
  store volatile i64 20509, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 20516, i64* @assembly_address
  %1269 = load i64* %rdi
  %1270 = inttoptr i64 %1269 to i64*
  %1271 = bitcast i64* %1270 to i8*
  %1272 = call i64 @last_component(i8* %1271)
  store i64 %1272, i64* %rax
  store i64 %1272, i64* %rax
  store volatile i64 20521, i64* @assembly_address
  %1273 = load i64* %rax
  %1274 = inttoptr i64 %1273 to i8*
  store i8* %1274, i8** %stack_var_-160
  store volatile i64 20528, i64* @assembly_address
  %1275 = load i8** %stack_var_-160
  %1276 = ptrtoint i8* %1275 to i64
  store i64 %1276, i64* %rdx
  store volatile i64 20535, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rax
  store volatile i64 20542, i64* @assembly_address
  %1277 = load i64* %rdx
  %1278 = load i64* %rax
  %1279 = sub i64 %1277, %1278
  %1280 = and i64 %1277, 15
  %1281 = and i64 %1278, 15
  %1282 = sub i64 %1280, %1281
  %1283 = icmp ugt i64 %1282, 15
  %1284 = icmp ult i64 %1277, %1278
  %1285 = xor i64 %1277, %1278
  %1286 = xor i64 %1277, %1279
  %1287 = and i64 %1285, %1286
  %1288 = icmp slt i64 %1287, 0
  store i1 %1283, i1* %az
  store i1 %1284, i1* %cf
  store i1 %1288, i1* %of
  %1289 = icmp eq i64 %1279, 0
  store i1 %1289, i1* %zf
  %1290 = icmp slt i64 %1279, 0
  store i1 %1290, i1* %sf
  %1291 = trunc i64 %1279 to i8
  %1292 = call i8 @llvm.ctpop.i8(i8 %1291)
  %1293 = and i8 %1292, 1
  %1294 = icmp eq i8 %1293, 0
  store i1 %1294, i1* %pf
  store i64 %1279, i64* %rdx
  store volatile i64 20545, i64* @assembly_address
  %1295 = load i64* %rdx
  store i64 %1295, i64* %rax
  store volatile i64 20548, i64* @assembly_address
  %1296 = load i64* %rax
  store i64 %1296, i64* %rsi
  store volatile i64 20551, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 20558, i64* @assembly_address
  %1297 = load i64* %rdi
  %1298 = load i64* %rsi
  %1299 = trunc i64 %1298 to i32
  %1300 = call i64 @atdir_eq(i64 %1297, i32 %1299)
  store i64 %1300, i64* %rax
  store i64 %1300, i64* %rax
  store volatile i64 20563, i64* @assembly_address
  %1301 = load i64* %rax
  %1302 = trunc i64 %1301 to i8
  %1303 = load i64* %rax
  %1304 = trunc i64 %1303 to i8
  %1305 = and i8 %1302, %1304
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1306 = icmp eq i8 %1305, 0
  store i1 %1306, i1* %zf
  %1307 = icmp slt i8 %1305, 0
  store i1 %1307, i1* %sf
  %1308 = call i8 @llvm.ctpop.i8(i8 %1305)
  %1309 = and i8 %1308, 1
  %1310 = icmp eq i8 %1309, 0
  store i1 %1310, i1* %pf
  store volatile i64 20565, i64* @assembly_address
  %1311 = load i1* %zf
  br i1 %1311, label %block_505f, label %block_5057

block_5057:                                       ; preds = %block_501d
  store volatile i64 20567, i64* @assembly_address
  %1312 = load i32* bitcast (i64* @global_var_2160a8 to i32*)
  %1313 = zext i32 %1312 to i64
  store i64 %1313, i64* %rax
  store volatile i64 20573, i64* @assembly_address
  br label %block_5064

block_505f:                                       ; preds = %block_501d
  store volatile i64 20575, i64* @assembly_address
  store i64 4294967295, i64* %rax
  br label %block_5064

block_5064:                                       ; preds = %block_505f, %block_5057
  store volatile i64 20580, i64* @assembly_address
  %1314 = load i64* %rax
  %1315 = trunc i64 %1314 to i32
  store i32 %1315, i32* %stack_var_-176
  store volatile i64 20586, i64* @assembly_address
  %1316 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %1316, i64* %rax
  store volatile i64 20593, i64* @assembly_address
  %1317 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %1317, i64* %rdx
  store volatile i64 20596, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216640 to i64), i64* %rsi
  store volatile i64 20603, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 20608, i64* @assembly_address
  %1318 = load i64* %rdi
  %1319 = trunc i64 %1318 to i32
  %1320 = load i64* %rsi
  %1321 = inttoptr i64 %1320 to %_TYPEDEF_sigset_t*
  %1322 = load i64* %rdx
  %1323 = inttoptr i64 %1322 to %_TYPEDEF_sigset_t*
  %1324 = call i32 @sigprocmask(i32 %1319, %_TYPEDEF_sigset_t* %1321, %_TYPEDEF_sigset_t* %1323)
  %1325 = sext i32 %1324 to i64
  store i64 %1325, i64* %rax
  %1326 = sext i32 %1324 to i64
  store i64 %1326, i64* %rax
  store volatile i64 20613, i64* @assembly_address
  store i32 -1, i32* bitcast (i64* @global_var_2160a4 to i32*)
  store volatile i64 20623, i64* @assembly_address
  %1327 = load i32* %stack_var_-176
  %1328 = and i32 %1327, 15
  %1329 = icmp ugt i32 %1328, 15
  %1330 = icmp ult i32 %1327, 0
  %1331 = xor i32 %1327, 0
  %1332 = and i32 %1331, 0
  %1333 = icmp slt i32 %1332, 0
  store i1 %1329, i1* %az
  store i1 %1330, i1* %cf
  store i1 %1333, i1* %of
  %1334 = icmp eq i32 %1327, 0
  store i1 %1334, i1* %zf
  %1335 = icmp slt i32 %1327, 0
  store i1 %1335, i1* %sf
  %1336 = trunc i32 %1327 to i8
  %1337 = call i8 @llvm.ctpop.i8(i8 %1336)
  %1338 = and i8 %1337, 1
  %1339 = icmp eq i8 %1338, 0
  store i1 %1339, i1* %pf
  store volatile i64 20630, i64* @assembly_address
  %1340 = load i1* %sf
  %1341 = icmp eq i1 %1340, false
  br i1 %1341, label %block_50a6, label %block_5098

block_5098:                                       ; preds = %block_5064
  store volatile i64 20632, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 20639, i64* @assembly_address
  %1342 = load i64* %rdi
  %1343 = inttoptr i64 %1342 to i64*
  %1344 = bitcast i64* %1343 to i8*
  %1345 = call i64 @xunlink(i8* %1344)
  store i64 %1345, i64* %rax
  store i64 %1345, i64* %rax
  store volatile i64 20644, i64* @assembly_address
  br label %block_50c2

block_50a6:                                       ; preds = %block_5064
  store volatile i64 20646, i64* @assembly_address
  %1346 = load i8** %stack_var_-160
  %1347 = ptrtoint i8* %1346 to i64
  store i64 %1347, i64* %rcx
  store volatile i64 20653, i64* @assembly_address
  %1348 = load i32* %stack_var_-176
  %1349 = zext i32 %1348 to i64
  store i64 %1349, i64* %rax
  store volatile i64 20659, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 20664, i64* @assembly_address
  %1350 = load i64* %rcx
  store i64 %1350, i64* %rsi
  store volatile i64 20667, i64* @assembly_address
  %1351 = load i64* %rax
  %1352 = trunc i64 %1351 to i32
  %1353 = zext i32 %1352 to i64
  store i64 %1353, i64* %rdi
  store volatile i64 20669, i64* @assembly_address
  %1354 = load i64* %rdi
  %1355 = trunc i64 %1354 to i32
  %1356 = load i64* %rsi
  %1357 = inttoptr i64 %1356 to i8*
  %1358 = load i64* %rdx
  %1359 = trunc i64 %1358 to i32
  %1360 = call i32 @unlinkat(i32 %1355, i8* %1357, i32 %1359)
  %1361 = sext i32 %1360 to i64
  store i64 %1361, i64* %rax
  %1362 = sext i32 %1360 to i64
  store i64 %1362, i64* %rax
  br label %block_50c2

block_50c2:                                       ; preds = %block_50a6, %block_5098
  store volatile i64 20674, i64* @assembly_address
  %1363 = load i64* %rax
  %1364 = trunc i64 %1363 to i32
  store i32 %1364, i32* %stack_var_-172
  store volatile i64 20680, i64* @assembly_address
  %1365 = load i32* %stack_var_-172
  %1366 = and i32 %1365, 15
  %1367 = icmp ugt i32 %1366, 15
  %1368 = icmp ult i32 %1365, 0
  %1369 = xor i32 %1365, 0
  %1370 = and i32 %1369, 0
  %1371 = icmp slt i32 %1370, 0
  store i1 %1367, i1* %az
  store i1 %1368, i1* %cf
  store i1 %1371, i1* %of
  %1372 = icmp eq i32 %1365, 0
  store i1 %1372, i1* %zf
  %1373 = icmp slt i32 %1365, 0
  store i1 %1373, i1* %sf
  %1374 = trunc i32 %1365 to i8
  %1375 = call i8 @llvm.ctpop.i8(i8 %1374)
  %1376 = and i8 %1375, 1
  %1377 = icmp eq i8 %1376, 0
  store i1 %1377, i1* %pf
  store volatile i64 20687, i64* @assembly_address
  %1378 = load i1* %zf
  br i1 %1378, label %block_50da, label %block_50d1

block_50d1:                                       ; preds = %block_50c2
  store volatile i64 20689, i64* @assembly_address
  %1379 = call i32* @__errno_location()
  %1380 = ptrtoint i32* %1379 to i64
  store i64 %1380, i64* %rax
  %1381 = ptrtoint i32* %1379 to i64
  store i64 %1381, i64* %rax
  %1382 = ptrtoint i32* %1379 to i64
  store i64 %1382, i64* %rax
  store volatile i64 20694, i64* @assembly_address
  %1383 = load i64* %rax
  %1384 = inttoptr i64 %1383 to i32*
  %1385 = load i32* %1384
  %1386 = zext i32 %1385 to i64
  store i64 %1386, i64* %rax
  store volatile i64 20696, i64* @assembly_address
  br label %block_50df

block_50da:                                       ; preds = %block_50c2
  store volatile i64 20698, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_50df

block_50df:                                       ; preds = %block_50da, %block_50d1
  store volatile i64 20703, i64* @assembly_address
  %1387 = load i64* %rax
  %1388 = trunc i64 %1387 to i32
  store i32 %1388, i32* %stack_var_-168
  store volatile i64 20709, i64* @assembly_address
  %1389 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %1389, i64* %rax
  store volatile i64 20716, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 20721, i64* @assembly_address
  %1390 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %1390, i64* %rsi
  store volatile i64 20724, i64* @assembly_address
  store i64 2, i64* %rdi
  store volatile i64 20729, i64* @assembly_address
  %1391 = load i64* %rdi
  %1392 = trunc i64 %1391 to i32
  %1393 = load i64* %rsi
  %1394 = inttoptr i64 %1393 to %_TYPEDEF_sigset_t*
  %1395 = load i64* %rdx
  %1396 = inttoptr i64 %1395 to %_TYPEDEF_sigset_t*
  %1397 = call i32 @sigprocmask(i32 %1392, %_TYPEDEF_sigset_t* %1394, %_TYPEDEF_sigset_t* %1396)
  %1398 = sext i32 %1397 to i64
  store i64 %1398, i64* %rax
  %1399 = sext i32 %1397 to i64
  store i64 %1399, i64* %rax
  store volatile i64 20734, i64* @assembly_address
  %1400 = load i32* %stack_var_-168
  %1401 = and i32 %1400, 15
  %1402 = icmp ugt i32 %1401, 15
  %1403 = icmp ult i32 %1400, 0
  %1404 = xor i32 %1400, 0
  %1405 = and i32 %1404, 0
  %1406 = icmp slt i32 %1405, 0
  store i1 %1402, i1* %az
  store i1 %1403, i1* %cf
  store i1 %1406, i1* %of
  %1407 = icmp eq i32 %1400, 0
  store i1 %1407, i1* %zf
  %1408 = icmp slt i32 %1400, 0
  store i1 %1408, i1* %sf
  %1409 = trunc i32 %1400 to i8
  %1410 = call i8 @llvm.ctpop.i8(i8 %1409)
  %1411 = and i8 %1410, 1
  %1412 = icmp eq i8 %1411, 0
  store i1 %1412, i1* %pf
  store volatile i64 20741, i64* @assembly_address
  %1413 = load i1* %zf
  br i1 %1413, label %block_516d, label %block_5107

block_5107:                                       ; preds = %block_50df
  store volatile i64 20743, i64* @assembly_address
  %1414 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %1415 = zext i32 %1414 to i64
  store i64 %1415, i64* %rax
  store volatile i64 20749, i64* @assembly_address
  %1416 = load i64* %rax
  %1417 = trunc i64 %1416 to i32
  %1418 = load i64* %rax
  %1419 = trunc i64 %1418 to i32
  %1420 = and i32 %1417, %1419
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1421 = icmp eq i32 %1420, 0
  store i1 %1421, i1* %zf
  %1422 = icmp slt i32 %1420, 0
  store i1 %1422, i1* %sf
  %1423 = trunc i32 %1420 to i8
  %1424 = call i8 @llvm.ctpop.i8(i8 %1423)
  %1425 = and i8 %1424, 1
  %1426 = icmp eq i8 %1425, 0
  store i1 %1426, i1* %pf
  store volatile i64 20751, i64* @assembly_address
  %1427 = load i1* %zf
  %1428 = icmp eq i1 %1427, false
  br i1 %1428, label %block_5133, label %block_5111

block_5111:                                       ; preds = %block_5107
  store volatile i64 20753, i64* @assembly_address
  %1429 = load i64* @global_var_25f4c8
  store i64 %1429, i64* %rdx
  store volatile i64 20760, i64* @assembly_address
  %1430 = load i64* @global_var_216580
  store i64 %1430, i64* %rax
  store volatile i64 20767, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11035 to i64), i64* %rsi
  store volatile i64 20774, i64* @assembly_address
  %1431 = load i64* %rax
  store i64 %1431, i64* %rdi
  store volatile i64 20777, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 20782, i64* @assembly_address
  %1432 = load i64* %rdi
  %1433 = inttoptr i64 %1432 to %_IO_FILE*
  %1434 = load i64* %rsi
  %1435 = inttoptr i64 %1434 to i8*
  %1436 = load i64* %rdx
  %1437 = inttoptr i64 %1436 to i8*
  %1438 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1433, i8* %1435, i8* %1437)
  %1439 = sext i32 %1438 to i64
  store i64 %1439, i64* %rax
  %1440 = sext i32 %1438 to i64
  store i64 %1440, i64* %rax
  br label %block_5133

block_5133:                                       ; preds = %block_5111, %block_5107
  store volatile i64 20787, i64* @assembly_address
  %1441 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %1442 = zext i32 %1441 to i64
  store i64 %1442, i64* %rax
  store volatile i64 20793, i64* @assembly_address
  %1443 = load i64* %rax
  %1444 = trunc i64 %1443 to i32
  %1445 = load i64* %rax
  %1446 = trunc i64 %1445 to i32
  %1447 = and i32 %1444, %1446
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1448 = icmp eq i32 %1447, 0
  store i1 %1448, i1* %zf
  %1449 = icmp slt i32 %1447, 0
  store i1 %1449, i1* %sf
  %1450 = trunc i32 %1447 to i8
  %1451 = call i8 @llvm.ctpop.i8(i8 %1450)
  %1452 = and i8 %1451, 1
  %1453 = icmp eq i8 %1452, 0
  store i1 %1453, i1* %pf
  store volatile i64 20795, i64* @assembly_address
  %1454 = load i1* %zf
  %1455 = icmp eq i1 %1454, false
  br i1 %1455, label %block_5147, label %block_513d

block_513d:                                       ; preds = %block_5133
  store volatile i64 20797, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_5147

block_5147:                                       ; preds = %block_513d, %block_5133
  store volatile i64 20807, i64* @assembly_address
  %1456 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %1457 = zext i32 %1456 to i64
  store i64 %1457, i64* %rax
  store volatile i64 20813, i64* @assembly_address
  %1458 = load i64* %rax
  %1459 = trunc i64 %1458 to i32
  %1460 = load i64* %rax
  %1461 = trunc i64 %1460 to i32
  %1462 = and i32 %1459, %1461
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1463 = icmp eq i32 %1462, 0
  store i1 %1463, i1* %zf
  %1464 = icmp slt i32 %1462, 0
  store i1 %1464, i1* %sf
  %1465 = trunc i32 %1462 to i8
  %1466 = call i8 @llvm.ctpop.i8(i8 %1465)
  %1467 = and i8 %1466, 1
  %1468 = icmp eq i8 %1467, 0
  store i1 %1468, i1* %pf
  store volatile i64 20815, i64* @assembly_address
  %1469 = load i1* %zf
  %1470 = icmp eq i1 %1469, false
  br i1 %1470, label %block_516d, label %block_5151

block_5151:                                       ; preds = %block_5147
  store volatile i64 20817, i64* @assembly_address
  %1471 = call i32* @__errno_location()
  %1472 = ptrtoint i32* %1471 to i64
  store i64 %1472, i64* %rax
  %1473 = ptrtoint i32* %1471 to i64
  store i64 %1473, i64* %rax
  %1474 = ptrtoint i32* %1471 to i64
  store i64 %1474, i64* %rax
  store volatile i64 20822, i64* @assembly_address
  %1475 = load i64* %rax
  store i64 %1475, i64* %rdx
  store volatile i64 20825, i64* @assembly_address
  %1476 = load i32* %stack_var_-168
  %1477 = zext i32 %1476 to i64
  store i64 %1477, i64* %rax
  store volatile i64 20831, i64* @assembly_address
  %1478 = load i64* %rax
  %1479 = trunc i64 %1478 to i32
  %1480 = load i64* %rdx
  %1481 = inttoptr i64 %1480 to i32*
  store i32 %1479, i32* %1481
  store volatile i64 20833, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 20840, i64* @assembly_address
  %1482 = load i64* %rdi
  %1483 = inttoptr i64 %1482 to i8*
  call void @perror(i8* %1483)
  br label %block_516d

block_516d:                                       ; preds = %block_5151, %block_5147, %block_50df, %block_500f, %block_4f90
  store volatile i64 20845, i64* @assembly_address
  %1484 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %1485 = zext i32 %1484 to i64
  store i64 %1485, i64* %rax
  store volatile i64 20851, i64* @assembly_address
  %1486 = load i64* %rax
  %1487 = trunc i64 %1486 to i32
  %1488 = sub i32 %1487, -1
  %1489 = and i32 %1487, 15
  %1490 = sub i32 %1489, 15
  %1491 = icmp ugt i32 %1490, 15
  %1492 = icmp ult i32 %1487, -1
  %1493 = xor i32 %1487, -1
  %1494 = xor i32 %1487, %1488
  %1495 = and i32 %1493, %1494
  %1496 = icmp slt i32 %1495, 0
  store i1 %1491, i1* %az
  store i1 %1492, i1* %cf
  store i1 %1496, i1* %of
  %1497 = icmp eq i32 %1488, 0
  store i1 %1497, i1* %zf
  %1498 = icmp slt i32 %1488, 0
  store i1 %1498, i1* %sf
  %1499 = trunc i32 %1488 to i8
  %1500 = call i8 @llvm.ctpop.i8(i8 %1499)
  %1501 = and i8 %1500, 1
  %1502 = icmp eq i8 %1501, 0
  store i1 %1502, i1* %pf
  store volatile i64 20854, i64* @assembly_address
  %1503 = load i1* %zf
  %1504 = icmp eq i1 %1503, false
  br i1 %1504, label %block_5195, label %block_5178

block_5178:                                       ; preds = %block_516d
  store volatile i64 20856, i64* @assembly_address
  %1505 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1506 = zext i32 %1505 to i64
  store i64 %1506, i64* %rax
  store volatile i64 20862, i64* @assembly_address
  %1507 = load i64* %rax
  %1508 = trunc i64 %1507 to i32
  %1509 = load i64* %rax
  %1510 = trunc i64 %1509 to i32
  %1511 = and i32 %1508, %1510
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1512 = icmp eq i32 %1511, 0
  store i1 %1512, i1* %zf
  %1513 = icmp slt i32 %1511, 0
  store i1 %1513, i1* %sf
  %1514 = trunc i32 %1511 to i8
  %1515 = call i8 @llvm.ctpop.i8(i8 %1514)
  %1516 = and i8 %1515, 1
  %1517 = icmp eq i8 %1516, 0
  store i1 %1517, i1* %pf
  store volatile i64 20864, i64* @assembly_address
  %1518 = load i1* %zf
  %1519 = icmp eq i1 %1518, false
  br i1 %1519, label %block_52bb, label %block_5186

block_5186:                                       ; preds = %block_5178
  store volatile i64 20870, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 20875, i64* @assembly_address
  %1520 = load i64* %rdi
  %1521 = trunc i64 %1520 to i32
  %1522 = call i64 @remove_output_file(i32 %1521)
  store i64 %1522, i64* %rax
  store i64 %1522, i64* %rax
  store volatile i64 20880, i64* @assembly_address
  br label %block_52bb

block_5195:                                       ; preds = %block_516d
  store volatile i64 20885, i64* @assembly_address
  %1523 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %1524 = zext i32 %1523 to i64
  store i64 %1524, i64* %rax
  store volatile i64 20891, i64* @assembly_address
  %1525 = load i64* %rax
  %1526 = trunc i64 %1525 to i32
  %1527 = load i64* %rax
  %1528 = trunc i64 %1527 to i32
  %1529 = and i32 %1526, %1528
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1530 = icmp eq i32 %1529, 0
  store i1 %1530, i1* %zf
  %1531 = icmp slt i32 %1529, 0
  store i1 %1531, i1* %sf
  %1532 = trunc i32 %1529 to i8
  %1533 = call i8 @llvm.ctpop.i8(i8 %1532)
  %1534 = and i8 %1533, 1
  %1535 = icmp eq i8 %1534, 0
  store i1 %1535, i1* %pf
  store volatile i64 20893, i64* @assembly_address
  %1536 = load i1* %zf
  br i1 %1536, label %block_52bc, label %block_51a3

block_51a3:                                       ; preds = %block_5195
  store volatile i64 20899, i64* @assembly_address
  %1537 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %1538 = zext i32 %1537 to i64
  store i64 %1538, i64* %rax
  store volatile i64 20905, i64* @assembly_address
  %1539 = load i64* %rax
  %1540 = trunc i64 %1539 to i32
  %1541 = load i64* %rax
  %1542 = trunc i64 %1541 to i32
  %1543 = and i32 %1540, %1542
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1544 = icmp eq i32 %1543, 0
  store i1 %1544, i1* %zf
  %1545 = icmp slt i32 %1543, 0
  store i1 %1545, i1* %sf
  %1546 = trunc i32 %1543 to i8
  %1547 = call i8 @llvm.ctpop.i8(i8 %1546)
  %1548 = and i8 %1547, 1
  %1549 = icmp eq i8 %1548, 0
  store i1 %1549, i1* %pf
  store volatile i64 20907, i64* @assembly_address
  %1550 = load i1* %zf
  br i1 %1550, label %block_51cf, label %block_51ad

block_51ad:                                       ; preds = %block_51a3
  store volatile i64 20909, i64* @assembly_address
  %1551 = load i64* @global_var_216580
  store i64 %1551, i64* %rax
  store volatile i64 20916, i64* @assembly_address
  %1552 = load i64* %rax
  store i64 %1552, i64* %rcx
  store volatile i64 20919, i64* @assembly_address
  store i64 3, i64* %rdx
  store volatile i64 20924, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 20929, i64* @assembly_address
  store i64 ptrtoint ([4 x i8]* @global_var_116c8 to i64), i64* %rdi
  store volatile i64 20936, i64* @assembly_address
  %1553 = load i64* %rdi
  %1554 = inttoptr i64 %1553 to i64*
  %1555 = load i64* %rsi
  %1556 = trunc i64 %1555 to i32
  %1557 = load i64* %rdx
  %1558 = trunc i64 %1557 to i32
  %1559 = load i64* %rcx
  %1560 = inttoptr i64 %1559 to %_IO_FILE*
  %1561 = call i32 @fwrite(i64* %1554, i32 %1556, i32 %1558, %_IO_FILE* %1560)
  %1562 = sext i32 %1561 to i64
  store i64 %1562, i64* %rax
  %1563 = sext i32 %1561 to i64
  store i64 %1563, i64* %rax
  store volatile i64 20941, i64* @assembly_address
  br label %block_5249

block_51cf:                                       ; preds = %block_51a3
  store volatile i64 20943, i64* @assembly_address
  %1564 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %1565 = zext i32 %1564 to i64
  store i64 %1565, i64* %rax
  store volatile i64 20949, i64* @assembly_address
  %1566 = load i64* %rax
  %1567 = trunc i64 %1566 to i32
  %1568 = load i64* %rax
  %1569 = trunc i64 %1568 to i32
  %1570 = and i32 %1567, %1569
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1571 = icmp eq i32 %1570, 0
  store i1 %1571, i1* %zf
  %1572 = icmp slt i32 %1570, 0
  store i1 %1572, i1* %sf
  %1573 = trunc i32 %1570 to i8
  %1574 = call i8 @llvm.ctpop.i8(i8 %1573)
  %1575 = and i8 %1574, 1
  %1576 = icmp eq i8 %1575, 0
  store i1 %1576, i1* %pf
  store volatile i64 20951, i64* @assembly_address
  %1577 = load i1* %zf
  br i1 %1577, label %block_5212, label %block_51d9

block_51d9:                                       ; preds = %block_51cf
  store volatile i64 20953, i64* @assembly_address
  %1578 = load i64* @global_var_216580
  store i64 %1578, i64* %rdx
  store volatile i64 20960, i64* @assembly_address
  %1579 = load i64* @global_var_25f4c0
  store i64 %1579, i64* %rax
  store volatile i64 20967, i64* @assembly_address
  %1580 = load i64* @global_var_25f4c0
  store i64 %1580, i64* %rcx
  store volatile i64 20974, i64* @assembly_address
  %1581 = load i64* @global_var_21a860
  store i64 %1581, i64* %rdi
  store volatile i64 20981, i64* @assembly_address
  %1582 = load i64* @global_var_267540
  store i64 %1582, i64* %rsi
  store volatile i64 20988, i64* @assembly_address
  %1583 = load i64* %rdi
  %1584 = load i64* %rsi
  %1585 = sub i64 %1583, %1584
  %1586 = and i64 %1583, 15
  %1587 = and i64 %1584, 15
  %1588 = sub i64 %1586, %1587
  %1589 = icmp ugt i64 %1588, 15
  %1590 = icmp ult i64 %1583, %1584
  %1591 = xor i64 %1583, %1584
  %1592 = xor i64 %1583, %1585
  %1593 = and i64 %1591, %1592
  %1594 = icmp slt i64 %1593, 0
  store i1 %1589, i1* %az
  store i1 %1590, i1* %cf
  store i1 %1594, i1* %of
  %1595 = icmp eq i64 %1585, 0
  store i1 %1595, i1* %zf
  %1596 = icmp slt i64 %1585, 0
  store i1 %1596, i1* %sf
  %1597 = trunc i64 %1585 to i8
  %1598 = call i8 @llvm.ctpop.i8(i8 %1597)
  %1599 = and i8 %1598, 1
  %1600 = icmp eq i8 %1599, 0
  store i1 %1600, i1* %pf
  store i64 %1585, i64* %rdi
  store volatile i64 20991, i64* @assembly_address
  %1601 = load i64* %rdi
  store i64 %1601, i64* %rsi
  store volatile i64 20994, i64* @assembly_address
  %1602 = load i64* %rcx
  %1603 = load i64* %rsi
  %1604 = sub i64 %1602, %1603
  %1605 = and i64 %1602, 15
  %1606 = and i64 %1603, 15
  %1607 = sub i64 %1605, %1606
  %1608 = icmp ugt i64 %1607, 15
  %1609 = icmp ult i64 %1602, %1603
  %1610 = xor i64 %1602, %1603
  %1611 = xor i64 %1602, %1604
  %1612 = and i64 %1610, %1611
  %1613 = icmp slt i64 %1612, 0
  store i1 %1608, i1* %az
  store i1 %1609, i1* %cf
  store i1 %1613, i1* %of
  %1614 = icmp eq i64 %1604, 0
  store i1 %1614, i1* %zf
  %1615 = icmp slt i64 %1604, 0
  store i1 %1615, i1* %sf
  %1616 = trunc i64 %1604 to i8
  %1617 = call i8 @llvm.ctpop.i8(i8 %1616)
  %1618 = and i8 %1617, 1
  %1619 = icmp eq i8 %1618, 0
  store i1 %1619, i1* %pf
  store i64 %1604, i64* %rcx
  store volatile i64 20997, i64* @assembly_address
  %1620 = load i64* %rax
  store i64 %1620, i64* %rsi
  store volatile i64 21000, i64* @assembly_address
  %1621 = load i64* %rcx
  store i64 %1621, i64* %rdi
  store volatile i64 21003, i64* @assembly_address
  %1622 = load i64* %rdi
  %1623 = load i64* %rsi
  %1624 = load i64* %rdx
  %1625 = inttoptr i64 %1624 to %_IO_FILE*
  %1626 = call i64 @display_ratio(i64 %1622, i64 %1623, %_IO_FILE* %1625)
  store i64 %1626, i64* %rax
  store i64 %1626, i64* %rax
  store volatile i64 21008, i64* @assembly_address
  br label %block_5249

block_5212:                                       ; preds = %block_51cf
  store volatile i64 21010, i64* @assembly_address
  %1627 = load i64* @global_var_216580
  store i64 %1627, i64* %rdx
  store volatile i64 21017, i64* @assembly_address
  %1628 = load i64* @global_var_21a860
  store i64 %1628, i64* %rax
  store volatile i64 21024, i64* @assembly_address
  %1629 = load i64* @global_var_21a860
  store i64 %1629, i64* %rcx
  store volatile i64 21031, i64* @assembly_address
  %1630 = load i64* @global_var_25f4c0
  store i64 %1630, i64* %rdi
  store volatile i64 21038, i64* @assembly_address
  %1631 = load i64* @global_var_267540
  store i64 %1631, i64* %rsi
  store volatile i64 21045, i64* @assembly_address
  %1632 = load i64* %rdi
  %1633 = load i64* %rsi
  %1634 = sub i64 %1632, %1633
  %1635 = and i64 %1632, 15
  %1636 = and i64 %1633, 15
  %1637 = sub i64 %1635, %1636
  %1638 = icmp ugt i64 %1637, 15
  %1639 = icmp ult i64 %1632, %1633
  %1640 = xor i64 %1632, %1633
  %1641 = xor i64 %1632, %1634
  %1642 = and i64 %1640, %1641
  %1643 = icmp slt i64 %1642, 0
  store i1 %1638, i1* %az
  store i1 %1639, i1* %cf
  store i1 %1643, i1* %of
  %1644 = icmp eq i64 %1634, 0
  store i1 %1644, i1* %zf
  %1645 = icmp slt i64 %1634, 0
  store i1 %1645, i1* %sf
  %1646 = trunc i64 %1634 to i8
  %1647 = call i8 @llvm.ctpop.i8(i8 %1646)
  %1648 = and i8 %1647, 1
  %1649 = icmp eq i8 %1648, 0
  store i1 %1649, i1* %pf
  store i64 %1634, i64* %rdi
  store volatile i64 21048, i64* @assembly_address
  %1650 = load i64* %rdi
  store i64 %1650, i64* %rsi
  store volatile i64 21051, i64* @assembly_address
  %1651 = load i64* %rcx
  %1652 = load i64* %rsi
  %1653 = sub i64 %1651, %1652
  %1654 = and i64 %1651, 15
  %1655 = and i64 %1652, 15
  %1656 = sub i64 %1654, %1655
  %1657 = icmp ugt i64 %1656, 15
  %1658 = icmp ult i64 %1651, %1652
  %1659 = xor i64 %1651, %1652
  %1660 = xor i64 %1651, %1653
  %1661 = and i64 %1659, %1660
  %1662 = icmp slt i64 %1661, 0
  store i1 %1657, i1* %az
  store i1 %1658, i1* %cf
  store i1 %1662, i1* %of
  %1663 = icmp eq i64 %1653, 0
  store i1 %1663, i1* %zf
  %1664 = icmp slt i64 %1653, 0
  store i1 %1664, i1* %sf
  %1665 = trunc i64 %1653 to i8
  %1666 = call i8 @llvm.ctpop.i8(i8 %1665)
  %1667 = and i8 %1666, 1
  %1668 = icmp eq i8 %1667, 0
  store i1 %1668, i1* %pf
  store i64 %1653, i64* %rcx
  store volatile i64 21054, i64* @assembly_address
  %1669 = load i64* %rax
  store i64 %1669, i64* %rsi
  store volatile i64 21057, i64* @assembly_address
  %1670 = load i64* %rcx
  store i64 %1670, i64* %rdi
  store volatile i64 21060, i64* @assembly_address
  %1671 = load i64* %rdi
  %1672 = load i64* %rsi
  %1673 = load i64* %rdx
  %1674 = inttoptr i64 %1673 to %_IO_FILE*
  %1675 = call i64 @display_ratio(i64 %1671, i64 %1672, %_IO_FILE* %1674)
  store i64 %1675, i64* %rax
  store i64 %1675, i64* %rax
  br label %block_5249

block_5249:                                       ; preds = %block_5212, %block_51d9, %block_51ad
  store volatile i64 21065, i64* @assembly_address
  %1676 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %1677 = zext i32 %1676 to i64
  store i64 %1677, i64* %rax
  store volatile i64 21071, i64* @assembly_address
  %1678 = load i64* %rax
  %1679 = trunc i64 %1678 to i32
  %1680 = load i64* %rax
  %1681 = trunc i64 %1680 to i32
  %1682 = and i32 %1679, %1681
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1683 = icmp eq i32 %1682, 0
  store i1 %1683, i1* %zf
  %1684 = icmp slt i32 %1682, 0
  store i1 %1684, i1* %sf
  %1685 = trunc i32 %1682 to i8
  %1686 = call i8 @llvm.ctpop.i8(i8 %1685)
  %1687 = and i8 %1686, 1
  %1688 = icmp eq i8 %1687, 0
  store i1 %1688, i1* %pf
  store volatile i64 21073, i64* @assembly_address
  %1689 = load i1* %zf
  %1690 = icmp eq i1 %1689, false
  br i1 %1690, label %block_5299, label %block_5253

block_5253:                                       ; preds = %block_5249
  store volatile i64 21075, i64* @assembly_address
  %1691 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1692 = zext i32 %1691 to i64
  store i64 %1692, i64* %rax
  store volatile i64 21081, i64* @assembly_address
  %1693 = load i64* %rax
  %1694 = trunc i64 %1693 to i32
  %1695 = load i64* %rax
  %1696 = trunc i64 %1695 to i32
  %1697 = and i32 %1694, %1696
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1698 = icmp eq i32 %1697, 0
  store i1 %1698, i1* %zf
  %1699 = icmp slt i32 %1697, 0
  store i1 %1699, i1* %sf
  %1700 = trunc i32 %1697 to i8
  %1701 = call i8 @llvm.ctpop.i8(i8 %1700)
  %1702 = and i8 %1701, 1
  %1703 = icmp eq i8 %1702, 0
  store i1 %1703, i1* %pf
  store volatile i64 21083, i64* @assembly_address
  %1704 = load i1* %zf
  %1705 = icmp eq i1 %1704, false
  br i1 %1705, label %block_5299, label %block_525d

block_525d:                                       ; preds = %block_5253
  store volatile i64 21085, i64* @assembly_address
  %1706 = load i32* bitcast (i64* @global_var_216608 to i32*)
  %1707 = zext i32 %1706 to i64
  store i64 %1707, i64* %rax
  store volatile i64 21091, i64* @assembly_address
  %1708 = load i64* %rax
  %1709 = trunc i64 %1708 to i32
  %1710 = load i64* %rax
  %1711 = trunc i64 %1710 to i32
  %1712 = and i32 %1709, %1711
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1713 = icmp eq i32 %1712, 0
  store i1 %1713, i1* %zf
  %1714 = icmp slt i32 %1712, 0
  store i1 %1714, i1* %sf
  %1715 = trunc i32 %1712 to i8
  %1716 = call i8 @llvm.ctpop.i8(i8 %1715)
  %1717 = and i8 %1716, 1
  %1718 = icmp eq i8 %1717, 0
  store i1 %1718, i1* %pf
  store volatile i64 21093, i64* @assembly_address
  %1719 = load i1* %zf
  br i1 %1719, label %block_5270, label %block_5267

block_5267:                                       ; preds = %block_525d
  store volatile i64 21095, i64* @assembly_address
  store i64 ptrtoint ([8 x i8]* @global_var_116cc to i64), i64* %rax
  store volatile i64 21102, i64* @assembly_address
  br label %block_5277

block_5270:                                       ; preds = %block_525d
  store volatile i64 21104, i64* @assembly_address
  store i64 ptrtoint ([14 x i8]* @global_var_116d4 to i64), i64* %rax
  br label %block_5277

block_5277:                                       ; preds = %block_5270, %block_5267
  store volatile i64 21111, i64* @assembly_address
  %1720 = load i64* @global_var_216580
  store i64 %1720, i64* %rdi
  store volatile i64 21118, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rcx
  store volatile i64 21125, i64* @assembly_address
  %1721 = load i64* %rax
  store i64 %1721, i64* %rdx
  store volatile i64 21128, i64* @assembly_address
  store i64 ptrtoint ([10 x i8]* @global_var_116e2 to i64), i64* %rsi
  store volatile i64 21135, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 21140, i64* @assembly_address
  %1722 = load i64* %rdi
  %1723 = inttoptr i64 %1722 to %_IO_FILE*
  %1724 = load i64* %rsi
  %1725 = inttoptr i64 %1724 to i8*
  %1726 = load i64* %rdx
  %1727 = inttoptr i64 %1726 to i8*
  %1728 = load i64* %rcx
  %1729 = inttoptr i64 %1728 to i8*
  %1730 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1723, i8* %1725, i8* %1727, i8* %1729)
  %1731 = sext i32 %1730 to i64
  store i64 %1731, i64* %rax
  %1732 = sext i32 %1730 to i64
  store i64 %1732, i64* %rax
  br label %block_5299

block_5299:                                       ; preds = %block_5277, %block_5253, %block_5249
  store volatile i64 21145, i64* @assembly_address
  %1733 = load i64* @global_var_216580
  store i64 %1733, i64* %rax
  store volatile i64 21152, i64* @assembly_address
  %1734 = load i64* %rax
  store i64 %1734, i64* %rsi
  store volatile i64 21155, i64* @assembly_address
  store i64 10, i64* %rdi
  store volatile i64 21160, i64* @assembly_address
  %1735 = load i64* %rdi
  %1736 = trunc i64 %1735 to i32
  %1737 = load i64* %rsi
  %1738 = inttoptr i64 %1737 to %_IO_FILE*
  %1739 = call i32 @fputc(i32 %1736, %_IO_FILE* %1738)
  %1740 = sext i32 %1739 to i64
  store i64 %1740, i64* %rax
  %1741 = sext i32 %1739 to i64
  store i64 %1741, i64* %rax
  store volatile i64 21165, i64* @assembly_address
  br label %block_52bc

block_52af:                                       ; preds = %block_4a64
  store volatile i64 21167, i64* @assembly_address
  store volatile i64 21168, i64* @assembly_address
  br label %block_52bc

block_52b2:                                       ; preds = %block_4b18, %block_4b0a
  store volatile i64 21170, i64* @assembly_address
  store volatile i64 21171, i64* @assembly_address
  br label %block_52bc

block_52b5:                                       ; preds = %block_4e26
  store volatile i64 21173, i64* @assembly_address
  store volatile i64 21174, i64* @assembly_address
  br label %block_52bc

block_52b8:                                       ; preds = %block_4e6b
  store volatile i64 21176, i64* @assembly_address
  store volatile i64 21177, i64* @assembly_address
  br label %block_52bc

block_52bb:                                       ; preds = %block_5186, %block_5178
  store volatile i64 21179, i64* @assembly_address
  br label %block_52bc

block_52bc:                                       ; preds = %block_52bb, %block_52b8, %block_52b5, %block_52b2, %block_52af, %block_5299, %block_5195, %block_4e0a, %block_4dc2, %block_4d68, %block_4cd4, %block_4c5e, %block_4bf6, %block_4b8e, %block_4aae, %block_4a42
  store volatile i64 21180, i64* @assembly_address
  %1742 = load i64* %stack_var_-16
  store i64 %1742, i64* %rax
  store volatile i64 21184, i64* @assembly_address
  %1743 = load i64* %rax
  %1744 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %1745 = xor i64 %1743, %1744
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1746 = icmp eq i64 %1745, 0
  store i1 %1746, i1* %zf
  %1747 = icmp slt i64 %1745, 0
  store i1 %1747, i1* %sf
  %1748 = trunc i64 %1745 to i8
  %1749 = call i8 @llvm.ctpop.i8(i8 %1748)
  %1750 = and i8 %1749, 1
  %1751 = icmp eq i8 %1750, 0
  store i1 %1751, i1* %pf
  store i64 %1745, i64* %rax
  store volatile i64 21193, i64* @assembly_address
  %1752 = load i1* %zf
  br i1 %1752, label %block_52d0, label %block_52cb

block_52cb:                                       ; preds = %block_52bc
  store volatile i64 21195, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_52d0:                                       ; preds = %block_52bc
  store volatile i64 21200, i64* @assembly_address
  %1753 = load i64* %stack_var_-8
  store i64 %1753, i64* %rbp
  %1754 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1754, i64* %rsp
  store volatile i64 21201, i64* @assembly_address
  %1755 = load i64* %rax
  ret i64 %1755
}

declare i64 @173(i64)

define i64 @volatile_strcpy(i8* %arg1, i64* %arg2) {
block_52d2:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = bitcast i8* %arg1 to i64*
  %2 = ptrtoint i64* %1 to i64
  store i64 %2, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-24 = alloca i8*
  %3 = alloca i64
  %stack_var_-16 = alloca i8*
  %4 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 21202, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 21203, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 21206, i64* @assembly_address
  %8 = load i64* %rdi
  %9 = inttoptr i64 %8 to i8*
  store i8* %9, i8** %stack_var_-16
  store volatile i64 21210, i64* @assembly_address
  %10 = load i64* %rsi
  %11 = inttoptr i64 %10 to i8*
  store i8* %11, i8** %stack_var_-24
  store volatile i64 21214, i64* @assembly_address
  br label %block_52e1

block_52e0:                                       ; preds = %block_52e1
  store volatile i64 21216, i64* @assembly_address
  br label %block_52e1

block_52e1:                                       ; preds = %block_52e0, %block_52d2
  store volatile i64 21217, i64* @assembly_address
  %12 = load i8** %stack_var_-24
  %13 = ptrtoint i8* %12 to i64
  store i64 %13, i64* %rax
  store volatile i64 21221, i64* @assembly_address
  %14 = load i64* %rax
  %15 = add i64 %14, 1
  store i64 %15, i64* %rdx
  store volatile i64 21225, i64* @assembly_address
  %16 = load i64* %rdx
  %17 = inttoptr i64 %16 to i8*
  store i8* %17, i8** %stack_var_-24
  store volatile i64 21229, i64* @assembly_address
  %18 = load i8** %stack_var_-16
  %19 = ptrtoint i8* %18 to i64
  store i64 %19, i64* %rdx
  store volatile i64 21233, i64* @assembly_address
  %20 = load i64* %rdx
  %21 = add i64 %20, 1
  store i64 %21, i64* %rcx
  store volatile i64 21237, i64* @assembly_address
  %22 = load i64* %rcx
  %23 = inttoptr i64 %22 to i8*
  store i8* %23, i8** %stack_var_-16
  store volatile i64 21241, i64* @assembly_address
  %24 = load i64* %rax
  %25 = inttoptr i64 %24 to i8*
  %26 = load i8* %25
  %27 = zext i8 %26 to i64
  store i64 %27, i64* %rax
  store volatile i64 21244, i64* @assembly_address
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i8
  %30 = load i64* %rdx
  %31 = inttoptr i64 %30 to i8*
  store i8 %29, i8* %31
  store volatile i64 21246, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i8
  %34 = load i64* %rax
  %35 = trunc i64 %34 to i8
  %36 = and i8 %33, %35
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i8 %36, 0
  store i1 %38, i1* %sf
  %39 = call i8 @llvm.ctpop.i8(i8 %36)
  %40 = and i8 %39, 1
  %41 = icmp eq i8 %40, 0
  store i1 %41, i1* %pf
  store volatile i64 21248, i64* @assembly_address
  %42 = load i1* %zf
  %43 = icmp eq i1 %42, false
  br i1 %43, label %block_52e0, label %block_5302

block_5302:                                       ; preds = %block_52e1
  store volatile i64 21250, i64* @assembly_address
  store volatile i64 21251, i64* @assembly_address
  %44 = load i64* %stack_var_-8
  store i64 %44, i64* %rbp
  %45 = ptrtoint i64* %stack_var_0 to i64
  store i64 %45, i64* %rsp
  store volatile i64 21252, i64* @assembly_address
  %46 = load i64* %rax
  ret i64 %46
}

declare i64 @174(i64*, i8*)

declare i64 @175(i64*, i64*)

define i64 @create_outfile(i64 %arg1, i64 %arg2, i64 %arg3, i64 %arg4, i64 %arg5, i64 %arg6) {
block_5305:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg6, i64* %r9
  store i64 %arg5, i64* %r8
  store i64 %arg4, i64* %rcx
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-172 = alloca i32
  %stack_var_-152 = alloca i64
  %stack_var_-176 = alloca i32
  %stack_var_-160 = alloca i64
  %stack_var_-184 = alloca i32
  %stack_var_-168 = alloca i8*
  %0 = alloca i64
  %stack_var_-180 = alloca i32
  %stack_var_-188 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-200 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 21253, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 21254, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 21257, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 192
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 192
  %9 = xor i64 %4, 192
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-200 to i64
  store i64 %19, i64* %rsp
  store volatile i64 21264, i64* @assembly_address
  %20 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %20, i64* %rax
  store volatile i64 21273, i64* @assembly_address
  %21 = load i64* %rax
  store i64 %21, i64* %stack_var_-16
  store volatile i64 21277, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %22 = icmp eq i32 0, 0
  store i1 %22, i1* %zf
  %23 = icmp slt i32 0, 0
  store i1 %23, i1* %sf
  %24 = trunc i32 0 to i8
  %25 = call i8 @llvm.ctpop.i8(i8 %24)
  %26 = and i8 %25, 1
  %27 = icmp eq i8 %26, 0
  store i1 %27, i1* %pf
  %28 = zext i32 0 to i64
  store i64 %28, i64* %rax
  store volatile i64 21279, i64* @assembly_address
  store i32 0, i32* %stack_var_-188
  store volatile i64 21289, i64* @assembly_address
  store i32 193, i32* %stack_var_-180
  store volatile i64 21299, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rax
  store volatile i64 21306, i64* @assembly_address
  %29 = load i64* %rax
  %30 = inttoptr i64 %29 to i8*
  store i8* %30, i8** %stack_var_-168
  store volatile i64 21313, i64* @assembly_address
  store i32 -100, i32* %stack_var_-184
  store volatile i64 21323, i64* @assembly_address
  %31 = load i32* bitcast (i64* @global_var_216608 to i32*)
  %32 = zext i32 %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 21329, i64* @assembly_address
  %33 = load i64* %rax
  %34 = trunc i64 %33 to i32
  %35 = load i64* %rax
  %36 = trunc i64 %35 to i32
  %37 = and i32 %34, %36
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %38 = icmp eq i32 %37, 0
  store i1 %38, i1* %zf
  %39 = icmp slt i32 %37, 0
  store i1 %39, i1* %sf
  %40 = trunc i32 %37 to i8
  %41 = call i8 @llvm.ctpop.i8(i8 %40)
  %42 = and i8 %41, 1
  %43 = icmp eq i8 %42, 0
  store i1 %43, i1* %pf
  store volatile i64 21331, i64* @assembly_address
  %44 = load i1* %zf
  %45 = icmp eq i1 %44, false
  br i1 %45, label %block_53b4, label %block_5355

block_5355:                                       ; preds = %block_5305
  store volatile i64 21333, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 21340, i64* @assembly_address
  %46 = load i64* %rdi
  %47 = inttoptr i64 %46 to i64*
  %48 = bitcast i64* %47 to i8*
  %49 = call i64 @last_component(i8* %48)
  store i64 %49, i64* %rax
  store i64 %49, i64* %rax
  store volatile i64 21345, i64* @assembly_address
  %50 = load i64* %rax
  store i64 %50, i64* %stack_var_-160
  store volatile i64 21352, i64* @assembly_address
  %51 = load i64* %stack_var_-160
  store i64 %51, i64* %rdx
  store volatile i64 21359, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rax
  store volatile i64 21366, i64* @assembly_address
  %52 = load i64* %rdx
  %53 = load i64* %rax
  %54 = sub i64 %52, %53
  %55 = and i64 %52, 15
  %56 = and i64 %53, 15
  %57 = sub i64 %55, %56
  %58 = icmp ugt i64 %57, 15
  %59 = icmp ult i64 %52, %53
  %60 = xor i64 %52, %53
  %61 = xor i64 %52, %54
  %62 = and i64 %60, %61
  %63 = icmp slt i64 %62, 0
  store i1 %58, i1* %az
  store i1 %59, i1* %cf
  store i1 %63, i1* %of
  %64 = icmp eq i64 %54, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i64 %54, 0
  store i1 %65, i1* %sf
  %66 = trunc i64 %54 to i8
  %67 = call i8 @llvm.ctpop.i8(i8 %66)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  store i64 %54, i64* %rdx
  store volatile i64 21369, i64* @assembly_address
  %70 = load i64* %rdx
  store i64 %70, i64* %rax
  store volatile i64 21372, i64* @assembly_address
  %71 = load i64* %rax
  store i64 %71, i64* %rsi
  store volatile i64 21375, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 21382, i64* @assembly_address
  %72 = load i64* %rdi
  %73 = inttoptr i64 %72 to i64*
  %74 = load i64* %rsi
  %75 = trunc i64 %74 to i32
  %76 = call i64 @atdir_set(i64* %73, i32 %75)
  store i64 %76, i64* %rax
  store i64 %76, i64* %rax
  store volatile i64 21387, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i32
  store i32 %78, i32* %stack_var_-176
  store volatile i64 21393, i64* @assembly_address
  %79 = load i32* %stack_var_-176
  %80 = and i32 %79, 15
  %81 = icmp ugt i32 %80, 15
  %82 = icmp ult i32 %79, 0
  %83 = xor i32 %79, 0
  %84 = and i32 %83, 0
  %85 = icmp slt i32 %84, 0
  store i1 %81, i1* %az
  store i1 %82, i1* %cf
  store i1 %85, i1* %of
  %86 = icmp eq i32 %79, 0
  store i1 %86, i1* %zf
  %87 = icmp slt i32 %79, 0
  store i1 %87, i1* %sf
  %88 = trunc i32 %79 to i8
  %89 = call i8 @llvm.ctpop.i8(i8 %88)
  %90 = and i8 %89, 1
  %91 = icmp eq i8 %90, 0
  store i1 %91, i1* %pf
  store volatile i64 21400, i64* @assembly_address
  %92 = load i1* %sf
  br i1 %92, label %block_53b4, label %block_539a

block_539a:                                       ; preds = %block_5355
  store volatile i64 21402, i64* @assembly_address
  %93 = load i64* %stack_var_-160
  store i64 %93, i64* %rax
  store volatile i64 21409, i64* @assembly_address
  %94 = load i64* %rax
  %95 = inttoptr i64 %94 to i8*
  store i8* %95, i8** %stack_var_-168
  store volatile i64 21416, i64* @assembly_address
  %96 = load i32* %stack_var_-176
  %97 = zext i32 %96 to i64
  store i64 %97, i64* %rax
  store volatile i64 21422, i64* @assembly_address
  %98 = load i64* %rax
  %99 = trunc i64 %98 to i32
  store i32 %99, i32* %stack_var_-184
  br label %block_53b4

block_53b4:                                       ; preds = %block_54bc, %block_539a, %block_5355, %block_5305
  store volatile i64 21428, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rsi
  store volatile i64 21435, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2166e0 to i64), i64* %rdi
  store volatile i64 21442, i64* @assembly_address
  %100 = load i64* %rdi
  %101 = inttoptr i64 %100 to i64*
  %102 = load i64* %rsi
  %103 = inttoptr i64 %102 to i64*
  %104 = bitcast i64* %101 to i8*
  %105 = call i64 @volatile_strcpy(i8* %104, i64* %103)
  store i64 %105, i64* %rax
  store i64 %105, i64* %rax
  store volatile i64 21447, i64* @assembly_address
  %106 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %106, i64* %rax
  store volatile i64 21454, i64* @assembly_address
  %107 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %107, i64* %rdx
  store volatile i64 21457, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216640 to i64), i64* %rsi
  store volatile i64 21464, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 21469, i64* @assembly_address
  %108 = load i64* %rdi
  %109 = trunc i64 %108 to i32
  %110 = load i64* %rsi
  %111 = inttoptr i64 %110 to %_TYPEDEF_sigset_t*
  %112 = load i64* %rdx
  %113 = inttoptr i64 %112 to %_TYPEDEF_sigset_t*
  %114 = call i32 @sigprocmask(i32 %109, %_TYPEDEF_sigset_t* %111, %_TYPEDEF_sigset_t* %113)
  %115 = sext i32 %114 to i64
  store i64 %115, i64* %rax
  %116 = sext i32 %114 to i64
  store i64 %116, i64* %rax
  store volatile i64 21474, i64* @assembly_address
  %117 = load i32* %stack_var_-180
  %118 = zext i32 %117 to i64
  store i64 %118, i64* %rdx
  store volatile i64 21480, i64* @assembly_address
  %119 = load i8** %stack_var_-168
  %120 = ptrtoint i8* %119 to i64
  store i64 %120, i64* %rsi
  store volatile i64 21487, i64* @assembly_address
  %121 = load i32* %stack_var_-184
  %122 = zext i32 %121 to i64
  store i64 %122, i64* %rax
  store volatile i64 21493, i64* @assembly_address
  store i64 384, i64* %rcx
  store volatile i64 21498, i64* @assembly_address
  %123 = load i64* %rax
  %124 = trunc i64 %123 to i32
  %125 = zext i32 %124 to i64
  store i64 %125, i64* %rdi
  store volatile i64 21500, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 21505, i64* @assembly_address
  %126 = load i64* %rdi
  %127 = load i64* %rsi
  %128 = load i64* %rdx
  %129 = trunc i64 %128 to i32
  %130 = load i64* %rcx
  %131 = load i64* %r8
  %132 = load i64* %r9
  %133 = trunc i64 %126 to i32
  %134 = call i64 @openat_safer(i32 %133, i64 %127, i32 %129, i64 %130, i64 %131, i64 %132)
  store i64 %134, i64* %rax
  store i64 %134, i64* %rax
  store volatile i64 21510, i64* @assembly_address
  %135 = load i64* %rax
  %136 = trunc i64 %135 to i32
  store i32 %136, i32* bitcast (i64* @global_var_24a880 to i32*)
  store volatile i64 21516, i64* @assembly_address
  %137 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %138 = zext i32 %137 to i64
  store i64 %138, i64* %rax
  store volatile i64 21522, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  store i32 %140, i32* bitcast (i64* @global_var_2160a4 to i32*)
  store volatile i64 21528, i64* @assembly_address
  %141 = call i32* @__errno_location()
  %142 = ptrtoint i32* %141 to i64
  store i64 %142, i64* %rax
  %143 = ptrtoint i32* %141 to i64
  store i64 %143, i64* %rax
  %144 = ptrtoint i32* %141 to i64
  store i64 %144, i64* %rax
  store volatile i64 21533, i64* @assembly_address
  %145 = load i64* %rax
  %146 = inttoptr i64 %145 to i32*
  %147 = load i32* %146
  %148 = zext i32 %147 to i64
  store i64 %148, i64* %rax
  store volatile i64 21535, i64* @assembly_address
  %149 = load i64* %rax
  %150 = trunc i64 %149 to i32
  store i32 %150, i32* %stack_var_-172
  store volatile i64 21541, i64* @assembly_address
  %151 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %151, i64* %rax
  store volatile i64 21548, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 21553, i64* @assembly_address
  %152 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %152, i64* %rsi
  store volatile i64 21556, i64* @assembly_address
  store i64 2, i64* %rdi
  store volatile i64 21561, i64* @assembly_address
  %153 = load i64* %rdi
  %154 = trunc i64 %153 to i32
  %155 = load i64* %rsi
  %156 = inttoptr i64 %155 to %_TYPEDEF_sigset_t*
  %157 = load i64* %rdx
  %158 = inttoptr i64 %157 to %_TYPEDEF_sigset_t*
  %159 = call i32 @sigprocmask(i32 %154, %_TYPEDEF_sigset_t* %156, %_TYPEDEF_sigset_t* %158)
  %160 = sext i32 %159 to i64
  store i64 %160, i64* %rax
  %161 = sext i32 %159 to i64
  store i64 %161, i64* %rax
  store volatile i64 21566, i64* @assembly_address
  %162 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %163 = zext i32 %162 to i64
  store i64 %163, i64* %rax
  store volatile i64 21572, i64* @assembly_address
  %164 = load i64* %rax
  %165 = trunc i64 %164 to i32
  %166 = load i64* %rax
  %167 = trunc i64 %166 to i32
  %168 = and i32 %165, %167
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %169 = icmp eq i32 %168, 0
  store i1 %169, i1* %zf
  %170 = icmp slt i32 %168, 0
  store i1 %170, i1* %sf
  %171 = trunc i32 %168 to i8
  %172 = call i8 @llvm.ctpop.i8(i8 %171)
  %173 = and i8 %172, 1
  %174 = icmp eq i8 %173, 0
  store i1 %174, i1* %pf
  store volatile i64 21574, i64* @assembly_address
  %175 = load i1* %sf
  br i1 %175, label %block_5456, label %block_5448

block_5448:                                       ; preds = %block_53b4
  store volatile i64 21576, i64* @assembly_address
  %176 = load i32* %stack_var_-188
  %177 = and i32 %176, 15
  %178 = icmp ugt i32 %177, 15
  %179 = icmp ult i32 %176, 0
  %180 = xor i32 %176, 0
  %181 = and i32 %180, 0
  %182 = icmp slt i32 %181, 0
  store i1 %178, i1* %az
  store i1 %179, i1* %cf
  store i1 %182, i1* %of
  %183 = icmp eq i32 %176, 0
  store i1 %183, i1* %zf
  %184 = icmp slt i32 %176, 0
  store i1 %184, i1* %sf
  %185 = trunc i32 %176 to i8
  %186 = call i8 @llvm.ctpop.i8(i8 %185)
  %187 = and i8 %186, 1
  %188 = icmp eq i8 %187, 0
  store i1 %188, i1* %pf
  store volatile i64 21583, i64* @assembly_address
  %189 = load i1* %zf
  %190 = icmp eq i1 %189, false
  br i1 %190, label %block_54c1, label %block_5451

block_5451:                                       ; preds = %block_5448
  store volatile i64 21585, i64* @assembly_address
  br label %block_5512

block_5456:                                       ; preds = %block_53b4
  store volatile i64 21590, i64* @assembly_address
  %191 = load i32* %stack_var_-172
  %192 = zext i32 %191 to i64
  store i64 %192, i64* %rax
  store volatile i64 21596, i64* @assembly_address
  %193 = load i64* %rax
  %194 = trunc i64 %193 to i32
  %195 = sub i32 %194, 17
  %196 = and i32 %194, 15
  %197 = sub i32 %196, 1
  %198 = icmp ugt i32 %197, 15
  %199 = icmp ult i32 %194, 17
  %200 = xor i32 %194, 17
  %201 = xor i32 %194, %195
  %202 = and i32 %200, %201
  %203 = icmp slt i32 %202, 0
  store i1 %198, i1* %az
  store i1 %199, i1* %cf
  store i1 %203, i1* %of
  %204 = icmp eq i32 %195, 0
  store i1 %204, i1* %zf
  %205 = icmp slt i32 %195, 0
  store i1 %205, i1* %sf
  %206 = trunc i32 %195 to i8
  %207 = call i8 @llvm.ctpop.i8(i8 %206)
  %208 = and i8 %207, 1
  %209 = icmp eq i8 %208, 0
  store i1 %209, i1* %pf
  store volatile i64 21599, i64* @assembly_address
  %210 = load i1* %zf
  br i1 %210, label %block_547e, label %block_5461

block_5461:                                       ; preds = %block_5456
  store volatile i64 21601, i64* @assembly_address
  %211 = load i64* %rax
  %212 = trunc i64 %211 to i32
  %213 = sub i32 %212, 36
  %214 = and i32 %212, 15
  %215 = sub i32 %214, 4
  %216 = icmp ugt i32 %215, 15
  %217 = icmp ult i32 %212, 36
  %218 = xor i32 %212, 36
  %219 = xor i32 %212, %213
  %220 = and i32 %218, %219
  %221 = icmp slt i32 %220, 0
  store i1 %216, i1* %az
  store i1 %217, i1* %cf
  store i1 %221, i1* %of
  %222 = icmp eq i32 %213, 0
  store i1 %222, i1* %zf
  %223 = icmp slt i32 %213, 0
  store i1 %223, i1* %sf
  %224 = trunc i32 %213 to i8
  %225 = call i8 @llvm.ctpop.i8(i8 %224)
  %226 = and i8 %225, 1
  %227 = icmp eq i8 %226, 0
  store i1 %227, i1* %pf
  store volatile i64 21604, i64* @assembly_address
  %228 = load i1* %zf
  %229 = icmp eq i1 %228, false
  br i1 %229, label %block_549b, label %block_5466

block_5466:                                       ; preds = %block_5461
  store volatile i64 21606, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 21613, i64* @assembly_address
  %230 = load i64* %rdi
  %231 = inttoptr i64 %230 to i64*
  %232 = bitcast i64* %231 to i8*
  %233 = call i64 @shorten_name(i8* %232)
  store i64 %233, i64* %rax
  store i64 %233, i64* %rax
  store volatile i64 21618, i64* @assembly_address
  store i32 1, i32* %stack_var_-188
  store volatile i64 21628, i64* @assembly_address
  br label %block_54bc

block_547e:                                       ; preds = %block_5456
  store volatile i64 21630, i64* @assembly_address
  %234 = call i64 @check_ofname()
  store i64 %234, i64* %rax
  store i64 %234, i64* %rax
  store i64 %234, i64* %rax
  store volatile i64 21635, i64* @assembly_address
  %235 = load i64* %rax
  %236 = trunc i64 %235 to i32
  %237 = load i64* %rax
  %238 = trunc i64 %237 to i32
  %239 = and i32 %236, %238
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %240 = icmp eq i32 %239, 0
  store i1 %240, i1* %zf
  %241 = icmp slt i32 %239, 0
  store i1 %241, i1* %sf
  %242 = trunc i32 %239 to i8
  %243 = call i8 @llvm.ctpop.i8(i8 %242)
  %244 = and i8 %243, 1
  %245 = icmp eq i8 %244, 0
  store i1 %245, i1* %pf
  store volatile i64 21637, i64* @assembly_address
  %246 = load i1* %zf
  br i1 %246, label %block_54bb, label %block_5487

block_5487:                                       ; preds = %block_547e
  store volatile i64 21639, i64* @assembly_address
  %247 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %248 = zext i32 %247 to i64
  store i64 %248, i64* %rax
  store volatile i64 21645, i64* @assembly_address
  %249 = load i64* %rax
  %250 = trunc i64 %249 to i32
  %251 = zext i32 %250 to i64
  store i64 %251, i64* %rdi
  store volatile i64 21647, i64* @assembly_address
  %252 = load i64* %rdi
  %253 = trunc i64 %252 to i32
  %254 = call i32 @close(i32 %253)
  %255 = sext i32 %254 to i64
  store i64 %255, i64* %rax
  %256 = sext i32 %254 to i64
  store i64 %256, i64* %rax
  store volatile i64 21652, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 21657, i64* @assembly_address
  br label %block_5517

block_549b:                                       ; preds = %block_5461
  store volatile i64 21659, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 21666, i64* @assembly_address
  %257 = load i64* %rdi
  %258 = inttoptr i64 %257 to i8*
  %259 = call i64 @progerror(i8* %258)
  store i64 %259, i64* %rax
  store i64 %259, i64* %rax
  store volatile i64 21671, i64* @assembly_address
  %260 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %261 = zext i32 %260 to i64
  store i64 %261, i64* %rax
  store volatile i64 21677, i64* @assembly_address
  %262 = load i64* %rax
  %263 = trunc i64 %262 to i32
  %264 = zext i32 %263 to i64
  store i64 %264, i64* %rdi
  store volatile i64 21679, i64* @assembly_address
  %265 = load i64* %rdi
  %266 = trunc i64 %265 to i32
  %267 = call i32 @close(i32 %266)
  %268 = sext i32 %267 to i64
  store i64 %268, i64* %rax
  %269 = sext i32 %267 to i64
  store i64 %269, i64* %rax
  store volatile i64 21684, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 21689, i64* @assembly_address
  br label %block_5517

block_54bb:                                       ; preds = %block_547e
  store volatile i64 21691, i64* @assembly_address
  br label %block_54bc

block_54bc:                                       ; preds = %block_54bb, %block_5466
  store volatile i64 21692, i64* @assembly_address
  br label %block_53b4

block_54c1:                                       ; preds = %block_5448
  store volatile i64 21697, i64* @assembly_address
  %270 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %271 = zext i32 %270 to i64
  store i64 %271, i64* %rax
  store volatile i64 21703, i64* @assembly_address
  %272 = load i64* %rax
  %273 = trunc i64 %272 to i32
  %274 = load i64* %rax
  %275 = trunc i64 %274 to i32
  %276 = and i32 %273, %275
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %277 = icmp eq i32 %276, 0
  store i1 %277, i1* %zf
  %278 = icmp slt i32 %276, 0
  store i1 %278, i1* %sf
  %279 = trunc i32 %276 to i8
  %280 = call i8 @llvm.ctpop.i8(i8 %279)
  %281 = and i8 %280, 1
  %282 = icmp eq i8 %281, 0
  store i1 %282, i1* %pf
  store volatile i64 21705, i64* @assembly_address
  %283 = load i1* %zf
  br i1 %283, label %block_5512, label %block_54cb

block_54cb:                                       ; preds = %block_54c1
  store volatile i64 21707, i64* @assembly_address
  %284 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %285 = zext i32 %284 to i64
  store i64 %285, i64* %rax
  store volatile i64 21713, i64* @assembly_address
  %286 = load i64* %rax
  %287 = trunc i64 %286 to i32
  %288 = load i64* %rax
  %289 = trunc i64 %288 to i32
  %290 = and i32 %287, %289
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %291 = icmp eq i32 %290, 0
  store i1 %291, i1* %zf
  %292 = icmp slt i32 %290, 0
  store i1 %292, i1* %sf
  %293 = trunc i32 %290 to i8
  %294 = call i8 @llvm.ctpop.i8(i8 %293)
  %295 = and i8 %294, 1
  %296 = icmp eq i8 %295, 0
  store i1 %296, i1* %pf
  store volatile i64 21715, i64* @assembly_address
  %297 = load i1* %zf
  %298 = icmp eq i1 %297, false
  br i1 %298, label %block_54fe, label %block_54d5

block_54d5:                                       ; preds = %block_54cb
  store volatile i64 21717, i64* @assembly_address
  %299 = load i64* @global_var_25f4c8
  store i64 %299, i64* %rdx
  store volatile i64 21724, i64* @assembly_address
  %300 = load i64* @global_var_216580
  store i64 %300, i64* %rax
  store volatile i64 21731, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rcx
  store volatile i64 21738, i64* @assembly_address
  store i64 ptrtoint ([33 x i8]* @global_var_116f0 to i64), i64* %rsi
  store volatile i64 21745, i64* @assembly_address
  %301 = load i64* %rax
  store i64 %301, i64* %rdi
  store volatile i64 21748, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 21753, i64* @assembly_address
  %302 = load i64* %rdi
  %303 = inttoptr i64 %302 to %_IO_FILE*
  %304 = load i64* %rsi
  %305 = inttoptr i64 %304 to i8*
  %306 = load i64* %rdx
  %307 = inttoptr i64 %306 to i8*
  %308 = load i64* %rcx
  %309 = inttoptr i64 %308 to i8*
  %310 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %303, i8* %305, i8* %307, i8* %309)
  %311 = sext i32 %310 to i64
  store i64 %311, i64* %rax
  %312 = sext i32 %310 to i64
  store i64 %312, i64* %rax
  br label %block_54fe

block_54fe:                                       ; preds = %block_54d5, %block_54cb
  store volatile i64 21758, i64* @assembly_address
  %313 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %314 = zext i32 %313 to i64
  store i64 %314, i64* %rax
  store volatile i64 21764, i64* @assembly_address
  %315 = load i64* %rax
  %316 = trunc i64 %315 to i32
  %317 = load i64* %rax
  %318 = trunc i64 %317 to i32
  %319 = and i32 %316, %318
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %320 = icmp eq i32 %319, 0
  store i1 %320, i1* %zf
  %321 = icmp slt i32 %319, 0
  store i1 %321, i1* %sf
  %322 = trunc i32 %319 to i8
  %323 = call i8 @llvm.ctpop.i8(i8 %322)
  %324 = and i8 %323, 1
  %325 = icmp eq i8 %324, 0
  store i1 %325, i1* %pf
  store volatile i64 21766, i64* @assembly_address
  %326 = load i1* %zf
  %327 = icmp eq i1 %326, false
  br i1 %327, label %block_5512, label %block_5508

block_5508:                                       ; preds = %block_54fe
  store volatile i64 21768, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_5512

block_5512:                                       ; preds = %block_5508, %block_54fe, %block_54c1, %block_5451
  store volatile i64 21778, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_5517

block_5517:                                       ; preds = %block_5512, %block_549b, %block_5487
  store volatile i64 21783, i64* @assembly_address
  %328 = load i64* %stack_var_-16
  store i64 %328, i64* %rcx
  store volatile i64 21787, i64* @assembly_address
  %329 = load i64* %rcx
  %330 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %331 = xor i64 %329, %330
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %332 = icmp eq i64 %331, 0
  store i1 %332, i1* %zf
  %333 = icmp slt i64 %331, 0
  store i1 %333, i1* %sf
  %334 = trunc i64 %331 to i8
  %335 = call i8 @llvm.ctpop.i8(i8 %334)
  %336 = and i8 %335, 1
  %337 = icmp eq i8 %336, 0
  store i1 %337, i1* %pf
  store i64 %331, i64* %rcx
  store volatile i64 21796, i64* @assembly_address
  %338 = load i1* %zf
  br i1 %338, label %block_552b, label %block_5526

block_5526:                                       ; preds = %block_5517
  store volatile i64 21798, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_552b:                                       ; preds = %block_5517
  store volatile i64 21803, i64* @assembly_address
  %339 = load i64* %stack_var_-8
  store i64 %339, i64* %rbp
  %340 = ptrtoint i64* %stack_var_0 to i64
  store i64 %340, i64* %rsp
  store volatile i64 21804, i64* @assembly_address
  %341 = load i64* %rax
  %342 = load i64* %rax
  ret i64 %342
}

define i64 @get_suffix(i8* %arg1) {
block_552d:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = bitcast i8* %arg1 to i64*
  %1 = ptrtoint i64* %0 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-92 = alloca i8*
  %2 = alloca i32
  %stack_var_-80 = alloca i8*
  %3 = alloca i64
  %stack_var_-96 = alloca i32
  %stack_var_-56 = alloca i64
  %stack_var_-100 = alloca i32
  %stack_var_-64 = alloca i64
  %stack_var_-72 = alloca i64
  %stack_var_-88 = alloca i8**
  %4 = alloca i64
  %stack_var_-101 = alloca i8
  %stack_var_-16 = alloca i64
  %stack_var_-112 = alloca i8*
  %5 = alloca i64
  %stack_var_-120 = alloca i64
  %stack_var_-8 = alloca i64
  %6 = alloca i8*
  %7 = alloca i32
  %8 = alloca i32
  %9 = alloca i64
  %10 = alloca i32
  %11 = alloca i32
  store volatile i64 21805, i64* @assembly_address
  %12 = load i64* %rbp
  store i64 %12, i64* %stack_var_-8
  %13 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %13, i64* %rsp
  store volatile i64 21806, i64* @assembly_address
  %14 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %14, i64* %rbp
  store volatile i64 21809, i64* @assembly_address
  %15 = load i64* %rsp
  %16 = sub i64 %15, 112
  %17 = and i64 %15, 15
  %18 = icmp ugt i64 %17, 15
  %19 = icmp ult i64 %15, 112
  %20 = xor i64 %15, 112
  %21 = xor i64 %15, %16
  %22 = and i64 %20, %21
  %23 = icmp slt i64 %22, 0
  store i1 %18, i1* %az
  store i1 %19, i1* %cf
  store i1 %23, i1* %of
  %24 = icmp eq i64 %16, 0
  store i1 %24, i1* %zf
  %25 = icmp slt i64 %16, 0
  store i1 %25, i1* %sf
  %26 = trunc i64 %16 to i8
  %27 = call i8 @llvm.ctpop.i8(i8 %26)
  %28 = and i8 %27, 1
  %29 = icmp eq i8 %28, 0
  store i1 %29, i1* %pf
  %30 = ptrtoint i64* %stack_var_-120 to i64
  store i64 %30, i64* %rsp
  store volatile i64 21813, i64* @assembly_address
  %31 = load i64* %rdi
  %32 = inttoptr i64 %31 to i8*
  store i8* %32, i8** %stack_var_-112
  store volatile i64 21817, i64* @assembly_address
  %33 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %33, i64* %rax
  store volatile i64 21826, i64* @assembly_address
  %34 = load i64* %rax
  store i64 %34, i64* %stack_var_-16
  store volatile i64 21830, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %35 = icmp eq i32 0, 0
  store i1 %35, i1* %zf
  %36 = icmp slt i32 0, 0
  store i1 %36, i1* %sf
  %37 = trunc i32 0 to i8
  %38 = call i8 @llvm.ctpop.i8(i8 %37)
  %39 = and i8 %38, 1
  %40 = icmp eq i8 %39, 0
  store i1 %40, i1* %pf
  %41 = zext i32 0 to i64
  store i64 %41, i64* %rax
  store volatile i64 21832, i64* @assembly_address
  store i8 0, i8* %stack_var_-101
  store volatile i64 21836, i64* @assembly_address
  store i64 ptrtoint ([7 x i8*]* @global_var_2160e8 to i64), i64* %rax
  store volatile i64 21843, i64* @assembly_address
  %42 = load i64* %rax
  %43 = inttoptr i64 %42 to i8**
  store i8** %43, i8*** %stack_var_-88
  store volatile i64 21847, i64* @assembly_address
  br label %block_55b5

block_5559:                                       ; preds = %block_55b5
  store volatile i64 21849, i64* @assembly_address
  %44 = load i8*** %stack_var_-88
  %45 = ptrtoint i8** %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 21853, i64* @assembly_address
  %46 = load i64* %rax
  %47 = inttoptr i64 %46 to i64*
  %48 = load i64* %47
  store i64 %48, i64* %rax
  store volatile i64 21856, i64* @assembly_address
  %49 = load i64* %rax
  store i64 %49, i64* %rdi
  store volatile i64 21859, i64* @assembly_address
  %50 = load i64* %rdi
  %51 = inttoptr i64 %50 to i8*
  %52 = call i32 @strlen(i8* %51)
  %53 = sext i32 %52 to i64
  store i64 %53, i64* %rax
  %54 = sext i32 %52 to i64
  store i64 %54, i64* %rax
  store volatile i64 21864, i64* @assembly_address
  %55 = load i64* %rax
  store i64 %55, i64* %stack_var_-72
  store volatile i64 21868, i64* @assembly_address
  %56 = load i64* @global_var_216638
  store i64 %56, i64* %rax
  store volatile i64 21875, i64* @assembly_address
  %57 = load i64* %stack_var_-72
  %58 = load i64* %rax
  %59 = sub i64 %57, %58
  %60 = and i64 %57, 15
  %61 = and i64 %58, 15
  %62 = sub i64 %60, %61
  %63 = icmp ugt i64 %62, 15
  %64 = icmp ult i64 %57, %58
  %65 = xor i64 %57, %58
  %66 = xor i64 %57, %59
  %67 = and i64 %65, %66
  %68 = icmp slt i64 %67, 0
  store i1 %63, i1* %az
  store i1 %64, i1* %cf
  store i1 %68, i1* %of
  %69 = icmp eq i64 %59, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i64 %59, 0
  store i1 %70, i1* %sf
  %71 = trunc i64 %59 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  store volatile i64 21879, i64* @assembly_address
  %75 = load i1* %cf
  %76 = load i1* %zf
  %77 = or i1 %75, %76
  br i1 %77, label %block_55b0, label %block_5579

block_5579:                                       ; preds = %block_5559
  store volatile i64 21881, i64* @assembly_address
  %78 = load i8*** %stack_var_-88
  %79 = ptrtoint i8** %78 to i64
  store i64 %79, i64* %rax
  store volatile i64 21885, i64* @assembly_address
  %80 = load i64* %rax
  %81 = inttoptr i64 %80 to i64*
  %82 = load i64* %81
  store i64 %82, i64* %rax
  store volatile i64 21888, i64* @assembly_address
  %83 = load i64* @global_var_216638
  store i64 %83, i64* %rdx
  store volatile i64 21895, i64* @assembly_address
  %84 = load i64* %stack_var_-72
  store i64 %84, i64* %rcx
  store volatile i64 21899, i64* @assembly_address
  %85 = load i64* %rcx
  %86 = load i64* %rdx
  %87 = sub i64 %85, %86
  %88 = and i64 %85, 15
  %89 = and i64 %86, 15
  %90 = sub i64 %88, %89
  %91 = icmp ugt i64 %90, 15
  %92 = icmp ult i64 %85, %86
  %93 = xor i64 %85, %86
  %94 = xor i64 %85, %87
  %95 = and i64 %93, %94
  %96 = icmp slt i64 %95, 0
  store i1 %91, i1* %az
  store i1 %92, i1* %cf
  store i1 %96, i1* %of
  %97 = icmp eq i64 %87, 0
  store i1 %97, i1* %zf
  %98 = icmp slt i64 %87, 0
  store i1 %98, i1* %sf
  %99 = trunc i64 %87 to i8
  %100 = call i8 @llvm.ctpop.i8(i8 %99)
  %101 = and i8 %100, 1
  %102 = icmp eq i8 %101, 0
  store i1 %102, i1* %pf
  store i64 %87, i64* %rcx
  store volatile i64 21902, i64* @assembly_address
  %103 = load i64* %rcx
  store i64 %103, i64* %rdx
  store volatile i64 21905, i64* @assembly_address
  %104 = load i64* %rdx
  %105 = load i64* %rax
  %106 = add i64 %104, %105
  %107 = and i64 %104, 15
  %108 = and i64 %105, 15
  %109 = add i64 %107, %108
  %110 = icmp ugt i64 %109, 15
  %111 = icmp ult i64 %106, %104
  %112 = xor i64 %104, %106
  %113 = xor i64 %105, %106
  %114 = and i64 %112, %113
  %115 = icmp slt i64 %114, 0
  store i1 %110, i1* %az
  store i1 %111, i1* %cf
  store i1 %115, i1* %of
  %116 = icmp eq i64 %106, 0
  store i1 %116, i1* %zf
  %117 = icmp slt i64 %106, 0
  store i1 %117, i1* %sf
  %118 = trunc i64 %106 to i8
  %119 = call i8 @llvm.ctpop.i8(i8 %118)
  %120 = and i8 %119, 1
  %121 = icmp eq i8 %120, 0
  store i1 %121, i1* %pf
  store i64 %106, i64* %rdx
  store volatile i64 21908, i64* @assembly_address
  %122 = load i64* @global_var_216630
  store i64 %122, i64* %rax
  store volatile i64 21915, i64* @assembly_address
  %123 = load i64* %rdx
  store i64 %123, i64* %rsi
  store volatile i64 21918, i64* @assembly_address
  %124 = load i64* %rax
  store i64 %124, i64* %rdi
  store volatile i64 21921, i64* @assembly_address
  %125 = load i64* %rdi
  %126 = inttoptr i64 %125 to i8*
  %127 = load i64* %rsi
  %128 = inttoptr i64 %127 to i8*
  %129 = call i32 @strcmp(i8* %126, i8* %128)
  %130 = sext i32 %129 to i64
  store i64 %130, i64* %rax
  %131 = sext i32 %129 to i64
  store i64 %131, i64* %rax
  store volatile i64 21926, i64* @assembly_address
  %132 = load i64* %rax
  %133 = trunc i64 %132 to i32
  %134 = load i64* %rax
  %135 = trunc i64 %134 to i32
  %136 = and i32 %133, %135
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %137 = icmp eq i32 %136, 0
  store i1 %137, i1* %zf
  %138 = icmp slt i32 %136, 0
  store i1 %138, i1* %sf
  %139 = trunc i32 %136 to i8
  %140 = call i8 @llvm.ctpop.i8(i8 %139)
  %141 = and i8 %140, 1
  %142 = icmp eq i8 %141, 0
  store i1 %142, i1* %pf
  store volatile i64 21928, i64* @assembly_address
  %143 = load i1* %zf
  %144 = icmp eq i1 %143, false
  br i1 %144, label %block_55b0, label %block_55aa

block_55aa:                                       ; preds = %block_5579
  store volatile i64 21930, i64* @assembly_address
  store i8 1, i8* %stack_var_-101
  store volatile i64 21934, i64* @assembly_address
  br label %block_55c1

block_55b0:                                       ; preds = %block_5579, %block_5559
  store volatile i64 21936, i64* @assembly_address
  %145 = load i8*** %stack_var_-88
  %146 = ptrtoint i8** %145 to i64
  %147 = add i64 %146, 8
  %148 = and i64 %146, 15
  %149 = add i64 %148, 8
  %150 = icmp ugt i64 %149, 15
  %151 = icmp ult i64 %147, %146
  %152 = xor i64 %146, %147
  %153 = xor i64 8, %147
  %154 = and i64 %152, %153
  %155 = icmp slt i64 %154, 0
  store i1 %150, i1* %az
  store i1 %151, i1* %cf
  store i1 %155, i1* %of
  %156 = icmp eq i64 %147, 0
  store i1 %156, i1* %zf
  %157 = icmp slt i64 %147, 0
  store i1 %157, i1* %sf
  %158 = trunc i64 %147 to i8
  %159 = call i8 @llvm.ctpop.i8(i8 %158)
  %160 = and i8 %159, 1
  %161 = icmp eq i8 %160, 0
  store i1 %161, i1* %pf
  %162 = inttoptr i64 %147 to i8**
  store i8** %162, i8*** %stack_var_-88
  br label %block_55b5

block_55b5:                                       ; preds = %block_55b0, %block_552d
  store volatile i64 21941, i64* @assembly_address
  %163 = load i8*** %stack_var_-88
  %164 = ptrtoint i8** %163 to i64
  store i64 %164, i64* %rax
  store volatile i64 21945, i64* @assembly_address
  %165 = load i64* %rax
  %166 = inttoptr i64 %165 to i64*
  %167 = load i64* %166
  store i64 %167, i64* %rax
  store volatile i64 21948, i64* @assembly_address
  %168 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %169 = icmp eq i64 %168, 0
  store i1 %169, i1* %zf
  %170 = icmp slt i64 %168, 0
  store i1 %170, i1* %sf
  %171 = trunc i64 %168 to i8
  %172 = call i8 @llvm.ctpop.i8(i8 %171)
  %173 = and i8 %172, 1
  %174 = icmp eq i8 %173, 0
  store i1 %174, i1* %pf
  store volatile i64 21951, i64* @assembly_address
  %175 = load i1* %zf
  %176 = icmp eq i1 %175, false
  br i1 %176, label %block_5559, label %block_55c1

block_55c1:                                       ; preds = %block_55aa, %block_55b5
  store volatile i64 21953, i64* @assembly_address
  %177 = load i64* @global_var_216630
  store i64 %177, i64* %rax
  store volatile i64 21960, i64* @assembly_address
  %178 = load i64* %rax
  store i64 %178, i64* %rdi
  store volatile i64 21963, i64* @assembly_address
  %179 = load i64* %rdi
  %180 = inttoptr i64 %179 to i8*
  %181 = call i64 @xstrdup(i8* %180)
  store i64 %181, i64* %rax
  store i64 %181, i64* %rax
  store volatile i64 21968, i64* @assembly_address
  %182 = load i64* %rax
  store i64 %182, i64* %stack_var_-64
  store volatile i64 21972, i64* @assembly_address
  %183 = load i64* %stack_var_-64
  store i64 %183, i64* %rax
  store volatile i64 21976, i64* @assembly_address
  %184 = load i64* %rax
  store i64 %184, i64* %rdi
  store volatile i64 21979, i64* @assembly_address
  %185 = load i64* %rdi
  %186 = inttoptr i64 %185 to i8*
  %187 = call i64 @strlwr(i8* %186)
  store i64 %187, i64* %rax
  store i64 %187, i64* %rax
  store volatile i64 21984, i64* @assembly_address
  %188 = load i8* %stack_var_-101
  %189 = and i8 %188, 15
  %190 = icmp ugt i8 %189, 15
  %191 = icmp ult i8 %188, 0
  %192 = xor i8 %188, 0
  %193 = and i8 %192, 0
  %194 = icmp slt i8 %193, 0
  store i1 %190, i1* %az
  store i1 %191, i1* %cf
  store i1 %194, i1* %of
  %195 = icmp eq i8 %188, 0
  store i1 %195, i1* %zf
  %196 = icmp slt i8 %188, 0
  store i1 %196, i1* %sf
  %197 = call i8 @llvm.ctpop.i8(i8 %188)
  %198 = and i8 %197, 1
  %199 = icmp eq i8 %198, 0
  store i1 %199, i1* %pf
  store volatile i64 21988, i64* @assembly_address
  %200 = load i1* %zf
  br i1 %200, label %block_55ed, label %block_55e6

block_55e6:                                       ; preds = %block_55c1
  store volatile i64 21990, i64* @assembly_address
  store i64 8, i64* %rax
  store volatile i64 21995, i64* @assembly_address
  br label %block_55f2

block_55ed:                                       ; preds = %block_55c1
  store volatile i64 21997, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_55f2

block_55f2:                                       ; preds = %block_55ed, %block_55e6
  store volatile i64 22002, i64* @assembly_address
  %201 = load i64* %rax
  %202 = mul i64 %201, 8
  store i64 %202, i64* %rcx
  store volatile i64 22010, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2160e0 to i64), i64* %rax
  store volatile i64 22017, i64* @assembly_address
  %203 = load i64* %stack_var_-64
  store i64 %203, i64* %rdx
  store volatile i64 22021, i64* @assembly_address
  %204 = load i64* %rdx
  %205 = load i64* %rcx
  %206 = load i64* %rax
  %207 = mul i64 %206, 1
  %208 = add i64 %205, %207
  %209 = inttoptr i64 %208 to i64*
  store i64 %204, i64* %209
  store volatile i64 22025, i64* @assembly_address
  %210 = load i8* %stack_var_-101
  %211 = zext i8 %210 to i64
  store i64 %211, i64* %rax
  store volatile i64 22029, i64* @assembly_address
  %212 = load i64* %rax
  %213 = mul i64 %212, 8
  store i64 %213, i64* %rdx
  store volatile i64 22037, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2160e0 to i64), i64* %rax
  store volatile i64 22044, i64* @assembly_address
  %214 = load i64* %rax
  %215 = load i64* %rdx
  %216 = add i64 %214, %215
  %217 = and i64 %214, 15
  %218 = and i64 %215, 15
  %219 = add i64 %217, %218
  %220 = icmp ugt i64 %219, 15
  %221 = icmp ult i64 %216, %214
  %222 = xor i64 %214, %216
  %223 = xor i64 %215, %216
  %224 = and i64 %222, %223
  %225 = icmp slt i64 %224, 0
  store i1 %220, i1* %az
  store i1 %221, i1* %cf
  store i1 %225, i1* %of
  %226 = icmp eq i64 %216, 0
  store i1 %226, i1* %zf
  %227 = icmp slt i64 %216, 0
  store i1 %227, i1* %sf
  %228 = trunc i64 %216 to i8
  %229 = call i8 @llvm.ctpop.i8(i8 %228)
  %230 = and i8 %229, 1
  %231 = icmp eq i8 %230, 0
  store i1 %231, i1* %pf
  store i64 %216, i64* %rax
  store volatile i64 22047, i64* @assembly_address
  %232 = load i64* %rax
  %233 = inttoptr i64 %232 to i8**
  store i8** %233, i8*** %stack_var_-88
  store volatile i64 22051, i64* @assembly_address
  %234 = load i8** %stack_var_-112
  %235 = ptrtoint i8* %234 to i64
  store i64 %235, i64* %rax
  store volatile i64 22055, i64* @assembly_address
  %236 = load i64* %rax
  store i64 %236, i64* %rdi
  store volatile i64 22058, i64* @assembly_address
  %237 = load i64* %rdi
  %238 = inttoptr i64 %237 to i8*
  %239 = call i32 @strlen(i8* %238)
  %240 = sext i32 %239 to i64
  store i64 %240, i64* %rax
  %241 = sext i32 %239 to i64
  store i64 %241, i64* %rax
  store volatile i64 22063, i64* @assembly_address
  %242 = load i64* %rax
  %243 = trunc i64 %242 to i32
  store i32 %243, i32* %stack_var_-100
  store volatile i64 22066, i64* @assembly_address
  %244 = load i32* %stack_var_-100
  store i32 %244, i32* %11
  store i32 32, i32* %10
  %245 = sub i32 %244, 32
  %246 = and i32 %244, 15
  %247 = icmp ugt i32 %246, 15
  %248 = icmp ult i32 %244, 32
  %249 = xor i32 %244, 32
  %250 = xor i32 %244, %245
  %251 = and i32 %249, %250
  %252 = icmp slt i32 %251, 0
  store i1 %247, i1* %az
  store i1 %248, i1* %cf
  store i1 %252, i1* %of
  %253 = icmp eq i32 %245, 0
  store i1 %253, i1* %zf
  %254 = icmp slt i32 %245, 0
  store i1 %254, i1* %sf
  %255 = trunc i32 %245 to i8
  %256 = call i8 @llvm.ctpop.i8(i8 %255)
  %257 = and i8 %256, 1
  %258 = icmp eq i8 %257, 0
  store i1 %258, i1* %pf
  store volatile i64 22070, i64* @assembly_address
  %259 = load i32* %11
  %260 = load i32* %10
  %261 = icmp sgt i32 %259, %260
  br i1 %261, label %block_564d, label %block_5638

block_5638:                                       ; preds = %block_55f2
  store volatile i64 22072, i64* @assembly_address
  %262 = load i8** %stack_var_-112
  %263 = ptrtoint i8* %262 to i64
  store i64 %263, i64* %rdx
  store volatile i64 22076, i64* @assembly_address
  %264 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %264, i64* %rax
  store volatile i64 22080, i64* @assembly_address
  %265 = load i64* %rdx
  store i64 %265, i64* %rsi
  store volatile i64 22083, i64* @assembly_address
  %266 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %266, i64* %rdi
  store volatile i64 22086, i64* @assembly_address
  %267 = load i64* %rdi
  %268 = inttoptr i64 %267 to i8*
  %269 = load i64* %rsi
  %270 = inttoptr i64 %269 to i8*
  %271 = call i8* @strcpy(i8* %268, i8* %270)
  %272 = ptrtoint i8* %271 to i64
  store i64 %272, i64* %rax
  %273 = ptrtoint i8* %271 to i64
  store i64 %273, i64* %rax
  store volatile i64 22091, i64* @assembly_address
  br label %block_566c

block_564d:                                       ; preds = %block_55f2
  store volatile i64 22093, i64* @assembly_address
  %274 = load i32* %stack_var_-100
  %275 = zext i32 %274 to i64
  store i64 %275, i64* %rax
  store volatile i64 22096, i64* @assembly_address
  %276 = load i64* %rax
  %277 = trunc i64 %276 to i32
  %278 = sext i32 %277 to i64
  store i64 %278, i64* %rax
  store volatile i64 22098, i64* @assembly_address
  %279 = load i64* %rax
  %280 = add i64 %279, -32
  store i64 %280, i64* %rdx
  store volatile i64 22102, i64* @assembly_address
  %281 = load i8** %stack_var_-112
  %282 = ptrtoint i8* %281 to i64
  store i64 %282, i64* %rax
  store volatile i64 22106, i64* @assembly_address
  %283 = load i64* %rdx
  %284 = load i64* %rax
  %285 = add i64 %283, %284
  %286 = and i64 %283, 15
  %287 = and i64 %284, 15
  %288 = add i64 %286, %287
  %289 = icmp ugt i64 %288, 15
  %290 = icmp ult i64 %285, %283
  %291 = xor i64 %283, %285
  %292 = xor i64 %284, %285
  %293 = and i64 %291, %292
  %294 = icmp slt i64 %293, 0
  store i1 %289, i1* %az
  store i1 %290, i1* %cf
  store i1 %294, i1* %of
  %295 = icmp eq i64 %285, 0
  store i1 %295, i1* %zf
  %296 = icmp slt i64 %285, 0
  store i1 %296, i1* %sf
  %297 = trunc i64 %285 to i8
  %298 = call i8 @llvm.ctpop.i8(i8 %297)
  %299 = and i8 %298, 1
  %300 = icmp eq i8 %299, 0
  store i1 %300, i1* %pf
  store i64 %285, i64* %rdx
  store volatile i64 22109, i64* @assembly_address
  %301 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %301, i64* %rax
  store volatile i64 22113, i64* @assembly_address
  %302 = load i64* %rdx
  store i64 %302, i64* %rsi
  store volatile i64 22116, i64* @assembly_address
  %303 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %303, i64* %rdi
  store volatile i64 22119, i64* @assembly_address
  %304 = load i64* %rdi
  %305 = inttoptr i64 %304 to i8*
  %306 = load i64* %rsi
  %307 = inttoptr i64 %306 to i8*
  %308 = call i8* @strcpy(i8* %305, i8* %307)
  %309 = ptrtoint i8* %308 to i64
  store i64 %309, i64* %rax
  %310 = ptrtoint i8* %308 to i64
  store i64 %310, i64* %rax
  br label %block_566c

block_566c:                                       ; preds = %block_564d, %block_5638
  store volatile i64 22124, i64* @assembly_address
  %311 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %311, i64* %rax
  store volatile i64 22128, i64* @assembly_address
  %312 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %312, i64* %rdi
  store volatile i64 22131, i64* @assembly_address
  %313 = load i64* %rdi
  %314 = inttoptr i64 %313 to i8*
  %315 = call i64 @strlwr(i8* %314)
  store i64 %315, i64* %rax
  store i64 %315, i64* %rax
  store volatile i64 22136, i64* @assembly_address
  %316 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %316, i64* %rax
  store volatile i64 22140, i64* @assembly_address
  %317 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %317, i64* %rdi
  store volatile i64 22143, i64* @assembly_address
  %318 = load i64* %rdi
  %319 = inttoptr i64 %318 to i8*
  %320 = call i32 @strlen(i8* %319)
  %321 = sext i32 %320 to i64
  store i64 %321, i64* %rax
  %322 = sext i32 %320 to i64
  store i64 %322, i64* %rax
  store volatile i64 22148, i64* @assembly_address
  %323 = load i64* %rax
  %324 = trunc i64 %323 to i32
  store i32 %324, i32* %stack_var_-96
  store volatile i64 22151, i64* @assembly_address
  %325 = inttoptr i64 0 to i8*
  store i8* %325, i8** %stack_var_-80
  br label %block_568f

block_568f:                                       ; preds = %block_5704, %block_566c
  store volatile i64 22159, i64* @assembly_address
  %326 = load i8*** %stack_var_-88
  %327 = ptrtoint i8** %326 to i64
  store i64 %327, i64* %rax
  store volatile i64 22163, i64* @assembly_address
  %328 = load i64* %rax
  %329 = inttoptr i64 %328 to i64*
  %330 = load i64* %329
  store i64 %330, i64* %rax
  store volatile i64 22166, i64* @assembly_address
  %331 = load i64* %rax
  store i64 %331, i64* %rdi
  store volatile i64 22169, i64* @assembly_address
  %332 = load i64* %rdi
  %333 = inttoptr i64 %332 to i8*
  %334 = call i32 @strlen(i8* %333)
  %335 = sext i32 %334 to i64
  store i64 %335, i64* %rax
  %336 = sext i32 %334 to i64
  store i64 %336, i64* %rax
  store volatile i64 22174, i64* @assembly_address
  %337 = load i64* %rax
  %338 = trunc i64 %337 to i32
  %339 = inttoptr i32 %338 to i8*
  store i8* %339, i8** %stack_var_-92
  store volatile i64 22177, i64* @assembly_address
  %340 = load i32* %stack_var_-96
  %341 = zext i32 %340 to i64
  store i64 %341, i64* %rax
  store volatile i64 22180, i64* @assembly_address
  %342 = load i64* %rax
  %343 = trunc i64 %342 to i32
  %344 = load i8** %stack_var_-92
  %345 = ptrtoint i8* %344 to i32
  %346 = trunc i64 %342 to i32
  store i32 %346, i32* %8
  store i8* %344, i8** %6
  %347 = sub i32 %343, %345
  %348 = and i32 %343, 15
  %349 = and i32 %345, 15
  %350 = sub i32 %348, %349
  %351 = icmp ugt i32 %350, 15
  %352 = icmp ult i32 %343, %345
  %353 = xor i32 %343, %345
  %354 = xor i32 %343, %347
  %355 = and i32 %353, %354
  %356 = icmp slt i32 %355, 0
  store i1 %351, i1* %az
  store i1 %352, i1* %cf
  store i1 %356, i1* %of
  %357 = icmp eq i32 %347, 0
  store i1 %357, i1* %zf
  %358 = icmp slt i32 %347, 0
  store i1 %358, i1* %sf
  %359 = trunc i32 %347 to i8
  %360 = call i8 @llvm.ctpop.i8(i8 %359)
  %361 = and i8 %360, 1
  %362 = icmp eq i8 %361, 0
  store i1 %362, i1* %pf
  store volatile i64 22183, i64* @assembly_address
  %363 = load i32* %8
  %364 = sext i32 %363 to i64
  %365 = load i8** %6
  %366 = ptrtoint i8* %365 to i32
  %367 = trunc i64 %364 to i32
  %368 = icmp sle i32 %367, %366
  br i1 %368, label %block_5704, label %block_56a9

block_56a9:                                       ; preds = %block_568f
  store volatile i64 22185, i64* @assembly_address
  %369 = load i32* %stack_var_-96
  %370 = zext i32 %369 to i64
  store i64 %370, i64* %rax
  store volatile i64 22188, i64* @assembly_address
  %371 = load i64* %rax
  %372 = trunc i64 %371 to i32
  %373 = load i8** %stack_var_-92
  %374 = ptrtoint i8* %373 to i32
  %375 = sub i32 %372, %374
  %376 = and i32 %372, 15
  %377 = and i32 %374, 15
  %378 = sub i32 %376, %377
  %379 = icmp ugt i32 %378, 15
  %380 = icmp ult i32 %372, %374
  %381 = xor i32 %372, %374
  %382 = xor i32 %372, %375
  %383 = and i32 %381, %382
  %384 = icmp slt i32 %383, 0
  store i1 %379, i1* %az
  store i1 %380, i1* %cf
  store i1 %384, i1* %of
  %385 = icmp eq i32 %375, 0
  store i1 %385, i1* %zf
  %386 = icmp slt i32 %375, 0
  store i1 %386, i1* %sf
  %387 = trunc i32 %375 to i8
  %388 = call i8 @llvm.ctpop.i8(i8 %387)
  %389 = and i8 %388, 1
  %390 = icmp eq i8 %389, 0
  store i1 %390, i1* %pf
  %391 = zext i32 %375 to i64
  store i64 %391, i64* %rax
  store volatile i64 22191, i64* @assembly_address
  %392 = load i64* %rax
  %393 = trunc i64 %392 to i32
  %394 = sub i32 %393, 1
  %395 = and i32 %393, 15
  %396 = sub i32 %395, 1
  %397 = icmp ugt i32 %396, 15
  %398 = icmp ult i32 %393, 1
  %399 = xor i32 %393, 1
  %400 = xor i32 %393, %394
  %401 = and i32 %399, %400
  %402 = icmp slt i32 %401, 0
  store i1 %397, i1* %az
  store i1 %398, i1* %cf
  store i1 %402, i1* %of
  %403 = icmp eq i32 %394, 0
  store i1 %403, i1* %zf
  %404 = icmp slt i32 %394, 0
  store i1 %404, i1* %sf
  %405 = trunc i32 %394 to i8
  %406 = call i8 @llvm.ctpop.i8(i8 %405)
  %407 = and i8 %406, 1
  %408 = icmp eq i8 %407, 0
  store i1 %408, i1* %pf
  %409 = zext i32 %394 to i64
  store i64 %409, i64* %rax
  store volatile i64 22194, i64* @assembly_address
  %410 = load i64* %rax
  %411 = trunc i64 %410 to i32
  %412 = sext i32 %411 to i64
  store i64 %412, i64* %rax
  store volatile i64 22196, i64* @assembly_address
  %413 = load i64* %rbp
  %414 = load i64* %rax
  %415 = mul i64 %414, 1
  %416 = add i64 %413, -48
  %417 = add i64 %416, %415
  %418 = inttoptr i64 %417 to i8*
  %419 = load i8* %418
  %420 = zext i8 %419 to i64
  store i64 %420, i64* %rax
  store volatile i64 22201, i64* @assembly_address
  %421 = load i64* %rax
  %422 = trunc i64 %421 to i8
  %423 = sub i8 %422, 47
  %424 = and i8 %422, 15
  %425 = sub i8 %424, 15
  %426 = icmp ugt i8 %425, 15
  %427 = icmp ult i8 %422, 47
  %428 = xor i8 %422, 47
  %429 = xor i8 %422, %423
  %430 = and i8 %428, %429
  %431 = icmp slt i8 %430, 0
  store i1 %426, i1* %az
  store i1 %427, i1* %cf
  store i1 %431, i1* %of
  %432 = icmp eq i8 %423, 0
  store i1 %432, i1* %zf
  %433 = icmp slt i8 %423, 0
  store i1 %433, i1* %sf
  %434 = call i8 @llvm.ctpop.i8(i8 %423)
  %435 = and i8 %434, 1
  %436 = icmp eq i8 %435, 0
  store i1 %436, i1* %pf
  store volatile i64 22203, i64* @assembly_address
  %437 = load i1* %zf
  br i1 %437, label %block_5704, label %block_56bd

block_56bd:                                       ; preds = %block_56a9
  store volatile i64 22205, i64* @assembly_address
  %438 = load i8*** %stack_var_-88
  %439 = ptrtoint i8** %438 to i64
  store i64 %439, i64* %rax
  store volatile i64 22209, i64* @assembly_address
  %440 = load i64* %rax
  %441 = inttoptr i64 %440 to i64*
  %442 = load i64* %441
  store i64 %442, i64* %rax
  store volatile i64 22212, i64* @assembly_address
  %443 = load i32* %stack_var_-96
  %444 = zext i32 %443 to i64
  store i64 %444, i64* %rdx
  store volatile i64 22215, i64* @assembly_address
  %445 = load i64* %rdx
  %446 = trunc i64 %445 to i32
  %447 = sext i32 %446 to i64
  store i64 %447, i64* %rcx
  store volatile i64 22218, i64* @assembly_address
  %448 = load i8** %stack_var_-92
  %449 = ptrtoint i8* %448 to i32
  %450 = zext i32 %449 to i64
  store i64 %450, i64* %rdx
  store volatile i64 22221, i64* @assembly_address
  %451 = load i64* %rdx
  %452 = trunc i64 %451 to i32
  %453 = sext i32 %452 to i64
  store i64 %453, i64* %rdx
  store volatile i64 22224, i64* @assembly_address
  %454 = load i64* %rcx
  %455 = load i64* %rdx
  %456 = sub i64 %454, %455
  %457 = and i64 %454, 15
  %458 = and i64 %455, 15
  %459 = sub i64 %457, %458
  %460 = icmp ugt i64 %459, 15
  %461 = icmp ult i64 %454, %455
  %462 = xor i64 %454, %455
  %463 = xor i64 %454, %456
  %464 = and i64 %462, %463
  %465 = icmp slt i64 %464, 0
  store i1 %460, i1* %az
  store i1 %461, i1* %cf
  store i1 %465, i1* %of
  %466 = icmp eq i64 %456, 0
  store i1 %466, i1* %zf
  %467 = icmp slt i64 %456, 0
  store i1 %467, i1* %sf
  %468 = trunc i64 %456 to i8
  %469 = call i8 @llvm.ctpop.i8(i8 %468)
  %470 = and i8 %469, 1
  %471 = icmp eq i8 %470, 0
  store i1 %471, i1* %pf
  store i64 %456, i64* %rcx
  store volatile i64 22227, i64* @assembly_address
  %472 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %472, i64* %rdx
  store volatile i64 22231, i64* @assembly_address
  %473 = load i64* %rdx
  %474 = load i64* %rcx
  %475 = add i64 %473, %474
  %476 = and i64 %473, 15
  %477 = and i64 %474, 15
  %478 = add i64 %476, %477
  %479 = icmp ugt i64 %478, 15
  %480 = icmp ult i64 %475, %473
  %481 = xor i64 %473, %475
  %482 = xor i64 %474, %475
  %483 = and i64 %481, %482
  %484 = icmp slt i64 %483, 0
  store i1 %479, i1* %az
  store i1 %480, i1* %cf
  store i1 %484, i1* %of
  %485 = icmp eq i64 %475, 0
  store i1 %485, i1* %zf
  %486 = icmp slt i64 %475, 0
  store i1 %486, i1* %sf
  %487 = trunc i64 %475 to i8
  %488 = call i8 @llvm.ctpop.i8(i8 %487)
  %489 = and i8 %488, 1
  %490 = icmp eq i8 %489, 0
  store i1 %490, i1* %pf
  store i64 %475, i64* %rdx
  store volatile i64 22234, i64* @assembly_address
  %491 = load i64* %rax
  store i64 %491, i64* %rsi
  store volatile i64 22237, i64* @assembly_address
  %492 = load i64* %rdx
  store i64 %492, i64* %rdi
  store volatile i64 22240, i64* @assembly_address
  %493 = load i64* %rdi
  %494 = inttoptr i64 %493 to i8*
  %495 = load i64* %rsi
  %496 = inttoptr i64 %495 to i8*
  %497 = call i32 @strcmp(i8* %494, i8* %496)
  %498 = sext i32 %497 to i64
  store i64 %498, i64* %rax
  %499 = sext i32 %497 to i64
  store i64 %499, i64* %rax
  store volatile i64 22245, i64* @assembly_address
  %500 = load i64* %rax
  %501 = trunc i64 %500 to i32
  %502 = load i64* %rax
  %503 = trunc i64 %502 to i32
  %504 = and i32 %501, %503
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %505 = icmp eq i32 %504, 0
  store i1 %505, i1* %zf
  %506 = icmp slt i32 %504, 0
  store i1 %506, i1* %sf
  %507 = trunc i32 %504 to i8
  %508 = call i8 @llvm.ctpop.i8(i8 %507)
  %509 = and i8 %508, 1
  %510 = icmp eq i8 %509, 0
  store i1 %510, i1* %pf
  store volatile i64 22247, i64* @assembly_address
  %511 = load i1* %zf
  %512 = icmp eq i1 %511, false
  br i1 %512, label %block_5704, label %block_56e9

block_56e9:                                       ; preds = %block_56bd
  store volatile i64 22249, i64* @assembly_address
  %513 = load i32* %stack_var_-100
  %514 = zext i32 %513 to i64
  store i64 %514, i64* %rax
  store volatile i64 22252, i64* @assembly_address
  %515 = load i64* %rax
  %516 = trunc i64 %515 to i32
  %517 = sext i32 %516 to i64
  store i64 %517, i64* %rdx
  store volatile i64 22255, i64* @assembly_address
  %518 = load i8** %stack_var_-92
  %519 = ptrtoint i8* %518 to i32
  %520 = zext i32 %519 to i64
  store i64 %520, i64* %rax
  store volatile i64 22258, i64* @assembly_address
  %521 = load i64* %rax
  %522 = trunc i64 %521 to i32
  %523 = sext i32 %522 to i64
  store i64 %523, i64* %rax
  store volatile i64 22260, i64* @assembly_address
  %524 = load i64* %rdx
  %525 = load i64* %rax
  %526 = sub i64 %524, %525
  %527 = and i64 %524, 15
  %528 = and i64 %525, 15
  %529 = sub i64 %527, %528
  %530 = icmp ugt i64 %529, 15
  %531 = icmp ult i64 %524, %525
  %532 = xor i64 %524, %525
  %533 = xor i64 %524, %526
  %534 = and i64 %532, %533
  %535 = icmp slt i64 %534, 0
  store i1 %530, i1* %az
  store i1 %531, i1* %cf
  store i1 %535, i1* %of
  %536 = icmp eq i64 %526, 0
  store i1 %536, i1* %zf
  %537 = icmp slt i64 %526, 0
  store i1 %537, i1* %sf
  %538 = trunc i64 %526 to i8
  %539 = call i8 @llvm.ctpop.i8(i8 %538)
  %540 = and i8 %539, 1
  %541 = icmp eq i8 %540, 0
  store i1 %541, i1* %pf
  store i64 %526, i64* %rdx
  store volatile i64 22263, i64* @assembly_address
  %542 = load i8** %stack_var_-112
  %543 = ptrtoint i8* %542 to i64
  store i64 %543, i64* %rax
  store volatile i64 22267, i64* @assembly_address
  %544 = load i64* %rax
  %545 = load i64* %rdx
  %546 = add i64 %544, %545
  %547 = and i64 %544, 15
  %548 = and i64 %545, 15
  %549 = add i64 %547, %548
  %550 = icmp ugt i64 %549, 15
  %551 = icmp ult i64 %546, %544
  %552 = xor i64 %544, %546
  %553 = xor i64 %545, %546
  %554 = and i64 %552, %553
  %555 = icmp slt i64 %554, 0
  store i1 %550, i1* %az
  store i1 %551, i1* %cf
  store i1 %555, i1* %of
  %556 = icmp eq i64 %546, 0
  store i1 %556, i1* %zf
  %557 = icmp slt i64 %546, 0
  store i1 %557, i1* %sf
  %558 = trunc i64 %546 to i8
  %559 = call i8 @llvm.ctpop.i8(i8 %558)
  %560 = and i8 %559, 1
  %561 = icmp eq i8 %560, 0
  store i1 %561, i1* %pf
  store i64 %546, i64* %rax
  store volatile i64 22270, i64* @assembly_address
  %562 = load i64* %rax
  %563 = inttoptr i64 %562 to i8*
  store i8* %563, i8** %stack_var_-80
  store volatile i64 22274, i64* @assembly_address
  br label %block_5719

block_5704:                                       ; preds = %block_56bd, %block_56a9, %block_568f
  store volatile i64 22276, i64* @assembly_address
  %564 = load i8*** %stack_var_-88
  %565 = ptrtoint i8** %564 to i64
  %566 = add i64 %565, 8
  %567 = and i64 %565, 15
  %568 = add i64 %567, 8
  %569 = icmp ugt i64 %568, 15
  %570 = icmp ult i64 %566, %565
  %571 = xor i64 %565, %566
  %572 = xor i64 8, %566
  %573 = and i64 %571, %572
  %574 = icmp slt i64 %573, 0
  store i1 %569, i1* %az
  store i1 %570, i1* %cf
  store i1 %574, i1* %of
  %575 = icmp eq i64 %566, 0
  store i1 %575, i1* %zf
  %576 = icmp slt i64 %566, 0
  store i1 %576, i1* %sf
  %577 = trunc i64 %566 to i8
  %578 = call i8 @llvm.ctpop.i8(i8 %577)
  %579 = and i8 %578, 1
  %580 = icmp eq i8 %579, 0
  store i1 %580, i1* %pf
  %581 = inttoptr i64 %566 to i8**
  store i8** %581, i8*** %stack_var_-88
  store volatile i64 22281, i64* @assembly_address
  %582 = load i8*** %stack_var_-88
  %583 = ptrtoint i8** %582 to i64
  store i64 %583, i64* %rax
  store volatile i64 22285, i64* @assembly_address
  %584 = load i64* %rax
  %585 = inttoptr i64 %584 to i64*
  %586 = load i64* %585
  store i64 %586, i64* %rax
  store volatile i64 22288, i64* @assembly_address
  %587 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %588 = icmp eq i64 %587, 0
  store i1 %588, i1* %zf
  %589 = icmp slt i64 %587, 0
  store i1 %589, i1* %sf
  %590 = trunc i64 %587 to i8
  %591 = call i8 @llvm.ctpop.i8(i8 %590)
  %592 = and i8 %591, 1
  %593 = icmp eq i8 %592, 0
  store i1 %593, i1* %pf
  store volatile i64 22291, i64* @assembly_address
  %594 = load i1* %zf
  %595 = icmp eq i1 %594, false
  br i1 %595, label %block_568f, label %block_5719

block_5719:                                       ; preds = %block_5704, %block_56e9
  store volatile i64 22297, i64* @assembly_address
  %596 = load i64* %stack_var_-64
  store i64 %596, i64* %rax
  store volatile i64 22301, i64* @assembly_address
  %597 = load i64* %rax
  store i64 %597, i64* %rdi
  store volatile i64 22304, i64* @assembly_address
  %598 = load i64* %rdi
  %599 = inttoptr i64 %598 to i64*
  call void @free(i64* %599)
  store volatile i64 22309, i64* @assembly_address
  %600 = load i8** %stack_var_-80
  %601 = ptrtoint i8* %600 to i64
  store i64 %601, i64* %rax
  store volatile i64 22313, i64* @assembly_address
  %602 = load i64* %stack_var_-16
  store i64 %602, i64* %rsi
  store volatile i64 22317, i64* @assembly_address
  %603 = load i64* %rsi
  %604 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %605 = xor i64 %603, %604
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %606 = icmp eq i64 %605, 0
  store i1 %606, i1* %zf
  %607 = icmp slt i64 %605, 0
  store i1 %607, i1* %sf
  %608 = trunc i64 %605 to i8
  %609 = call i8 @llvm.ctpop.i8(i8 %608)
  %610 = and i8 %609, 1
  %611 = icmp eq i8 %610, 0
  store i1 %611, i1* %pf
  store i64 %605, i64* %rsi
  store volatile i64 22326, i64* @assembly_address
  %612 = load i1* %zf
  br i1 %612, label %block_573d, label %block_5738

block_5738:                                       ; preds = %block_5719
  store volatile i64 22328, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_573d:                                       ; preds = %block_5719
  store volatile i64 22333, i64* @assembly_address
  %613 = load i64* %stack_var_-8
  store i64 %613, i64* %rbp
  %614 = ptrtoint i64* %stack_var_0 to i64
  store i64 %614, i64* %rsp
  store volatile i64 22334, i64* @assembly_address
  %615 = load i64* %rax
  ret i64 %615
}

declare i64 @176(i64*)

define i64 @open_and_stat(i64* %arg1, i32 %arg2, i64 %arg3) {
block_573f:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i64* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-40 = alloca i32
  %stack_var_-64 = alloca %stat*
  %2 = alloca i64
  %stack_var_-52 = alloca i32
  %stack_var_-48 = alloca i64
  %stack_var_-72 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 22335, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 22336, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 22339, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 64
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 64
  %11 = xor i64 %6, 64
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-72 to i64
  store i64 %21, i64* %rsp
  store volatile i64 22343, i64* @assembly_address
  %22 = load i64* %rdi
  store i64 %22, i64* %stack_var_-48
  store volatile i64 22347, i64* @assembly_address
  %23 = load i64* %rsi
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-52
  store volatile i64 22350, i64* @assembly_address
  %25 = load i64* %rdx
  %26 = inttoptr i64 %25 to %stat*
  store %stat* %26, %stat** %stack_var_-64
  store volatile i64 22354, i64* @assembly_address
  store i32 -100, i32* %stack_var_-40
  store volatile i64 22361, i64* @assembly_address
  %27 = load i64* %stack_var_-48
  store i64 %27, i64* %rax
  store volatile i64 22365, i64* @assembly_address
  %28 = load i64* %rax
  store i64 %28, i64* %stack_var_-24
  store volatile i64 22369, i64* @assembly_address
  %29 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %30 = zext i32 %29 to i64
  store i64 %30, i64* %rax
  store volatile i64 22375, i64* @assembly_address
  %31 = load i64* %rax
  %32 = trunc i64 %31 to i32
  %33 = load i64* %rax
  %34 = trunc i64 %33 to i32
  %35 = and i32 %32, %34
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %36 = icmp eq i32 %35, 0
  store i1 %36, i1* %zf
  %37 = icmp slt i32 %35, 0
  store i1 %37, i1* %sf
  %38 = trunc i32 %35 to i8
  %39 = call i8 @llvm.ctpop.i8(i8 %38)
  %40 = and i8 %39, 1
  %41 = icmp eq i8 %40, 0
  store i1 %41, i1* %pf
  store volatile i64 22377, i64* @assembly_address
  %42 = load i1* %zf
  %43 = icmp eq i1 %42, false
  br i1 %43, label %block_577c, label %block_576b

block_576b:                                       ; preds = %block_573f
  store volatile i64 22379, i64* @assembly_address
  %44 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 22385, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = load i64* %rax
  %49 = trunc i64 %48 to i32
  %50 = and i32 %47, %49
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %51 = icmp eq i32 %50, 0
  store i1 %51, i1* %zf
  %52 = icmp slt i32 %50, 0
  store i1 %52, i1* %sf
  %53 = trunc i32 %50 to i8
  %54 = call i8 @llvm.ctpop.i8(i8 %53)
  %55 = and i8 %54, 1
  %56 = icmp eq i8 %55, 0
  store i1 %56, i1* %pf
  store volatile i64 22387, i64* @assembly_address
  %57 = load i1* %zf
  %58 = icmp eq i1 %57, false
  br i1 %58, label %block_577c, label %block_5775

block_5775:                                       ; preds = %block_576b
  store volatile i64 22389, i64* @assembly_address
  %59 = load i32* %stack_var_-52
  %60 = or i32 %59, 131072
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %61 = icmp eq i32 %60, 0
  store i1 %61, i1* %zf
  %62 = icmp slt i32 %60, 0
  store i1 %62, i1* %sf
  %63 = trunc i32 %60 to i8
  %64 = call i8 @llvm.ctpop.i8(i8 %63)
  %65 = and i8 %64, 1
  %66 = icmp eq i8 %65, 0
  store i1 %66, i1* %pf
  store i32 %60, i32* %stack_var_-52
  br label %block_577c

block_577c:                                       ; preds = %block_5775, %block_576b, %block_573f
  store volatile i64 22396, i64* @assembly_address
  %67 = load i32* bitcast (i64* @global_var_216608 to i32*)
  %68 = zext i32 %67 to i64
  store i64 %68, i64* %rax
  store volatile i64 22402, i64* @assembly_address
  %69 = load i64* %rax
  %70 = trunc i64 %69 to i32
  %71 = load i64* %rax
  %72 = trunc i64 %71 to i32
  %73 = and i32 %70, %72
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %74 = icmp eq i32 %73, 0
  store i1 %74, i1* %zf
  %75 = icmp slt i32 %73, 0
  store i1 %75, i1* %sf
  %76 = trunc i32 %73 to i8
  %77 = call i8 @llvm.ctpop.i8(i8 %76)
  %78 = and i8 %77, 1
  %79 = icmp eq i8 %78, 0
  store i1 %79, i1* %pf
  store volatile i64 22404, i64* @assembly_address
  %80 = load i1* %zf
  %81 = icmp eq i1 %80, false
  br i1 %81, label %block_57c7, label %block_5786

block_5786:                                       ; preds = %block_577c
  store volatile i64 22406, i64* @assembly_address
  %82 = load i64* %stack_var_-48
  store i64 %82, i64* %rax
  store volatile i64 22410, i64* @assembly_address
  %83 = load i64* %rax
  store i64 %83, i64* %rdi
  store volatile i64 22413, i64* @assembly_address
  %84 = load i64* %rdi
  %85 = inttoptr i64 %84 to i64*
  %86 = bitcast i64* %85 to i8*
  %87 = call i64 @last_component(i8* %86)
  store i64 %87, i64* %rax
  store i64 %87, i64* %rax
  store volatile i64 22418, i64* @assembly_address
  %88 = load i64* %rax
  store i64 %88, i64* %stack_var_-16
  store volatile i64 22422, i64* @assembly_address
  %89 = load i64* %stack_var_-16
  store i64 %89, i64* %rdx
  store volatile i64 22426, i64* @assembly_address
  %90 = load i64* %stack_var_-48
  store i64 %90, i64* %rax
  store volatile i64 22430, i64* @assembly_address
  %91 = load i64* %rdx
  %92 = load i64* %rax
  %93 = sub i64 %91, %92
  %94 = and i64 %91, 15
  %95 = and i64 %92, 15
  %96 = sub i64 %94, %95
  %97 = icmp ugt i64 %96, 15
  %98 = icmp ult i64 %91, %92
  %99 = xor i64 %91, %92
  %100 = xor i64 %91, %93
  %101 = and i64 %99, %100
  %102 = icmp slt i64 %101, 0
  store i1 %97, i1* %az
  store i1 %98, i1* %cf
  store i1 %102, i1* %of
  %103 = icmp eq i64 %93, 0
  store i1 %103, i1* %zf
  %104 = icmp slt i64 %93, 0
  store i1 %104, i1* %sf
  %105 = trunc i64 %93 to i8
  %106 = call i8 @llvm.ctpop.i8(i8 %105)
  %107 = and i8 %106, 1
  %108 = icmp eq i8 %107, 0
  store i1 %108, i1* %pf
  store i64 %93, i64* %rdx
  store volatile i64 22433, i64* @assembly_address
  %109 = load i64* %stack_var_-48
  store i64 %109, i64* %rax
  store volatile i64 22437, i64* @assembly_address
  %110 = load i64* %rdx
  store i64 %110, i64* %rsi
  store volatile i64 22440, i64* @assembly_address
  %111 = load i64* %rax
  store i64 %111, i64* %rdi
  store volatile i64 22443, i64* @assembly_address
  %112 = load i64* %rdi
  %113 = inttoptr i64 %112 to i64*
  %114 = load i64* %rsi
  %115 = trunc i64 %114 to i32
  %116 = call i64 @atdir_set(i64* %113, i32 %115)
  store i64 %116, i64* %rax
  store i64 %116, i64* %rax
  store volatile i64 22448, i64* @assembly_address
  %117 = load i64* %rax
  %118 = trunc i64 %117 to i32
  store i32 %118, i32* %stack_var_-36
  store volatile i64 22451, i64* @assembly_address
  %119 = load i32* %stack_var_-36
  %120 = and i32 %119, 15
  %121 = icmp ugt i32 %120, 15
  %122 = icmp ult i32 %119, 0
  %123 = xor i32 %119, 0
  %124 = and i32 %123, 0
  %125 = icmp slt i32 %124, 0
  store i1 %121, i1* %az
  store i1 %122, i1* %cf
  store i1 %125, i1* %of
  %126 = icmp eq i32 %119, 0
  store i1 %126, i1* %zf
  %127 = icmp slt i32 %119, 0
  store i1 %127, i1* %sf
  %128 = trunc i32 %119 to i8
  %129 = call i8 @llvm.ctpop.i8(i8 %128)
  %130 = and i8 %129, 1
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %pf
  store volatile i64 22455, i64* @assembly_address
  %132 = load i1* %sf
  br i1 %132, label %block_57c7, label %block_57b9

block_57b9:                                       ; preds = %block_5786
  store volatile i64 22457, i64* @assembly_address
  %133 = load i64* %stack_var_-16
  store i64 %133, i64* %rax
  store volatile i64 22461, i64* @assembly_address
  %134 = load i64* %rax
  store i64 %134, i64* %stack_var_-24
  store volatile i64 22465, i64* @assembly_address
  %135 = load i32* %stack_var_-36
  %136 = zext i32 %135 to i64
  store i64 %136, i64* %rax
  store volatile i64 22468, i64* @assembly_address
  %137 = load i64* %rax
  %138 = trunc i64 %137 to i32
  store i32 %138, i32* %stack_var_-40
  br label %block_57c7

block_57c7:                                       ; preds = %block_57b9, %block_5786, %block_577c
  store volatile i64 22471, i64* @assembly_address
  %139 = load i32* %stack_var_-52
  %140 = zext i32 %139 to i64
  store i64 %140, i64* %rdx
  store volatile i64 22474, i64* @assembly_address
  %141 = load i64* %stack_var_-24
  store i64 %141, i64* %rcx
  store volatile i64 22478, i64* @assembly_address
  %142 = load i32* %stack_var_-40
  %143 = zext i32 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 22481, i64* @assembly_address
  %144 = load i64* %rcx
  store i64 %144, i64* %rsi
  store volatile i64 22484, i64* @assembly_address
  %145 = load i64* %rax
  %146 = trunc i64 %145 to i32
  %147 = zext i32 %146 to i64
  store i64 %147, i64* %rdi
  store volatile i64 22486, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 22491, i64* @assembly_address
  %148 = load i64* %rdi
  %149 = load i64* %rsi
  %150 = load i64* %rdx
  %151 = trunc i64 %150 to i32
  %152 = load i64* %rcx
  %153 = load i64* %r8
  %154 = load i64* %r9
  %155 = trunc i64 %148 to i32
  %156 = call i64 @openat_safer(i32 %155, i64 %149, i32 %151, i64 %152, i64 %153, i64 %154)
  store i64 %156, i64* %rax
  store i64 %156, i64* %rax
  store volatile i64 22496, i64* @assembly_address
  %157 = load i64* %rax
  %158 = trunc i64 %157 to i32
  store i32 %158, i32* %stack_var_-32
  store volatile i64 22499, i64* @assembly_address
  %159 = load i32* %stack_var_-32
  %160 = and i32 %159, 15
  %161 = icmp ugt i32 %160, 15
  %162 = icmp ult i32 %159, 0
  %163 = xor i32 %159, 0
  %164 = and i32 %163, 0
  %165 = icmp slt i32 %164, 0
  store i1 %161, i1* %az
  store i1 %162, i1* %cf
  store i1 %165, i1* %of
  %166 = icmp eq i32 %159, 0
  store i1 %166, i1* %zf
  %167 = icmp slt i32 %159, 0
  store i1 %167, i1* %sf
  %168 = trunc i32 %159 to i8
  %169 = call i8 @llvm.ctpop.i8(i8 %168)
  %170 = and i8 %169, 1
  %171 = icmp eq i8 %170, 0
  store i1 %171, i1* %pf
  store volatile i64 22503, i64* @assembly_address
  %172 = load i1* %sf
  br i1 %172, label %block_5826, label %block_57e9

block_57e9:                                       ; preds = %block_57c7
  store volatile i64 22505, i64* @assembly_address
  %173 = load %stat** %stack_var_-64
  %174 = ptrtoint %stat* %173 to i64
  store i64 %174, i64* %rdx
  store volatile i64 22509, i64* @assembly_address
  %175 = load i32* %stack_var_-32
  %176 = zext i32 %175 to i64
  store i64 %176, i64* %rax
  store volatile i64 22512, i64* @assembly_address
  %177 = load i64* %rdx
  store i64 %177, i64* %rsi
  store volatile i64 22515, i64* @assembly_address
  %178 = load i64* %rax
  %179 = trunc i64 %178 to i32
  %180 = zext i32 %179 to i64
  store i64 %180, i64* %rdi
  store volatile i64 22517, i64* @assembly_address
  %181 = load i64* %rdi
  %182 = trunc i64 %181 to i32
  %183 = load i64* %rsi
  %184 = inttoptr i64 %183 to %stat*
  %185 = call i32 @fstat(i32 %182, %stat* %184)
  %186 = sext i32 %185 to i64
  store i64 %186, i64* %rax
  %187 = sext i32 %185 to i64
  store i64 %187, i64* %rax
  store volatile i64 22522, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  %190 = load i64* %rax
  %191 = trunc i64 %190 to i32
  %192 = and i32 %189, %191
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %193 = icmp eq i32 %192, 0
  store i1 %193, i1* %zf
  %194 = icmp slt i32 %192, 0
  store i1 %194, i1* %sf
  %195 = trunc i32 %192 to i8
  %196 = call i8 @llvm.ctpop.i8(i8 %195)
  %197 = and i8 %196, 1
  %198 = icmp eq i8 %197, 0
  store i1 %198, i1* %pf
  store volatile i64 22524, i64* @assembly_address
  %199 = load i1* %zf
  br i1 %199, label %block_5826, label %block_57fe

block_57fe:                                       ; preds = %block_57e9
  store volatile i64 22526, i64* @assembly_address
  %200 = call i32* @__errno_location()
  %201 = ptrtoint i32* %200 to i64
  store i64 %201, i64* %rax
  %202 = ptrtoint i32* %200 to i64
  store i64 %202, i64* %rax
  %203 = ptrtoint i32* %200 to i64
  store i64 %203, i64* %rax
  store volatile i64 22531, i64* @assembly_address
  %204 = load i64* %rax
  %205 = inttoptr i64 %204 to i32*
  %206 = load i32* %205
  %207 = zext i32 %206 to i64
  store i64 %207, i64* %rax
  store volatile i64 22533, i64* @assembly_address
  %208 = load i64* %rax
  %209 = trunc i64 %208 to i32
  store i32 %209, i32* %stack_var_-28
  store volatile i64 22536, i64* @assembly_address
  %210 = load i32* %stack_var_-32
  %211 = zext i32 %210 to i64
  store i64 %211, i64* %rax
  store volatile i64 22539, i64* @assembly_address
  %212 = load i64* %rax
  %213 = trunc i64 %212 to i32
  %214 = zext i32 %213 to i64
  store i64 %214, i64* %rdi
  store volatile i64 22541, i64* @assembly_address
  %215 = load i64* %rdi
  %216 = trunc i64 %215 to i32
  %217 = call i32 @close(i32 %216)
  %218 = sext i32 %217 to i64
  store i64 %218, i64* %rax
  %219 = sext i32 %217 to i64
  store i64 %219, i64* %rax
  store volatile i64 22546, i64* @assembly_address
  %220 = call i32* @__errno_location()
  %221 = ptrtoint i32* %220 to i64
  store i64 %221, i64* %rax
  %222 = ptrtoint i32* %220 to i64
  store i64 %222, i64* %rax
  %223 = ptrtoint i32* %220 to i64
  store i64 %223, i64* %rax
  store volatile i64 22551, i64* @assembly_address
  %224 = load i64* %rax
  store i64 %224, i64* %rdx
  store volatile i64 22554, i64* @assembly_address
  %225 = load i32* %stack_var_-28
  %226 = zext i32 %225 to i64
  store i64 %226, i64* %rax
  store volatile i64 22557, i64* @assembly_address
  %227 = load i64* %rax
  %228 = trunc i64 %227 to i32
  %229 = load i64* %rdx
  %230 = inttoptr i64 %229 to i32*
  store i32 %228, i32* %230
  store volatile i64 22559, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 22564, i64* @assembly_address
  br label %block_5829

block_5826:                                       ; preds = %block_57e9, %block_57c7
  store volatile i64 22566, i64* @assembly_address
  %231 = load i32* %stack_var_-32
  %232 = zext i32 %231 to i64
  store i64 %232, i64* %rax
  br label %block_5829

block_5829:                                       ; preds = %block_5826, %block_57fe
  store volatile i64 22569, i64* @assembly_address
  %233 = load i64* %stack_var_-8
  store i64 %233, i64* %rbp
  %234 = ptrtoint i64* %stack_var_0 to i64
  store i64 %234, i64* %rsp
  store volatile i64 22570, i64* @assembly_address
  %235 = load i64* %rax
  ret i64 %235
}

declare i64 @177(i64*, i64, %stat*)

declare i64 @178(i64*, i64, i64)

define i64 @open_input_file(i8* %arg1, i64* %arg2) {
block_582b:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i8*
  %2 = alloca i64
  %stack_var_-52 = alloca i32
  %stack_var_-40 = alloca i8*
  %3 = alloca i64
  %stack_var_-56 = alloca i32
  %stack_var_-60 = alloca i32
  %stack_var_-48 = alloca i64
  %stack_var_-64 = alloca i32
  %stack_var_-80 = alloca i8*
  %4 = alloca i64
  %stack_var_-88 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 22571, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 22572, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 22575, i64* @assembly_address
  %8 = load i64* %rbx
  store i64 %8, i64* %stack_var_-16
  %9 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %9, i64* %rsp
  store volatile i64 22576, i64* @assembly_address
  %10 = load i64* %rsp
  %11 = sub i64 %10, 72
  %12 = and i64 %10, 15
  %13 = sub i64 %12, 8
  %14 = icmp ugt i64 %13, 15
  %15 = icmp ult i64 %10, 72
  %16 = xor i64 %10, 72
  %17 = xor i64 %10, %11
  %18 = and i64 %16, %17
  %19 = icmp slt i64 %18, 0
  store i1 %14, i1* %az
  store i1 %15, i1* %cf
  store i1 %19, i1* %of
  %20 = icmp eq i64 %11, 0
  store i1 %20, i1* %zf
  %21 = icmp slt i64 %11, 0
  store i1 %21, i1* %sf
  %22 = trunc i64 %11 to i8
  %23 = call i8 @llvm.ctpop.i8(i8 %22)
  %24 = and i8 %23, 1
  %25 = icmp eq i8 %24, 0
  store i1 %25, i1* %pf
  %26 = ptrtoint i64* %stack_var_-88 to i64
  store i64 %26, i64* %rsp
  store volatile i64 22580, i64* @assembly_address
  %27 = load i64* %rdi
  %28 = inttoptr i64 %27 to i8*
  store i8* %28, i8** %stack_var_-80
  store volatile i64 22584, i64* @assembly_address
  %29 = load i64* %rsi
  store i64 %29, i64* %stack_var_-88
  store volatile i64 22588, i64* @assembly_address
  store i32 0, i32* %stack_var_-64
  store volatile i64 22595, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216140 to i64), i64* %rax
  store volatile i64 22602, i64* @assembly_address
  %30 = load i64* %rax
  store i64 %30, i64* %stack_var_-48
  store volatile i64 22606, i64* @assembly_address
  store i32 ptrtoint (i64* @global_var_900 to i32), i32* %stack_var_-60
  store volatile i64 22613, i64* @assembly_address
  %31 = load i64* @global_var_216630
  store i64 %31, i64* %rdx
  store volatile i64 22620, i64* @assembly_address
  %32 = load i64* %stack_var_-48
  store i64 %32, i64* %rax
  store volatile i64 22624, i64* @assembly_address
  %33 = load i64* %rdx
  %34 = load i64* %rax
  %35 = inttoptr i64 %34 to i64*
  store i64 %33, i64* %35
  store volatile i64 22627, i64* @assembly_address
  %36 = load i8** %stack_var_-80
  %37 = ptrtoint i8* %36 to i64
  store i64 %37, i64* %rax
  store volatile i64 22631, i64* @assembly_address
  %38 = load i64* %rax
  store i64 %38, i64* %rdi
  store volatile i64 22634, i64* @assembly_address
  %39 = load i64* %rdi
  %40 = inttoptr i64 %39 to i8*
  %41 = call i32 @strlen(i8* %40)
  %42 = sext i32 %41 to i64
  store i64 %42, i64* %rax
  %43 = sext i32 %41 to i64
  store i64 %43, i64* %rax
  store volatile i64 22639, i64* @assembly_address
  %44 = load i64* %rax
  %45 = sub i64 %44, 1022
  %46 = and i64 %44, 15
  %47 = sub i64 %46, 14
  %48 = icmp ugt i64 %47, 15
  %49 = icmp ult i64 %44, 1022
  %50 = xor i64 %44, 1022
  %51 = xor i64 %44, %45
  %52 = and i64 %50, %51
  %53 = icmp slt i64 %52, 0
  store i1 %48, i1* %az
  store i1 %49, i1* %cf
  store i1 %53, i1* %of
  %54 = icmp eq i64 %45, 0
  store i1 %54, i1* %zf
  %55 = icmp slt i64 %45, 0
  store i1 %55, i1* %sf
  %56 = trunc i64 %45 to i8
  %57 = call i8 @llvm.ctpop.i8(i8 %56)
  %58 = and i8 %57, 1
  %59 = icmp eq i8 %58, 0
  store i1 %59, i1* %pf
  store volatile i64 22645, i64* @assembly_address
  %60 = load i1* %cf
  %61 = load i1* %zf
  %62 = or i1 %60, %61
  %63 = icmp ne i1 %62, true
  br i1 %63, label %block_5a5f, label %block_587b

block_587b:                                       ; preds = %block_582b
  store volatile i64 22651, i64* @assembly_address
  %64 = load i8** %stack_var_-80
  %65 = ptrtoint i8* %64 to i64
  store i64 %65, i64* %rax
  store volatile i64 22655, i64* @assembly_address
  %66 = load i64* %rax
  store i64 %66, i64* %rsi
  store volatile i64 22658, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22665, i64* @assembly_address
  %67 = load i64* %rdi
  %68 = inttoptr i64 %67 to i8*
  %69 = load i64* %rsi
  %70 = inttoptr i64 %69 to i8*
  %71 = call i8* @strcpy(i8* %68, i8* %70)
  %72 = ptrtoint i8* %71 to i64
  store i64 %72, i64* %rax
  %73 = ptrtoint i8* %71 to i64
  store i64 %73, i64* %rax
  store volatile i64 22670, i64* @assembly_address
  %74 = load i64* %stack_var_-88
  store i64 %74, i64* %rdx
  store volatile i64 22674, i64* @assembly_address
  %75 = load i32* %stack_var_-60
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 22677, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i32
  %79 = zext i32 %78 to i64
  store i64 %79, i64* %rsi
  store volatile i64 22679, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22686, i64* @assembly_address
  %80 = load i64* %rdi
  %81 = inttoptr i64 %80 to i64*
  %82 = load i64* %rsi
  %83 = load i64* %rdx
  %84 = trunc i64 %82 to i32
  %85 = call i64 @open_and_stat(i64* %81, i32 %84, i64 %83)
  store i64 %85, i64* %rax
  store i64 %85, i64* %rax
  store volatile i64 22691, i64* @assembly_address
  %86 = load i64* %rax
  %87 = trunc i64 %86 to i32
  store i32 %87, i32* %stack_var_-56
  store volatile i64 22694, i64* @assembly_address
  %88 = load i32* %stack_var_-56
  %89 = and i32 %88, 15
  %90 = icmp ugt i32 %89, 15
  %91 = icmp ult i32 %88, 0
  %92 = xor i32 %88, 0
  %93 = and i32 %92, 0
  %94 = icmp slt i32 %93, 0
  store i1 %90, i1* %az
  store i1 %91, i1* %cf
  store i1 %94, i1* %of
  %95 = icmp eq i32 %88, 0
  store i1 %95, i1* %zf
  %96 = icmp slt i32 %88, 0
  store i1 %96, i1* %sf
  %97 = trunc i32 %88 to i8
  %98 = call i8 @llvm.ctpop.i8(i8 %97)
  %99 = and i8 %98, 1
  %100 = icmp eq i8 %99, 0
  store i1 %100, i1* %pf
  store volatile i64 22698, i64* @assembly_address
  %101 = load i1* %sf
  br i1 %101, label %block_58b4, label %block_58ac

block_58ac:                                       ; preds = %block_587b
  store volatile i64 22700, i64* @assembly_address
  %102 = load i32* %stack_var_-56
  %103 = zext i32 %102 to i64
  store i64 %103, i64* %rax
  store volatile i64 22703, i64* @assembly_address
  br label %block_5a98

block_58b4:                                       ; preds = %block_587b
  store volatile i64 22708, i64* @assembly_address
  %104 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %105 = zext i32 %104 to i64
  store i64 %105, i64* %rax
  store volatile i64 22714, i64* @assembly_address
  %106 = load i64* %rax
  %107 = trunc i64 %106 to i32
  %108 = load i64* %rax
  %109 = trunc i64 %108 to i32
  %110 = and i32 %107, %109
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %111 = icmp eq i32 %110, 0
  store i1 %111, i1* %zf
  %112 = icmp slt i32 %110, 0
  store i1 %112, i1* %sf
  %113 = trunc i32 %110 to i8
  %114 = call i8 @llvm.ctpop.i8(i8 %113)
  %115 = and i8 %114, 1
  %116 = icmp eq i8 %115, 0
  store i1 %116, i1* %pf
  store volatile i64 22716, i64* @assembly_address
  %117 = load i1* %zf
  br i1 %117, label %block_58ca, label %block_58be

block_58be:                                       ; preds = %block_58b4
  store volatile i64 22718, i64* @assembly_address
  %118 = call i32* @__errno_location()
  %119 = ptrtoint i32* %118 to i64
  store i64 %119, i64* %rax
  %120 = ptrtoint i32* %118 to i64
  store i64 %120, i64* %rax
  %121 = ptrtoint i32* %118 to i64
  store i64 %121, i64* %rax
  store volatile i64 22723, i64* @assembly_address
  %122 = load i64* %rax
  %123 = inttoptr i64 %122 to i32*
  %124 = load i32* %123
  %125 = zext i32 %124 to i64
  store i64 %125, i64* %rax
  store volatile i64 22725, i64* @assembly_address
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i32
  %128 = sub i32 %127, 2
  %129 = and i32 %127, 15
  %130 = sub i32 %129, 2
  %131 = icmp ugt i32 %130, 15
  %132 = icmp ult i32 %127, 2
  %133 = xor i32 %127, 2
  %134 = xor i32 %127, %128
  %135 = and i32 %133, %134
  %136 = icmp slt i32 %135, 0
  store i1 %131, i1* %az
  store i1 %132, i1* %cf
  store i1 %136, i1* %of
  %137 = icmp eq i32 %128, 0
  store i1 %137, i1* %zf
  %138 = icmp slt i32 %128, 0
  store i1 %138, i1* %sf
  %139 = trunc i32 %128 to i8
  %140 = call i8 @llvm.ctpop.i8(i8 %139)
  %141 = and i8 %140, 1
  %142 = icmp eq i8 %141, 0
  store i1 %142, i1* %pf
  store volatile i64 22728, i64* @assembly_address
  %143 = load i1* %zf
  br i1 %143, label %block_58e0, label %block_58ca

block_58ca:                                       ; preds = %block_58be, %block_58b4
  store volatile i64 22730, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22737, i64* @assembly_address
  %144 = load i64* %rdi
  %145 = inttoptr i64 %144 to i8*
  %146 = call i64 @progerror(i8* %145)
  store i64 %146, i64* %rax
  store i64 %146, i64* %rax
  store volatile i64 22742, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 22747, i64* @assembly_address
  br label %block_5a98

block_58e0:                                       ; preds = %block_58be
  store volatile i64 22752, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22759, i64* @assembly_address
  %147 = load i64* %rdi
  %148 = inttoptr i64 %147 to i64*
  %149 = bitcast i64* %148 to i8*
  %150 = call i64 @get_suffix(i8* %149)
  store i64 %150, i64* %rax
  store i64 %150, i64* %rax
  store volatile i64 22764, i64* @assembly_address
  %151 = load i64* %rax
  %152 = inttoptr i64 %151 to i8*
  store i8* %152, i8** %stack_var_-40
  store volatile i64 22768, i64* @assembly_address
  %153 = load i8** %stack_var_-40
  %154 = ptrtoint i8* %153 to i64
  %155 = and i64 %154, 15
  %156 = icmp ugt i64 %155, 15
  %157 = icmp ult i64 %154, 0
  %158 = xor i64 %154, 0
  %159 = and i64 %158, 0
  %160 = icmp slt i64 %159, 0
  store i1 %156, i1* %az
  store i1 %157, i1* %cf
  store i1 %160, i1* %of
  %161 = icmp eq i64 %154, 0
  store i1 %161, i1* %zf
  %162 = icmp slt i64 %154, 0
  store i1 %162, i1* %sf
  %163 = trunc i64 %154 to i8
  %164 = call i8 @llvm.ctpop.i8(i8 %163)
  %165 = and i8 %164, 1
  %166 = icmp eq i8 %165, 0
  store i1 %166, i1* %pf
  store volatile i64 22773, i64* @assembly_address
  %167 = load i1* %zf
  br i1 %167, label %block_590d, label %block_58f7

block_58f7:                                       ; preds = %block_58e0
  store volatile i64 22775, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22782, i64* @assembly_address
  %168 = load i64* %rdi
  %169 = inttoptr i64 %168 to i8*
  %170 = call i64 @progerror(i8* %169)
  store i64 %170, i64* %rax
  store i64 %170, i64* %rax
  store volatile i64 22787, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 22792, i64* @assembly_address
  br label %block_5a98

block_590d:                                       ; preds = %block_58e0
  store volatile i64 22797, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22804, i64* @assembly_address
  %171 = load i64* %rdi
  %172 = inttoptr i64 %171 to i8*
  %173 = call i32 @strlen(i8* %172)
  %174 = sext i32 %173 to i64
  store i64 %174, i64* %rax
  %175 = sext i32 %173 to i64
  store i64 %175, i64* %rax
  store volatile i64 22809, i64* @assembly_address
  %176 = load i64* %rax
  %177 = trunc i64 %176 to i32
  store i32 %177, i32* %stack_var_-52
  store volatile i64 22812, i64* @assembly_address
  %178 = load i64* @global_var_216630
  store i64 %178, i64* %rax
  store volatile i64 22819, i64* @assembly_address
  store i64 ptrtoint ([4 x i8]* @global_var_11044 to i64), i64* %rsi
  store volatile i64 22826, i64* @assembly_address
  %179 = load i64* %rax
  store i64 %179, i64* %rdi
  store volatile i64 22829, i64* @assembly_address
  %180 = load i64* %rdi
  %181 = inttoptr i64 %180 to i8*
  %182 = load i64* %rsi
  %183 = inttoptr i64 %182 to i8*
  %184 = call i32 @strcmp(i8* %181, i8* %183)
  %185 = sext i32 %184 to i64
  store i64 %185, i64* %rax
  %186 = sext i32 %184 to i64
  store i64 %186, i64* %rax
  store volatile i64 22834, i64* @assembly_address
  %187 = load i64* %rax
  %188 = trunc i64 %187 to i32
  %189 = load i64* %rax
  %190 = trunc i64 %189 to i32
  %191 = and i32 %188, %190
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %192 = icmp eq i32 %191, 0
  store i1 %192, i1* %zf
  %193 = icmp slt i32 %191, 0
  store i1 %193, i1* %sf
  %194 = trunc i32 %191 to i8
  %195 = call i8 @llvm.ctpop.i8(i8 %194)
  %196 = and i8 %195, 1
  %197 = icmp eq i8 %196, 0
  store i1 %197, i1* %pf
  store volatile i64 22836, i64* @assembly_address
  %198 = load i1* %zf
  %199 = icmp eq i1 %198, false
  br i1 %199, label %block_593b, label %block_5936

block_5936:                                       ; preds = %block_590d
  store volatile i64 22838, i64* @assembly_address
  %200 = load i64* %stack_var_-48
  %201 = add i64 %200, 8
  %202 = and i64 %200, 15
  %203 = add i64 %202, 8
  %204 = icmp ugt i64 %203, 15
  %205 = icmp ult i64 %201, %200
  %206 = xor i64 %200, %201
  %207 = xor i64 8, %201
  %208 = and i64 %206, %207
  %209 = icmp slt i64 %208, 0
  store i1 %204, i1* %az
  store i1 %205, i1* %cf
  store i1 %209, i1* %of
  %210 = icmp eq i64 %201, 0
  store i1 %210, i1* %zf
  %211 = icmp slt i64 %201, 0
  store i1 %211, i1* %sf
  %212 = trunc i64 %201 to i8
  %213 = call i8 @llvm.ctpop.i8(i8 %212)
  %214 = and i8 %213, 1
  %215 = icmp eq i8 %214, 0
  store i1 %215, i1* %pf
  store i64 ptrtoint ([4 x i8*]* @global_var_216148 to i64), i64* %stack_var_-48
  br label %block_593b

block_593b:                                       ; preds = %block_5a01, %block_5936, %block_590d
  store volatile i64 22843, i64* @assembly_address
  %216 = load i64* %stack_var_-48
  store i64 %216, i64* %rax
  store volatile i64 22847, i64* @assembly_address
  %217 = load i64* %rax
  %218 = inttoptr i64 %217 to i64*
  %219 = load i64* %218
  store i64 %219, i64* %rax
  store volatile i64 22850, i64* @assembly_address
  %220 = load i64* %rax
  %221 = inttoptr i64 %220 to i8*
  store i8* %221, i8** %stack_var_-40
  store volatile i64 22854, i64* @assembly_address
  %222 = load i8** %stack_var_-40
  %223 = ptrtoint i8* %222 to i64
  store i64 %223, i64* %rax
  store volatile i64 22858, i64* @assembly_address
  %224 = load i64* %rax
  %225 = inttoptr i64 %224 to i8*
  store i8* %225, i8** %stack_var_-32
  store volatile i64 22862, i64* @assembly_address
  %226 = load i8** %stack_var_-80
  %227 = ptrtoint i8* %226 to i64
  store i64 %227, i64* %rax
  store volatile i64 22866, i64* @assembly_address
  %228 = load i64* %rax
  store i64 %228, i64* %rsi
  store volatile i64 22869, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22876, i64* @assembly_address
  %229 = load i64* %rdi
  %230 = inttoptr i64 %229 to i8*
  %231 = load i64* %rsi
  %232 = inttoptr i64 %231 to i8*
  %233 = call i8* @strcpy(i8* %230, i8* %232)
  %234 = ptrtoint i8* %233 to i64
  store i64 %234, i64* %rax
  %235 = ptrtoint i8* %233 to i64
  store i64 %235, i64* %rax
  store volatile i64 22881, i64* @assembly_address
  %236 = load i32* %stack_var_-52
  %237 = zext i32 %236 to i64
  store i64 %237, i64* %rax
  store volatile i64 22884, i64* @assembly_address
  %238 = load i64* %rax
  %239 = trunc i64 %238 to i32
  %240 = sext i32 %239 to i64
  store i64 %240, i64* %rbx
  store volatile i64 22887, i64* @assembly_address
  %241 = load i8** %stack_var_-40
  %242 = ptrtoint i8* %241 to i64
  store i64 %242, i64* %rax
  store volatile i64 22891, i64* @assembly_address
  %243 = load i64* %rax
  store i64 %243, i64* %rdi
  store volatile i64 22894, i64* @assembly_address
  %244 = load i64* %rdi
  %245 = inttoptr i64 %244 to i8*
  %246 = call i32 @strlen(i8* %245)
  %247 = sext i32 %246 to i64
  store i64 %247, i64* %rax
  %248 = sext i32 %246 to i64
  store i64 %248, i64* %rax
  store volatile i64 22899, i64* @assembly_address
  %249 = load i64* %rax
  %250 = load i64* %rbx
  %251 = add i64 %249, %250
  %252 = and i64 %249, 15
  %253 = and i64 %250, 15
  %254 = add i64 %252, %253
  %255 = icmp ugt i64 %254, 15
  %256 = icmp ult i64 %251, %249
  %257 = xor i64 %249, %251
  %258 = xor i64 %250, %251
  %259 = and i64 %257, %258
  %260 = icmp slt i64 %259, 0
  store i1 %255, i1* %az
  store i1 %256, i1* %cf
  store i1 %260, i1* %of
  %261 = icmp eq i64 %251, 0
  store i1 %261, i1* %zf
  %262 = icmp slt i64 %251, 0
  store i1 %262, i1* %sf
  %263 = trunc i64 %251 to i8
  %264 = call i8 @llvm.ctpop.i8(i8 %263)
  %265 = and i8 %264, 1
  %266 = icmp eq i8 %265, 0
  store i1 %266, i1* %pf
  store i64 %251, i64* %rax
  store volatile i64 22902, i64* @assembly_address
  %267 = load i64* %rax
  %268 = sub i64 %267, 1023
  %269 = and i64 %267, 15
  %270 = sub i64 %269, 15
  %271 = icmp ugt i64 %270, 15
  %272 = icmp ult i64 %267, 1023
  %273 = xor i64 %267, 1023
  %274 = xor i64 %267, %268
  %275 = and i64 %273, %274
  %276 = icmp slt i64 %275, 0
  store i1 %271, i1* %az
  store i1 %272, i1* %cf
  store i1 %276, i1* %of
  %277 = icmp eq i64 %268, 0
  store i1 %277, i1* %zf
  %278 = icmp slt i64 %268, 0
  store i1 %278, i1* %sf
  %279 = trunc i64 %268 to i8
  %280 = call i8 @llvm.ctpop.i8(i8 %279)
  %281 = and i8 %280, 1
  %282 = icmp eq i8 %281, 0
  store i1 %282, i1* %pf
  store volatile i64 22908, i64* @assembly_address
  %283 = load i1* %cf
  %284 = load i1* %zf
  %285 = or i1 %283, %284
  %286 = icmp ne i1 %285, true
  br i1 %286, label %block_5a62, label %block_5982

block_5982:                                       ; preds = %block_593b
  store volatile i64 22914, i64* @assembly_address
  %287 = load i8** %stack_var_-40
  %288 = ptrtoint i8* %287 to i64
  store i64 %288, i64* %rax
  store volatile i64 22918, i64* @assembly_address
  %289 = load i64* %rax
  store i64 %289, i64* %rsi
  store volatile i64 22921, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22928, i64* @assembly_address
  %290 = load i64* %rdi
  %291 = inttoptr i64 %290 to i8*
  %292 = load i64* %rsi
  %293 = inttoptr i64 %292 to i8*
  %294 = call i8* @strcat(i8* %291, i8* %293)
  %295 = ptrtoint i8* %294 to i64
  store i64 %295, i64* %rax
  %296 = ptrtoint i8* %294 to i64
  store i64 %296, i64* %rax
  store volatile i64 22933, i64* @assembly_address
  %297 = load i64* %stack_var_-88
  store i64 %297, i64* %rdx
  store volatile i64 22937, i64* @assembly_address
  %298 = load i32* %stack_var_-60
  %299 = zext i32 %298 to i64
  store i64 %299, i64* %rax
  store volatile i64 22940, i64* @assembly_address
  %300 = load i64* %rax
  %301 = trunc i64 %300 to i32
  %302 = zext i32 %301 to i64
  store i64 %302, i64* %rsi
  store volatile i64 22942, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22949, i64* @assembly_address
  %303 = load i64* %rdi
  %304 = inttoptr i64 %303 to i64*
  %305 = load i64* %rsi
  %306 = load i64* %rdx
  %307 = trunc i64 %305 to i32
  %308 = call i64 @open_and_stat(i64* %304, i32 %307, i64 %306)
  store i64 %308, i64* %rax
  store i64 %308, i64* %rax
  store volatile i64 22954, i64* @assembly_address
  %309 = load i64* %rax
  %310 = trunc i64 %309 to i32
  store i32 %310, i32* %stack_var_-56
  store volatile i64 22957, i64* @assembly_address
  %311 = load i32* %stack_var_-56
  %312 = and i32 %311, 15
  %313 = icmp ugt i32 %312, 15
  %314 = icmp ult i32 %311, 0
  %315 = xor i32 %311, 0
  %316 = and i32 %315, 0
  %317 = icmp slt i32 %316, 0
  store i1 %313, i1* %az
  store i1 %314, i1* %cf
  store i1 %317, i1* %of
  %318 = icmp eq i32 %311, 0
  store i1 %318, i1* %zf
  %319 = icmp slt i32 %311, 0
  store i1 %319, i1* %sf
  %320 = trunc i32 %311 to i8
  %321 = call i8 @llvm.ctpop.i8(i8 %320)
  %322 = and i8 %321, 1
  %323 = icmp eq i8 %322, 0
  store i1 %323, i1* %pf
  store volatile i64 22961, i64* @assembly_address
  %324 = load i1* %sf
  br i1 %324, label %block_59bb, label %block_59b3

block_59b3:                                       ; preds = %block_5982
  store volatile i64 22963, i64* @assembly_address
  %325 = load i32* %stack_var_-56
  %326 = zext i32 %325 to i64
  store i64 %326, i64* %rax
  store volatile i64 22966, i64* @assembly_address
  br label %block_5a98

block_59bb:                                       ; preds = %block_5982
  store volatile i64 22971, i64* @assembly_address
  %327 = call i32* @__errno_location()
  %328 = ptrtoint i32* %327 to i64
  store i64 %328, i64* %rax
  %329 = ptrtoint i32* %327 to i64
  store i64 %329, i64* %rax
  %330 = ptrtoint i32* %327 to i64
  store i64 %330, i64* %rax
  store volatile i64 22976, i64* @assembly_address
  %331 = load i64* %rax
  %332 = inttoptr i64 %331 to i32*
  %333 = load i32* %332
  %334 = zext i32 %333 to i64
  store i64 %334, i64* %rax
  store volatile i64 22978, i64* @assembly_address
  %335 = load i64* %rax
  %336 = trunc i64 %335 to i32
  %337 = sub i32 %336, 2
  %338 = and i32 %336, 15
  %339 = sub i32 %338, 2
  %340 = icmp ugt i32 %339, 15
  %341 = icmp ult i32 %336, 2
  %342 = xor i32 %336, 2
  %343 = xor i32 %336, %337
  %344 = and i32 %342, %343
  %345 = icmp slt i32 %344, 0
  store i1 %340, i1* %az
  store i1 %341, i1* %cf
  store i1 %345, i1* %of
  %346 = icmp eq i32 %337, 0
  store i1 %346, i1* %zf
  %347 = icmp slt i32 %337, 0
  store i1 %347, i1* %sf
  %348 = trunc i32 %337 to i8
  %349 = call i8 @llvm.ctpop.i8(i8 %348)
  %350 = and i8 %349, 1
  %351 = icmp eq i8 %350, 0
  store i1 %351, i1* %pf
  store volatile i64 22981, i64* @assembly_address
  %352 = load i1* %zf
  br i1 %352, label %block_59dd, label %block_59c7

block_59c7:                                       ; preds = %block_59bb
  store volatile i64 22983, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 22990, i64* @assembly_address
  %353 = load i64* %rdi
  %354 = inttoptr i64 %353 to i8*
  %355 = call i64 @progerror(i8* %354)
  store i64 %355, i64* %rax
  store i64 %355, i64* %rax
  store volatile i64 22995, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 23000, i64* @assembly_address
  br label %block_5a98

block_59dd:                                       ; preds = %block_59bb
  store volatile i64 23005, i64* @assembly_address
  %356 = load i64* @global_var_216630
  store i64 %356, i64* %rdx
  store volatile i64 23012, i64* @assembly_address
  %357 = load i8** %stack_var_-32
  %358 = ptrtoint i8* %357 to i64
  store i64 %358, i64* %rax
  store volatile i64 23016, i64* @assembly_address
  %359 = load i64* %rdx
  store i64 %359, i64* %rsi
  store volatile i64 23019, i64* @assembly_address
  %360 = load i64* %rax
  store i64 %360, i64* %rdi
  store volatile i64 23022, i64* @assembly_address
  %361 = load i64* %rdi
  %362 = inttoptr i64 %361 to i8*
  %363 = load i64* %rsi
  %364 = inttoptr i64 %363 to i8*
  %365 = call i32 @strcmp(i8* %362, i8* %364)
  %366 = sext i32 %365 to i64
  store i64 %366, i64* %rax
  %367 = sext i32 %365 to i64
  store i64 %367, i64* %rax
  store volatile i64 23027, i64* @assembly_address
  %368 = load i64* %rax
  %369 = trunc i64 %368 to i32
  %370 = load i64* %rax
  %371 = trunc i64 %370 to i32
  %372 = and i32 %369, %371
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %373 = icmp eq i32 %372, 0
  store i1 %373, i1* %zf
  %374 = icmp slt i32 %372, 0
  store i1 %374, i1* %sf
  %375 = trunc i32 %372 to i8
  %376 = call i8 @llvm.ctpop.i8(i8 %375)
  %377 = and i8 %376, 1
  %378 = icmp eq i8 %377, 0
  store i1 %378, i1* %pf
  store volatile i64 23029, i64* @assembly_address
  %379 = load i1* %zf
  %380 = icmp eq i1 %379, false
  br i1 %380, label %block_5a01, label %block_59f7

block_59f7:                                       ; preds = %block_59dd
  store volatile i64 23031, i64* @assembly_address
  %381 = call i32* @__errno_location()
  %382 = ptrtoint i32* %381 to i64
  store i64 %382, i64* %rax
  %383 = ptrtoint i32* %381 to i64
  store i64 %383, i64* %rax
  %384 = ptrtoint i32* %381 to i64
  store i64 %384, i64* %rax
  store volatile i64 23036, i64* @assembly_address
  %385 = load i64* %rax
  %386 = inttoptr i64 %385 to i32*
  %387 = load i32* %386
  %388 = zext i32 %387 to i64
  store i64 %388, i64* %rax
  store volatile i64 23038, i64* @assembly_address
  %389 = load i64* %rax
  %390 = trunc i64 %389 to i32
  store i32 %390, i32* %stack_var_-64
  br label %block_5a01

block_5a01:                                       ; preds = %block_59f7, %block_59dd
  store volatile i64 23041, i64* @assembly_address
  %391 = load i64* %stack_var_-48
  %392 = add i64 %391, 8
  %393 = and i64 %391, 15
  %394 = add i64 %393, 8
  %395 = icmp ugt i64 %394, 15
  %396 = icmp ult i64 %392, %391
  %397 = xor i64 %391, %392
  %398 = xor i64 8, %392
  %399 = and i64 %397, %398
  %400 = icmp slt i64 %399, 0
  store i1 %395, i1* %az
  store i1 %396, i1* %cf
  store i1 %400, i1* %of
  %401 = icmp eq i64 %392, 0
  store i1 %401, i1* %zf
  %402 = icmp slt i64 %392, 0
  store i1 %402, i1* %sf
  %403 = trunc i64 %392 to i8
  %404 = call i8 @llvm.ctpop.i8(i8 %403)
  %405 = and i8 %404, 1
  %406 = icmp eq i8 %405, 0
  store i1 %406, i1* %pf
  store i64 %392, i64* %stack_var_-48
  store volatile i64 23046, i64* @assembly_address
  %407 = load i64* %stack_var_-48
  store i64 %407, i64* %rax
  store volatile i64 23050, i64* @assembly_address
  %408 = load i64* %rax
  %409 = inttoptr i64 %408 to i64*
  %410 = load i64* %409
  store i64 %410, i64* %rax
  store volatile i64 23053, i64* @assembly_address
  %411 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %412 = icmp eq i64 %411, 0
  store i1 %412, i1* %zf
  %413 = icmp slt i64 %411, 0
  store i1 %413, i1* %sf
  %414 = trunc i64 %411 to i8
  %415 = call i8 @llvm.ctpop.i8(i8 %414)
  %416 = and i8 %415, 1
  %417 = icmp eq i8 %416, 0
  store i1 %417, i1* %pf
  store volatile i64 23056, i64* @assembly_address
  %418 = load i1* %zf
  %419 = icmp eq i1 %418, false
  br i1 %419, label %block_593b, label %block_5a16

block_5a16:                                       ; preds = %block_5a01
  store volatile i64 23062, i64* @assembly_address
  %420 = load i8** %stack_var_-80
  %421 = ptrtoint i8* %420 to i64
  store i64 %421, i64* %rax
  store volatile i64 23066, i64* @assembly_address
  %422 = load i64* %rax
  store i64 %422, i64* %rsi
  store volatile i64 23069, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 23076, i64* @assembly_address
  %423 = load i64* %rdi
  %424 = inttoptr i64 %423 to i8*
  %425 = load i64* %rsi
  %426 = inttoptr i64 %425 to i8*
  %427 = call i8* @strcpy(i8* %424, i8* %426)
  %428 = ptrtoint i8* %427 to i64
  store i64 %428, i64* %rax
  %429 = ptrtoint i8* %427 to i64
  store i64 %429, i64* %rax
  store volatile i64 23081, i64* @assembly_address
  %430 = load i64* @global_var_216630
  store i64 %430, i64* %rax
  store volatile i64 23088, i64* @assembly_address
  %431 = load i64* %rax
  store i64 %431, i64* %rsi
  store volatile i64 23091, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 23098, i64* @assembly_address
  %432 = load i64* %rdi
  %433 = inttoptr i64 %432 to i8*
  %434 = load i64* %rsi
  %435 = inttoptr i64 %434 to i8*
  %436 = call i8* @strcat(i8* %433, i8* %435)
  %437 = ptrtoint i8* %436 to i64
  store i64 %437, i64* %rax
  %438 = ptrtoint i8* %436 to i64
  store i64 %438, i64* %rax
  store volatile i64 23103, i64* @assembly_address
  %439 = call i32* @__errno_location()
  %440 = ptrtoint i32* %439 to i64
  store i64 %440, i64* %rax
  %441 = ptrtoint i32* %439 to i64
  store i64 %441, i64* %rax
  %442 = ptrtoint i32* %439 to i64
  store i64 %442, i64* %rax
  store volatile i64 23108, i64* @assembly_address
  %443 = load i64* %rax
  store i64 %443, i64* %rdx
  store volatile i64 23111, i64* @assembly_address
  %444 = load i32* %stack_var_-64
  %445 = zext i32 %444 to i64
  store i64 %445, i64* %rax
  store volatile i64 23114, i64* @assembly_address
  %446 = load i64* %rax
  %447 = trunc i64 %446 to i32
  %448 = load i64* %rdx
  %449 = inttoptr i64 %448 to i32*
  store i32 %447, i32* %449
  store volatile i64 23116, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 23123, i64* @assembly_address
  %450 = load i64* %rdi
  %451 = inttoptr i64 %450 to i8*
  %452 = call i64 @progerror(i8* %451)
  store i64 %452, i64* %rax
  store i64 %452, i64* %rax
  store volatile i64 23128, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 23133, i64* @assembly_address
  br label %block_5a98

block_5a5f:                                       ; preds = %block_582b
  store volatile i64 23135, i64* @assembly_address
  store volatile i64 23136, i64* @assembly_address
  br label %block_5a63

block_5a62:                                       ; preds = %block_593b
  store volatile i64 23138, i64* @assembly_address
  br label %block_5a63

block_5a63:                                       ; preds = %block_5a62, %block_5a5f
  store volatile i64 23139, i64* @assembly_address
  %453 = load i64* @global_var_25f4c8
  store i64 %453, i64* %rdx
  store volatile i64 23146, i64* @assembly_address
  %454 = load i64* @global_var_216580
  store i64 %454, i64* %rax
  store volatile i64 23153, i64* @assembly_address
  %455 = load i8** %stack_var_-80
  %456 = ptrtoint i8* %455 to i64
  store i64 %456, i64* %rcx
  store volatile i64 23157, i64* @assembly_address
  store i64 ptrtoint ([28 x i8]* @global_var_11711 to i64), i64* %rsi
  store volatile i64 23164, i64* @assembly_address
  %457 = load i64* %rax
  store i64 %457, i64* %rdi
  store volatile i64 23167, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 23172, i64* @assembly_address
  %458 = load i64* %rdi
  %459 = inttoptr i64 %458 to %_IO_FILE*
  %460 = load i64* %rsi
  %461 = inttoptr i64 %460 to i8*
  %462 = load i64* %rdx
  %463 = inttoptr i64 %462 to i8*
  %464 = load i64* %rcx
  %465 = inttoptr i64 %464 to i8*
  %466 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %459, i8* %461, i8* %463, i8* %465)
  %467 = sext i32 %466 to i64
  store i64 %467, i64* %rax
  %468 = sext i32 %466 to i64
  store i64 %468, i64* %rax
  store volatile i64 23177, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 23187, i64* @assembly_address
  store i64 4294967295, i64* %rax
  br label %block_5a98

block_5a98:                                       ; preds = %block_5a63, %block_5a16, %block_59c7, %block_59b3, %block_58f7, %block_58ca, %block_58ac
  store volatile i64 23192, i64* @assembly_address
  %469 = load i64* %rsp
  %470 = add i64 %469, 72
  %471 = and i64 %469, 15
  %472 = add i64 %471, 8
  %473 = icmp ugt i64 %472, 15
  %474 = icmp ult i64 %470, %469
  %475 = xor i64 %469, %470
  %476 = xor i64 72, %470
  %477 = and i64 %475, %476
  %478 = icmp slt i64 %477, 0
  store i1 %473, i1* %az
  store i1 %474, i1* %cf
  store i1 %478, i1* %of
  %479 = icmp eq i64 %470, 0
  store i1 %479, i1* %zf
  %480 = icmp slt i64 %470, 0
  store i1 %480, i1* %sf
  %481 = trunc i64 %470 to i8
  %482 = call i8 @llvm.ctpop.i8(i8 %481)
  %483 = and i8 %482, 1
  %484 = icmp eq i8 %483, 0
  store i1 %484, i1* %pf
  %485 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %485, i64* %rsp
  store volatile i64 23196, i64* @assembly_address
  %486 = load i64* %stack_var_-16
  store i64 %486, i64* %rbx
  %487 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %487, i64* %rsp
  store volatile i64 23197, i64* @assembly_address
  %488 = load i64* %stack_var_-8
  store i64 %488, i64* %rbp
  %489 = ptrtoint i64* %stack_var_0 to i64
  store i64 %489, i64* %rsp
  store volatile i64 23198, i64* @assembly_address
  %490 = load i64* %rax
  ret i64 %490
}

declare i64 @179(i64, i64*)

define i64 @make_ofname() {
block_5a9f:
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %0 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 23199, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 23200, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 23203, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 23207, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rsi
  store volatile i64 23214, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 23221, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = inttoptr i64 %20 to i8*
  %22 = load i64* %rsi
  %23 = inttoptr i64 %22 to i8*
  %24 = call i8* @strcpy(i8* %21, i8* %23)
  %25 = ptrtoint i8* %24 to i64
  store i64 %25, i64* %rax
  %26 = ptrtoint i8* %24 to i64
  store i64 %26, i64* %rax
  store volatile i64 23226, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 23233, i64* @assembly_address
  %27 = load i64* %rdi
  %28 = inttoptr i64 %27 to i64*
  %29 = bitcast i64* %28 to i8*
  %30 = call i64 @get_suffix(i8* %29)
  store i64 %30, i64* %rax
  store i64 %30, i64* %rax
  store volatile i64 23238, i64* @assembly_address
  %31 = load i64* %rax
  %32 = inttoptr i64 %31 to i8*
  store i8* %32, i8** %stack_var_-16
  store volatile i64 23242, i64* @assembly_address
  %33 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %34 = zext i32 %33 to i64
  store i64 %34, i64* %rax
  store volatile i64 23248, i64* @assembly_address
  %35 = load i64* %rax
  %36 = trunc i64 %35 to i32
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i32
  %39 = and i32 %36, %38
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %40 = icmp eq i32 %39, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i32 %39, 0
  store i1 %41, i1* %sf
  %42 = trunc i32 %39 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  store volatile i64 23250, i64* @assembly_address
  %46 = load i1* %zf
  br i1 %46, label %block_5bd3, label %block_5ad8

block_5ad8:                                       ; preds = %block_5a9f
  store volatile i64 23256, i64* @assembly_address
  %47 = load i8** %stack_var_-16
  %48 = ptrtoint i8* %47 to i64
  %49 = and i64 %48, 15
  %50 = icmp ugt i64 %49, 15
  %51 = icmp ult i64 %48, 0
  %52 = xor i64 %48, 0
  %53 = and i64 %52, 0
  %54 = icmp slt i64 %53, 0
  store i1 %50, i1* %az
  store i1 %51, i1* %cf
  store i1 %54, i1* %of
  %55 = icmp eq i64 %48, 0
  store i1 %55, i1* %zf
  %56 = icmp slt i64 %48, 0
  store i1 %56, i1* %sf
  %57 = trunc i64 %48 to i8
  %58 = call i8 @llvm.ctpop.i8(i8 %57)
  %59 = and i8 %58, 1
  %60 = icmp eq i8 %59, 0
  store i1 %60, i1* %pf
  store volatile i64 23261, i64* @assembly_address
  %61 = load i1* %zf
  %62 = icmp eq i1 %61, false
  br i1 %62, label %block_5b7a, label %block_5ae3

block_5ae3:                                       ; preds = %block_5ad8
  store volatile i64 23267, i64* @assembly_address
  %63 = load i32* bitcast (i64* @global_var_21660c to i32*)
  %64 = zext i32 %63 to i64
  store i64 %64, i64* %rax
  store volatile i64 23273, i64* @assembly_address
  %65 = load i64* %rax
  %66 = trunc i64 %65 to i32
  %67 = load i64* %rax
  %68 = trunc i64 %67 to i32
  %69 = and i32 %66, %68
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %70 = icmp eq i32 %69, 0
  store i1 %70, i1* %zf
  %71 = icmp slt i32 %69, 0
  store i1 %71, i1* %sf
  %72 = trunc i32 %69 to i8
  %73 = call i8 @llvm.ctpop.i8(i8 %72)
  %74 = and i8 %73, 1
  %75 = icmp eq i8 %74, 0
  store i1 %75, i1* %pf
  store volatile i64 23275, i64* @assembly_address
  %76 = load i1* %zf
  %77 = icmp eq i1 %76, false
  br i1 %77, label %block_5b0b, label %block_5aed

block_5aed:                                       ; preds = %block_5ae3
  store volatile i64 23277, i64* @assembly_address
  %78 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %79 = zext i32 %78 to i64
  store i64 %79, i64* %rax
  store volatile i64 23283, i64* @assembly_address
  %80 = load i64* %rax
  %81 = trunc i64 %80 to i32
  %82 = load i64* %rax
  %83 = trunc i64 %82 to i32
  %84 = and i32 %81, %83
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %85 = icmp eq i32 %84, 0
  store i1 %85, i1* %zf
  %86 = icmp slt i32 %84, 0
  store i1 %86, i1* %sf
  %87 = trunc i32 %84 to i8
  %88 = call i8 @llvm.ctpop.i8(i8 %87)
  %89 = and i8 %88, 1
  %90 = icmp eq i8 %89, 0
  store i1 %90, i1* %pf
  store volatile i64 23285, i64* @assembly_address
  %91 = load i1* %zf
  %92 = icmp eq i1 %91, false
  br i1 %92, label %block_5b01, label %block_5af7

block_5af7:                                       ; preds = %block_5aed
  store volatile i64 23287, i64* @assembly_address
  %93 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %94 = zext i32 %93 to i64
  store i64 %94, i64* %rax
  store volatile i64 23293, i64* @assembly_address
  %95 = load i64* %rax
  %96 = trunc i64 %95 to i32
  %97 = load i64* %rax
  %98 = trunc i64 %97 to i32
  %99 = and i32 %96, %98
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %100 = icmp eq i32 %99, 0
  store i1 %100, i1* %zf
  %101 = icmp slt i32 %99, 0
  store i1 %101, i1* %sf
  %102 = trunc i32 %99 to i8
  %103 = call i8 @llvm.ctpop.i8(i8 %102)
  %104 = and i8 %103, 1
  %105 = icmp eq i8 %104, 0
  store i1 %105, i1* %pf
  store volatile i64 23295, i64* @assembly_address
  %106 = load i1* %zf
  br i1 %106, label %block_5b0b, label %block_5b01

block_5b01:                                       ; preds = %block_5af7, %block_5aed
  store volatile i64 23297, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 23302, i64* @assembly_address
  br label %block_5cd1

block_5b0b:                                       ; preds = %block_5af7, %block_5ae3
  store volatile i64 23307, i64* @assembly_address
  %107 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %108 = zext i32 %107 to i64
  store i64 %108, i64* %rax
  store volatile i64 23313, i64* @assembly_address
  %109 = load i64* %rax
  %110 = trunc i64 %109 to i32
  %111 = load i64* %rax
  %112 = trunc i64 %111 to i32
  %113 = and i32 %110, %112
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %114 = icmp eq i32 %113, 0
  store i1 %114, i1* %zf
  %115 = icmp slt i32 %113, 0
  store i1 %115, i1* %sf
  %116 = trunc i32 %113 to i8
  %117 = call i8 @llvm.ctpop.i8(i8 %116)
  %118 = and i8 %117, 1
  %119 = icmp eq i8 %118, 0
  store i1 %119, i1* %pf
  store volatile i64 23315, i64* @assembly_address
  %120 = load i1* %zf
  %121 = icmp eq i1 %120, false
  br i1 %121, label %block_5b29, label %block_5b15

block_5b15:                                       ; preds = %block_5b0b
  store volatile i64 23317, i64* @assembly_address
  %122 = load i32* bitcast (i64* @global_var_21660c to i32*)
  %123 = zext i32 %122 to i64
  store i64 %123, i64* %rax
  store volatile i64 23323, i64* @assembly_address
  %124 = load i64* %rax
  %125 = trunc i64 %124 to i32
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i32
  %128 = and i32 %125, %127
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %129 = icmp eq i32 %128, 0
  store i1 %129, i1* %zf
  %130 = icmp slt i32 %128, 0
  store i1 %130, i1* %sf
  %131 = trunc i32 %128 to i8
  %132 = call i8 @llvm.ctpop.i8(i8 %131)
  %133 = and i8 %132, 1
  %134 = icmp eq i8 %133, 0
  store i1 %134, i1* %pf
  store volatile i64 23325, i64* @assembly_address
  %135 = load i1* %zf
  %136 = icmp eq i1 %135, false
  br i1 %136, label %block_5b70, label %block_5b1f

block_5b1f:                                       ; preds = %block_5b15
  store volatile i64 23327, i64* @assembly_address
  %137 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %138 = zext i32 %137 to i64
  store i64 %138, i64* %rax
  store volatile i64 23333, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  %143 = and i32 %140, %142
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %144 = icmp eq i32 %143, 0
  store i1 %144, i1* %zf
  %145 = icmp slt i32 %143, 0
  store i1 %145, i1* %sf
  %146 = trunc i32 %143 to i8
  %147 = call i8 @llvm.ctpop.i8(i8 %146)
  %148 = and i8 %147, 1
  %149 = icmp eq i8 %148, 0
  store i1 %149, i1* %pf
  store volatile i64 23335, i64* @assembly_address
  %150 = load i1* %zf
  %151 = icmp eq i1 %150, false
  br i1 %151, label %block_5b70, label %block_5b29

block_5b29:                                       ; preds = %block_5b1f, %block_5b0b
  store volatile i64 23337, i64* @assembly_address
  %152 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %153 = zext i32 %152 to i64
  store i64 %153, i64* %rax
  store volatile i64 23343, i64* @assembly_address
  %154 = load i64* %rax
  %155 = trunc i64 %154 to i32
  %156 = load i64* %rax
  %157 = trunc i64 %156 to i32
  %158 = and i32 %155, %157
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %159 = icmp eq i32 %158, 0
  store i1 %159, i1* %zf
  %160 = icmp slt i32 %158, 0
  store i1 %160, i1* %sf
  %161 = trunc i32 %158 to i8
  %162 = call i8 @llvm.ctpop.i8(i8 %161)
  %163 = and i8 %162, 1
  %164 = icmp eq i8 %163, 0
  store i1 %164, i1* %pf
  store volatile i64 23345, i64* @assembly_address
  %165 = load i1* %zf
  %166 = icmp eq i1 %165, false
  br i1 %166, label %block_5b5c, label %block_5b33

block_5b33:                                       ; preds = %block_5b29
  store volatile i64 23347, i64* @assembly_address
  %167 = load i64* @global_var_25f4c8
  store i64 %167, i64* %rdx
  store volatile i64 23354, i64* @assembly_address
  %168 = load i64* @global_var_216580
  store i64 %168, i64* %rax
  store volatile i64 23361, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 23368, i64* @assembly_address
  store i64 ptrtoint ([35 x i8]* @global_var_11730 to i64), i64* %rsi
  store volatile i64 23375, i64* @assembly_address
  %169 = load i64* %rax
  store i64 %169, i64* %rdi
  store volatile i64 23378, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 23383, i64* @assembly_address
  %170 = load i64* %rdi
  %171 = inttoptr i64 %170 to %_IO_FILE*
  %172 = load i64* %rsi
  %173 = inttoptr i64 %172 to i8*
  %174 = load i64* %rdx
  %175 = inttoptr i64 %174 to i8*
  %176 = load i64* %rcx
  %177 = inttoptr i64 %176 to i8*
  %178 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %171, i8* %173, i8* %175, i8* %177)
  %179 = sext i32 %178 to i64
  store i64 %179, i64* %rax
  %180 = sext i32 %178 to i64
  store i64 %180, i64* %rax
  br label %block_5b5c

block_5b5c:                                       ; preds = %block_5b33, %block_5b29
  store volatile i64 23388, i64* @assembly_address
  %181 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %182 = zext i32 %181 to i64
  store i64 %182, i64* %rax
  store volatile i64 23394, i64* @assembly_address
  %183 = load i64* %rax
  %184 = trunc i64 %183 to i32
  %185 = load i64* %rax
  %186 = trunc i64 %185 to i32
  %187 = and i32 %184, %186
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %188 = icmp eq i32 %187, 0
  store i1 %188, i1* %zf
  %189 = icmp slt i32 %187, 0
  store i1 %189, i1* %sf
  %190 = trunc i32 %187 to i8
  %191 = call i8 @llvm.ctpop.i8(i8 %190)
  %192 = and i8 %191, 1
  %193 = icmp eq i8 %192, 0
  store i1 %193, i1* %pf
  store volatile i64 23396, i64* @assembly_address
  %194 = load i1* %zf
  %195 = icmp eq i1 %194, false
  br i1 %195, label %block_5b70, label %block_5b66

block_5b66:                                       ; preds = %block_5b5c
  store volatile i64 23398, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_5b70

block_5b70:                                       ; preds = %block_5b66, %block_5b5c, %block_5b1f, %block_5b15
  store volatile i64 23408, i64* @assembly_address
  store i64 2, i64* %rax
  store volatile i64 23413, i64* @assembly_address
  br label %block_5cd1

block_5b7a:                                       ; preds = %block_5ad8
  store volatile i64 23418, i64* @assembly_address
  %196 = load i8** %stack_var_-16
  %197 = ptrtoint i8* %196 to i64
  store i64 %197, i64* %rax
  store volatile i64 23422, i64* @assembly_address
  %198 = load i64* %rax
  store i64 %198, i64* %rdi
  store volatile i64 23425, i64* @assembly_address
  %199 = load i64* %rdi
  %200 = inttoptr i64 %199 to i8*
  %201 = call i64 @strlwr(i8* %200)
  store i64 %201, i64* %rax
  store i64 %201, i64* %rax
  store volatile i64 23430, i64* @assembly_address
  %202 = load i8** %stack_var_-16
  %203 = ptrtoint i8* %202 to i64
  store i64 %203, i64* %rax
  store volatile i64 23434, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11753 to i64), i64* %rsi
  store volatile i64 23441, i64* @assembly_address
  %204 = load i64* %rax
  store i64 %204, i64* %rdi
  store volatile i64 23444, i64* @assembly_address
  %205 = load i64* %rdi
  %206 = inttoptr i64 %205 to i8*
  %207 = load i64* %rsi
  %208 = inttoptr i64 %207 to i8*
  %209 = call i32 @strcmp(i8* %206, i8* %208)
  %210 = sext i32 %209 to i64
  store i64 %210, i64* %rax
  %211 = sext i32 %209 to i64
  store i64 %211, i64* %rax
  store volatile i64 23449, i64* @assembly_address
  %212 = load i64* %rax
  %213 = trunc i64 %212 to i32
  %214 = load i64* %rax
  %215 = trunc i64 %214 to i32
  %216 = and i32 %213, %215
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %217 = icmp eq i32 %216, 0
  store i1 %217, i1* %zf
  %218 = icmp slt i32 %216, 0
  store i1 %218, i1* %sf
  %219 = trunc i32 %216 to i8
  %220 = call i8 @llvm.ctpop.i8(i8 %219)
  %221 = and i8 %220, 1
  %222 = icmp eq i8 %221, 0
  store i1 %222, i1* %pf
  store volatile i64 23451, i64* @assembly_address
  %223 = load i1* %zf
  br i1 %223, label %block_5bb4, label %block_5b9d

block_5b9d:                                       ; preds = %block_5b7a
  store volatile i64 23453, i64* @assembly_address
  %224 = load i8** %stack_var_-16
  %225 = ptrtoint i8* %224 to i64
  store i64 %225, i64* %rax
  store volatile i64 23457, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11758 to i64), i64* %rsi
  store volatile i64 23464, i64* @assembly_address
  %226 = load i64* %rax
  store i64 %226, i64* %rdi
  store volatile i64 23467, i64* @assembly_address
  %227 = load i64* %rdi
  %228 = inttoptr i64 %227 to i8*
  %229 = load i64* %rsi
  %230 = inttoptr i64 %229 to i8*
  %231 = call i32 @strcmp(i8* %228, i8* %230)
  %232 = sext i32 %231 to i64
  store i64 %232, i64* %rax
  %233 = sext i32 %231 to i64
  store i64 %233, i64* %rax
  store volatile i64 23472, i64* @assembly_address
  %234 = load i64* %rax
  %235 = trunc i64 %234 to i32
  %236 = load i64* %rax
  %237 = trunc i64 %236 to i32
  %238 = and i32 %235, %237
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %239 = icmp eq i32 %238, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i32 %238, 0
  store i1 %240, i1* %sf
  %241 = trunc i32 %238 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  store volatile i64 23474, i64* @assembly_address
  %245 = load i1* %zf
  %246 = icmp eq i1 %245, false
  br i1 %246, label %block_5bc7, label %block_5bb4

block_5bb4:                                       ; preds = %block_5b9d, %block_5b7a
  store volatile i64 23476, i64* @assembly_address
  %247 = load i8** %stack_var_-16
  %248 = ptrtoint i8* %247 to i64
  store i64 %248, i64* %rax
  store volatile i64 23480, i64* @assembly_address
  %249 = load i64* %rax
  %250 = inttoptr i64 %249 to i32*
  store i32 1918989358, i32* %250
  store volatile i64 23486, i64* @assembly_address
  %251 = load i64* %rax
  %252 = add i64 %251, 4
  %253 = inttoptr i64 %252 to i8*
  store i8 0, i8* %253
  store volatile i64 23490, i64* @assembly_address
  br label %block_5c7d

block_5bc7:                                       ; preds = %block_5b9d
  store volatile i64 23495, i64* @assembly_address
  %254 = load i8** %stack_var_-16
  %255 = ptrtoint i8* %254 to i64
  store i64 %255, i64* %rax
  store volatile i64 23499, i64* @assembly_address
  %256 = load i64* %rax
  %257 = inttoptr i64 %256 to i8*
  store i8 0, i8* %257
  store volatile i64 23502, i64* @assembly_address
  br label %block_5c7d

block_5bd3:                                       ; preds = %block_5a9f
  store volatile i64 23507, i64* @assembly_address
  %258 = load i8** %stack_var_-16
  %259 = ptrtoint i8* %258 to i64
  %260 = and i64 %259, 15
  %261 = icmp ugt i64 %260, 15
  %262 = icmp ult i64 %259, 0
  %263 = xor i64 %259, 0
  %264 = and i64 %263, 0
  %265 = icmp slt i64 %264, 0
  store i1 %261, i1* %az
  store i1 %262, i1* %cf
  store i1 %265, i1* %of
  %266 = icmp eq i64 %259, 0
  store i1 %266, i1* %zf
  %267 = icmp slt i64 %259, 0
  store i1 %267, i1* %sf
  %268 = trunc i64 %259 to i8
  %269 = call i8 @llvm.ctpop.i8(i8 %268)
  %270 = and i8 %269, 1
  %271 = icmp eq i8 %270, 0
  store i1 %271, i1* %pf
  store volatile i64 23512, i64* @assembly_address
  %272 = load i1* %zf
  br i1 %272, label %block_5c3c, label %block_5bda

block_5bda:                                       ; preds = %block_5bd3
  store volatile i64 23514, i64* @assembly_address
  %273 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %274 = zext i32 %273 to i64
  store i64 %274, i64* %rax
  store volatile i64 23520, i64* @assembly_address
  %275 = load i64* %rax
  %276 = trunc i64 %275 to i32
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i32
  %279 = and i32 %276, %278
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %280 = icmp eq i32 %279, 0
  store i1 %280, i1* %zf
  %281 = icmp slt i32 %279, 0
  store i1 %281, i1* %sf
  %282 = trunc i32 %279 to i8
  %283 = call i8 @llvm.ctpop.i8(i8 %282)
  %284 = and i8 %283, 1
  %285 = icmp eq i8 %284, 0
  store i1 %285, i1* %pf
  store volatile i64 23522, i64* @assembly_address
  %286 = load i1* %zf
  %287 = icmp eq i1 %286, false
  br i1 %287, label %block_5c3c, label %block_5be4

block_5be4:                                       ; preds = %block_5bda
  store volatile i64 23524, i64* @assembly_address
  %288 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %289 = zext i32 %288 to i64
  store i64 %289, i64* %rax
  store volatile i64 23530, i64* @assembly_address
  %290 = load i64* %rax
  %291 = trunc i64 %290 to i32
  %292 = load i64* %rax
  %293 = trunc i64 %292 to i32
  %294 = and i32 %291, %293
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %295 = icmp eq i32 %294, 0
  store i1 %295, i1* %zf
  %296 = icmp slt i32 %294, 0
  store i1 %296, i1* %sf
  %297 = trunc i32 %294 to i8
  %298 = call i8 @llvm.ctpop.i8(i8 %297)
  %299 = and i8 %298, 1
  %300 = icmp eq i8 %299, 0
  store i1 %300, i1* %pf
  store volatile i64 23532, i64* @assembly_address
  %301 = load i1* %zf
  %302 = icmp eq i1 %301, false
  br i1 %302, label %block_5c02, label %block_5bee

block_5bee:                                       ; preds = %block_5be4
  store volatile i64 23534, i64* @assembly_address
  %303 = load i32* bitcast (i64* @global_var_21660c to i32*)
  %304 = zext i32 %303 to i64
  store i64 %304, i64* %rax
  store volatile i64 23540, i64* @assembly_address
  %305 = load i64* %rax
  %306 = trunc i64 %305 to i32
  %307 = load i64* %rax
  %308 = trunc i64 %307 to i32
  %309 = and i32 %306, %308
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %310 = icmp eq i32 %309, 0
  store i1 %310, i1* %zf
  %311 = icmp slt i32 %309, 0
  store i1 %311, i1* %sf
  %312 = trunc i32 %309 to i8
  %313 = call i8 @llvm.ctpop.i8(i8 %312)
  %314 = and i8 %313, 1
  %315 = icmp eq i8 %314, 0
  store i1 %315, i1* %pf
  store volatile i64 23542, i64* @assembly_address
  %316 = load i1* %zf
  %317 = icmp eq i1 %316, false
  br i1 %317, label %block_5c32, label %block_5bf8

block_5bf8:                                       ; preds = %block_5bee
  store volatile i64 23544, i64* @assembly_address
  %318 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %319 = zext i32 %318 to i64
  store i64 %319, i64* %rax
  store volatile i64 23550, i64* @assembly_address
  %320 = load i64* %rax
  %321 = trunc i64 %320 to i32
  %322 = load i64* %rax
  %323 = trunc i64 %322 to i32
  %324 = and i32 %321, %323
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %325 = icmp eq i32 %324, 0
  store i1 %325, i1* %zf
  %326 = icmp slt i32 %324, 0
  store i1 %326, i1* %sf
  %327 = trunc i32 %324 to i8
  %328 = call i8 @llvm.ctpop.i8(i8 %327)
  %329 = and i8 %328, 1
  %330 = icmp eq i8 %329, 0
  store i1 %330, i1* %pf
  store volatile i64 23552, i64* @assembly_address
  %331 = load i1* %zf
  %332 = icmp eq i1 %331, false
  br i1 %332, label %block_5c32, label %block_5c02

block_5c02:                                       ; preds = %block_5bf8, %block_5be4
  store volatile i64 23554, i64* @assembly_address
  %333 = load i64* @global_var_25f4c8
  store i64 %333, i64* %rdx
  store volatile i64 23561, i64* @assembly_address
  %334 = load i64* @global_var_216580
  store i64 %334, i64* %rax
  store volatile i64 23568, i64* @assembly_address
  %335 = load i8** %stack_var_-16
  %336 = ptrtoint i8* %335 to i64
  store i64 %336, i64* %rcx
  store volatile i64 23572, i64* @assembly_address
  %337 = load i64* %rcx
  store i64 %337, i64* %r8
  store volatile i64 23575, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 23582, i64* @assembly_address
  store i64 ptrtoint ([43 x i8]* @global_var_11760 to i64), i64* %rsi
  store volatile i64 23589, i64* @assembly_address
  %338 = load i64* %rax
  store i64 %338, i64* %rdi
  store volatile i64 23592, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 23597, i64* @assembly_address
  %339 = load i64* %rdi
  %340 = inttoptr i64 %339 to %_IO_FILE*
  %341 = load i64* %rsi
  %342 = inttoptr i64 %341 to i8*
  %343 = load i64* %rdx
  %344 = inttoptr i64 %343 to i8*
  %345 = load i64* %rcx
  %346 = inttoptr i64 %345 to i8*
  %347 = load i64* %r8
  %348 = inttoptr i64 %347 to i8*
  %349 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %340, i8* %342, i8* %344, i8* %346, i8* %348)
  %350 = sext i32 %349 to i64
  store i64 %350, i64* %rax
  %351 = sext i32 %349 to i64
  store i64 %351, i64* %rax
  br label %block_5c32

block_5c32:                                       ; preds = %block_5c02, %block_5bf8, %block_5bee
  store volatile i64 23602, i64* @assembly_address
  store i64 2, i64* %rax
  store volatile i64 23607, i64* @assembly_address
  br label %block_5cd1

block_5c3c:                                       ; preds = %block_5bda, %block_5bd3
  store volatile i64 23612, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_24a888 to i32*)
  store volatile i64 23622, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 23629, i64* @assembly_address
  %352 = load i64* %rdi
  %353 = inttoptr i64 %352 to i8*
  %354 = call i32 @strlen(i8* %353)
  %355 = sext i32 %354 to i64
  store i64 %355, i64* %rax
  %356 = sext i32 %354 to i64
  store i64 %356, i64* %rax
  store volatile i64 23634, i64* @assembly_address
  %357 = load i64* %rax
  store i64 %357, i64* %rdx
  store volatile i64 23637, i64* @assembly_address
  %358 = load i64* @global_var_216638
  store i64 %358, i64* %rax
  store volatile i64 23644, i64* @assembly_address
  %359 = load i64* %rax
  %360 = load i64* %rdx
  %361 = add i64 %359, %360
  %362 = and i64 %359, 15
  %363 = and i64 %360, 15
  %364 = add i64 %362, %363
  %365 = icmp ugt i64 %364, 15
  %366 = icmp ult i64 %361, %359
  %367 = xor i64 %359, %361
  %368 = xor i64 %360, %361
  %369 = and i64 %367, %368
  %370 = icmp slt i64 %369, 0
  store i1 %365, i1* %az
  store i1 %366, i1* %cf
  store i1 %370, i1* %of
  %371 = icmp eq i64 %361, 0
  store i1 %371, i1* %zf
  %372 = icmp slt i64 %361, 0
  store i1 %372, i1* %sf
  %373 = trunc i64 %361 to i8
  %374 = call i8 @llvm.ctpop.i8(i8 %373)
  %375 = and i8 %374, 1
  %376 = icmp eq i8 %375, 0
  store i1 %376, i1* %pf
  store i64 %361, i64* %rax
  store volatile i64 23647, i64* @assembly_address
  %377 = load i64* %rax
  %378 = sub i64 %377, 1023
  %379 = and i64 %377, 15
  %380 = sub i64 %379, 15
  %381 = icmp ugt i64 %380, 15
  %382 = icmp ult i64 %377, 1023
  %383 = xor i64 %377, 1023
  %384 = xor i64 %377, %378
  %385 = and i64 %383, %384
  %386 = icmp slt i64 %385, 0
  store i1 %381, i1* %az
  store i1 %382, i1* %cf
  store i1 %386, i1* %of
  %387 = icmp eq i64 %378, 0
  store i1 %387, i1* %zf
  %388 = icmp slt i64 %378, 0
  store i1 %388, i1* %sf
  %389 = trunc i64 %378 to i8
  %390 = call i8 @llvm.ctpop.i8(i8 %389)
  %391 = and i8 %390, 1
  %392 = icmp eq i8 %391, 0
  store i1 %392, i1* %pf
  store volatile i64 23653, i64* @assembly_address
  %393 = load i1* %cf
  %394 = load i1* %zf
  %395 = or i1 %393, %394
  %396 = icmp ne i1 %395, true
  br i1 %396, label %block_5c84, label %block_5c67

block_5c67:                                       ; preds = %block_5c3c
  store volatile i64 23655, i64* @assembly_address
  %397 = load i64* @global_var_216630
  store i64 %397, i64* %rax
  store volatile i64 23662, i64* @assembly_address
  %398 = load i64* %rax
  store i64 %398, i64* %rsi
  store volatile i64 23665, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 23672, i64* @assembly_address
  %399 = load i64* %rdi
  %400 = inttoptr i64 %399 to i8*
  %401 = load i64* %rsi
  %402 = inttoptr i64 %401 to i8*
  %403 = call i8* @strcat(i8* %400, i8* %402)
  %404 = ptrtoint i8* %403 to i64
  store i64 %404, i64* %rax
  %405 = ptrtoint i8* %403 to i64
  store i64 %405, i64* %rax
  br label %block_5c7d

block_5c7d:                                       ; preds = %block_5c67, %block_5bc7, %block_5bb4
  store volatile i64 23677, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 23682, i64* @assembly_address
  br label %block_5cd1

block_5c84:                                       ; preds = %block_5c3c
  store volatile i64 23684, i64* @assembly_address
  store volatile i64 23685, i64* @assembly_address
  %406 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %407 = zext i32 %406 to i64
  store i64 %407, i64* %rax
  store volatile i64 23691, i64* @assembly_address
  %408 = load i64* %rax
  %409 = trunc i64 %408 to i32
  %410 = load i64* %rax
  %411 = trunc i64 %410 to i32
  %412 = and i32 %409, %411
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %413 = icmp eq i32 %412, 0
  store i1 %413, i1* %zf
  %414 = icmp slt i32 %412, 0
  store i1 %414, i1* %sf
  %415 = trunc i32 %412 to i8
  %416 = call i8 @llvm.ctpop.i8(i8 %415)
  %417 = and i8 %416, 1
  %418 = icmp eq i8 %417, 0
  store i1 %418, i1* %pf
  store volatile i64 23693, i64* @assembly_address
  %419 = load i1* %zf
  %420 = icmp eq i1 %419, false
  br i1 %420, label %block_5cb8, label %block_5c8f

block_5c8f:                                       ; preds = %block_5c84
  store volatile i64 23695, i64* @assembly_address
  %421 = load i64* @global_var_25f4c8
  store i64 %421, i64* %rdx
  store volatile i64 23702, i64* @assembly_address
  %422 = load i64* @global_var_216580
  store i64 %422, i64* %rax
  store volatile i64 23709, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 23716, i64* @assembly_address
  store i64 ptrtoint ([28 x i8]* @global_var_11711 to i64), i64* %rsi
  store volatile i64 23723, i64* @assembly_address
  %423 = load i64* %rax
  store i64 %423, i64* %rdi
  store volatile i64 23726, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 23731, i64* @assembly_address
  %424 = load i64* %rdi
  %425 = inttoptr i64 %424 to %_IO_FILE*
  %426 = load i64* %rsi
  %427 = inttoptr i64 %426 to i8*
  %428 = load i64* %rdx
  %429 = inttoptr i64 %428 to i8*
  %430 = load i64* %rcx
  %431 = inttoptr i64 %430 to i8*
  %432 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %425, i8* %427, i8* %429, i8* %431)
  %433 = sext i32 %432 to i64
  store i64 %433, i64* %rax
  %434 = sext i32 %432 to i64
  store i64 %434, i64* %rax
  br label %block_5cb8

block_5cb8:                                       ; preds = %block_5c8f, %block_5c84
  store volatile i64 23736, i64* @assembly_address
  %435 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %436 = zext i32 %435 to i64
  store i64 %436, i64* %rax
  store volatile i64 23742, i64* @assembly_address
  %437 = load i64* %rax
  %438 = trunc i64 %437 to i32
  %439 = load i64* %rax
  %440 = trunc i64 %439 to i32
  %441 = and i32 %438, %440
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %442 = icmp eq i32 %441, 0
  store i1 %442, i1* %zf
  %443 = icmp slt i32 %441, 0
  store i1 %443, i1* %sf
  %444 = trunc i32 %441 to i8
  %445 = call i8 @llvm.ctpop.i8(i8 %444)
  %446 = and i8 %445, 1
  %447 = icmp eq i8 %446, 0
  store i1 %447, i1* %pf
  store volatile i64 23744, i64* @assembly_address
  %448 = load i1* %zf
  %449 = icmp eq i1 %448, false
  br i1 %449, label %block_5ccc, label %block_5cc2

block_5cc2:                                       ; preds = %block_5cb8
  store volatile i64 23746, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_5ccc

block_5ccc:                                       ; preds = %block_5cc2, %block_5cb8
  store volatile i64 23756, i64* @assembly_address
  store i64 2, i64* %rax
  br label %block_5cd1

block_5cd1:                                       ; preds = %block_5ccc, %block_5c7d, %block_5c32, %block_5b70, %block_5b01
  store volatile i64 23761, i64* @assembly_address
  %450 = load i64* %stack_var_-8
  store i64 %450, i64* %rbp
  %451 = ptrtoint i64* %stack_var_0 to i64
  store i64 %451, i64* %rsp
  store volatile i64 23762, i64* @assembly_address
  %452 = load i64* %rax
  %453 = load i64* %rax
  ret i64 %453
}

define i64 @discard_input_bytes(i64 %arg1, i32 %arg2) {
block_5cd3:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-17 = alloca i8
  %stack_var_-16 = alloca i64
  %stack_var_-36 = alloca i32
  %stack_var_-32 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 23763, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 23764, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 23767, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 23771, i64* @assembly_address
  %20 = load i64* %rdi
  store i64 %20, i64* %stack_var_-32
  store volatile i64 23775, i64* @assembly_address
  %21 = load i64* %rsi
  %22 = trunc i64 %21 to i32
  store i32 %22, i32* %stack_var_-36
  store volatile i64 23778, i64* @assembly_address
  %23 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %23, i64* %rax
  store volatile i64 23787, i64* @assembly_address
  %24 = load i64* %rax
  store i64 %24, i64* %stack_var_-16
  store volatile i64 23791, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %25 = icmp eq i32 0, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i32 0, 0
  store i1 %26, i1* %sf
  %27 = trunc i32 0 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  %31 = zext i32 0 to i64
  store i64 %31, i64* %rax
  store volatile i64 23793, i64* @assembly_address
  br label %block_5d62

block_5cf3:                                       ; preds = %block_5d62
  store volatile i64 23795, i64* @assembly_address
  %32 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %33 = zext i32 %32 to i64
  store i64 %33, i64* %rdx
  store volatile i64 23801, i64* @assembly_address
  %34 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %35 = zext i32 %34 to i64
  store i64 %35, i64* %rax
  store volatile i64 23807, i64* @assembly_address
  %36 = load i64* %rdx
  %37 = trunc i64 %36 to i32
  %38 = load i64* %rax
  %39 = trunc i64 %38 to i32
  %40 = sub i32 %37, %39
  %41 = and i32 %37, 15
  %42 = and i32 %39, 15
  %43 = sub i32 %41, %42
  %44 = icmp ugt i32 %43, 15
  %45 = icmp ult i32 %37, %39
  %46 = xor i32 %37, %39
  %47 = xor i32 %37, %40
  %48 = and i32 %46, %47
  %49 = icmp slt i32 %48, 0
  store i1 %44, i1* %az
  store i1 %45, i1* %cf
  store i1 %49, i1* %of
  %50 = icmp eq i32 %40, 0
  store i1 %50, i1* %zf
  %51 = icmp slt i32 %40, 0
  store i1 %51, i1* %sf
  %52 = trunc i32 %40 to i8
  %53 = call i8 @llvm.ctpop.i8(i8 %52)
  %54 = and i8 %53, 1
  %55 = icmp eq i8 %54, 0
  store i1 %55, i1* %pf
  store volatile i64 23809, i64* @assembly_address
  %56 = load i1* %cf
  %57 = icmp eq i1 %56, false
  br i1 %57, label %block_5d21, label %block_5d03

block_5d03:                                       ; preds = %block_5cf3
  store volatile i64 23811, i64* @assembly_address
  %58 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %59 = zext i32 %58 to i64
  store i64 %59, i64* %rax
  store volatile i64 23817, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 23820, i64* @assembly_address
  %60 = load i64* %rdx
  %61 = trunc i64 %60 to i32
  store i32 %61, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 23826, i64* @assembly_address
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i32
  %64 = zext i32 %63 to i64
  store i64 %64, i64* %rdx
  store volatile i64 23828, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 23835, i64* @assembly_address
  %65 = load i64* %rdx
  %66 = load i64* %rax
  %67 = mul i64 %66, 1
  %68 = add i64 %65, %67
  %69 = inttoptr i64 %68 to i8*
  %70 = load i8* %69
  %71 = zext i8 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 23839, i64* @assembly_address
  br label %block_5d2b

block_5d21:                                       ; preds = %block_5cf3
  store volatile i64 23841, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 23846, i64* @assembly_address
  %72 = load i64* %rdi
  %73 = trunc i64 %72 to i32
  %74 = call i64 @fill_inbuf(i32 %73)
  store i64 %74, i64* %rax
  store i64 %74, i64* %rax
  br label %block_5d2b

block_5d2b:                                       ; preds = %block_5d21, %block_5d03
  store volatile i64 23851, i64* @assembly_address
  %75 = load i64* %rax
  %76 = trunc i64 %75 to i8
  store i8 %76, i8* %stack_var_-17
  store volatile i64 23854, i64* @assembly_address
  %77 = load i32* %stack_var_-36
  %78 = zext i32 %77 to i64
  store i64 %78, i64* %rax
  store volatile i64 23857, i64* @assembly_address
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  %81 = and i32 %80, 2
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %82 = icmp eq i32 %81, 0
  store i1 %82, i1* %zf
  %83 = icmp slt i32 %81, 0
  store i1 %83, i1* %sf
  %84 = trunc i32 %81 to i8
  %85 = call i8 @llvm.ctpop.i8(i8 %84)
  %86 = and i8 %85, 1
  %87 = icmp eq i8 %86, 0
  store i1 %87, i1* %pf
  %88 = zext i32 %81 to i64
  store i64 %88, i64* %rax
  store volatile i64 23860, i64* @assembly_address
  %89 = load i64* %rax
  %90 = trunc i64 %89 to i32
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i32
  %93 = and i32 %90, %92
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %94 = icmp eq i32 %93, 0
  store i1 %94, i1* %zf
  %95 = icmp slt i32 %93, 0
  store i1 %95, i1* %sf
  %96 = trunc i32 %93 to i8
  %97 = call i8 @llvm.ctpop.i8(i8 %96)
  %98 = and i8 %97, 1
  %99 = icmp eq i8 %98, 0
  store i1 %99, i1* %pf
  store volatile i64 23862, i64* @assembly_address
  %100 = load i1* %zf
  br i1 %100, label %block_5d49, label %block_5d38

block_5d38:                                       ; preds = %block_5d2b
  store volatile i64 23864, i64* @assembly_address
  %101 = ptrtoint i8* %stack_var_-17 to i64
  store i64 %101, i64* %rax
  store volatile i64 23868, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 23873, i64* @assembly_address
  %102 = ptrtoint i8* %stack_var_-17 to i64
  store i64 %102, i64* %rdi
  store volatile i64 23876, i64* @assembly_address
  %103 = load i64* %rdi
  %104 = inttoptr i64 %103 to i8*
  %105 = load i64* %rsi
  %106 = trunc i64 %105 to i32
  %107 = call i64 @updcrc(i8* %104, i32 %106)
  store i64 %107, i64* %rax
  store i64 %107, i64* %rax
  br label %block_5d49

block_5d49:                                       ; preds = %block_5d38, %block_5d2b
  store volatile i64 23881, i64* @assembly_address
  %108 = load i64* %stack_var_-32
  %109 = sub i64 %108, -1
  %110 = and i64 %108, 15
  %111 = sub i64 %110, 15
  %112 = icmp ugt i64 %111, 15
  %113 = icmp ult i64 %108, -1
  %114 = xor i64 %108, -1
  %115 = xor i64 %108, %109
  %116 = and i64 %114, %115
  %117 = icmp slt i64 %116, 0
  store i1 %112, i1* %az
  store i1 %113, i1* %cf
  store i1 %117, i1* %of
  %118 = icmp eq i64 %109, 0
  store i1 %118, i1* %zf
  %119 = icmp slt i64 %109, 0
  store i1 %119, i1* %sf
  %120 = trunc i64 %109 to i8
  %121 = call i8 @llvm.ctpop.i8(i8 %120)
  %122 = and i8 %121, 1
  %123 = icmp eq i8 %122, 0
  store i1 %123, i1* %pf
  store volatile i64 23886, i64* @assembly_address
  %124 = load i1* %zf
  br i1 %124, label %block_5d57, label %block_5d50

block_5d50:                                       ; preds = %block_5d49
  store volatile i64 23888, i64* @assembly_address
  %125 = load i64* %stack_var_-32
  %126 = sub i64 %125, 1
  %127 = and i64 %125, 15
  %128 = sub i64 %127, 1
  %129 = icmp ugt i64 %128, 15
  %130 = icmp ult i64 %125, 1
  %131 = xor i64 %125, 1
  %132 = xor i64 %125, %126
  %133 = and i64 %131, %132
  %134 = icmp slt i64 %133, 0
  store i1 %129, i1* %az
  store i1 %130, i1* %cf
  store i1 %134, i1* %of
  %135 = icmp eq i64 %126, 0
  store i1 %135, i1* %zf
  %136 = icmp slt i64 %126, 0
  store i1 %136, i1* %sf
  %137 = trunc i64 %126 to i8
  %138 = call i8 @llvm.ctpop.i8(i8 %137)
  %139 = and i8 %138, 1
  %140 = icmp eq i8 %139, 0
  store i1 %140, i1* %pf
  store i64 %126, i64* %stack_var_-32
  store volatile i64 23893, i64* @assembly_address
  br label %block_5d62

block_5d57:                                       ; preds = %block_5d49
  store volatile i64 23895, i64* @assembly_address
  %141 = load i8* %stack_var_-17
  %142 = zext i8 %141 to i64
  store i64 %142, i64* %rax
  store volatile i64 23899, i64* @assembly_address
  %143 = load i64* %rax
  %144 = trunc i64 %143 to i8
  %145 = load i64* %rax
  %146 = trunc i64 %145 to i8
  %147 = and i8 %144, %146
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %148 = icmp eq i8 %147, 0
  store i1 %148, i1* %zf
  %149 = icmp slt i8 %147, 0
  store i1 %149, i1* %sf
  %150 = call i8 @llvm.ctpop.i8(i8 %147)
  %151 = and i8 %150, 1
  %152 = icmp eq i8 %151, 0
  store i1 %152, i1* %pf
  store volatile i64 23901, i64* @assembly_address
  %153 = load i1* %zf
  %154 = icmp eq i1 %153, false
  br i1 %154, label %block_5d62, label %block_5d5f

block_5d5f:                                       ; preds = %block_5d57
  store volatile i64 23903, i64* @assembly_address
  store volatile i64 23904, i64* @assembly_address
  br label %block_5d69

block_5d62:                                       ; preds = %block_5d57, %block_5d50, %block_5cd3
  store volatile i64 23906, i64* @assembly_address
  %155 = load i64* %stack_var_-32
  %156 = and i64 %155, 15
  %157 = icmp ugt i64 %156, 15
  %158 = icmp ult i64 %155, 0
  %159 = xor i64 %155, 0
  %160 = and i64 %159, 0
  %161 = icmp slt i64 %160, 0
  store i1 %157, i1* %az
  store i1 %158, i1* %cf
  store i1 %161, i1* %of
  %162 = icmp eq i64 %155, 0
  store i1 %162, i1* %zf
  %163 = icmp slt i64 %155, 0
  store i1 %163, i1* %sf
  %164 = trunc i64 %155 to i8
  %165 = call i8 @llvm.ctpop.i8(i8 %164)
  %166 = and i8 %165, 1
  %167 = icmp eq i8 %166, 0
  store i1 %167, i1* %pf
  store volatile i64 23911, i64* @assembly_address
  %168 = load i1* %zf
  %169 = icmp eq i1 %168, false
  br i1 %169, label %block_5cf3, label %block_5d69

block_5d69:                                       ; preds = %block_5d5f, %block_5d62
  store volatile i64 23913, i64* @assembly_address
  store volatile i64 23914, i64* @assembly_address
  %170 = load i64* %stack_var_-16
  store i64 %170, i64* %rax
  store volatile i64 23918, i64* @assembly_address
  %171 = load i64* %rax
  %172 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %173 = xor i64 %171, %172
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %174 = icmp eq i64 %173, 0
  store i1 %174, i1* %zf
  %175 = icmp slt i64 %173, 0
  store i1 %175, i1* %sf
  %176 = trunc i64 %173 to i8
  %177 = call i8 @llvm.ctpop.i8(i8 %176)
  %178 = and i8 %177, 1
  %179 = icmp eq i8 %178, 0
  store i1 %179, i1* %pf
  store i64 %173, i64* %rax
  store volatile i64 23927, i64* @assembly_address
  %180 = load i1* %zf
  br i1 %180, label %block_5d7e, label %block_5d79

block_5d79:                                       ; preds = %block_5d69
  store volatile i64 23929, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_5d7e:                                       ; preds = %block_5d69
  store volatile i64 23934, i64* @assembly_address
  %181 = load i64* %stack_var_-8
  store i64 %181, i64* %rbp
  %182 = ptrtoint i64* %stack_var_0 to i64
  store i64 %182, i64* %rsp
  store volatile i64 23935, i64* @assembly_address
  %183 = load i64* %rax
  ret i64 %183
}

declare i64 @180(i64, i64)

define i64 @get_method(i32 %arg1) {
block_5d80:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-72 = alloca i32
  %stack_var_-60 = alloca i32
  %stack_var_-64 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-56 = alloca i8*
  %1 = alloca i64
  %stack_var_-27 = alloca i32
  %2 = alloca i8
  %stack_var_-68 = alloca i32
  %stack_var_-28 = alloca i32
  %3 = alloca i8
  %stack_var_-19 = alloca i8
  %stack_var_-20 = alloca i8
  %stack_var_-21 = alloca i8
  %stack_var_-22 = alloca i8
  %stack_var_-23 = alloca i8
  %stack_var_-24 = alloca i8
  %stack_var_-17 = alloca i8
  %stack_var_-18 = alloca i8
  %stack_var_-48 = alloca i32
  %4 = alloca i64
  %stack_var_-81 = alloca i32
  %5 = alloca i8
  %stack_var_-25 = alloca i8
  %stack_var_-76 = alloca i32
  %stack_var_-26 = alloca i8
  %stack_var_-80 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-92 = alloca i32
  %stack_var_-104 = alloca i64
  %stack_var_-8 = alloca i64
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i64
  %9 = alloca i32
  %10 = alloca i32
  %11 = alloca i64
  %12 = alloca i32
  %13 = alloca i32
  %14 = alloca i64
  store volatile i64 23936, i64* @assembly_address
  %15 = load i64* %rbp
  store i64 %15, i64* %stack_var_-8
  %16 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %16, i64* %rsp
  store volatile i64 23937, i64* @assembly_address
  %17 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %17, i64* %rbp
  store volatile i64 23940, i64* @assembly_address
  %18 = load i64* %rsp
  %19 = sub i64 %18, 96
  %20 = and i64 %18, 15
  %21 = icmp ugt i64 %20, 15
  %22 = icmp ult i64 %18, 96
  %23 = xor i64 %18, 96
  %24 = xor i64 %18, %19
  %25 = and i64 %23, %24
  %26 = icmp slt i64 %25, 0
  store i1 %21, i1* %az
  store i1 %22, i1* %cf
  store i1 %26, i1* %of
  %27 = icmp eq i64 %19, 0
  store i1 %27, i1* %zf
  %28 = icmp slt i64 %19, 0
  store i1 %28, i1* %sf
  %29 = trunc i64 %19 to i8
  %30 = call i8 @llvm.ctpop.i8(i8 %29)
  %31 = and i8 %30, 1
  %32 = icmp eq i8 %31, 0
  store i1 %32, i1* %pf
  %33 = ptrtoint i64* %stack_var_-104 to i64
  store i64 %33, i64* %rsp
  store volatile i64 23944, i64* @assembly_address
  %34 = load i64* %rdi
  %35 = trunc i64 %34 to i32
  store i32 %35, i32* %stack_var_-92
  store volatile i64 23947, i64* @assembly_address
  %36 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %36, i64* %rax
  store volatile i64 23956, i64* @assembly_address
  %37 = load i64* %rax
  store i64 %37, i64* %stack_var_-16
  store volatile i64 23960, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %38 = icmp eq i32 0, 0
  store i1 %38, i1* %zf
  %39 = icmp slt i32 0, 0
  store i1 %39, i1* %sf
  %40 = trunc i32 0 to i8
  %41 = call i8 @llvm.ctpop.i8(i8 %40)
  %42 = and i8 %41, 1
  %43 = icmp eq i8 %42, 0
  store i1 %43, i1* %pf
  %44 = zext i32 0 to i64
  store i64 %44, i64* %rax
  store volatile i64 23962, i64* @assembly_address
  %45 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %46 = zext i32 %45 to i64
  store i64 %46, i64* %rax
  store volatile i64 23968, i64* @assembly_address
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i32
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  %51 = and i32 %48, %50
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %52 = icmp eq i32 %51, 0
  store i1 %52, i1* %zf
  %53 = icmp slt i32 %51, 0
  store i1 %53, i1* %sf
  %54 = trunc i32 %51 to i8
  %55 = call i8 @llvm.ctpop.i8(i8 %54)
  %56 = and i8 %55, 1
  %57 = icmp eq i8 %56, 0
  store i1 %57, i1* %pf
  store volatile i64 23970, i64* @assembly_address
  %58 = load i1* %zf
  br i1 %58, label %block_5e43, label %block_5da8

block_5da8:                                       ; preds = %block_5d80
  store volatile i64 23976, i64* @assembly_address
  %59 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %60 = zext i32 %59 to i64
  store i64 %60, i64* %rax
  store volatile i64 23982, i64* @assembly_address
  %61 = load i64* %rax
  %62 = trunc i64 %61 to i32
  %63 = load i64* %rax
  %64 = trunc i64 %63 to i32
  %65 = and i32 %62, %64
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %66 = icmp eq i32 %65, 0
  store i1 %66, i1* %zf
  %67 = icmp slt i32 %65, 0
  store i1 %67, i1* %sf
  %68 = trunc i32 %65 to i8
  %69 = call i8 @llvm.ctpop.i8(i8 %68)
  %70 = and i8 %69, 1
  %71 = icmp eq i8 %70, 0
  store i1 %71, i1* %pf
  store volatile i64 23984, i64* @assembly_address
  %72 = load i1* %zf
  br i1 %72, label %block_5e43, label %block_5db6

block_5db6:                                       ; preds = %block_5da8
  store volatile i64 23990, i64* @assembly_address
  %73 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %74 = zext i32 %73 to i64
  store i64 %74, i64* %rdx
  store volatile i64 23996, i64* @assembly_address
  %75 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 24002, i64* @assembly_address
  %77 = load i64* %rdx
  %78 = trunc i64 %77 to i32
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  %81 = sub i32 %78, %80
  %82 = and i32 %78, 15
  %83 = and i32 %80, 15
  %84 = sub i32 %82, %83
  %85 = icmp ugt i32 %84, 15
  %86 = icmp ult i32 %78, %80
  %87 = xor i32 %78, %80
  %88 = xor i32 %78, %81
  %89 = and i32 %87, %88
  %90 = icmp slt i32 %89, 0
  store i1 %85, i1* %az
  store i1 %86, i1* %cf
  store i1 %90, i1* %of
  %91 = icmp eq i32 %81, 0
  store i1 %91, i1* %zf
  %92 = icmp slt i32 %81, 0
  store i1 %92, i1* %sf
  %93 = trunc i32 %81 to i8
  %94 = call i8 @llvm.ctpop.i8(i8 %93)
  %95 = and i8 %94, 1
  %96 = icmp eq i8 %95, 0
  store i1 %96, i1* %pf
  store volatile i64 24004, i64* @assembly_address
  %97 = load i1* %cf
  %98 = icmp eq i1 %97, false
  br i1 %98, label %block_5de7, label %block_5dc6

block_5dc6:                                       ; preds = %block_5db6
  store volatile i64 24006, i64* @assembly_address
  %99 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %100 = zext i32 %99 to i64
  store i64 %100, i64* %rax
  store volatile i64 24012, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24015, i64* @assembly_address
  %101 = load i64* %rdx
  %102 = trunc i64 %101 to i32
  store i32 %102, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24021, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  %105 = zext i32 %104 to i64
  store i64 %105, i64* %rdx
  store volatile i64 24023, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24030, i64* @assembly_address
  %106 = load i64* %rdx
  %107 = load i64* %rax
  %108 = mul i64 %107, 1
  %109 = add i64 %106, %108
  %110 = inttoptr i64 %109 to i8*
  %111 = load i8* %110
  %112 = zext i8 %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 24034, i64* @assembly_address
  %113 = load i64* %rax
  %114 = trunc i64 %113 to i8
  %115 = zext i8 %114 to i64
  store i64 %115, i64* %rax
  store volatile i64 24037, i64* @assembly_address
  br label %block_5df1

block_5de7:                                       ; preds = %block_5db6
  store volatile i64 24039, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 24044, i64* @assembly_address
  %116 = load i64* %rdi
  %117 = trunc i64 %116 to i32
  %118 = call i64 @fill_inbuf(i32 %117)
  store i64 %118, i64* %rax
  store i64 %118, i64* %rax
  br label %block_5df1

block_5df1:                                       ; preds = %block_5de7, %block_5dc6
  store volatile i64 24049, i64* @assembly_address
  %119 = load i64* %rax
  %120 = trunc i64 %119 to i32
  store i32 %120, i32* %stack_var_-80
  store volatile i64 24052, i64* @assembly_address
  %121 = load i32* %stack_var_-80
  %122 = zext i32 %121 to i64
  store i64 %122, i64* %rax
  store volatile i64 24055, i64* @assembly_address
  %123 = load i64* %rax
  %124 = trunc i64 %123 to i8
  store i8 %124, i8* %stack_var_-26
  store volatile i64 24058, i64* @assembly_address
  %125 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %126 = zext i32 %125 to i64
  store i64 %126, i64* %rdx
  store volatile i64 24064, i64* @assembly_address
  %127 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %128 = zext i32 %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 24070, i64* @assembly_address
  %129 = load i64* %rdx
  %130 = trunc i64 %129 to i32
  %131 = load i64* %rax
  %132 = trunc i64 %131 to i32
  %133 = sub i32 %130, %132
  %134 = and i32 %130, 15
  %135 = and i32 %132, 15
  %136 = sub i32 %134, %135
  %137 = icmp ugt i32 %136, 15
  %138 = icmp ult i32 %130, %132
  %139 = xor i32 %130, %132
  %140 = xor i32 %130, %133
  %141 = and i32 %139, %140
  %142 = icmp slt i32 %141, 0
  store i1 %137, i1* %az
  store i1 %138, i1* %cf
  store i1 %142, i1* %of
  %143 = icmp eq i32 %133, 0
  store i1 %143, i1* %zf
  %144 = icmp slt i32 %133, 0
  store i1 %144, i1* %sf
  %145 = trunc i32 %133 to i8
  %146 = call i8 @llvm.ctpop.i8(i8 %145)
  %147 = and i8 %146, 1
  %148 = icmp eq i8 %147, 0
  store i1 %148, i1* %pf
  store volatile i64 24072, i64* @assembly_address
  %149 = load i1* %cf
  %150 = icmp eq i1 %149, false
  br i1 %150, label %block_5e2b, label %block_5e0a

block_5e0a:                                       ; preds = %block_5df1
  store volatile i64 24074, i64* @assembly_address
  %151 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %152 = zext i32 %151 to i64
  store i64 %152, i64* %rax
  store volatile i64 24080, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24083, i64* @assembly_address
  %153 = load i64* %rdx
  %154 = trunc i64 %153 to i32
  store i32 %154, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24089, i64* @assembly_address
  %155 = load i64* %rax
  %156 = trunc i64 %155 to i32
  %157 = zext i32 %156 to i64
  store i64 %157, i64* %rdx
  store volatile i64 24091, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24098, i64* @assembly_address
  %158 = load i64* %rdx
  %159 = load i64* %rax
  %160 = mul i64 %159, 1
  %161 = add i64 %158, %160
  %162 = inttoptr i64 %161 to i8*
  %163 = load i8* %162
  %164 = zext i8 %163 to i64
  store i64 %164, i64* %rax
  store volatile i64 24102, i64* @assembly_address
  %165 = load i64* %rax
  %166 = trunc i64 %165 to i8
  %167 = zext i8 %166 to i64
  store i64 %167, i64* %rax
  store volatile i64 24105, i64* @assembly_address
  br label %block_5e35

block_5e2b:                                       ; preds = %block_5df1
  store volatile i64 24107, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 24112, i64* @assembly_address
  %168 = load i64* %rdi
  %169 = trunc i64 %168 to i32
  %170 = call i64 @fill_inbuf(i32 %169)
  store i64 %170, i64* %rax
  store i64 %170, i64* %rax
  br label %block_5e35

block_5e35:                                       ; preds = %block_5e2b, %block_5e0a
  store volatile i64 24117, i64* @assembly_address
  %171 = load i64* %rax
  %172 = trunc i64 %171 to i32
  store i32 %172, i32* %stack_var_-76
  store volatile i64 24120, i64* @assembly_address
  %173 = load i32* %stack_var_-76
  %174 = zext i32 %173 to i64
  store i64 %174, i64* %rax
  store volatile i64 24123, i64* @assembly_address
  %175 = load i64* %rax
  %176 = trunc i64 %175 to i8
  store i8 %176, i8* %stack_var_-25
  store volatile i64 24126, i64* @assembly_address
  br label %block_5f15

block_5e43:                                       ; preds = %block_5da8, %block_5d80
  store volatile i64 24131, i64* @assembly_address
  %177 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %178 = zext i32 %177 to i64
  store i64 %178, i64* %rdx
  store volatile i64 24137, i64* @assembly_address
  %179 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %180 = zext i32 %179 to i64
  store i64 %180, i64* %rax
  store volatile i64 24143, i64* @assembly_address
  %181 = load i64* %rdx
  %182 = trunc i64 %181 to i32
  %183 = load i64* %rax
  %184 = trunc i64 %183 to i32
  %185 = sub i32 %182, %184
  %186 = and i32 %182, 15
  %187 = and i32 %184, 15
  %188 = sub i32 %186, %187
  %189 = icmp ugt i32 %188, 15
  %190 = icmp ult i32 %182, %184
  %191 = xor i32 %182, %184
  %192 = xor i32 %182, %185
  %193 = and i32 %191, %192
  %194 = icmp slt i32 %193, 0
  store i1 %189, i1* %az
  store i1 %190, i1* %cf
  store i1 %194, i1* %of
  %195 = icmp eq i32 %185, 0
  store i1 %195, i1* %zf
  %196 = icmp slt i32 %185, 0
  store i1 %196, i1* %sf
  %197 = trunc i32 %185 to i8
  %198 = call i8 @llvm.ctpop.i8(i8 %197)
  %199 = and i8 %198, 1
  %200 = icmp eq i8 %199, 0
  store i1 %200, i1* %pf
  store volatile i64 24145, i64* @assembly_address
  %201 = load i1* %cf
  %202 = icmp eq i1 %201, false
  br i1 %202, label %block_5e71, label %block_5e53

block_5e53:                                       ; preds = %block_5e43
  store volatile i64 24147, i64* @assembly_address
  %203 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %204 = zext i32 %203 to i64
  store i64 %204, i64* %rax
  store volatile i64 24153, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24156, i64* @assembly_address
  %205 = load i64* %rdx
  %206 = trunc i64 %205 to i32
  store i32 %206, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24162, i64* @assembly_address
  %207 = load i64* %rax
  %208 = trunc i64 %207 to i32
  %209 = zext i32 %208 to i64
  store i64 %209, i64* %rdx
  store volatile i64 24164, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24171, i64* @assembly_address
  %210 = load i64* %rdx
  %211 = load i64* %rax
  %212 = mul i64 %211, 1
  %213 = add i64 %210, %212
  %214 = inttoptr i64 %213 to i8*
  %215 = load i8* %214
  %216 = zext i8 %215 to i64
  store i64 %216, i64* %rax
  store volatile i64 24175, i64* @assembly_address
  br label %block_5e7b

block_5e71:                                       ; preds = %block_5e43
  store volatile i64 24177, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 24182, i64* @assembly_address
  %217 = load i64* %rdi
  %218 = trunc i64 %217 to i32
  %219 = call i64 @fill_inbuf(i32 %218)
  store i64 %219, i64* %rax
  store i64 %219, i64* %rax
  br label %block_5e7b

block_5e7b:                                       ; preds = %block_5e71, %block_5e53
  store volatile i64 24187, i64* @assembly_address
  %220 = load i64* %rax
  %221 = trunc i64 %220 to i8
  store i8 %221, i8* %stack_var_-26
  store volatile i64 24190, i64* @assembly_address
  store i32 0, i32* %stack_var_-80
  store volatile i64 24197, i64* @assembly_address
  %222 = load i8* %stack_var_-26
  %223 = zext i8 %222 to i64
  store i64 %223, i64* %rax
  store volatile i64 24201, i64* @assembly_address
  %224 = load i64* %rax
  %225 = trunc i64 %224 to i8
  %226 = load i64* %rax
  %227 = trunc i64 %226 to i8
  %228 = and i8 %225, %227
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %229 = icmp eq i8 %228, 0
  store i1 %229, i1* %zf
  %230 = icmp slt i8 %228, 0
  store i1 %230, i1* %sf
  %231 = call i8 @llvm.ctpop.i8(i8 %228)
  %232 = and i8 %231, 1
  %233 = icmp eq i8 %232, 0
  store i1 %233, i1* %pf
  store volatile i64 24203, i64* @assembly_address
  %234 = load i1* %zf
  br i1 %234, label %block_5ed1, label %block_5e8d

block_5e8d:                                       ; preds = %block_5e7b
  store volatile i64 24205, i64* @assembly_address
  %235 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %236 = zext i32 %235 to i64
  store i64 %236, i64* %rdx
  store volatile i64 24211, i64* @assembly_address
  %237 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %238 = zext i32 %237 to i64
  store i64 %238, i64* %rax
  store volatile i64 24217, i64* @assembly_address
  %239 = load i64* %rdx
  %240 = trunc i64 %239 to i32
  %241 = load i64* %rax
  %242 = trunc i64 %241 to i32
  %243 = sub i32 %240, %242
  %244 = and i32 %240, 15
  %245 = and i32 %242, 15
  %246 = sub i32 %244, %245
  %247 = icmp ugt i32 %246, 15
  %248 = icmp ult i32 %240, %242
  %249 = xor i32 %240, %242
  %250 = xor i32 %240, %243
  %251 = and i32 %249, %250
  %252 = icmp slt i32 %251, 0
  store i1 %247, i1* %az
  store i1 %248, i1* %cf
  store i1 %252, i1* %of
  %253 = icmp eq i32 %243, 0
  store i1 %253, i1* %zf
  %254 = icmp slt i32 %243, 0
  store i1 %254, i1* %sf
  %255 = trunc i32 %243 to i8
  %256 = call i8 @llvm.ctpop.i8(i8 %255)
  %257 = and i8 %256, 1
  %258 = icmp eq i8 %257, 0
  store i1 %258, i1* %pf
  store volatile i64 24219, i64* @assembly_address
  %259 = load i1* %cf
  %260 = icmp eq i1 %259, false
  br i1 %260, label %block_5ebb, label %block_5e9d

block_5e9d:                                       ; preds = %block_5e8d
  store volatile i64 24221, i64* @assembly_address
  %261 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %262 = zext i32 %261 to i64
  store i64 %262, i64* %rax
  store volatile i64 24227, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24230, i64* @assembly_address
  %263 = load i64* %rdx
  %264 = trunc i64 %263 to i32
  store i32 %264, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24236, i64* @assembly_address
  %265 = load i64* %rax
  %266 = trunc i64 %265 to i32
  %267 = zext i32 %266 to i64
  store i64 %267, i64* %rdx
  store volatile i64 24238, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24245, i64* @assembly_address
  %268 = load i64* %rdx
  %269 = load i64* %rax
  %270 = mul i64 %269, 1
  %271 = add i64 %268, %270
  %272 = inttoptr i64 %271 to i8*
  %273 = load i8* %272
  %274 = zext i8 %273 to i64
  store i64 %274, i64* %rax
  store volatile i64 24249, i64* @assembly_address
  br label %block_5ec5

block_5ebb:                                       ; preds = %block_5e8d
  store volatile i64 24251, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 24256, i64* @assembly_address
  %275 = load i64* %rdi
  %276 = trunc i64 %275 to i32
  %277 = call i64 @fill_inbuf(i32 %276)
  store i64 %277, i64* %rax
  store i64 %277, i64* %rax
  br label %block_5ec5

block_5ec5:                                       ; preds = %block_5ebb, %block_5e9d
  store volatile i64 24261, i64* @assembly_address
  %278 = load i64* %rax
  %279 = trunc i64 %278 to i8
  store i8 %279, i8* %stack_var_-25
  store volatile i64 24264, i64* @assembly_address
  store i32 0, i32* %stack_var_-76
  store volatile i64 24271, i64* @assembly_address
  br label %block_5f15

block_5ed1:                                       ; preds = %block_5e7b
  store volatile i64 24273, i64* @assembly_address
  %280 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %281 = zext i32 %280 to i64
  store i64 %281, i64* %rdx
  store volatile i64 24279, i64* @assembly_address
  %282 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %283 = zext i32 %282 to i64
  store i64 %283, i64* %rax
  store volatile i64 24285, i64* @assembly_address
  %284 = load i64* %rdx
  %285 = trunc i64 %284 to i32
  %286 = load i64* %rax
  %287 = trunc i64 %286 to i32
  %288 = sub i32 %285, %287
  %289 = and i32 %285, 15
  %290 = and i32 %287, 15
  %291 = sub i32 %289, %290
  %292 = icmp ugt i32 %291, 15
  %293 = icmp ult i32 %285, %287
  %294 = xor i32 %285, %287
  %295 = xor i32 %285, %288
  %296 = and i32 %294, %295
  %297 = icmp slt i32 %296, 0
  store i1 %292, i1* %az
  store i1 %293, i1* %cf
  store i1 %297, i1* %of
  %298 = icmp eq i32 %288, 0
  store i1 %298, i1* %zf
  %299 = icmp slt i32 %288, 0
  store i1 %299, i1* %sf
  %300 = trunc i32 %288 to i8
  %301 = call i8 @llvm.ctpop.i8(i8 %300)
  %302 = and i8 %301, 1
  %303 = icmp eq i8 %302, 0
  store i1 %303, i1* %pf
  store volatile i64 24287, i64* @assembly_address
  %304 = load i1* %cf
  %305 = icmp eq i1 %304, false
  br i1 %305, label %block_5f02, label %block_5ee1

block_5ee1:                                       ; preds = %block_5ed1
  store volatile i64 24289, i64* @assembly_address
  %306 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %307 = zext i32 %306 to i64
  store i64 %307, i64* %rax
  store volatile i64 24295, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24298, i64* @assembly_address
  %308 = load i64* %rdx
  %309 = trunc i64 %308 to i32
  store i32 %309, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24304, i64* @assembly_address
  %310 = load i64* %rax
  %311 = trunc i64 %310 to i32
  %312 = zext i32 %311 to i64
  store i64 %312, i64* %rdx
  store volatile i64 24306, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24313, i64* @assembly_address
  %313 = load i64* %rdx
  %314 = load i64* %rax
  %315 = mul i64 %314, 1
  %316 = add i64 %313, %315
  %317 = inttoptr i64 %316 to i8*
  %318 = load i8* %317
  %319 = zext i8 %318 to i64
  store i64 %319, i64* %rax
  store volatile i64 24317, i64* @assembly_address
  %320 = load i64* %rax
  %321 = trunc i64 %320 to i8
  %322 = zext i8 %321 to i64
  store i64 %322, i64* %rax
  store volatile i64 24320, i64* @assembly_address
  br label %block_5f0c

block_5f02:                                       ; preds = %block_5ed1
  store volatile i64 24322, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 24327, i64* @assembly_address
  %323 = load i64* %rdi
  %324 = trunc i64 %323 to i32
  %325 = call i64 @fill_inbuf(i32 %324)
  store i64 %325, i64* %rax
  store i64 %325, i64* %rax
  br label %block_5f0c

block_5f0c:                                       ; preds = %block_5f02, %block_5ee1
  store volatile i64 24332, i64* @assembly_address
  %326 = load i64* %rax
  %327 = trunc i64 %326 to i32
  store i32 %327, i32* %stack_var_-76
  store volatile i64 24335, i64* @assembly_address
  %328 = load i32* %stack_var_-76
  %329 = zext i32 %328 to i64
  store i64 %329, i64* %rax
  store volatile i64 24338, i64* @assembly_address
  %330 = load i64* %rax
  %331 = trunc i64 %330 to i8
  store i8 %331, i8* %stack_var_-25
  br label %block_5f15

block_5f15:                                       ; preds = %block_5f0c, %block_5ec5, %block_5e35
  store volatile i64 24341, i64* @assembly_address
  store i32 -1, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 24351, i64* @assembly_address
  %332 = load i32* bitcast (i64* @global_var_216620 to i32*)
  %333 = zext i32 %332 to i64
  store i64 %333, i64* %rax
  store volatile i64 24357, i64* @assembly_address
  %334 = load i64* %rax
  %335 = trunc i64 %334 to i32
  %336 = add i32 %335, 1
  %337 = and i32 %335, 15
  %338 = add i32 %337, 1
  %339 = icmp ugt i32 %338, 15
  %340 = icmp ult i32 %336, %335
  %341 = xor i32 %335, %336
  %342 = xor i32 1, %336
  %343 = and i32 %341, %342
  %344 = icmp slt i32 %343, 0
  store i1 %339, i1* %az
  store i1 %340, i1* %cf
  store i1 %344, i1* %of
  %345 = icmp eq i32 %336, 0
  store i1 %345, i1* %zf
  %346 = icmp slt i32 %336, 0
  store i1 %346, i1* %sf
  %347 = trunc i32 %336 to i8
  %348 = call i8 @llvm.ctpop.i8(i8 %347)
  %349 = and i8 %348, 1
  %350 = icmp eq i8 %349, 0
  store i1 %350, i1* %pf
  store i64 ptrtoint (i64* @global_var_216621 to i64), i64* %rax
  store volatile i64 24360, i64* @assembly_address
  %351 = load i64* %rax
  %352 = trunc i64 %351 to i32
  store i32 %352, i32* bitcast (i64* @global_var_216620 to i32*)
  store volatile i64 24366, i64* @assembly_address
  store i64 0, i64* @global_var_267540
  store volatile i64 24377, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21661c to i32*)
  store volatile i64 24387, i64* @assembly_address
  %353 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %353, i64* %rax
  store volatile i64 24391, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 24396, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_1178b to i64), i64* %rsi
  store volatile i64 24403, i64* @assembly_address
  %354 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %354, i64* %rdi
  store volatile i64 24406, i64* @assembly_address
  %355 = load i64* %rdi
  %356 = inttoptr i64 %355 to i64*
  %357 = load i64* %rsi
  %358 = inttoptr i64 %357 to i64*
  %359 = load i64* %rdx
  %360 = trunc i64 %359 to i32
  %361 = call i32 @memcmp(i64* %356, i64* %358, i32 %360)
  %362 = sext i32 %361 to i64
  store i64 %362, i64* %rax
  %363 = sext i32 %361 to i64
  store i64 %363, i64* %rax
  store volatile i64 24411, i64* @assembly_address
  %364 = load i64* %rax
  %365 = trunc i64 %364 to i32
  %366 = load i64* %rax
  %367 = trunc i64 %366 to i32
  %368 = and i32 %365, %367
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %369 = icmp eq i32 %368, 0
  store i1 %369, i1* %zf
  %370 = icmp slt i32 %368, 0
  store i1 %370, i1* %sf
  %371 = trunc i32 %368 to i8
  %372 = call i8 @llvm.ctpop.i8(i8 %371)
  %373 = and i8 %372, 1
  %374 = icmp eq i8 %373, 0
  store i1 %374, i1* %pf
  store volatile i64 24413, i64* @assembly_address
  %375 = load i1* %zf
  br i1 %375, label %block_5f7f, label %block_5f5f

block_5f5f:                                       ; preds = %block_5f15
  store volatile i64 24415, i64* @assembly_address
  %376 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %376, i64* %rax
  store volatile i64 24419, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 24424, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_1178e to i64), i64* %rsi
  store volatile i64 24431, i64* @assembly_address
  %377 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %377, i64* %rdi
  store volatile i64 24434, i64* @assembly_address
  %378 = load i64* %rdi
  %379 = inttoptr i64 %378 to i64*
  %380 = load i64* %rsi
  %381 = inttoptr i64 %380 to i64*
  %382 = load i64* %rdx
  %383 = trunc i64 %382 to i32
  %384 = call i32 @memcmp(i64* %379, i64* %381, i32 %383)
  %385 = sext i32 %384 to i64
  store i64 %385, i64* %rax
  %386 = sext i32 %384 to i64
  store i64 %386, i64* %rax
  store volatile i64 24439, i64* @assembly_address
  %387 = load i64* %rax
  %388 = trunc i64 %387 to i32
  %389 = load i64* %rax
  %390 = trunc i64 %389 to i32
  %391 = and i32 %388, %390
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %392 = icmp eq i32 %391, 0
  store i1 %392, i1* %zf
  %393 = icmp slt i32 %391, 0
  store i1 %393, i1* %sf
  %394 = trunc i32 %391 to i8
  %395 = call i8 @llvm.ctpop.i8(i8 %394)
  %396 = and i8 %395, 1
  %397 = icmp eq i8 %396, 0
  store i1 %397, i1* %pf
  store volatile i64 24441, i64* @assembly_address
  %398 = load i1* %zf
  %399 = icmp eq i1 %398, false
  br i1 %399, label %block_672a, label %block_5f7f

block_5f7f:                                       ; preds = %block_5f5f, %block_5f15
  store volatile i64 24447, i64* @assembly_address
  %400 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %401 = zext i32 %400 to i64
  store i64 %401, i64* %rdx
  store volatile i64 24453, i64* @assembly_address
  %402 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %403 = zext i32 %402 to i64
  store i64 %403, i64* %rax
  store volatile i64 24459, i64* @assembly_address
  %404 = load i64* %rdx
  %405 = trunc i64 %404 to i32
  %406 = load i64* %rax
  %407 = trunc i64 %406 to i32
  %408 = sub i32 %405, %407
  %409 = and i32 %405, 15
  %410 = and i32 %407, 15
  %411 = sub i32 %409, %410
  %412 = icmp ugt i32 %411, 15
  %413 = icmp ult i32 %405, %407
  %414 = xor i32 %405, %407
  %415 = xor i32 %405, %408
  %416 = and i32 %414, %415
  %417 = icmp slt i32 %416, 0
  store i1 %412, i1* %az
  store i1 %413, i1* %cf
  store i1 %417, i1* %of
  %418 = icmp eq i32 %408, 0
  store i1 %418, i1* %zf
  %419 = icmp slt i32 %408, 0
  store i1 %419, i1* %sf
  %420 = trunc i32 %408 to i8
  %421 = call i8 @llvm.ctpop.i8(i8 %420)
  %422 = and i8 %421, 1
  %423 = icmp eq i8 %422, 0
  store i1 %423, i1* %pf
  store volatile i64 24461, i64* @assembly_address
  %424 = load i1* %cf
  %425 = icmp eq i1 %424, false
  br i1 %425, label %block_5fb0, label %block_5f8f

block_5f8f:                                       ; preds = %block_5f7f
  store volatile i64 24463, i64* @assembly_address
  %426 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %427 = zext i32 %426 to i64
  store i64 %427, i64* %rax
  store volatile i64 24469, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24472, i64* @assembly_address
  %428 = load i64* %rdx
  %429 = trunc i64 %428 to i32
  store i32 %429, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24478, i64* @assembly_address
  %430 = load i64* %rax
  %431 = trunc i64 %430 to i32
  %432 = zext i32 %431 to i64
  store i64 %432, i64* %rdx
  store volatile i64 24480, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24487, i64* @assembly_address
  %433 = load i64* %rdx
  %434 = load i64* %rax
  %435 = mul i64 %434, 1
  %436 = add i64 %433, %435
  %437 = inttoptr i64 %436 to i8*
  %438 = load i8* %437
  %439 = zext i8 %438 to i64
  store i64 %439, i64* %rax
  store volatile i64 24491, i64* @assembly_address
  %440 = load i64* %rax
  %441 = trunc i64 %440 to i8
  %442 = zext i8 %441 to i64
  store i64 %442, i64* %rax
  store volatile i64 24494, i64* @assembly_address
  br label %block_5fba

block_5fb0:                                       ; preds = %block_5f7f
  store volatile i64 24496, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 24501, i64* @assembly_address
  %443 = load i64* %rdi
  %444 = trunc i64 %443 to i32
  %445 = call i64 @fill_inbuf(i32 %444)
  store i64 %445, i64* %rax
  store i64 %445, i64* %rax
  br label %block_5fba

block_5fba:                                       ; preds = %block_5fb0, %block_5f8f
  store volatile i64 24506, i64* @assembly_address
  %446 = load i64* %rax
  %447 = trunc i64 %446 to i32
  store i32 %447, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 24512, i64* @assembly_address
  %448 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %449 = zext i32 %448 to i64
  store i64 %449, i64* %rax
  store volatile i64 24518, i64* @assembly_address
  %450 = load i64* %rax
  %451 = trunc i64 %450 to i32
  %452 = sub i32 %451, 8
  %453 = and i32 %451, 15
  %454 = sub i32 %453, 8
  %455 = icmp ugt i32 %454, 15
  %456 = icmp ult i32 %451, 8
  %457 = xor i32 %451, 8
  %458 = xor i32 %451, %452
  %459 = and i32 %457, %458
  %460 = icmp slt i32 %459, 0
  store i1 %455, i1* %az
  store i1 %456, i1* %cf
  store i1 %460, i1* %of
  %461 = icmp eq i32 %452, 0
  store i1 %461, i1* %zf
  %462 = icmp slt i32 %452, 0
  store i1 %462, i1* %sf
  %463 = trunc i32 %452 to i8
  %464 = call i8 @llvm.ctpop.i8(i8 %463)
  %465 = and i8 %464, 1
  %466 = icmp eq i8 %465, 0
  store i1 %466, i1* %pf
  store volatile i64 24521, i64* @assembly_address
  %467 = load i1* %zf
  br i1 %467, label %block_6011, label %block_5fcb

block_5fcb:                                       ; preds = %block_5fba
  store volatile i64 24523, i64* @assembly_address
  %468 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %469 = zext i32 %468 to i64
  store i64 %469, i64* %rcx
  store volatile i64 24529, i64* @assembly_address
  %470 = load i64* @global_var_25f4c8
  store i64 %470, i64* %rdx
  store volatile i64 24536, i64* @assembly_address
  %471 = load i64* @global_var_216580
  store i64 %471, i64* %rax
  store volatile i64 24543, i64* @assembly_address
  %472 = load i64* %rcx
  %473 = trunc i64 %472 to i32
  %474 = zext i32 %473 to i64
  store i64 %474, i64* %r8
  store volatile i64 24546, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 24553, i64* @assembly_address
  store i64 ptrtoint ([44 x i8]* @global_var_11798 to i64), i64* %rsi
  store volatile i64 24560, i64* @assembly_address
  %475 = load i64* %rax
  store i64 %475, i64* %rdi
  store volatile i64 24563, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 24568, i64* @assembly_address
  %476 = load i64* %rdi
  %477 = inttoptr i64 %476 to %_IO_FILE*
  %478 = load i64* %rsi
  %479 = inttoptr i64 %478 to i8*
  %480 = load i64* %rdx
  %481 = inttoptr i64 %480 to i8*
  %482 = load i64* %rcx
  %483 = inttoptr i64 %482 to i8*
  %484 = load i64* %r8
  %485 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %477, i8* %479, i8* %481, i8* %483, i64 %484)
  %486 = sext i32 %485 to i64
  store i64 %486, i64* %rax
  %487 = sext i32 %485 to i64
  store i64 %487, i64* %rax
  store volatile i64 24573, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 24583, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 24588, i64* @assembly_address
  br label %block_6a4d

block_6011:                                       ; preds = %block_5fba
  store volatile i64 24593, i64* @assembly_address
  store i64 54102, i64* %rax
  store volatile i64 24600, i64* @assembly_address
  %488 = load i64* %rax
  store i64 %488, i64* @global_var_2160d0
  store volatile i64 24607, i64* @assembly_address
  %489 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %490 = zext i32 %489 to i64
  store i64 %490, i64* %rdx
  store volatile i64 24613, i64* @assembly_address
  %491 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %492 = zext i32 %491 to i64
  store i64 %492, i64* %rax
  store volatile i64 24619, i64* @assembly_address
  %493 = load i64* %rdx
  %494 = trunc i64 %493 to i32
  %495 = load i64* %rax
  %496 = trunc i64 %495 to i32
  %497 = sub i32 %494, %496
  %498 = and i32 %494, 15
  %499 = and i32 %496, 15
  %500 = sub i32 %498, %499
  %501 = icmp ugt i32 %500, 15
  %502 = icmp ult i32 %494, %496
  %503 = xor i32 %494, %496
  %504 = xor i32 %494, %497
  %505 = and i32 %503, %504
  %506 = icmp slt i32 %505, 0
  store i1 %501, i1* %az
  store i1 %502, i1* %cf
  store i1 %506, i1* %of
  %507 = icmp eq i32 %497, 0
  store i1 %507, i1* %zf
  %508 = icmp slt i32 %497, 0
  store i1 %508, i1* %sf
  %509 = trunc i32 %497 to i8
  %510 = call i8 @llvm.ctpop.i8(i8 %509)
  %511 = and i8 %510, 1
  %512 = icmp eq i8 %511, 0
  store i1 %512, i1* %pf
  store volatile i64 24621, i64* @assembly_address
  %513 = load i1* %cf
  %514 = icmp eq i1 %513, false
  br i1 %514, label %block_604d, label %block_602f

block_602f:                                       ; preds = %block_6011
  store volatile i64 24623, i64* @assembly_address
  %515 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %516 = zext i32 %515 to i64
  store i64 %516, i64* %rax
  store volatile i64 24629, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24632, i64* @assembly_address
  %517 = load i64* %rdx
  %518 = trunc i64 %517 to i32
  store i32 %518, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24638, i64* @assembly_address
  %519 = load i64* %rax
  %520 = trunc i64 %519 to i32
  %521 = zext i32 %520 to i64
  store i64 %521, i64* %rdx
  store volatile i64 24640, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24647, i64* @assembly_address
  %522 = load i64* %rdx
  %523 = load i64* %rax
  %524 = mul i64 %523, 1
  %525 = add i64 %522, %524
  %526 = inttoptr i64 %525 to i8*
  %527 = load i8* %526
  %528 = zext i8 %527 to i64
  store i64 %528, i64* %rax
  store volatile i64 24651, i64* @assembly_address
  br label %block_6057

block_604d:                                       ; preds = %block_6011
  store volatile i64 24653, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 24658, i64* @assembly_address
  %529 = load i64* %rdi
  %530 = trunc i64 %529 to i32
  %531 = call i64 @fill_inbuf(i32 %530)
  store i64 %531, i64* %rax
  store i64 %531, i64* %rax
  br label %block_6057

block_6057:                                       ; preds = %block_604d, %block_602f
  store volatile i64 24663, i64* @assembly_address
  %532 = load i64* %rax
  %533 = trunc i64 %532 to i8
  %534 = sext i8 %533 to i32
  store i32 %534, i32* %stack_var_-81
  store volatile i64 24666, i64* @assembly_address
  %535 = load i32* %stack_var_-81
  %536 = trunc i32 %535 to i8
  %537 = zext i8 %536 to i64
  store i64 %537, i64* %rax
  store volatile i64 24670, i64* @assembly_address
  %538 = load i64* %rax
  %539 = trunc i64 %538 to i32
  %540 = and i32 %539, 32
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %541 = icmp eq i32 %540, 0
  store i1 %541, i1* %zf
  %542 = icmp slt i32 %540, 0
  store i1 %542, i1* %sf
  %543 = trunc i32 %540 to i8
  %544 = call i8 @llvm.ctpop.i8(i8 %543)
  %545 = and i8 %544, 1
  %546 = icmp eq i8 %545, 0
  store i1 %546, i1* %pf
  %547 = zext i32 %540 to i64
  store i64 %547, i64* %rax
  store volatile i64 24673, i64* @assembly_address
  %548 = load i64* %rax
  %549 = trunc i64 %548 to i32
  %550 = load i64* %rax
  %551 = trunc i64 %550 to i32
  %552 = and i32 %549, %551
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %553 = icmp eq i32 %552, 0
  store i1 %553, i1* %zf
  %554 = icmp slt i32 %552, 0
  store i1 %554, i1* %sf
  %555 = trunc i32 %552 to i8
  %556 = call i8 @llvm.ctpop.i8(i8 %555)
  %557 = and i8 %556, 1
  %558 = icmp eq i8 %557, 0
  store i1 %558, i1* %pf
  store volatile i64 24675, i64* @assembly_address
  %559 = load i1* %zf
  br i1 %559, label %block_60a2, label %block_6065

block_6065:                                       ; preds = %block_6057
  store volatile i64 24677, i64* @assembly_address
  %560 = load i64* @global_var_25f4c8
  store i64 %560, i64* %rdx
  store volatile i64 24684, i64* @assembly_address
  %561 = load i64* @global_var_216580
  store i64 %561, i64* %rax
  store volatile i64 24691, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 24698, i64* @assembly_address
  store i64 ptrtoint ([38 x i8]* @global_var_117c8 to i64), i64* %rsi
  store volatile i64 24705, i64* @assembly_address
  %562 = load i64* %rax
  store i64 %562, i64* %rdi
  store volatile i64 24708, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 24713, i64* @assembly_address
  %563 = load i64* %rdi
  %564 = inttoptr i64 %563 to %_IO_FILE*
  %565 = load i64* %rsi
  %566 = inttoptr i64 %565 to i8*
  %567 = load i64* %rdx
  %568 = inttoptr i64 %567 to i8*
  %569 = load i64* %rcx
  %570 = inttoptr i64 %569 to i8*
  %571 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %564, i8* %566, i8* %568, i8* %570)
  %572 = sext i32 %571 to i64
  store i64 %572, i64* %rax
  %573 = sext i32 %571 to i64
  store i64 %573, i64* %rax
  store volatile i64 24718, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 24728, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 24733, i64* @assembly_address
  br label %block_6a4d

block_60a2:                                       ; preds = %block_6057
  store volatile i64 24738, i64* @assembly_address
  %574 = load i32* %stack_var_-81
  %575 = trunc i32 %574 to i8
  %576 = zext i8 %575 to i64
  store i64 %576, i64* %rax
  store volatile i64 24742, i64* @assembly_address
  %577 = load i64* %rax
  %578 = trunc i64 %577 to i32
  %579 = and i32 %578, 192
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %580 = icmp eq i32 %579, 0
  store i1 %580, i1* %zf
  %581 = icmp slt i32 %579, 0
  store i1 %581, i1* %sf
  %582 = trunc i32 %579 to i8
  %583 = call i8 @llvm.ctpop.i8(i8 %582)
  %584 = and i8 %583, 1
  %585 = icmp eq i8 %584, 0
  store i1 %585, i1* %pf
  %586 = zext i32 %579 to i64
  store i64 %586, i64* %rax
  store volatile i64 24747, i64* @assembly_address
  %587 = load i64* %rax
  %588 = trunc i64 %587 to i32
  %589 = load i64* %rax
  %590 = trunc i64 %589 to i32
  %591 = and i32 %588, %590
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %592 = icmp eq i32 %591, 0
  store i1 %592, i1* %zf
  %593 = icmp slt i32 %591, 0
  store i1 %593, i1* %sf
  %594 = trunc i32 %591 to i8
  %595 = call i8 @llvm.ctpop.i8(i8 %594)
  %596 = and i8 %595, 1
  %597 = icmp eq i8 %596, 0
  store i1 %597, i1* %pf
  store volatile i64 24749, i64* @assembly_address
  %598 = load i1* %zf
  br i1 %598, label %block_60fe, label %block_60af

block_60af:                                       ; preds = %block_60a2
  store volatile i64 24751, i64* @assembly_address
  %599 = load i32* %stack_var_-81
  %600 = trunc i32 %599 to i8
  %601 = zext i8 %600 to i64
  store i64 %601, i64* %rcx
  store volatile i64 24755, i64* @assembly_address
  %602 = load i64* @global_var_25f4c8
  store i64 %602, i64* %rdx
  store volatile i64 24762, i64* @assembly_address
  %603 = load i64* @global_var_216580
  store i64 %603, i64* %rax
  store volatile i64 24769, i64* @assembly_address
  %604 = load i64* %rcx
  %605 = trunc i64 %604 to i32
  %606 = zext i32 %605 to i64
  store i64 %606, i64* %r8
  store volatile i64 24772, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 24779, i64* @assembly_address
  store i64 ptrtoint ([40 x i8]* @global_var_117f0 to i64), i64* %rsi
  store volatile i64 24786, i64* @assembly_address
  %607 = load i64* %rax
  store i64 %607, i64* %rdi
  store volatile i64 24789, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 24794, i64* @assembly_address
  %608 = load i64* %rdi
  %609 = inttoptr i64 %608 to %_IO_FILE*
  %610 = load i64* %rsi
  %611 = inttoptr i64 %610 to i8*
  %612 = load i64* %rdx
  %613 = inttoptr i64 %612 to i8*
  %614 = load i64* %rcx
  %615 = inttoptr i64 %614 to i8*
  %616 = load i64* %r8
  %617 = trunc i64 %616 to i32
  %618 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %609, i8* %611, i8* %613, i8* %615, i32 %617)
  %619 = sext i32 %618 to i64
  store i64 %619, i64* %rax
  %620 = sext i32 %618 to i64
  store i64 %620, i64* %rax
  store volatile i64 24799, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 24809, i64* @assembly_address
  %621 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %622 = zext i32 %621 to i64
  store i64 %622, i64* %rax
  store volatile i64 24815, i64* @assembly_address
  %623 = load i64* %rax
  %624 = trunc i64 %623 to i32
  %625 = trunc i64 %623 to i32
  store i32 %625, i32* %13
  store i32 1, i32* %12
  %626 = sub i32 %624, 1
  %627 = and i32 %624, 15
  %628 = sub i32 %627, 1
  %629 = icmp ugt i32 %628, 15
  %630 = icmp ult i32 %624, 1
  %631 = xor i32 %624, 1
  %632 = xor i32 %624, %626
  %633 = and i32 %631, %632
  %634 = icmp slt i32 %633, 0
  store i1 %629, i1* %az
  store i1 %630, i1* %cf
  store i1 %634, i1* %of
  %635 = icmp eq i32 %626, 0
  store i1 %635, i1* %zf
  %636 = icmp slt i32 %626, 0
  store i1 %636, i1* %sf
  %637 = trunc i32 %626 to i8
  %638 = call i8 @llvm.ctpop.i8(i8 %637)
  %639 = and i8 %638, 1
  %640 = icmp eq i8 %639, 0
  store i1 %640, i1* %pf
  store volatile i64 24818, i64* @assembly_address
  %641 = load i32* %13
  %642 = sext i32 %641 to i64
  %643 = load i32* %12
  %644 = trunc i64 %642 to i32
  %645 = icmp sgt i32 %644, %643
  br i1 %645, label %block_60fe, label %block_60f4

block_60f4:                                       ; preds = %block_60af
  store volatile i64 24820, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 24825, i64* @assembly_address
  br label %block_6a4d

block_60fe:                                       ; preds = %block_60af, %block_60a2
  store volatile i64 24830, i64* @assembly_address
  %646 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %647 = zext i32 %646 to i64
  store i64 %647, i64* %rdx
  store volatile i64 24836, i64* @assembly_address
  %648 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %649 = zext i32 %648 to i64
  store i64 %649, i64* %rax
  store volatile i64 24842, i64* @assembly_address
  %650 = load i64* %rdx
  %651 = trunc i64 %650 to i32
  %652 = load i64* %rax
  %653 = trunc i64 %652 to i32
  %654 = sub i32 %651, %653
  %655 = and i32 %651, 15
  %656 = and i32 %653, 15
  %657 = sub i32 %655, %656
  %658 = icmp ugt i32 %657, 15
  %659 = icmp ult i32 %651, %653
  %660 = xor i32 %651, %653
  %661 = xor i32 %651, %654
  %662 = and i32 %660, %661
  %663 = icmp slt i32 %662, 0
  store i1 %658, i1* %az
  store i1 %659, i1* %cf
  store i1 %663, i1* %of
  %664 = icmp eq i32 %654, 0
  store i1 %664, i1* %zf
  %665 = icmp slt i32 %654, 0
  store i1 %665, i1* %sf
  %666 = trunc i32 %654 to i8
  %667 = call i8 @llvm.ctpop.i8(i8 %666)
  %668 = and i8 %667, 1
  %669 = icmp eq i8 %668, 0
  store i1 %669, i1* %pf
  store volatile i64 24844, i64* @assembly_address
  %670 = load i1* %cf
  %671 = icmp eq i1 %670, false
  br i1 %671, label %block_612f, label %block_610e

block_610e:                                       ; preds = %block_60fe
  store volatile i64 24846, i64* @assembly_address
  %672 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %673 = zext i32 %672 to i64
  store i64 %673, i64* %rax
  store volatile i64 24852, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24855, i64* @assembly_address
  %674 = load i64* %rdx
  %675 = trunc i64 %674 to i32
  store i32 %675, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24861, i64* @assembly_address
  %676 = load i64* %rax
  %677 = trunc i64 %676 to i32
  %678 = zext i32 %677 to i64
  store i64 %678, i64* %rdx
  store volatile i64 24863, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24870, i64* @assembly_address
  %679 = load i64* %rdx
  %680 = load i64* %rax
  %681 = mul i64 %680, 1
  %682 = add i64 %679, %681
  %683 = inttoptr i64 %682 to i8*
  %684 = load i8* %683
  %685 = zext i8 %684 to i64
  store i64 %685, i64* %rax
  store volatile i64 24874, i64* @assembly_address
  %686 = load i64* %rax
  %687 = trunc i64 %686 to i8
  %688 = zext i8 %687 to i64
  store i64 %688, i64* %rax
  store volatile i64 24877, i64* @assembly_address
  br label %block_613b

block_612f:                                       ; preds = %block_60fe
  store volatile i64 24879, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 24884, i64* @assembly_address
  %689 = load i64* %rdi
  %690 = trunc i64 %689 to i32
  %691 = call i64 @fill_inbuf(i32 %690)
  store i64 %691, i64* %rax
  store i64 %691, i64* %rax
  store volatile i64 24889, i64* @assembly_address
  %692 = load i64* %rax
  %693 = trunc i64 %692 to i32
  %694 = sext i32 %693 to i64
  store i64 %694, i64* %rax
  br label %block_613b

block_613b:                                       ; preds = %block_612f, %block_610e
  store volatile i64 24891, i64* @assembly_address
  %695 = load i64* %rax
  %696 = trunc i64 %695 to i32
  store i32 %696, i32* %stack_var_-48
  store volatile i64 24895, i64* @assembly_address
  %697 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %698 = zext i32 %697 to i64
  store i64 %698, i64* %rdx
  store volatile i64 24901, i64* @assembly_address
  %699 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %700 = zext i32 %699 to i64
  store i64 %700, i64* %rax
  store volatile i64 24907, i64* @assembly_address
  %701 = load i64* %rdx
  %702 = trunc i64 %701 to i32
  %703 = load i64* %rax
  %704 = trunc i64 %703 to i32
  %705 = sub i32 %702, %704
  %706 = and i32 %702, 15
  %707 = and i32 %704, 15
  %708 = sub i32 %706, %707
  %709 = icmp ugt i32 %708, 15
  %710 = icmp ult i32 %702, %704
  %711 = xor i32 %702, %704
  %712 = xor i32 %702, %705
  %713 = and i32 %711, %712
  %714 = icmp slt i32 %713, 0
  store i1 %709, i1* %az
  store i1 %710, i1* %cf
  store i1 %714, i1* %of
  %715 = icmp eq i32 %705, 0
  store i1 %715, i1* %zf
  %716 = icmp slt i32 %705, 0
  store i1 %716, i1* %sf
  %717 = trunc i32 %705 to i8
  %718 = call i8 @llvm.ctpop.i8(i8 %717)
  %719 = and i8 %718, 1
  %720 = icmp eq i8 %719, 0
  store i1 %720, i1* %pf
  store volatile i64 24909, i64* @assembly_address
  %721 = load i1* %cf
  %722 = icmp eq i1 %721, false
  br i1 %722, label %block_6174, label %block_614f

block_614f:                                       ; preds = %block_613b
  store volatile i64 24911, i64* @assembly_address
  %723 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %724 = zext i32 %723 to i64
  store i64 %724, i64* %rax
  store volatile i64 24917, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24920, i64* @assembly_address
  %725 = load i64* %rdx
  %726 = trunc i64 %725 to i32
  store i32 %726, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24926, i64* @assembly_address
  %727 = load i64* %rax
  %728 = trunc i64 %727 to i32
  %729 = zext i32 %728 to i64
  store i64 %729, i64* %rdx
  store volatile i64 24928, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 24935, i64* @assembly_address
  %730 = load i64* %rdx
  %731 = load i64* %rax
  %732 = mul i64 %731, 1
  %733 = add i64 %730, %732
  %734 = inttoptr i64 %733 to i8*
  %735 = load i8* %734
  %736 = zext i8 %735 to i64
  store i64 %736, i64* %rax
  store volatile i64 24939, i64* @assembly_address
  %737 = load i64* %rax
  %738 = trunc i64 %737 to i8
  %739 = zext i8 %738 to i64
  store i64 %739, i64* %rax
  store volatile i64 24942, i64* @assembly_address
  %740 = load i64* %rax
  %741 = load i1* %of
  %742 = shl i64 %740, 8
  %743 = icmp eq i64 %742, 0
  store i1 %743, i1* %zf
  %744 = icmp slt i64 %742, 0
  store i1 %744, i1* %sf
  %745 = trunc i64 %742 to i8
  %746 = call i8 @llvm.ctpop.i8(i8 %745)
  %747 = and i8 %746, 1
  %748 = icmp eq i8 %747, 0
  store i1 %748, i1* %pf
  store i64 %742, i64* %rax
  %749 = shl i64 %740, 7
  %750 = lshr i64 %749, 63
  %751 = trunc i64 %750 to i1
  store i1 %751, i1* %cf
  %752 = lshr i64 %742, 63
  %753 = icmp ne i64 %752, %750
  %754 = select i1 false, i1 %753, i1 %741
  store i1 %754, i1* %of
  store volatile i64 24946, i64* @assembly_address
  br label %block_6184

block_6174:                                       ; preds = %block_613b
  store volatile i64 24948, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 24953, i64* @assembly_address
  %755 = load i64* %rdi
  %756 = trunc i64 %755 to i32
  %757 = call i64 @fill_inbuf(i32 %756)
  store i64 %757, i64* %rax
  store i64 %757, i64* %rax
  store volatile i64 24958, i64* @assembly_address
  %758 = load i64* %rax
  %759 = trunc i64 %758 to i32
  %760 = sext i32 %759 to i64
  store i64 %760, i64* %rax
  store volatile i64 24960, i64* @assembly_address
  %761 = load i64* %rax
  %762 = load i1* %of
  %763 = shl i64 %761, 8
  %764 = icmp eq i64 %763, 0
  store i1 %764, i1* %zf
  %765 = icmp slt i64 %763, 0
  store i1 %765, i1* %sf
  %766 = trunc i64 %763 to i8
  %767 = call i8 @llvm.ctpop.i8(i8 %766)
  %768 = and i8 %767, 1
  %769 = icmp eq i8 %768, 0
  store i1 %769, i1* %pf
  store i64 %763, i64* %rax
  %770 = shl i64 %761, 7
  %771 = lshr i64 %770, 63
  %772 = trunc i64 %771 to i1
  store i1 %772, i1* %cf
  %773 = lshr i64 %763, 63
  %774 = icmp ne i64 %773, %771
  %775 = select i1 false, i1 %774, i1 %762
  store i1 %775, i1* %of
  br label %block_6184

block_6184:                                       ; preds = %block_6174, %block_614f
  store volatile i64 24964, i64* @assembly_address
  %776 = load i32* %stack_var_-48
  %777 = sext i32 %776 to i64
  %778 = load i64* %rax
  %779 = or i64 %777, %778
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %780 = icmp eq i64 %779, 0
  store i1 %780, i1* %zf
  %781 = icmp slt i64 %779, 0
  store i1 %781, i1* %sf
  %782 = trunc i64 %779 to i8
  %783 = call i8 @llvm.ctpop.i8(i8 %782)
  %784 = and i8 %783, 1
  %785 = icmp eq i8 %784, 0
  store i1 %785, i1* %pf
  %786 = trunc i64 %779 to i32
  store i32 %786, i32* %stack_var_-48
  store volatile i64 24968, i64* @assembly_address
  %787 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %788 = zext i32 %787 to i64
  store i64 %788, i64* %rdx
  store volatile i64 24974, i64* @assembly_address
  %789 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %790 = zext i32 %789 to i64
  store i64 %790, i64* %rax
  store volatile i64 24980, i64* @assembly_address
  %791 = load i64* %rdx
  %792 = trunc i64 %791 to i32
  %793 = load i64* %rax
  %794 = trunc i64 %793 to i32
  %795 = sub i32 %792, %794
  %796 = and i32 %792, 15
  %797 = and i32 %794, 15
  %798 = sub i32 %796, %797
  %799 = icmp ugt i32 %798, 15
  %800 = icmp ult i32 %792, %794
  %801 = xor i32 %792, %794
  %802 = xor i32 %792, %795
  %803 = and i32 %801, %802
  %804 = icmp slt i32 %803, 0
  store i1 %799, i1* %az
  store i1 %800, i1* %cf
  store i1 %804, i1* %of
  %805 = icmp eq i32 %795, 0
  store i1 %805, i1* %zf
  %806 = icmp slt i32 %795, 0
  store i1 %806, i1* %sf
  %807 = trunc i32 %795 to i8
  %808 = call i8 @llvm.ctpop.i8(i8 %807)
  %809 = and i8 %808, 1
  %810 = icmp eq i8 %809, 0
  store i1 %810, i1* %pf
  store volatile i64 24982, i64* @assembly_address
  %811 = load i1* %cf
  %812 = icmp eq i1 %811, false
  br i1 %812, label %block_61bd, label %block_6198

block_6198:                                       ; preds = %block_6184
  store volatile i64 24984, i64* @assembly_address
  %813 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %814 = zext i32 %813 to i64
  store i64 %814, i64* %rax
  store volatile i64 24990, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 24993, i64* @assembly_address
  %815 = load i64* %rdx
  %816 = trunc i64 %815 to i32
  store i32 %816, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 24999, i64* @assembly_address
  %817 = load i64* %rax
  %818 = trunc i64 %817 to i32
  %819 = zext i32 %818 to i64
  store i64 %819, i64* %rdx
  store volatile i64 25001, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 25008, i64* @assembly_address
  %820 = load i64* %rdx
  %821 = load i64* %rax
  %822 = mul i64 %821, 1
  %823 = add i64 %820, %822
  %824 = inttoptr i64 %823 to i8*
  %825 = load i8* %824
  %826 = zext i8 %825 to i64
  store i64 %826, i64* %rax
  store volatile i64 25012, i64* @assembly_address
  %827 = load i64* %rax
  %828 = trunc i64 %827 to i8
  %829 = zext i8 %828 to i64
  store i64 %829, i64* %rax
  store volatile i64 25015, i64* @assembly_address
  %830 = load i64* %rax
  %831 = load i1* %of
  %832 = shl i64 %830, 16
  %833 = icmp eq i64 %832, 0
  store i1 %833, i1* %zf
  %834 = icmp slt i64 %832, 0
  store i1 %834, i1* %sf
  %835 = trunc i64 %832 to i8
  %836 = call i8 @llvm.ctpop.i8(i8 %835)
  %837 = and i8 %836, 1
  %838 = icmp eq i8 %837, 0
  store i1 %838, i1* %pf
  store i64 %832, i64* %rax
  %839 = shl i64 %830, 15
  %840 = lshr i64 %839, 63
  %841 = trunc i64 %840 to i1
  store i1 %841, i1* %cf
  %842 = lshr i64 %832, 63
  %843 = icmp ne i64 %842, %840
  %844 = select i1 false, i1 %843, i1 %831
  store i1 %844, i1* %of
  store volatile i64 25019, i64* @assembly_address
  br label %block_61cd

block_61bd:                                       ; preds = %block_6184
  store volatile i64 25021, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25026, i64* @assembly_address
  %845 = load i64* %rdi
  %846 = trunc i64 %845 to i32
  %847 = call i64 @fill_inbuf(i32 %846)
  store i64 %847, i64* %rax
  store i64 %847, i64* %rax
  store volatile i64 25031, i64* @assembly_address
  %848 = load i64* %rax
  %849 = trunc i64 %848 to i32
  %850 = sext i32 %849 to i64
  store i64 %850, i64* %rax
  store volatile i64 25033, i64* @assembly_address
  %851 = load i64* %rax
  %852 = load i1* %of
  %853 = shl i64 %851, 16
  %854 = icmp eq i64 %853, 0
  store i1 %854, i1* %zf
  %855 = icmp slt i64 %853, 0
  store i1 %855, i1* %sf
  %856 = trunc i64 %853 to i8
  %857 = call i8 @llvm.ctpop.i8(i8 %856)
  %858 = and i8 %857, 1
  %859 = icmp eq i8 %858, 0
  store i1 %859, i1* %pf
  store i64 %853, i64* %rax
  %860 = shl i64 %851, 15
  %861 = lshr i64 %860, 63
  %862 = trunc i64 %861 to i1
  store i1 %862, i1* %cf
  %863 = lshr i64 %853, 63
  %864 = icmp ne i64 %863, %861
  %865 = select i1 false, i1 %864, i1 %852
  store i1 %865, i1* %of
  br label %block_61cd

block_61cd:                                       ; preds = %block_61bd, %block_6198
  store volatile i64 25037, i64* @assembly_address
  %866 = load i32* %stack_var_-48
  %867 = sext i32 %866 to i64
  %868 = load i64* %rax
  %869 = or i64 %867, %868
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %870 = icmp eq i64 %869, 0
  store i1 %870, i1* %zf
  %871 = icmp slt i64 %869, 0
  store i1 %871, i1* %sf
  %872 = trunc i64 %869 to i8
  %873 = call i8 @llvm.ctpop.i8(i8 %872)
  %874 = and i8 %873, 1
  %875 = icmp eq i8 %874, 0
  store i1 %875, i1* %pf
  %876 = trunc i64 %869 to i32
  store i32 %876, i32* %stack_var_-48
  store volatile i64 25041, i64* @assembly_address
  %877 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %878 = zext i32 %877 to i64
  store i64 %878, i64* %rdx
  store volatile i64 25047, i64* @assembly_address
  %879 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %880 = zext i32 %879 to i64
  store i64 %880, i64* %rax
  store volatile i64 25053, i64* @assembly_address
  %881 = load i64* %rdx
  %882 = trunc i64 %881 to i32
  %883 = load i64* %rax
  %884 = trunc i64 %883 to i32
  %885 = sub i32 %882, %884
  %886 = and i32 %882, 15
  %887 = and i32 %884, 15
  %888 = sub i32 %886, %887
  %889 = icmp ugt i32 %888, 15
  %890 = icmp ult i32 %882, %884
  %891 = xor i32 %882, %884
  %892 = xor i32 %882, %885
  %893 = and i32 %891, %892
  %894 = icmp slt i32 %893, 0
  store i1 %889, i1* %az
  store i1 %890, i1* %cf
  store i1 %894, i1* %of
  %895 = icmp eq i32 %885, 0
  store i1 %895, i1* %zf
  %896 = icmp slt i32 %885, 0
  store i1 %896, i1* %sf
  %897 = trunc i32 %885 to i8
  %898 = call i8 @llvm.ctpop.i8(i8 %897)
  %899 = and i8 %898, 1
  %900 = icmp eq i8 %899, 0
  store i1 %900, i1* %pf
  store volatile i64 25055, i64* @assembly_address
  %901 = load i1* %cf
  %902 = icmp eq i1 %901, false
  br i1 %902, label %block_6206, label %block_61e1

block_61e1:                                       ; preds = %block_61cd
  store volatile i64 25057, i64* @assembly_address
  %903 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %904 = zext i32 %903 to i64
  store i64 %904, i64* %rax
  store volatile i64 25063, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 25066, i64* @assembly_address
  %905 = load i64* %rdx
  %906 = trunc i64 %905 to i32
  store i32 %906, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 25072, i64* @assembly_address
  %907 = load i64* %rax
  %908 = trunc i64 %907 to i32
  %909 = zext i32 %908 to i64
  store i64 %909, i64* %rdx
  store volatile i64 25074, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 25081, i64* @assembly_address
  %910 = load i64* %rdx
  %911 = load i64* %rax
  %912 = mul i64 %911, 1
  %913 = add i64 %910, %912
  %914 = inttoptr i64 %913 to i8*
  %915 = load i8* %914
  %916 = zext i8 %915 to i64
  store i64 %916, i64* %rax
  store volatile i64 25085, i64* @assembly_address
  %917 = load i64* %rax
  %918 = trunc i64 %917 to i8
  %919 = zext i8 %918 to i64
  store i64 %919, i64* %rax
  store volatile i64 25088, i64* @assembly_address
  %920 = load i64* %rax
  %921 = load i1* %of
  %922 = shl i64 %920, 24
  %923 = icmp eq i64 %922, 0
  store i1 %923, i1* %zf
  %924 = icmp slt i64 %922, 0
  store i1 %924, i1* %sf
  %925 = trunc i64 %922 to i8
  %926 = call i8 @llvm.ctpop.i8(i8 %925)
  %927 = and i8 %926, 1
  %928 = icmp eq i8 %927, 0
  store i1 %928, i1* %pf
  store i64 %922, i64* %rax
  %929 = shl i64 %920, 23
  %930 = lshr i64 %929, 63
  %931 = trunc i64 %930 to i1
  store i1 %931, i1* %cf
  %932 = lshr i64 %922, 63
  %933 = icmp ne i64 %932, %930
  %934 = select i1 false, i1 %933, i1 %921
  store i1 %934, i1* %of
  store volatile i64 25092, i64* @assembly_address
  br label %block_6216

block_6206:                                       ; preds = %block_61cd
  store volatile i64 25094, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25099, i64* @assembly_address
  %935 = load i64* %rdi
  %936 = trunc i64 %935 to i32
  %937 = call i64 @fill_inbuf(i32 %936)
  store i64 %937, i64* %rax
  store i64 %937, i64* %rax
  store volatile i64 25104, i64* @assembly_address
  %938 = load i64* %rax
  %939 = trunc i64 %938 to i32
  %940 = sext i32 %939 to i64
  store i64 %940, i64* %rax
  store volatile i64 25106, i64* @assembly_address
  %941 = load i64* %rax
  %942 = load i1* %of
  %943 = shl i64 %941, 24
  %944 = icmp eq i64 %943, 0
  store i1 %944, i1* %zf
  %945 = icmp slt i64 %943, 0
  store i1 %945, i1* %sf
  %946 = trunc i64 %943 to i8
  %947 = call i8 @llvm.ctpop.i8(i8 %946)
  %948 = and i8 %947, 1
  %949 = icmp eq i8 %948, 0
  store i1 %949, i1* %pf
  store i64 %943, i64* %rax
  %950 = shl i64 %941, 23
  %951 = lshr i64 %950, 63
  %952 = trunc i64 %951 to i1
  store i1 %952, i1* %cf
  %953 = lshr i64 %943, 63
  %954 = icmp ne i64 %953, %951
  %955 = select i1 false, i1 %954, i1 %942
  store i1 %955, i1* %of
  br label %block_6216

block_6216:                                       ; preds = %block_6206, %block_61e1
  store volatile i64 25110, i64* @assembly_address
  %956 = load i32* %stack_var_-48
  %957 = sext i32 %956 to i64
  %958 = load i64* %rax
  %959 = or i64 %957, %958
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %960 = icmp eq i64 %959, 0
  store i1 %960, i1* %zf
  %961 = icmp slt i64 %959, 0
  store i1 %961, i1* %sf
  %962 = trunc i64 %959 to i8
  %963 = call i8 @llvm.ctpop.i8(i8 %962)
  %964 = and i8 %963, 1
  %965 = icmp eq i8 %964, 0
  store i1 %965, i1* %pf
  %966 = trunc i64 %959 to i32
  store i32 %966, i32* %stack_var_-48
  store volatile i64 25114, i64* @assembly_address
  %967 = load i32* %stack_var_-48
  %968 = sext i32 %967 to i64
  %969 = and i64 %968, 15
  %970 = icmp ugt i64 %969, 15
  %971 = icmp ult i64 %968, 0
  %972 = xor i64 %968, 0
  %973 = and i64 %972, 0
  %974 = icmp slt i64 %973, 0
  store i1 %970, i1* %az
  store i1 %971, i1* %cf
  store i1 %974, i1* %of
  %975 = icmp eq i64 %968, 0
  store i1 %975, i1* %zf
  %976 = icmp slt i64 %968, 0
  store i1 %976, i1* %sf
  %977 = trunc i64 %968 to i8
  %978 = call i8 @llvm.ctpop.i8(i8 %977)
  %979 = and i8 %978, 1
  %980 = icmp eq i8 %979, 0
  store i1 %980, i1* %pf
  store volatile i64 25119, i64* @assembly_address
  %981 = load i1* %zf
  br i1 %981, label %block_62be, label %block_6225

block_6225:                                       ; preds = %block_6216
  store volatile i64 25125, i64* @assembly_address
  %982 = load i32* bitcast (i64* @global_var_216094 to i32*)
  %983 = zext i32 %982 to i64
  store i64 %983, i64* %rax
  store volatile i64 25131, i64* @assembly_address
  %984 = load i64* %rax
  %985 = trunc i64 %984 to i32
  %986 = load i64* %rax
  %987 = trunc i64 %986 to i32
  %988 = and i32 %985, %987
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %989 = icmp eq i32 %988, 0
  store i1 %989, i1* %zf
  %990 = icmp slt i32 %988, 0
  store i1 %990, i1* %sf
  %991 = trunc i32 %988 to i8
  %992 = call i8 @llvm.ctpop.i8(i8 %991)
  %993 = and i8 %992, 1
  %994 = icmp eq i8 %993, 0
  store i1 %994, i1* %pf
  store volatile i64 25133, i64* @assembly_address
  %995 = load i1* %zf
  %996 = icmp eq i1 %995, false
  br i1 %996, label %block_62be, label %block_6233

block_6233:                                       ; preds = %block_6225
  store volatile i64 25139, i64* @assembly_address
  %997 = load i32* %stack_var_-48
  %998 = sext i32 %997 to i64
  store i64 %998, i64* %rax
  store volatile i64 25143, i64* @assembly_address
  %999 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1000 = icmp eq i64 %999, 0
  store i1 %1000, i1* %zf
  %1001 = icmp slt i64 %999, 0
  store i1 %1001, i1* %sf
  %1002 = trunc i64 %999 to i8
  %1003 = call i8 @llvm.ctpop.i8(i8 %1002)
  %1004 = and i8 %1003, 1
  %1005 = icmp eq i8 %1004, 0
  store i1 %1005, i1* %pf
  store volatile i64 25146, i64* @assembly_address
  %1006 = load i1* %sf
  br i1 %1006, label %block_6254, label %block_623c

block_623c:                                       ; preds = %block_6233
  store volatile i64 25148, i64* @assembly_address
  %1007 = load i32* %stack_var_-48
  %1008 = sext i32 %1007 to i64
  store i64 %1008, i64* %rax
  store volatile i64 25152, i64* @assembly_address
  %1009 = load i64* %rax
  store i64 %1009, i64* @global_var_25f4d0
  store volatile i64 25159, i64* @assembly_address
  store i64 0, i64* @global_var_25f4d8
  store volatile i64 25170, i64* @assembly_address
  br label %block_62be

block_6254:                                       ; preds = %block_6233
  store volatile i64 25172, i64* @assembly_address
  %1010 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %1011 = zext i32 %1010 to i64
  store i64 %1011, i64* %rax
  store volatile i64 25178, i64* @assembly_address
  %1012 = load i64* %rax
  %1013 = trunc i64 %1012 to i32
  %1014 = load i64* %rax
  %1015 = trunc i64 %1014 to i32
  %1016 = and i32 %1013, %1015
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1017 = icmp eq i32 %1016, 0
  store i1 %1017, i1* %zf
  %1018 = icmp slt i32 %1016, 0
  store i1 %1018, i1* %sf
  %1019 = trunc i32 %1016 to i8
  %1020 = call i8 @llvm.ctpop.i8(i8 %1019)
  %1021 = and i8 %1020, 1
  %1022 = icmp eq i8 %1021, 0
  store i1 %1022, i1* %pf
  store volatile i64 25180, i64* @assembly_address
  %1023 = load i1* %zf
  %1024 = icmp eq i1 %1023, false
  br i1 %1024, label %block_628e, label %block_625e

block_625e:                                       ; preds = %block_6254
  store volatile i64 25182, i64* @assembly_address
  %1025 = load i64* @global_var_25f4c8
  store i64 %1025, i64* %rdx
  store volatile i64 25189, i64* @assembly_address
  %1026 = load i64* @global_var_216580
  store i64 %1026, i64* %rax
  store volatile i64 25196, i64* @assembly_address
  %1027 = load i32* %stack_var_-48
  %1028 = sext i32 %1027 to i64
  store i64 %1028, i64* %rcx
  store volatile i64 25200, i64* @assembly_address
  %1029 = load i64* %rcx
  store i64 %1029, i64* %r8
  store volatile i64 25203, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 25210, i64* @assembly_address
  store i64 ptrtoint ([50 x i8]* @global_var_11818 to i64), i64* %rsi
  store volatile i64 25217, i64* @assembly_address
  %1030 = load i64* %rax
  store i64 %1030, i64* %rdi
  store volatile i64 25220, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 25225, i64* @assembly_address
  %1031 = load i64* %rdi
  %1032 = inttoptr i64 %1031 to %_IO_FILE*
  %1033 = load i64* %rsi
  %1034 = inttoptr i64 %1033 to i8*
  %1035 = load i64* %rdx
  %1036 = inttoptr i64 %1035 to i8*
  %1037 = load i64* %rcx
  %1038 = inttoptr i64 %1037 to i8*
  %1039 = load i64* %r8
  %1040 = trunc i64 %1039 to i32
  %1041 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1032, i8* %1034, i8* %1036, i8* %1038, i32 %1040)
  %1042 = sext i32 %1041 to i64
  store i64 %1042, i64* %rax
  %1043 = sext i32 %1041 to i64
  store i64 %1043, i64* %rax
  br label %block_628e

block_628e:                                       ; preds = %block_625e, %block_6254
  store volatile i64 25230, i64* @assembly_address
  %1044 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %1045 = zext i32 %1044 to i64
  store i64 %1045, i64* %rax
  store volatile i64 25236, i64* @assembly_address
  %1046 = load i64* %rax
  %1047 = trunc i64 %1046 to i32
  %1048 = load i64* %rax
  %1049 = trunc i64 %1048 to i32
  %1050 = and i32 %1047, %1049
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1051 = icmp eq i32 %1050, 0
  store i1 %1051, i1* %zf
  %1052 = icmp slt i32 %1050, 0
  store i1 %1052, i1* %sf
  %1053 = trunc i32 %1050 to i8
  %1054 = call i8 @llvm.ctpop.i8(i8 %1053)
  %1055 = and i8 %1054, 1
  %1056 = icmp eq i8 %1055, 0
  store i1 %1056, i1* %pf
  store volatile i64 25238, i64* @assembly_address
  %1057 = load i1* %zf
  %1058 = icmp eq i1 %1057, false
  br i1 %1058, label %block_62a2, label %block_6298

block_6298:                                       ; preds = %block_628e
  store volatile i64 25240, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_62a2

block_62a2:                                       ; preds = %block_6298, %block_628e
  store volatile i64 25250, i64* @assembly_address
  store i64 9223372036854775807, i64* %rax
  store volatile i64 25260, i64* @assembly_address
  %1059 = load i64* %rax
  store i64 %1059, i64* @global_var_25f4d0
  store volatile i64 25267, i64* @assembly_address
  store i64 999999999, i64* @global_var_25f4d8
  br label %block_62be

block_62be:                                       ; preds = %block_62a2, %block_623c, %block_6225, %block_6216
  store volatile i64 25278, i64* @assembly_address
  %1060 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1061 = zext i32 %1060 to i64
  store i64 %1061, i64* %rdx
  store volatile i64 25284, i64* @assembly_address
  %1062 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1063 = zext i32 %1062 to i64
  store i64 %1063, i64* %rax
  store volatile i64 25290, i64* @assembly_address
  %1064 = load i64* %rdx
  %1065 = trunc i64 %1064 to i32
  %1066 = load i64* %rax
  %1067 = trunc i64 %1066 to i32
  %1068 = sub i32 %1065, %1067
  %1069 = and i32 %1065, 15
  %1070 = and i32 %1067, 15
  %1071 = sub i32 %1069, %1070
  %1072 = icmp ugt i32 %1071, 15
  %1073 = icmp ult i32 %1065, %1067
  %1074 = xor i32 %1065, %1067
  %1075 = xor i32 %1065, %1068
  %1076 = and i32 %1074, %1075
  %1077 = icmp slt i32 %1076, 0
  store i1 %1072, i1* %az
  store i1 %1073, i1* %cf
  store i1 %1077, i1* %of
  %1078 = icmp eq i32 %1068, 0
  store i1 %1078, i1* %zf
  %1079 = icmp slt i32 %1068, 0
  store i1 %1079, i1* %sf
  %1080 = trunc i32 %1068 to i8
  %1081 = call i8 @llvm.ctpop.i8(i8 %1080)
  %1082 = and i8 %1081, 1
  %1083 = icmp eq i8 %1082, 0
  store i1 %1083, i1* %pf
  store volatile i64 25292, i64* @assembly_address
  %1084 = load i1* %cf
  %1085 = icmp eq i1 %1084, false
  br i1 %1085, label %block_62ec, label %block_62ce

block_62ce:                                       ; preds = %block_62be
  store volatile i64 25294, i64* @assembly_address
  %1086 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1087 = zext i32 %1086 to i64
  store i64 %1087, i64* %rax
  store volatile i64 25300, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 25303, i64* @assembly_address
  %1088 = load i64* %rdx
  %1089 = trunc i64 %1088 to i32
  store i32 %1089, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 25309, i64* @assembly_address
  %1090 = load i64* %rax
  %1091 = trunc i64 %1090 to i32
  %1092 = zext i32 %1091 to i64
  store i64 %1092, i64* %rdx
  store volatile i64 25311, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 25318, i64* @assembly_address
  %1093 = load i64* %rdx
  %1094 = load i64* %rax
  %1095 = mul i64 %1094, 1
  %1096 = add i64 %1093, %1095
  %1097 = inttoptr i64 %1096 to i8*
  %1098 = load i8* %1097
  %1099 = zext i8 %1098 to i64
  store i64 %1099, i64* %rax
  store volatile i64 25322, i64* @assembly_address
  br label %block_62f6

block_62ec:                                       ; preds = %block_62be
  store volatile i64 25324, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25329, i64* @assembly_address
  %1100 = load i64* %rdi
  %1101 = trunc i64 %1100 to i32
  %1102 = call i64 @fill_inbuf(i32 %1101)
  store i64 %1102, i64* %rax
  store i64 %1102, i64* %rax
  br label %block_62f6

block_62f6:                                       ; preds = %block_62ec, %block_62ce
  store volatile i64 25334, i64* @assembly_address
  %1103 = load i64* %rax
  %1104 = trunc i64 %1103 to i8
  store i8 %1104, i8* %stack_var_-18
  store volatile i64 25337, i64* @assembly_address
  %1105 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1106 = zext i32 %1105 to i64
  store i64 %1106, i64* %rdx
  store volatile i64 25343, i64* @assembly_address
  %1107 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1108 = zext i32 %1107 to i64
  store i64 %1108, i64* %rax
  store volatile i64 25349, i64* @assembly_address
  %1109 = load i64* %rdx
  %1110 = trunc i64 %1109 to i32
  %1111 = load i64* %rax
  %1112 = trunc i64 %1111 to i32
  %1113 = sub i32 %1110, %1112
  %1114 = and i32 %1110, 15
  %1115 = and i32 %1112, 15
  %1116 = sub i32 %1114, %1115
  %1117 = icmp ugt i32 %1116, 15
  %1118 = icmp ult i32 %1110, %1112
  %1119 = xor i32 %1110, %1112
  %1120 = xor i32 %1110, %1113
  %1121 = and i32 %1119, %1120
  %1122 = icmp slt i32 %1121, 0
  store i1 %1117, i1* %az
  store i1 %1118, i1* %cf
  store i1 %1122, i1* %of
  %1123 = icmp eq i32 %1113, 0
  store i1 %1123, i1* %zf
  %1124 = icmp slt i32 %1113, 0
  store i1 %1124, i1* %sf
  %1125 = trunc i32 %1113 to i8
  %1126 = call i8 @llvm.ctpop.i8(i8 %1125)
  %1127 = and i8 %1126, 1
  %1128 = icmp eq i8 %1127, 0
  store i1 %1128, i1* %pf
  store volatile i64 25351, i64* @assembly_address
  %1129 = load i1* %cf
  %1130 = icmp eq i1 %1129, false
  br i1 %1130, label %block_6327, label %block_6309

block_6309:                                       ; preds = %block_62f6
  store volatile i64 25353, i64* @assembly_address
  %1131 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1132 = zext i32 %1131 to i64
  store i64 %1132, i64* %rax
  store volatile i64 25359, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 25362, i64* @assembly_address
  %1133 = load i64* %rdx
  %1134 = trunc i64 %1133 to i32
  store i32 %1134, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 25368, i64* @assembly_address
  %1135 = load i64* %rax
  %1136 = trunc i64 %1135 to i32
  %1137 = zext i32 %1136 to i64
  store i64 %1137, i64* %rdx
  store volatile i64 25370, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 25377, i64* @assembly_address
  %1138 = load i64* %rdx
  %1139 = load i64* %rax
  %1140 = mul i64 %1139, 1
  %1141 = add i64 %1138, %1140
  %1142 = inttoptr i64 %1141 to i8*
  %1143 = load i8* %1142
  %1144 = zext i8 %1143 to i64
  store i64 %1144, i64* %rax
  store volatile i64 25381, i64* @assembly_address
  br label %block_6331

block_6327:                                       ; preds = %block_62f6
  store volatile i64 25383, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25388, i64* @assembly_address
  %1145 = load i64* %rdi
  %1146 = trunc i64 %1145 to i32
  %1147 = call i64 @fill_inbuf(i32 %1146)
  store i64 %1147, i64* %rax
  store i64 %1147, i64* %rax
  br label %block_6331

block_6331:                                       ; preds = %block_6327, %block_6309
  store volatile i64 25393, i64* @assembly_address
  %1148 = load i64* %rax
  %1149 = trunc i64 %1148 to i8
  store i8 %1149, i8* %stack_var_-17
  store volatile i64 25396, i64* @assembly_address
  %1150 = load i32* %stack_var_-81
  %1151 = trunc i32 %1150 to i8
  %1152 = zext i8 %1151 to i64
  store i64 %1152, i64* %rax
  store volatile i64 25400, i64* @assembly_address
  %1153 = load i64* %rax
  %1154 = trunc i64 %1153 to i32
  %1155 = and i32 %1154, 2
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1156 = icmp eq i32 %1155, 0
  store i1 %1156, i1* %zf
  %1157 = icmp slt i32 %1155, 0
  store i1 %1157, i1* %sf
  %1158 = trunc i32 %1155 to i8
  %1159 = call i8 @llvm.ctpop.i8(i8 %1158)
  %1160 = and i8 %1159, 1
  %1161 = icmp eq i8 %1160, 0
  store i1 %1161, i1* %pf
  %1162 = zext i32 %1155 to i64
  store i64 %1162, i64* %rax
  store volatile i64 25403, i64* @assembly_address
  %1163 = load i64* %rax
  %1164 = trunc i64 %1163 to i32
  %1165 = load i64* %rax
  %1166 = trunc i64 %1165 to i32
  %1167 = and i32 %1164, %1166
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1168 = icmp eq i32 %1167, 0
  store i1 %1168, i1* %zf
  %1169 = icmp slt i32 %1167, 0
  store i1 %1169, i1* %sf
  %1170 = trunc i32 %1167 to i8
  %1171 = call i8 @llvm.ctpop.i8(i8 %1170)
  %1172 = and i8 %1171, 1
  %1173 = icmp eq i8 %1172, 0
  store i1 %1173, i1* %pf
  store volatile i64 25405, i64* @assembly_address
  %1174 = load i1* %zf
  br i1 %1174, label %block_6392, label %block_633f

block_633f:                                       ; preds = %block_6331
  store volatile i64 25407, i64* @assembly_address
  store i8 8, i8* %stack_var_-24
  store volatile i64 25411, i64* @assembly_address
  %1175 = load i32* %stack_var_-81
  %1176 = trunc i32 %1175 to i8
  %1177 = zext i8 %1176 to i64
  store i64 %1177, i64* %rax
  store volatile i64 25415, i64* @assembly_address
  %1178 = load i64* %rax
  %1179 = trunc i64 %1178 to i8
  store i8 %1179, i8* %stack_var_-23
  store volatile i64 25418, i64* @assembly_address
  %1180 = load i32* %stack_var_-48
  %1181 = sext i32 %1180 to i64
  store i64 %1181, i64* %rax
  store volatile i64 25422, i64* @assembly_address
  %1182 = load i64* %rax
  %1183 = trunc i64 %1182 to i8
  store i8 %1183, i8* %stack_var_-22
  store volatile i64 25425, i64* @assembly_address
  %1184 = load i32* %stack_var_-48
  %1185 = sext i32 %1184 to i64
  store i64 %1185, i64* %rax
  store volatile i64 25429, i64* @assembly_address
  %1186 = load i64* %rax
  %1187 = load i1* %of
  %1188 = lshr i64 %1186, 8
  %1189 = icmp eq i64 %1188, 0
  store i1 %1189, i1* %zf
  %1190 = icmp slt i64 %1188, 0
  store i1 %1190, i1* %sf
  %1191 = trunc i64 %1188 to i8
  %1192 = call i8 @llvm.ctpop.i8(i8 %1191)
  %1193 = and i8 %1192, 1
  %1194 = icmp eq i8 %1193, 0
  store i1 %1194, i1* %pf
  store i64 %1188, i64* %rax
  %1195 = and i64 128, %1186
  %1196 = icmp ne i64 %1195, 0
  store i1 %1196, i1* %cf
  %1197 = icmp slt i64 %1186, 0
  %1198 = select i1 false, i1 %1197, i1 %1187
  store i1 %1198, i1* %of
  store volatile i64 25433, i64* @assembly_address
  %1199 = load i64* %rax
  %1200 = trunc i64 %1199 to i8
  store i8 %1200, i8* %stack_var_-21
  store volatile i64 25436, i64* @assembly_address
  %1201 = load i32* %stack_var_-48
  %1202 = sext i32 %1201 to i64
  store i64 %1202, i64* %rax
  store volatile i64 25440, i64* @assembly_address
  %1203 = load i64* %rax
  %1204 = load i1* %of
  %1205 = lshr i64 %1203, 16
  %1206 = icmp eq i64 %1205, 0
  store i1 %1206, i1* %zf
  %1207 = icmp slt i64 %1205, 0
  store i1 %1207, i1* %sf
  %1208 = trunc i64 %1205 to i8
  %1209 = call i8 @llvm.ctpop.i8(i8 %1208)
  %1210 = and i8 %1209, 1
  %1211 = icmp eq i8 %1210, 0
  store i1 %1211, i1* %pf
  store i64 %1205, i64* %rax
  %1212 = and i64 32768, %1203
  %1213 = icmp ne i64 %1212, 0
  store i1 %1213, i1* %cf
  %1214 = icmp slt i64 %1203, 0
  %1215 = select i1 false, i1 %1214, i1 %1204
  store i1 %1215, i1* %of
  store volatile i64 25444, i64* @assembly_address
  %1216 = load i64* %rax
  %1217 = trunc i64 %1216 to i8
  store i8 %1217, i8* %stack_var_-20
  store volatile i64 25447, i64* @assembly_address
  %1218 = load i32* %stack_var_-48
  %1219 = sext i32 %1218 to i64
  store i64 %1219, i64* %rax
  store volatile i64 25451, i64* @assembly_address
  %1220 = load i64* %rax
  %1221 = load i1* %of
  %1222 = lshr i64 %1220, 24
  %1223 = icmp eq i64 %1222, 0
  store i1 %1223, i1* %zf
  %1224 = icmp slt i64 %1222, 0
  store i1 %1224, i1* %sf
  %1225 = trunc i64 %1222 to i8
  %1226 = call i8 @llvm.ctpop.i8(i8 %1225)
  %1227 = and i8 %1226, 1
  %1228 = icmp eq i8 %1227, 0
  store i1 %1228, i1* %pf
  store i64 %1222, i64* %rax
  %1229 = and i64 8388608, %1220
  %1230 = icmp ne i64 %1229, 0
  store i1 %1230, i1* %cf
  %1231 = icmp slt i64 %1220, 0
  %1232 = select i1 false, i1 %1231, i1 %1221
  store i1 %1232, i1* %of
  store volatile i64 25455, i64* @assembly_address
  %1233 = load i64* %rax
  %1234 = trunc i64 %1233 to i8
  store i8 %1234, i8* %stack_var_-19
  store volatile i64 25458, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 25463, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25468, i64* @assembly_address
  %1235 = load i64* %rdi
  %1236 = inttoptr i64 %1235 to i8*
  %1237 = load i64* %rsi
  %1238 = trunc i64 %1237 to i32
  %1239 = call i64 @updcrc(i8* %1236, i32 %1238)
  store i64 %1239, i64* %rax
  store i64 %1239, i64* %rax
  store volatile i64 25473, i64* @assembly_address
  %1240 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %1240, i64* %rax
  store volatile i64 25477, i64* @assembly_address
  store i64 10, i64* %rsi
  store volatile i64 25482, i64* @assembly_address
  %1241 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %1241, i64* %rdi
  store volatile i64 25485, i64* @assembly_address
  %1242 = load i64* %rdi
  %1243 = inttoptr i64 %1242 to i8*
  %1244 = load i64* %rsi
  %1245 = trunc i64 %1244 to i32
  %1246 = call i64 @updcrc(i8* %1243, i32 %1245)
  store i64 %1246, i64* %rax
  store i64 %1246, i64* %rax
  br label %block_6392

block_6392:                                       ; preds = %block_633f, %block_6331
  store volatile i64 25490, i64* @assembly_address
  %1247 = load i32* %stack_var_-81
  %1248 = trunc i32 %1247 to i8
  %1249 = zext i8 %1248 to i64
  store i64 %1249, i64* %rax
  store volatile i64 25494, i64* @assembly_address
  %1250 = load i64* %rax
  %1251 = trunc i64 %1250 to i32
  %1252 = and i32 %1251, 4
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1253 = icmp eq i32 %1252, 0
  store i1 %1253, i1* %zf
  %1254 = icmp slt i32 %1252, 0
  store i1 %1254, i1* %sf
  %1255 = trunc i32 %1252 to i8
  %1256 = call i8 @llvm.ctpop.i8(i8 %1255)
  %1257 = and i8 %1256, 1
  %1258 = icmp eq i8 %1257, 0
  store i1 %1258, i1* %pf
  %1259 = zext i32 %1252 to i64
  store i64 %1259, i64* %rax
  store volatile i64 25497, i64* @assembly_address
  %1260 = load i64* %rax
  %1261 = trunc i64 %1260 to i32
  %1262 = load i64* %rax
  %1263 = trunc i64 %1262 to i32
  %1264 = and i32 %1261, %1263
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1265 = icmp eq i32 %1264, 0
  store i1 %1265, i1* %zf
  %1266 = icmp slt i32 %1264, 0
  store i1 %1266, i1* %sf
  %1267 = trunc i32 %1264 to i8
  %1268 = call i8 @llvm.ctpop.i8(i8 %1267)
  %1269 = and i8 %1268, 1
  %1270 = icmp eq i8 %1269, 0
  store i1 %1270, i1* %pf
  store volatile i64 25499, i64* @assembly_address
  %1271 = load i1* %zf
  br i1 %1271, label %block_6494, label %block_63a1

block_63a1:                                       ; preds = %block_6392
  store volatile i64 25505, i64* @assembly_address
  %1272 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1273 = zext i32 %1272 to i64
  store i64 %1273, i64* %rdx
  store volatile i64 25511, i64* @assembly_address
  %1274 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1275 = zext i32 %1274 to i64
  store i64 %1275, i64* %rax
  store volatile i64 25517, i64* @assembly_address
  %1276 = load i64* %rdx
  %1277 = trunc i64 %1276 to i32
  %1278 = load i64* %rax
  %1279 = trunc i64 %1278 to i32
  %1280 = sub i32 %1277, %1279
  %1281 = and i32 %1277, 15
  %1282 = and i32 %1279, 15
  %1283 = sub i32 %1281, %1282
  %1284 = icmp ugt i32 %1283, 15
  %1285 = icmp ult i32 %1277, %1279
  %1286 = xor i32 %1277, %1279
  %1287 = xor i32 %1277, %1280
  %1288 = and i32 %1286, %1287
  %1289 = icmp slt i32 %1288, 0
  store i1 %1284, i1* %az
  store i1 %1285, i1* %cf
  store i1 %1289, i1* %of
  %1290 = icmp eq i32 %1280, 0
  store i1 %1290, i1* %zf
  %1291 = icmp slt i32 %1280, 0
  store i1 %1291, i1* %sf
  %1292 = trunc i32 %1280 to i8
  %1293 = call i8 @llvm.ctpop.i8(i8 %1292)
  %1294 = and i8 %1293, 1
  %1295 = icmp eq i8 %1294, 0
  store i1 %1295, i1* %pf
  store volatile i64 25519, i64* @assembly_address
  %1296 = load i1* %cf
  %1297 = icmp eq i1 %1296, false
  br i1 %1297, label %block_63cf, label %block_63b1

block_63b1:                                       ; preds = %block_63a1
  store volatile i64 25521, i64* @assembly_address
  %1298 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1299 = zext i32 %1298 to i64
  store i64 %1299, i64* %rax
  store volatile i64 25527, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 25530, i64* @assembly_address
  %1300 = load i64* %rdx
  %1301 = trunc i64 %1300 to i32
  store i32 %1301, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 25536, i64* @assembly_address
  %1302 = load i64* %rax
  %1303 = trunc i64 %1302 to i32
  %1304 = zext i32 %1303 to i64
  store i64 %1304, i64* %rdx
  store volatile i64 25538, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 25545, i64* @assembly_address
  %1305 = load i64* %rdx
  %1306 = load i64* %rax
  %1307 = mul i64 %1306, 1
  %1308 = add i64 %1305, %1307
  %1309 = inttoptr i64 %1308 to i8*
  %1310 = load i8* %1309
  %1311 = zext i8 %1310 to i64
  store i64 %1311, i64* %rax
  store volatile i64 25549, i64* @assembly_address
  br label %block_63d9

block_63cf:                                       ; preds = %block_63a1
  store volatile i64 25551, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25556, i64* @assembly_address
  %1312 = load i64* %rdi
  %1313 = trunc i64 %1312 to i32
  %1314 = call i64 @fill_inbuf(i32 %1313)
  store i64 %1314, i64* %rax
  store i64 %1314, i64* %rax
  br label %block_63d9

block_63d9:                                       ; preds = %block_63cf, %block_63b1
  store volatile i64 25561, i64* @assembly_address
  %1315 = load i64* %rax
  %1316 = trunc i64 %1315 to i8
  %1317 = sext i8 %1316 to i32
  store i32 %1317, i32* %stack_var_-28
  store volatile i64 25564, i64* @assembly_address
  %1318 = load i32* %stack_var_-28
  %1319 = trunc i32 %1318 to i8
  %1320 = zext i8 %1319 to i64
  store i64 %1320, i64* %rax
  store volatile i64 25568, i64* @assembly_address
  %1321 = load i64* %rax
  %1322 = trunc i64 %1321 to i8
  %1323 = zext i8 %1322 to i64
  store i64 %1323, i64* %rax
  store volatile i64 25571, i64* @assembly_address
  %1324 = load i64* %rax
  %1325 = trunc i64 %1324 to i32
  store i32 %1325, i32* %stack_var_-68
  store volatile i64 25574, i64* @assembly_address
  %1326 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1327 = zext i32 %1326 to i64
  store i64 %1327, i64* %rdx
  store volatile i64 25580, i64* @assembly_address
  %1328 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1329 = zext i32 %1328 to i64
  store i64 %1329, i64* %rax
  store volatile i64 25586, i64* @assembly_address
  %1330 = load i64* %rdx
  %1331 = trunc i64 %1330 to i32
  %1332 = load i64* %rax
  %1333 = trunc i64 %1332 to i32
  %1334 = sub i32 %1331, %1333
  %1335 = and i32 %1331, 15
  %1336 = and i32 %1333, 15
  %1337 = sub i32 %1335, %1336
  %1338 = icmp ugt i32 %1337, 15
  %1339 = icmp ult i32 %1331, %1333
  %1340 = xor i32 %1331, %1333
  %1341 = xor i32 %1331, %1334
  %1342 = and i32 %1340, %1341
  %1343 = icmp slt i32 %1342, 0
  store i1 %1338, i1* %az
  store i1 %1339, i1* %cf
  store i1 %1343, i1* %of
  %1344 = icmp eq i32 %1334, 0
  store i1 %1344, i1* %zf
  %1345 = icmp slt i32 %1334, 0
  store i1 %1345, i1* %sf
  %1346 = trunc i32 %1334 to i8
  %1347 = call i8 @llvm.ctpop.i8(i8 %1346)
  %1348 = and i8 %1347, 1
  %1349 = icmp eq i8 %1348, 0
  store i1 %1349, i1* %pf
  store volatile i64 25588, i64* @assembly_address
  %1350 = load i1* %cf
  %1351 = icmp eq i1 %1350, false
  br i1 %1351, label %block_6414, label %block_63f6

block_63f6:                                       ; preds = %block_63d9
  store volatile i64 25590, i64* @assembly_address
  %1352 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1353 = zext i32 %1352 to i64
  store i64 %1353, i64* %rax
  store volatile i64 25596, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 25599, i64* @assembly_address
  %1354 = load i64* %rdx
  %1355 = trunc i64 %1354 to i32
  store i32 %1355, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 25605, i64* @assembly_address
  %1356 = load i64* %rax
  %1357 = trunc i64 %1356 to i32
  %1358 = zext i32 %1357 to i64
  store i64 %1358, i64* %rdx
  store volatile i64 25607, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 25614, i64* @assembly_address
  %1359 = load i64* %rdx
  %1360 = load i64* %rax
  %1361 = mul i64 %1360, 1
  %1362 = add i64 %1359, %1361
  %1363 = inttoptr i64 %1362 to i8*
  %1364 = load i8* %1363
  %1365 = zext i8 %1364 to i64
  store i64 %1365, i64* %rax
  store volatile i64 25618, i64* @assembly_address
  br label %block_641e

block_6414:                                       ; preds = %block_63d9
  store volatile i64 25620, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25625, i64* @assembly_address
  %1366 = load i64* %rdi
  %1367 = trunc i64 %1366 to i32
  %1368 = call i64 @fill_inbuf(i32 %1367)
  store i64 %1368, i64* %rax
  store i64 %1368, i64* %rax
  br label %block_641e

block_641e:                                       ; preds = %block_6414, %block_63f6
  store volatile i64 25630, i64* @assembly_address
  %1369 = load i64* %rax
  %1370 = trunc i64 %1369 to i8
  %1371 = sext i8 %1370 to i32
  store i32 %1371, i32* %stack_var_-27
  store volatile i64 25633, i64* @assembly_address
  %1372 = load i32* %stack_var_-27
  %1373 = trunc i32 %1372 to i8
  %1374 = zext i8 %1373 to i64
  store i64 %1374, i64* %rax
  store volatile i64 25637, i64* @assembly_address
  %1375 = load i64* %rax
  %1376 = trunc i64 %1375 to i8
  %1377 = zext i8 %1376 to i64
  store i64 %1377, i64* %rax
  store volatile i64 25640, i64* @assembly_address
  %1378 = load i64* %rax
  %1379 = trunc i64 %1378 to i32
  %1380 = load i1* %of
  %1381 = shl i32 %1379, 8
  %1382 = icmp eq i32 %1381, 0
  store i1 %1382, i1* %zf
  %1383 = icmp slt i32 %1381, 0
  store i1 %1383, i1* %sf
  %1384 = trunc i32 %1381 to i8
  %1385 = call i8 @llvm.ctpop.i8(i8 %1384)
  %1386 = and i8 %1385, 1
  %1387 = icmp eq i8 %1386, 0
  store i1 %1387, i1* %pf
  %1388 = zext i32 %1381 to i64
  store i64 %1388, i64* %rax
  %1389 = shl i32 %1379, 7
  %1390 = lshr i32 %1389, 31
  %1391 = trunc i32 %1390 to i1
  store i1 %1391, i1* %cf
  %1392 = lshr i32 %1381, 31
  %1393 = icmp ne i32 %1392, %1390
  %1394 = select i1 false, i1 %1393, i1 %1380
  store i1 %1394, i1* %of
  store volatile i64 25643, i64* @assembly_address
  %1395 = load i32* %stack_var_-68
  %1396 = load i64* %rax
  %1397 = trunc i64 %1396 to i32
  %1398 = or i32 %1395, %1397
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1399 = icmp eq i32 %1398, 0
  store i1 %1399, i1* %zf
  %1400 = icmp slt i32 %1398, 0
  store i1 %1400, i1* %sf
  %1401 = trunc i32 %1398 to i8
  %1402 = call i8 @llvm.ctpop.i8(i8 %1401)
  %1403 = and i8 %1402, 1
  %1404 = icmp eq i8 %1403, 0
  store i1 %1404, i1* %pf
  store i32 %1398, i32* %stack_var_-68
  store volatile i64 25646, i64* @assembly_address
  %1405 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %1406 = zext i32 %1405 to i64
  store i64 %1406, i64* %rax
  store volatile i64 25652, i64* @assembly_address
  %1407 = load i64* %rax
  %1408 = trunc i64 %1407 to i32
  %1409 = load i64* %rax
  %1410 = trunc i64 %1409 to i32
  %1411 = and i32 %1408, %1410
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1412 = icmp eq i32 %1411, 0
  store i1 %1412, i1* %zf
  %1413 = icmp slt i32 %1411, 0
  store i1 %1413, i1* %sf
  %1414 = trunc i32 %1411 to i8
  %1415 = call i8 @llvm.ctpop.i8(i8 %1414)
  %1416 = and i8 %1415, 1
  %1417 = icmp eq i8 %1416, 0
  store i1 %1417, i1* %pf
  store volatile i64 25654, i64* @assembly_address
  %1418 = load i1* %zf
  br i1 %1418, label %block_6467, label %block_6438

block_6438:                                       ; preds = %block_641e
  store volatile i64 25656, i64* @assembly_address
  %1419 = load i64* @global_var_25f4c8
  store i64 %1419, i64* %rdx
  store volatile i64 25663, i64* @assembly_address
  %1420 = load i64* @global_var_216580
  store i64 %1420, i64* %rax
  store volatile i64 25670, i64* @assembly_address
  %1421 = load i32* %stack_var_-68
  %1422 = zext i32 %1421 to i64
  store i64 %1422, i64* %rcx
  store volatile i64 25673, i64* @assembly_address
  %1423 = load i64* %rcx
  %1424 = trunc i64 %1423 to i32
  %1425 = zext i32 %1424 to i64
  store i64 %1425, i64* %r8
  store volatile i64 25676, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 25683, i64* @assembly_address
  store i64 ptrtoint ([41 x i8]* @global_var_11850 to i64), i64* %rsi
  store volatile i64 25690, i64* @assembly_address
  %1426 = load i64* %rax
  store i64 %1426, i64* %rdi
  store volatile i64 25693, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 25698, i64* @assembly_address
  %1427 = load i64* %rdi
  %1428 = inttoptr i64 %1427 to %_IO_FILE*
  %1429 = load i64* %rsi
  %1430 = inttoptr i64 %1429 to i8*
  %1431 = load i64* %rdx
  %1432 = inttoptr i64 %1431 to i8*
  %1433 = load i64* %rcx
  %1434 = inttoptr i64 %1433 to i8*
  %1435 = load i64* %r8
  %1436 = trunc i64 %1435 to i32
  %1437 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1428, i8* %1430, i8* %1432, i8* %1434, i32 %1436)
  %1438 = sext i32 %1437 to i64
  store i64 %1438, i64* %rax
  %1439 = sext i32 %1437 to i64
  store i64 %1439, i64* %rax
  br label %block_6467

block_6467:                                       ; preds = %block_6438, %block_641e
  store volatile i64 25703, i64* @assembly_address
  %1440 = load i32* %stack_var_-81
  %1441 = trunc i32 %1440 to i8
  %1442 = zext i8 %1441 to i64
  store i64 %1442, i64* %rax
  store volatile i64 25707, i64* @assembly_address
  %1443 = load i64* %rax
  %1444 = trunc i64 %1443 to i32
  %1445 = and i32 %1444, 2
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1446 = icmp eq i32 %1445, 0
  store i1 %1446, i1* %zf
  %1447 = icmp slt i32 %1445, 0
  store i1 %1447, i1* %sf
  %1448 = trunc i32 %1445 to i8
  %1449 = call i8 @llvm.ctpop.i8(i8 %1448)
  %1450 = and i8 %1449, 1
  %1451 = icmp eq i8 %1450, 0
  store i1 %1451, i1* %pf
  %1452 = zext i32 %1445 to i64
  store i64 %1452, i64* %rax
  store volatile i64 25710, i64* @assembly_address
  %1453 = load i64* %rax
  %1454 = trunc i64 %1453 to i32
  %1455 = load i64* %rax
  %1456 = trunc i64 %1455 to i32
  %1457 = and i32 %1454, %1456
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1458 = icmp eq i32 %1457, 0
  store i1 %1458, i1* %zf
  %1459 = icmp slt i32 %1457, 0
  store i1 %1459, i1* %sf
  %1460 = trunc i32 %1457 to i8
  %1461 = call i8 @llvm.ctpop.i8(i8 %1460)
  %1462 = and i8 %1461, 1
  %1463 = icmp eq i8 %1462, 0
  store i1 %1463, i1* %pf
  store volatile i64 25712, i64* @assembly_address
  %1464 = load i1* %zf
  br i1 %1464, label %block_6483, label %block_6472

block_6472:                                       ; preds = %block_6467
  store volatile i64 25714, i64* @assembly_address
  %1465 = ptrtoint i32* %stack_var_-28 to i64
  store i64 %1465, i64* %rax
  store volatile i64 25718, i64* @assembly_address
  store i64 2, i64* %rsi
  store volatile i64 25723, i64* @assembly_address
  %1466 = ptrtoint i32* %stack_var_-28 to i64
  store i64 %1466, i64* %rdi
  store volatile i64 25726, i64* @assembly_address
  %1467 = load i64* %rdi
  %1468 = inttoptr i64 %1467 to i8*
  %1469 = load i64* %rsi
  %1470 = trunc i64 %1469 to i32
  %1471 = call i64 @updcrc(i8* %1468, i32 %1470)
  store i64 %1471, i64* %rax
  store i64 %1471, i64* %rax
  br label %block_6483

block_6483:                                       ; preds = %block_6472, %block_6467
  store volatile i64 25731, i64* @assembly_address
  %1472 = load i32* %stack_var_-81
  %1473 = trunc i32 %1472 to i8
  %1474 = zext i8 %1473 to i64
  store i64 %1474, i64* %rdx
  store volatile i64 25735, i64* @assembly_address
  %1475 = load i32* %stack_var_-68
  %1476 = zext i32 %1475 to i64
  store i64 %1476, i64* %rax
  store volatile i64 25738, i64* @assembly_address
  %1477 = load i64* %rdx
  %1478 = trunc i64 %1477 to i32
  %1479 = zext i32 %1478 to i64
  store i64 %1479, i64* %rsi
  store volatile i64 25740, i64* @assembly_address
  %1480 = load i64* %rax
  store i64 %1480, i64* %rdi
  store volatile i64 25743, i64* @assembly_address
  %1481 = load i64* %rdi
  %1482 = load i64* %rsi
  %1483 = trunc i64 %1482 to i32
  %1484 = call i64 @discard_input_bytes(i64 %1481, i32 %1483)
  store i64 %1484, i64* %rax
  store i64 %1484, i64* %rax
  br label %block_6494

block_6494:                                       ; preds = %block_6483, %block_6392
  store volatile i64 25748, i64* @assembly_address
  %1485 = load i32* %stack_var_-81
  %1486 = trunc i32 %1485 to i8
  %1487 = zext i8 %1486 to i64
  store i64 %1487, i64* %rax
  store volatile i64 25752, i64* @assembly_address
  %1488 = load i64* %rax
  %1489 = trunc i64 %1488 to i32
  %1490 = and i32 %1489, 8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1491 = icmp eq i32 %1490, 0
  store i1 %1491, i1* %zf
  %1492 = icmp slt i32 %1490, 0
  store i1 %1492, i1* %sf
  %1493 = trunc i32 %1490 to i8
  %1494 = call i8 @llvm.ctpop.i8(i8 %1493)
  %1495 = and i8 %1494, 1
  %1496 = icmp eq i8 %1495, 0
  store i1 %1496, i1* %pf
  %1497 = zext i32 %1490 to i64
  store i64 %1497, i64* %rax
  store volatile i64 25755, i64* @assembly_address
  %1498 = load i64* %rax
  %1499 = trunc i64 %1498 to i32
  %1500 = load i64* %rax
  %1501 = trunc i64 %1500 to i32
  %1502 = and i32 %1499, %1501
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1503 = icmp eq i32 %1502, 0
  store i1 %1503, i1* %zf
  %1504 = icmp slt i32 %1502, 0
  store i1 %1504, i1* %sf
  %1505 = trunc i32 %1502 to i8
  %1506 = call i8 @llvm.ctpop.i8(i8 %1505)
  %1507 = and i8 %1506, 1
  %1508 = icmp eq i8 %1507, 0
  store i1 %1508, i1* %pf
  store volatile i64 25757, i64* @assembly_address
  %1509 = load i1* %zf
  br i1 %1509, label %block_65e1, label %block_64a3

block_64a3:                                       ; preds = %block_6494
  store volatile i64 25763, i64* @assembly_address
  %1510 = load i32* bitcast (i64* @global_var_216090 to i32*)
  %1511 = zext i32 %1510 to i64
  store i64 %1511, i64* %rax
  store volatile i64 25769, i64* @assembly_address
  %1512 = load i64* %rax
  %1513 = trunc i64 %1512 to i32
  %1514 = load i64* %rax
  %1515 = trunc i64 %1514 to i32
  %1516 = and i32 %1513, %1515
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1517 = icmp eq i32 %1516, 0
  store i1 %1517, i1* %zf
  %1518 = icmp slt i32 %1516, 0
  store i1 %1518, i1* %sf
  %1519 = trunc i32 %1516 to i8
  %1520 = call i8 @llvm.ctpop.i8(i8 %1519)
  %1521 = and i8 %1520, 1
  %1522 = icmp eq i8 %1521, 0
  store i1 %1522, i1* %pf
  store volatile i64 25771, i64* @assembly_address
  %1523 = load i1* %zf
  %1524 = icmp eq i1 %1523, false
  br i1 %1524, label %block_64cc, label %block_64ad

block_64ad:                                       ; preds = %block_64a3
  store volatile i64 25773, i64* @assembly_address
  %1525 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1526 = zext i32 %1525 to i64
  store i64 %1526, i64* %rax
  store volatile i64 25779, i64* @assembly_address
  %1527 = load i64* %rax
  %1528 = trunc i64 %1527 to i32
  %1529 = load i64* %rax
  %1530 = trunc i64 %1529 to i32
  %1531 = and i32 %1528, %1530
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1532 = icmp eq i32 %1531, 0
  store i1 %1532, i1* %zf
  %1533 = icmp slt i32 %1531, 0
  store i1 %1533, i1* %sf
  %1534 = trunc i32 %1531 to i8
  %1535 = call i8 @llvm.ctpop.i8(i8 %1534)
  %1536 = and i8 %1535, 1
  %1537 = icmp eq i8 %1536, 0
  store i1 %1537, i1* %pf
  store volatile i64 25781, i64* @assembly_address
  %1538 = load i1* %zf
  br i1 %1538, label %block_64c1, label %block_64b7

block_64b7:                                       ; preds = %block_64ad
  store volatile i64 25783, i64* @assembly_address
  %1539 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %1540 = zext i32 %1539 to i64
  store i64 %1540, i64* %rax
  store volatile i64 25789, i64* @assembly_address
  %1541 = load i64* %rax
  %1542 = trunc i64 %1541 to i32
  %1543 = load i64* %rax
  %1544 = trunc i64 %1543 to i32
  %1545 = and i32 %1542, %1544
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1546 = icmp eq i32 %1545, 0
  store i1 %1546, i1* %zf
  %1547 = icmp slt i32 %1545, 0
  store i1 %1547, i1* %sf
  %1548 = trunc i32 %1545 to i8
  %1549 = call i8 @llvm.ctpop.i8(i8 %1548)
  %1550 = and i8 %1549, 1
  %1551 = icmp eq i8 %1550, 0
  store i1 %1551, i1* %pf
  store volatile i64 25791, i64* @assembly_address
  %1552 = load i1* %zf
  br i1 %1552, label %block_64cc, label %block_64c1

block_64c1:                                       ; preds = %block_64b7, %block_64ad
  store volatile i64 25793, i64* @assembly_address
  %1553 = load i32* bitcast (i64* @global_var_216620 to i32*)
  %1554 = zext i32 %1553 to i64
  store i64 %1554, i64* %rax
  store volatile i64 25799, i64* @assembly_address
  %1555 = load i64* %rax
  %1556 = trunc i64 %1555 to i32
  %1557 = trunc i64 %1555 to i32
  store i32 %1557, i32* %10
  store i32 1, i32* %9
  %1558 = sub i32 %1556, 1
  %1559 = and i32 %1556, 15
  %1560 = sub i32 %1559, 1
  %1561 = icmp ugt i32 %1560, 15
  %1562 = icmp ult i32 %1556, 1
  %1563 = xor i32 %1556, 1
  %1564 = xor i32 %1556, %1558
  %1565 = and i32 %1563, %1564
  %1566 = icmp slt i32 %1565, 0
  store i1 %1561, i1* %az
  store i1 %1562, i1* %cf
  store i1 %1566, i1* %of
  %1567 = icmp eq i32 %1558, 0
  store i1 %1567, i1* %zf
  %1568 = icmp slt i32 %1558, 0
  store i1 %1568, i1* %sf
  %1569 = trunc i32 %1558 to i8
  %1570 = call i8 @llvm.ctpop.i8(i8 %1569)
  %1571 = and i8 %1570, 1
  %1572 = icmp eq i8 %1571, 0
  store i1 %1572, i1* %pf
  store volatile i64 25802, i64* @assembly_address
  %1573 = load i32* %10
  %1574 = sext i32 %1573 to i64
  %1575 = load i32* %9
  %1576 = trunc i64 %1574 to i32
  %1577 = icmp sle i32 %1576, %1575
  br i1 %1577, label %block_64e3, label %block_64cc

block_64cc:                                       ; preds = %block_64c1, %block_64b7, %block_64a3
  store volatile i64 25804, i64* @assembly_address
  %1578 = load i32* %stack_var_-81
  %1579 = trunc i32 %1578 to i8
  %1580 = zext i8 %1579 to i64
  store i64 %1580, i64* %rax
  store volatile i64 25808, i64* @assembly_address
  %1581 = load i64* %rax
  %1582 = trunc i64 %1581 to i32
  %1583 = zext i32 %1582 to i64
  store i64 %1583, i64* %rsi
  store volatile i64 25810, i64* @assembly_address
  store i64 -1, i64* %rdi
  store volatile i64 25817, i64* @assembly_address
  %1584 = load i64* %rdi
  %1585 = load i64* %rsi
  %1586 = trunc i64 %1585 to i32
  %1587 = call i64 @discard_input_bytes(i64 %1584, i32 %1586)
  store i64 %1587, i64* %rax
  store i64 %1587, i64* %rax
  store volatile i64 25822, i64* @assembly_address
  br label %block_65e1

block_64e3:                                       ; preds = %block_64c1
  store volatile i64 25827, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 25834, i64* @assembly_address
  %1588 = load i64* %rdi
  %1589 = inttoptr i64 %1588 to i64*
  %1590 = call i64 @gzip_base_name(i64* %1589)
  store i64 %1590, i64* %rax
  store i64 %1590, i64* %rax
  store volatile i64 25839, i64* @assembly_address
  %1591 = load i64* %rax
  %1592 = inttoptr i64 %1591 to i8*
  store i8* %1592, i8** %stack_var_-56
  store volatile i64 25843, i64* @assembly_address
  %1593 = load i8** %stack_var_-56
  %1594 = ptrtoint i8* %1593 to i64
  store i64 %1594, i64* %rax
  store volatile i64 25847, i64* @assembly_address
  %1595 = load i64* %rax
  store i64 %1595, i64* %stack_var_-40
  br label %block_64fb

block_64fb:                                       ; preds = %block_6550, %block_64e3
  store volatile i64 25851, i64* @assembly_address
  %1596 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1597 = zext i32 %1596 to i64
  store i64 %1597, i64* %rdx
  store volatile i64 25857, i64* @assembly_address
  %1598 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1599 = zext i32 %1598 to i64
  store i64 %1599, i64* %rax
  store volatile i64 25863, i64* @assembly_address
  %1600 = load i64* %rdx
  %1601 = trunc i64 %1600 to i32
  %1602 = load i64* %rax
  %1603 = trunc i64 %1602 to i32
  %1604 = sub i32 %1601, %1603
  %1605 = and i32 %1601, 15
  %1606 = and i32 %1603, 15
  %1607 = sub i32 %1605, %1606
  %1608 = icmp ugt i32 %1607, 15
  %1609 = icmp ult i32 %1601, %1603
  %1610 = xor i32 %1601, %1603
  %1611 = xor i32 %1601, %1604
  %1612 = and i32 %1610, %1611
  %1613 = icmp slt i32 %1612, 0
  store i1 %1608, i1* %az
  store i1 %1609, i1* %cf
  store i1 %1613, i1* %of
  %1614 = icmp eq i32 %1604, 0
  store i1 %1614, i1* %zf
  %1615 = icmp slt i32 %1604, 0
  store i1 %1615, i1* %sf
  %1616 = trunc i32 %1604 to i8
  %1617 = call i8 @llvm.ctpop.i8(i8 %1616)
  %1618 = and i8 %1617, 1
  %1619 = icmp eq i8 %1618, 0
  store i1 %1619, i1* %pf
  store volatile i64 25865, i64* @assembly_address
  %1620 = load i1* %cf
  %1621 = icmp eq i1 %1620, false
  br i1 %1621, label %block_652b, label %block_650b

block_650b:                                       ; preds = %block_64fb
  store volatile i64 25867, i64* @assembly_address
  %1622 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1623 = zext i32 %1622 to i64
  store i64 %1623, i64* %rax
  store volatile i64 25873, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 25876, i64* @assembly_address
  %1624 = load i64* %rdx
  %1625 = trunc i64 %1624 to i32
  store i32 %1625, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 25882, i64* @assembly_address
  %1626 = load i64* %rax
  %1627 = trunc i64 %1626 to i32
  %1628 = zext i32 %1627 to i64
  store i64 %1628, i64* %rdx
  store volatile i64 25884, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 25891, i64* @assembly_address
  %1629 = load i64* %rdx
  %1630 = load i64* %rax
  %1631 = mul i64 %1630, 1
  %1632 = add i64 %1629, %1631
  %1633 = inttoptr i64 %1632 to i8*
  %1634 = load i8* %1633
  %1635 = zext i8 %1634 to i64
  store i64 %1635, i64* %rax
  store volatile i64 25895, i64* @assembly_address
  %1636 = load i64* %rax
  %1637 = trunc i64 %1636 to i32
  %1638 = zext i32 %1637 to i64
  store i64 %1638, i64* %rdx
  store volatile i64 25897, i64* @assembly_address
  br label %block_6537

block_652b:                                       ; preds = %block_64fb
  store volatile i64 25899, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 25904, i64* @assembly_address
  %1639 = load i64* %rdi
  %1640 = trunc i64 %1639 to i32
  %1641 = call i64 @fill_inbuf(i32 %1640)
  store i64 %1641, i64* %rax
  store i64 %1641, i64* %rax
  store volatile i64 25909, i64* @assembly_address
  %1642 = load i64* %rax
  %1643 = trunc i64 %1642 to i32
  %1644 = zext i32 %1643 to i64
  store i64 %1644, i64* %rdx
  br label %block_6537

block_6537:                                       ; preds = %block_652b, %block_650b
  store volatile i64 25911, i64* @assembly_address
  %1645 = load i8** %stack_var_-56
  %1646 = ptrtoint i8* %1645 to i64
  store i64 %1646, i64* %rax
  store volatile i64 25915, i64* @assembly_address
  %1647 = load i64* %rdx
  %1648 = trunc i64 %1647 to i8
  %1649 = load i64* %rax
  %1650 = inttoptr i64 %1649 to i8*
  store i8 %1648, i8* %1650
  store volatile i64 25917, i64* @assembly_address
  %1651 = load i8** %stack_var_-56
  %1652 = ptrtoint i8* %1651 to i64
  store i64 %1652, i64* %rax
  store volatile i64 25921, i64* @assembly_address
  %1653 = load i64* %rax
  %1654 = add i64 %1653, 1
  store i64 %1654, i64* %rdx
  store volatile i64 25925, i64* @assembly_address
  %1655 = load i64* %rdx
  %1656 = inttoptr i64 %1655 to i8*
  store i8* %1656, i8** %stack_var_-56
  store volatile i64 25929, i64* @assembly_address
  %1657 = load i64* %rax
  %1658 = inttoptr i64 %1657 to i8*
  %1659 = load i8* %1658
  %1660 = zext i8 %1659 to i64
  store i64 %1660, i64* %rax
  store volatile i64 25932, i64* @assembly_address
  %1661 = load i64* %rax
  %1662 = trunc i64 %1661 to i8
  %1663 = load i64* %rax
  %1664 = trunc i64 %1663 to i8
  %1665 = and i8 %1662, %1664
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1666 = icmp eq i8 %1665, 0
  store i1 %1666, i1* %zf
  %1667 = icmp slt i8 %1665, 0
  store i1 %1667, i1* %sf
  %1668 = call i8 @llvm.ctpop.i8(i8 %1665)
  %1669 = and i8 %1668, 1
  %1670 = icmp eq i8 %1669, 0
  store i1 %1670, i1* %pf
  store volatile i64 25934, i64* @assembly_address
  %1671 = load i1* %zf
  br i1 %1671, label %block_6569, label %block_6550

block_6550:                                       ; preds = %block_6537
  store volatile i64 25936, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 25943, i64* @assembly_address
  %1672 = load i8** %stack_var_-56
  %1673 = ptrtoint i8* %1672 to i64
  %1674 = load i64* %rax
  %1675 = sub i64 %1673, %1674
  %1676 = and i64 %1673, 15
  %1677 = and i64 %1674, 15
  %1678 = sub i64 %1676, %1677
  %1679 = icmp ugt i64 %1678, 15
  %1680 = icmp ult i64 %1673, %1674
  %1681 = xor i64 %1673, %1674
  %1682 = xor i64 %1673, %1675
  %1683 = and i64 %1681, %1682
  %1684 = icmp slt i64 %1683, 0
  store i1 %1679, i1* %az
  store i1 %1680, i1* %cf
  store i1 %1684, i1* %of
  %1685 = icmp eq i64 %1675, 0
  store i1 %1685, i1* %zf
  %1686 = icmp slt i64 %1675, 0
  store i1 %1686, i1* %sf
  %1687 = trunc i64 %1675 to i8
  %1688 = call i8 @llvm.ctpop.i8(i8 %1687)
  %1689 = and i8 %1688, 1
  %1690 = icmp eq i8 %1689, 0
  store i1 %1690, i1* %pf
  store volatile i64 25947, i64* @assembly_address
  %1691 = load i1* %cf
  br i1 %1691, label %block_64fb, label %block_655d

block_655d:                                       ; preds = %block_6550
  store volatile i64 25949, i64* @assembly_address
  store i64 ptrtoint ([39 x i8]* @global_var_11880 to i64), i64* %rdi
  store volatile i64 25956, i64* @assembly_address
  %1692 = load i64* %rdi
  %1693 = inttoptr i64 %1692 to i8*
  %1694 = call i64 @gzip_error(i8* %1693)
  store i64 %1694, i64* %rax
  store i64 %1694, i64* %rax
  unreachable

block_6569:                                       ; preds = %block_6537
  store volatile i64 25961, i64* @assembly_address
  store volatile i64 25962, i64* @assembly_address
  %1695 = load i32* %stack_var_-81
  %1696 = trunc i32 %1695 to i8
  %1697 = zext i8 %1696 to i64
  store i64 %1697, i64* %rax
  store volatile i64 25966, i64* @assembly_address
  %1698 = load i64* %rax
  %1699 = trunc i64 %1698 to i32
  %1700 = and i32 %1699, 2
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1701 = icmp eq i32 %1700, 0
  store i1 %1701, i1* %zf
  %1702 = icmp slt i32 %1700, 0
  store i1 %1702, i1* %sf
  %1703 = trunc i32 %1700 to i8
  %1704 = call i8 @llvm.ctpop.i8(i8 %1703)
  %1705 = and i8 %1704, 1
  %1706 = icmp eq i8 %1705, 0
  store i1 %1706, i1* %pf
  %1707 = zext i32 %1700 to i64
  store i64 %1707, i64* %rax
  store volatile i64 25969, i64* @assembly_address
  %1708 = load i64* %rax
  %1709 = trunc i64 %1708 to i32
  %1710 = load i64* %rax
  %1711 = trunc i64 %1710 to i32
  %1712 = and i32 %1709, %1711
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1713 = icmp eq i32 %1712, 0
  store i1 %1713, i1* %zf
  %1714 = icmp slt i32 %1712, 0
  store i1 %1714, i1* %sf
  %1715 = trunc i32 %1712 to i8
  %1716 = call i8 @llvm.ctpop.i8(i8 %1715)
  %1717 = and i8 %1716, 1
  %1718 = icmp eq i8 %1717, 0
  store i1 %1718, i1* %pf
  store volatile i64 25971, i64* @assembly_address
  %1719 = load i1* %zf
  br i1 %1719, label %block_6593, label %block_6575

block_6575:                                       ; preds = %block_6569
  store volatile i64 25973, i64* @assembly_address
  %1720 = load i8** %stack_var_-56
  %1721 = ptrtoint i8* %1720 to i64
  store i64 %1721, i64* %rdx
  store volatile i64 25977, i64* @assembly_address
  %1722 = load i64* %stack_var_-40
  store i64 %1722, i64* %rax
  store volatile i64 25981, i64* @assembly_address
  %1723 = load i64* %rdx
  %1724 = load i64* %rax
  %1725 = sub i64 %1723, %1724
  %1726 = and i64 %1723, 15
  %1727 = and i64 %1724, 15
  %1728 = sub i64 %1726, %1727
  %1729 = icmp ugt i64 %1728, 15
  %1730 = icmp ult i64 %1723, %1724
  %1731 = xor i64 %1723, %1724
  %1732 = xor i64 %1723, %1725
  %1733 = and i64 %1731, %1732
  %1734 = icmp slt i64 %1733, 0
  store i1 %1729, i1* %az
  store i1 %1730, i1* %cf
  store i1 %1734, i1* %of
  %1735 = icmp eq i64 %1725, 0
  store i1 %1735, i1* %zf
  %1736 = icmp slt i64 %1725, 0
  store i1 %1736, i1* %sf
  %1737 = trunc i64 %1725 to i8
  %1738 = call i8 @llvm.ctpop.i8(i8 %1737)
  %1739 = and i8 %1738, 1
  %1740 = icmp eq i8 %1739, 0
  store i1 %1740, i1* %pf
  store i64 %1725, i64* %rdx
  store volatile i64 25984, i64* @assembly_address
  %1741 = load i64* %rdx
  store i64 %1741, i64* %rax
  store volatile i64 25987, i64* @assembly_address
  %1742 = load i64* %rax
  %1743 = trunc i64 %1742 to i32
  %1744 = zext i32 %1743 to i64
  store i64 %1744, i64* %rdx
  store volatile i64 25989, i64* @assembly_address
  %1745 = load i64* %stack_var_-40
  store i64 %1745, i64* %rax
  store volatile i64 25993, i64* @assembly_address
  %1746 = load i64* %rdx
  %1747 = trunc i64 %1746 to i32
  %1748 = zext i32 %1747 to i64
  store i64 %1748, i64* %rsi
  store volatile i64 25995, i64* @assembly_address
  %1749 = load i64* %rax
  store i64 %1749, i64* %rdi
  store volatile i64 25998, i64* @assembly_address
  %1750 = load i64* %rdi
  %1751 = inttoptr i64 %1750 to i8*
  %1752 = load i64* %rsi
  %1753 = trunc i64 %1752 to i32
  %1754 = call i64 @updcrc(i8* %1751, i32 %1753)
  store i64 %1754, i64* %rax
  store i64 %1754, i64* %rax
  br label %block_6593

block_6593:                                       ; preds = %block_6575, %block_6569
  store volatile i64 26003, i64* @assembly_address
  %1755 = load i64* %stack_var_-40
  store i64 %1755, i64* %rax
  store volatile i64 26007, i64* @assembly_address
  %1756 = load i64* %rax
  store i64 %1756, i64* %rdi
  store volatile i64 26010, i64* @assembly_address
  %1757 = load i64* %rdi
  %1758 = inttoptr i64 %1757 to i64*
  %1759 = call i64 @gzip_base_name(i64* %1758)
  store i64 %1759, i64* %rax
  store i64 %1759, i64* %rax
  store volatile i64 26015, i64* @assembly_address
  %1760 = load i64* %rax
  %1761 = inttoptr i64 %1760 to i8*
  store i8* %1761, i8** %stack_var_-56
  store volatile i64 26019, i64* @assembly_address
  %1762 = load i8** %stack_var_-56
  %1763 = ptrtoint i8* %1762 to i64
  store i64 %1763, i64* %rax
  store volatile i64 26023, i64* @assembly_address
  %1764 = load i64* %rax
  store i64 %1764, i64* %rdi
  store volatile i64 26026, i64* @assembly_address
  %1765 = load i64* %rdi
  %1766 = inttoptr i64 %1765 to i8*
  %1767 = call i32 @strlen(i8* %1766)
  %1768 = sext i32 %1767 to i64
  store i64 %1768, i64* %rax
  %1769 = sext i32 %1767 to i64
  store i64 %1769, i64* %rax
  store volatile i64 26031, i64* @assembly_address
  %1770 = load i64* %rax
  %1771 = add i64 %1770, 1
  store i64 %1771, i64* %rdx
  store volatile i64 26035, i64* @assembly_address
  %1772 = load i8** %stack_var_-56
  %1773 = ptrtoint i8* %1772 to i64
  store i64 %1773, i64* %rcx
  store volatile i64 26039, i64* @assembly_address
  %1774 = load i64* %stack_var_-40
  store i64 %1774, i64* %rax
  store volatile i64 26043, i64* @assembly_address
  %1775 = load i64* %rcx
  store i64 %1775, i64* %rsi
  store volatile i64 26046, i64* @assembly_address
  %1776 = load i64* %rax
  store i64 %1776, i64* %rdi
  store volatile i64 26049, i64* @assembly_address
  %1777 = load i64* %rdi
  %1778 = inttoptr i64 %1777 to i64*
  %1779 = load i64* %rsi
  %1780 = inttoptr i64 %1779 to i64*
  %1781 = load i64* %rdx
  %1782 = trunc i64 %1781 to i32
  %1783 = call i64* @memmove(i64* %1778, i64* %1780, i32 %1782)
  %1784 = ptrtoint i64* %1783 to i64
  store i64 %1784, i64* %rax
  %1785 = ptrtoint i64* %1783 to i64
  store i64 %1785, i64* %rax
  store volatile i64 26054, i64* @assembly_address
  %1786 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %1787 = zext i32 %1786 to i64
  store i64 %1787, i64* %rax
  store volatile i64 26060, i64* @assembly_address
  %1788 = load i64* %rax
  %1789 = trunc i64 %1788 to i32
  %1790 = load i64* %rax
  %1791 = trunc i64 %1790 to i32
  %1792 = and i32 %1789, %1791
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1793 = icmp eq i32 %1792, 0
  store i1 %1793, i1* %zf
  %1794 = icmp slt i32 %1792, 0
  store i1 %1794, i1* %sf
  %1795 = trunc i32 %1792 to i8
  %1796 = call i8 @llvm.ctpop.i8(i8 %1795)
  %1797 = and i8 %1796, 1
  %1798 = icmp eq i8 %1797, 0
  store i1 %1798, i1* %pf
  store volatile i64 26062, i64* @assembly_address
  %1799 = load i1* %zf
  %1800 = icmp eq i1 %1799, false
  br i1 %1800, label %block_65e1, label %block_65d0

block_65d0:                                       ; preds = %block_6593
  store volatile i64 26064, i64* @assembly_address
  %1801 = load i64* %stack_var_-40
  %1802 = and i64 %1801, 15
  %1803 = icmp ugt i64 %1802, 15
  %1804 = icmp ult i64 %1801, 0
  %1805 = xor i64 %1801, 0
  %1806 = and i64 %1805, 0
  %1807 = icmp slt i64 %1806, 0
  store i1 %1803, i1* %az
  store i1 %1804, i1* %cf
  store i1 %1807, i1* %of
  %1808 = icmp eq i64 %1801, 0
  store i1 %1808, i1* %zf
  %1809 = icmp slt i64 %1801, 0
  store i1 %1809, i1* %sf
  %1810 = trunc i64 %1801 to i8
  %1811 = call i8 @llvm.ctpop.i8(i8 %1810)
  %1812 = and i8 %1811, 1
  %1813 = icmp eq i8 %1812, 0
  store i1 %1813, i1* %pf
  store volatile i64 26069, i64* @assembly_address
  %1814 = load i1* %zf
  br i1 %1814, label %block_65e1, label %block_65d7

block_65d7:                                       ; preds = %block_65d0
  store volatile i64 26071, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216610 to i32*)
  br label %block_65e1

block_65e1:                                       ; preds = %block_65d7, %block_65d0, %block_6593, %block_64cc, %block_6494
  store volatile i64 26081, i64* @assembly_address
  %1815 = load i32* %stack_var_-81
  %1816 = trunc i32 %1815 to i8
  %1817 = zext i8 %1816 to i64
  store i64 %1817, i64* %rax
  store volatile i64 26085, i64* @assembly_address
  %1818 = load i64* %rax
  %1819 = trunc i64 %1818 to i32
  %1820 = and i32 %1819, 16
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1821 = icmp eq i32 %1820, 0
  store i1 %1821, i1* %zf
  %1822 = icmp slt i32 %1820, 0
  store i1 %1822, i1* %sf
  %1823 = trunc i32 %1820 to i8
  %1824 = call i8 @llvm.ctpop.i8(i8 %1823)
  %1825 = and i8 %1824, 1
  %1826 = icmp eq i8 %1825, 0
  store i1 %1826, i1* %pf
  %1827 = zext i32 %1820 to i64
  store i64 %1827, i64* %rax
  store volatile i64 26088, i64* @assembly_address
  %1828 = load i64* %rax
  %1829 = trunc i64 %1828 to i32
  %1830 = load i64* %rax
  %1831 = trunc i64 %1830 to i32
  %1832 = and i32 %1829, %1831
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1833 = icmp eq i32 %1832, 0
  store i1 %1833, i1* %zf
  %1834 = icmp slt i32 %1832, 0
  store i1 %1834, i1* %sf
  %1835 = trunc i32 %1832 to i8
  %1836 = call i8 @llvm.ctpop.i8(i8 %1835)
  %1837 = and i8 %1836, 1
  %1838 = icmp eq i8 %1837, 0
  store i1 %1838, i1* %pf
  store volatile i64 26090, i64* @assembly_address
  %1839 = load i1* %zf
  br i1 %1839, label %block_65fe, label %block_65ec

block_65ec:                                       ; preds = %block_65e1
  store volatile i64 26092, i64* @assembly_address
  %1840 = load i32* %stack_var_-81
  %1841 = trunc i32 %1840 to i8
  %1842 = zext i8 %1841 to i64
  store i64 %1842, i64* %rax
  store volatile i64 26096, i64* @assembly_address
  %1843 = load i64* %rax
  %1844 = trunc i64 %1843 to i32
  %1845 = zext i32 %1844 to i64
  store i64 %1845, i64* %rsi
  store volatile i64 26098, i64* @assembly_address
  store i64 -1, i64* %rdi
  store volatile i64 26105, i64* @assembly_address
  %1846 = load i64* %rdi
  %1847 = load i64* %rsi
  %1848 = trunc i64 %1847 to i32
  %1849 = call i64 @discard_input_bytes(i64 %1846, i32 %1848)
  store i64 %1849, i64* %rax
  store i64 %1849, i64* %rax
  br label %block_65fe

block_65fe:                                       ; preds = %block_65ec, %block_65e1
  store volatile i64 26110, i64* @assembly_address
  %1850 = load i32* %stack_var_-81
  %1851 = trunc i32 %1850 to i8
  %1852 = zext i8 %1851 to i64
  store i64 %1852, i64* %rax
  store volatile i64 26114, i64* @assembly_address
  %1853 = load i64* %rax
  %1854 = trunc i64 %1853 to i32
  %1855 = and i32 %1854, 2
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1856 = icmp eq i32 %1855, 0
  store i1 %1856, i1* %zf
  %1857 = icmp slt i32 %1855, 0
  store i1 %1857, i1* %sf
  %1858 = trunc i32 %1855 to i8
  %1859 = call i8 @llvm.ctpop.i8(i8 %1858)
  %1860 = and i8 %1859, 1
  %1861 = icmp eq i8 %1860, 0
  store i1 %1861, i1* %pf
  %1862 = zext i32 %1855 to i64
  store i64 %1862, i64* %rax
  store volatile i64 26117, i64* @assembly_address
  %1863 = load i64* %rax
  %1864 = trunc i64 %1863 to i32
  %1865 = load i64* %rax
  %1866 = trunc i64 %1865 to i32
  %1867 = and i32 %1864, %1866
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1868 = icmp eq i32 %1867, 0
  store i1 %1868, i1* %zf
  %1869 = icmp slt i32 %1867, 0
  store i1 %1869, i1* %sf
  %1870 = trunc i32 %1867 to i8
  %1871 = call i8 @llvm.ctpop.i8(i8 %1870)
  %1872 = and i8 %1871, 1
  %1873 = icmp eq i8 %1872, 0
  store i1 %1873, i1* %pf
  store volatile i64 26119, i64* @assembly_address
  %1874 = load i1* %zf
  br i1 %1874, label %block_6704, label %block_660d

block_660d:                                       ; preds = %block_65fe
  store volatile i64 26125, i64* @assembly_address
  %1875 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %1875, i64* %rax
  store volatile i64 26129, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 26134, i64* @assembly_address
  %1876 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %1876, i64* %rdi
  store volatile i64 26137, i64* @assembly_address
  %1877 = load i64* %rdi
  %1878 = inttoptr i64 %1877 to i8*
  %1879 = load i64* %rsi
  %1880 = trunc i64 %1879 to i32
  %1881 = call i64 @updcrc(i8* %1878, i32 %1880)
  store i64 %1881, i64* %rax
  store i64 %1881, i64* %rax
  store volatile i64 26142, i64* @assembly_address
  %1882 = load i64* %rax
  %1883 = trunc i64 %1882 to i32
  %1884 = and i32 %1883, 65535
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1885 = icmp eq i32 %1884, 0
  store i1 %1885, i1* %zf
  %1886 = icmp slt i32 %1884, 0
  store i1 %1886, i1* %sf
  %1887 = trunc i32 %1884 to i8
  %1888 = call i8 @llvm.ctpop.i8(i8 %1887)
  %1889 = and i8 %1888, 1
  %1890 = icmp eq i8 %1889, 0
  store i1 %1890, i1* %pf
  %1891 = zext i32 %1884 to i64
  store i64 %1891, i64* %rax
  store volatile i64 26147, i64* @assembly_address
  %1892 = load i64* %rax
  %1893 = trunc i64 %1892 to i32
  store i32 %1893, i32* %stack_var_-64
  store volatile i64 26150, i64* @assembly_address
  %1894 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1895 = zext i32 %1894 to i64
  store i64 %1895, i64* %rdx
  store volatile i64 26156, i64* @assembly_address
  %1896 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1897 = zext i32 %1896 to i64
  store i64 %1897, i64* %rax
  store volatile i64 26162, i64* @assembly_address
  %1898 = load i64* %rdx
  %1899 = trunc i64 %1898 to i32
  %1900 = load i64* %rax
  %1901 = trunc i64 %1900 to i32
  %1902 = sub i32 %1899, %1901
  %1903 = and i32 %1899, 15
  %1904 = and i32 %1901, 15
  %1905 = sub i32 %1903, %1904
  %1906 = icmp ugt i32 %1905, 15
  %1907 = icmp ult i32 %1899, %1901
  %1908 = xor i32 %1899, %1901
  %1909 = xor i32 %1899, %1902
  %1910 = and i32 %1908, %1909
  %1911 = icmp slt i32 %1910, 0
  store i1 %1906, i1* %az
  store i1 %1907, i1* %cf
  store i1 %1911, i1* %of
  %1912 = icmp eq i32 %1902, 0
  store i1 %1912, i1* %zf
  %1913 = icmp slt i32 %1902, 0
  store i1 %1913, i1* %sf
  %1914 = trunc i32 %1902 to i8
  %1915 = call i8 @llvm.ctpop.i8(i8 %1914)
  %1916 = and i8 %1915, 1
  %1917 = icmp eq i8 %1916, 0
  store i1 %1917, i1* %pf
  store volatile i64 26164, i64* @assembly_address
  %1918 = load i1* %cf
  %1919 = icmp eq i1 %1918, false
  br i1 %1919, label %block_6657, label %block_6636

block_6636:                                       ; preds = %block_660d
  store volatile i64 26166, i64* @assembly_address
  %1920 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1921 = zext i32 %1920 to i64
  store i64 %1921, i64* %rax
  store volatile i64 26172, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 26175, i64* @assembly_address
  %1922 = load i64* %rdx
  %1923 = trunc i64 %1922 to i32
  store i32 %1923, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 26181, i64* @assembly_address
  %1924 = load i64* %rax
  %1925 = trunc i64 %1924 to i32
  %1926 = zext i32 %1925 to i64
  store i64 %1926, i64* %rdx
  store volatile i64 26183, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 26190, i64* @assembly_address
  %1927 = load i64* %rdx
  %1928 = load i64* %rax
  %1929 = mul i64 %1928, 1
  %1930 = add i64 %1927, %1929
  %1931 = inttoptr i64 %1930 to i8*
  %1932 = load i8* %1931
  %1933 = zext i8 %1932 to i64
  store i64 %1933, i64* %rax
  store volatile i64 26194, i64* @assembly_address
  %1934 = load i64* %rax
  %1935 = trunc i64 %1934 to i8
  %1936 = zext i8 %1935 to i64
  store i64 %1936, i64* %rax
  store volatile i64 26197, i64* @assembly_address
  br label %block_6661

block_6657:                                       ; preds = %block_660d
  store volatile i64 26199, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 26204, i64* @assembly_address
  %1937 = load i64* %rdi
  %1938 = trunc i64 %1937 to i32
  %1939 = call i64 @fill_inbuf(i32 %1938)
  store i64 %1939, i64* %rax
  store i64 %1939, i64* %rax
  br label %block_6661

block_6661:                                       ; preds = %block_6657, %block_6636
  store volatile i64 26209, i64* @assembly_address
  %1940 = load i64* %rax
  %1941 = trunc i64 %1940 to i32
  store i32 %1941, i32* %stack_var_-60
  store volatile i64 26212, i64* @assembly_address
  %1942 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1943 = zext i32 %1942 to i64
  store i64 %1943, i64* %rdx
  store volatile i64 26218, i64* @assembly_address
  %1944 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1945 = zext i32 %1944 to i64
  store i64 %1945, i64* %rax
  store volatile i64 26224, i64* @assembly_address
  %1946 = load i64* %rdx
  %1947 = trunc i64 %1946 to i32
  %1948 = load i64* %rax
  %1949 = trunc i64 %1948 to i32
  %1950 = sub i32 %1947, %1949
  %1951 = and i32 %1947, 15
  %1952 = and i32 %1949, 15
  %1953 = sub i32 %1951, %1952
  %1954 = icmp ugt i32 %1953, 15
  %1955 = icmp ult i32 %1947, %1949
  %1956 = xor i32 %1947, %1949
  %1957 = xor i32 %1947, %1950
  %1958 = and i32 %1956, %1957
  %1959 = icmp slt i32 %1958, 0
  store i1 %1954, i1* %az
  store i1 %1955, i1* %cf
  store i1 %1959, i1* %of
  %1960 = icmp eq i32 %1950, 0
  store i1 %1960, i1* %zf
  %1961 = icmp slt i32 %1950, 0
  store i1 %1961, i1* %sf
  %1962 = trunc i32 %1950 to i8
  %1963 = call i8 @llvm.ctpop.i8(i8 %1962)
  %1964 = and i8 %1963, 1
  %1965 = icmp eq i8 %1964, 0
  store i1 %1965, i1* %pf
  store volatile i64 26226, i64* @assembly_address
  %1966 = load i1* %cf
  %1967 = icmp eq i1 %1966, false
  br i1 %1967, label %block_6698, label %block_6674

block_6674:                                       ; preds = %block_6661
  store volatile i64 26228, i64* @assembly_address
  %1968 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1969 = zext i32 %1968 to i64
  store i64 %1969, i64* %rax
  store volatile i64 26234, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 26237, i64* @assembly_address
  %1970 = load i64* %rdx
  %1971 = trunc i64 %1970 to i32
  store i32 %1971, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 26243, i64* @assembly_address
  %1972 = load i64* %rax
  %1973 = trunc i64 %1972 to i32
  %1974 = zext i32 %1973 to i64
  store i64 %1974, i64* %rdx
  store volatile i64 26245, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 26252, i64* @assembly_address
  %1975 = load i64* %rdx
  %1976 = load i64* %rax
  %1977 = mul i64 %1976, 1
  %1978 = add i64 %1975, %1977
  %1979 = inttoptr i64 %1978 to i8*
  %1980 = load i8* %1979
  %1981 = zext i8 %1980 to i64
  store i64 %1981, i64* %rax
  store volatile i64 26256, i64* @assembly_address
  %1982 = load i64* %rax
  %1983 = trunc i64 %1982 to i8
  %1984 = zext i8 %1983 to i64
  store i64 %1984, i64* %rax
  store volatile i64 26259, i64* @assembly_address
  %1985 = load i64* %rax
  %1986 = trunc i64 %1985 to i32
  %1987 = load i1* %of
  %1988 = shl i32 %1986, 8
  %1989 = icmp eq i32 %1988, 0
  store i1 %1989, i1* %zf
  %1990 = icmp slt i32 %1988, 0
  store i1 %1990, i1* %sf
  %1991 = trunc i32 %1988 to i8
  %1992 = call i8 @llvm.ctpop.i8(i8 %1991)
  %1993 = and i8 %1992, 1
  %1994 = icmp eq i8 %1993, 0
  store i1 %1994, i1* %pf
  %1995 = zext i32 %1988 to i64
  store i64 %1995, i64* %rax
  %1996 = shl i32 %1986, 7
  %1997 = lshr i32 %1996, 31
  %1998 = trunc i32 %1997 to i1
  store i1 %1998, i1* %cf
  %1999 = lshr i32 %1988, 31
  %2000 = icmp ne i32 %1999, %1997
  %2001 = select i1 false, i1 %2000, i1 %1987
  store i1 %2001, i1* %of
  store volatile i64 26262, i64* @assembly_address
  br label %block_66a5

block_6698:                                       ; preds = %block_6661
  store volatile i64 26264, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 26269, i64* @assembly_address
  %2002 = load i64* %rdi
  %2003 = trunc i64 %2002 to i32
  %2004 = call i64 @fill_inbuf(i32 %2003)
  store i64 %2004, i64* %rax
  store i64 %2004, i64* %rax
  store volatile i64 26274, i64* @assembly_address
  %2005 = load i64* %rax
  %2006 = trunc i64 %2005 to i32
  %2007 = load i1* %of
  %2008 = shl i32 %2006, 8
  %2009 = icmp eq i32 %2008, 0
  store i1 %2009, i1* %zf
  %2010 = icmp slt i32 %2008, 0
  store i1 %2010, i1* %sf
  %2011 = trunc i32 %2008 to i8
  %2012 = call i8 @llvm.ctpop.i8(i8 %2011)
  %2013 = and i8 %2012, 1
  %2014 = icmp eq i8 %2013, 0
  store i1 %2014, i1* %pf
  %2015 = zext i32 %2008 to i64
  store i64 %2015, i64* %rax
  %2016 = shl i32 %2006, 7
  %2017 = lshr i32 %2016, 31
  %2018 = trunc i32 %2017 to i1
  store i1 %2018, i1* %cf
  %2019 = lshr i32 %2008, 31
  %2020 = icmp ne i32 %2019, %2017
  %2021 = select i1 false, i1 %2020, i1 %2007
  store i1 %2021, i1* %of
  br label %block_66a5

block_66a5:                                       ; preds = %block_6698, %block_6674
  store volatile i64 26277, i64* @assembly_address
  %2022 = load i32* %stack_var_-60
  %2023 = load i64* %rax
  %2024 = trunc i64 %2023 to i32
  %2025 = or i32 %2022, %2024
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2026 = icmp eq i32 %2025, 0
  store i1 %2026, i1* %zf
  %2027 = icmp slt i32 %2025, 0
  store i1 %2027, i1* %sf
  %2028 = trunc i32 %2025 to i8
  %2029 = call i8 @llvm.ctpop.i8(i8 %2028)
  %2030 = and i8 %2029, 1
  %2031 = icmp eq i8 %2030, 0
  store i1 %2031, i1* %pf
  store i32 %2025, i32* %stack_var_-60
  store volatile i64 26280, i64* @assembly_address
  %2032 = load i32* %stack_var_-60
  %2033 = zext i32 %2032 to i64
  store i64 %2033, i64* %rax
  store volatile i64 26283, i64* @assembly_address
  %2034 = load i64* %rax
  %2035 = trunc i64 %2034 to i32
  %2036 = load i32* %stack_var_-64
  %2037 = sub i32 %2035, %2036
  %2038 = and i32 %2035, 15
  %2039 = and i32 %2036, 15
  %2040 = sub i32 %2038, %2039
  %2041 = icmp ugt i32 %2040, 15
  %2042 = icmp ult i32 %2035, %2036
  %2043 = xor i32 %2035, %2036
  %2044 = xor i32 %2035, %2037
  %2045 = and i32 %2043, %2044
  %2046 = icmp slt i32 %2045, 0
  store i1 %2041, i1* %az
  store i1 %2042, i1* %cf
  store i1 %2046, i1* %of
  %2047 = icmp eq i32 %2037, 0
  store i1 %2047, i1* %zf
  %2048 = icmp slt i32 %2037, 0
  store i1 %2048, i1* %sf
  %2049 = trunc i32 %2037 to i8
  %2050 = call i8 @llvm.ctpop.i8(i8 %2049)
  %2051 = and i8 %2050, 1
  %2052 = icmp eq i8 %2051, 0
  store i1 %2052, i1* %pf
  store volatile i64 26286, i64* @assembly_address
  %2053 = load i1* %zf
  br i1 %2053, label %block_6704, label %block_66b0

block_66b0:                                       ; preds = %block_66a5
  store volatile i64 26288, i64* @assembly_address
  %2054 = load i64* @global_var_25f4c8
  store i64 %2054, i64* %rdx
  store volatile i64 26295, i64* @assembly_address
  %2055 = load i64* @global_var_216580
  store i64 %2055, i64* %rax
  store volatile i64 26302, i64* @assembly_address
  %2056 = load i32* %stack_var_-64
  %2057 = zext i32 %2056 to i64
  store i64 %2057, i64* %rsi
  store volatile i64 26305, i64* @assembly_address
  %2058 = load i32* %stack_var_-60
  %2059 = zext i32 %2058 to i64
  store i64 %2059, i64* %rcx
  store volatile i64 26308, i64* @assembly_address
  %2060 = load i64* %rsi
  %2061 = trunc i64 %2060 to i32
  %2062 = zext i32 %2061 to i64
  store i64 %2062, i64* %r9
  store volatile i64 26311, i64* @assembly_address
  %2063 = load i64* %rcx
  %2064 = trunc i64 %2063 to i32
  %2065 = zext i32 %2064 to i64
  store i64 %2065, i64* %r8
  store volatile i64 26314, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 26321, i64* @assembly_address
  store i64 ptrtoint ([60 x i8]* @global_var_118a8 to i64), i64* %rsi
  store volatile i64 26328, i64* @assembly_address
  %2066 = load i64* %rax
  store i64 %2066, i64* %rdi
  store volatile i64 26331, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 26336, i64* @assembly_address
  %2067 = load i64* %rdi
  %2068 = inttoptr i64 %2067 to %_IO_FILE*
  %2069 = load i64* %rsi
  %2070 = inttoptr i64 %2069 to i8*
  %2071 = load i64* %rdx
  %2072 = inttoptr i64 %2071 to i8*
  %2073 = load i64* %rcx
  %2074 = inttoptr i64 %2073 to i8*
  %2075 = load i64* %r8
  %2076 = trunc i64 %2075 to i32
  %2077 = load i64* %r9
  %2078 = trunc i64 %2077 to i32
  %2079 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %2068, i8* %2070, i8* %2072, i8* %2074, i32 %2076, i32 %2078)
  %2080 = sext i32 %2079 to i64
  store i64 %2080, i64* %rax
  %2081 = sext i32 %2079 to i64
  store i64 %2081, i64* %rax
  store volatile i64 26341, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 26351, i64* @assembly_address
  %2082 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %2083 = zext i32 %2082 to i64
  store i64 %2083, i64* %rax
  store volatile i64 26357, i64* @assembly_address
  %2084 = load i64* %rax
  %2085 = trunc i64 %2084 to i32
  %2086 = trunc i64 %2084 to i32
  store i32 %2086, i32* %7
  store i32 1, i32* %6
  %2087 = sub i32 %2085, 1
  %2088 = and i32 %2085, 15
  %2089 = sub i32 %2088, 1
  %2090 = icmp ugt i32 %2089, 15
  %2091 = icmp ult i32 %2085, 1
  %2092 = xor i32 %2085, 1
  %2093 = xor i32 %2085, %2087
  %2094 = and i32 %2092, %2093
  %2095 = icmp slt i32 %2094, 0
  store i1 %2090, i1* %az
  store i1 %2091, i1* %cf
  store i1 %2095, i1* %of
  %2096 = icmp eq i32 %2087, 0
  store i1 %2096, i1* %zf
  %2097 = icmp slt i32 %2087, 0
  store i1 %2097, i1* %sf
  %2098 = trunc i32 %2087 to i8
  %2099 = call i8 @llvm.ctpop.i8(i8 %2098)
  %2100 = and i8 %2099, 1
  %2101 = icmp eq i8 %2100, 0
  store i1 %2101, i1* %pf
  store volatile i64 26360, i64* @assembly_address
  %2102 = load i32* %7
  %2103 = sext i32 %2102 to i64
  %2104 = load i32* %6
  %2105 = trunc i64 %2103 to i32
  %2106 = icmp sgt i32 %2105, %2104
  br i1 %2106, label %block_6704, label %block_66fa

block_66fa:                                       ; preds = %block_66b0
  store volatile i64 26362, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 26367, i64* @assembly_address
  br label %block_6a4d

block_6704:                                       ; preds = %block_66b0, %block_66a5, %block_65fe
  store volatile i64 26372, i64* @assembly_address
  %2107 = load i32* bitcast (i64* @global_var_216620 to i32*)
  %2108 = zext i32 %2107 to i64
  store i64 %2108, i64* %rax
  store volatile i64 26378, i64* @assembly_address
  %2109 = load i64* %rax
  %2110 = trunc i64 %2109 to i32
  %2111 = sub i32 %2110, 1
  %2112 = and i32 %2110, 15
  %2113 = sub i32 %2112, 1
  %2114 = icmp ugt i32 %2113, 15
  %2115 = icmp ult i32 %2110, 1
  %2116 = xor i32 %2110, 1
  %2117 = xor i32 %2110, %2111
  %2118 = and i32 %2116, %2117
  %2119 = icmp slt i32 %2118, 0
  store i1 %2114, i1* %az
  store i1 %2115, i1* %cf
  store i1 %2119, i1* %of
  %2120 = icmp eq i32 %2111, 0
  store i1 %2120, i1* %zf
  %2121 = icmp slt i32 %2111, 0
  store i1 %2121, i1* %sf
  %2122 = trunc i32 %2111 to i8
  %2123 = call i8 @llvm.ctpop.i8(i8 %2122)
  %2124 = and i8 %2123, 1
  %2125 = icmp eq i8 %2124, 0
  store i1 %2125, i1* %pf
  store volatile i64 26381, i64* @assembly_address
  %2126 = load i1* %zf
  %2127 = icmp eq i1 %2126, false
  br i1 %2127, label %block_68ee, label %block_6713

block_6713:                                       ; preds = %block_6704
  store volatile i64 26387, i64* @assembly_address
  %2128 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %2129 = zext i32 %2128 to i64
  store i64 %2129, i64* %rax
  store volatile i64 26393, i64* @assembly_address
  %2130 = load i64* %rax
  %2131 = trunc i64 %2130 to i32
  %2132 = add i32 %2131, 8
  %2133 = and i32 %2131, 15
  %2134 = add i32 %2133, 8
  %2135 = icmp ugt i32 %2134, 15
  %2136 = icmp ult i32 %2132, %2131
  %2137 = xor i32 %2131, %2132
  %2138 = xor i32 8, %2132
  %2139 = and i32 %2137, %2138
  %2140 = icmp slt i32 %2139, 0
  store i1 %2135, i1* %az
  store i1 %2136, i1* %cf
  store i1 %2140, i1* %of
  %2141 = icmp eq i32 %2132, 0
  store i1 %2141, i1* %zf
  %2142 = icmp slt i32 %2132, 0
  store i1 %2142, i1* %sf
  %2143 = trunc i32 %2132 to i8
  %2144 = call i8 @llvm.ctpop.i8(i8 %2143)
  %2145 = and i8 %2144, 1
  %2146 = icmp eq i8 %2145, 0
  store i1 %2146, i1* %pf
  store i64 ptrtoint (i64* @global_var_24a88c to i64), i64* %rax
  store volatile i64 26396, i64* @assembly_address
  %2147 = load i64* %rax
  %2148 = trunc i64 %2147 to i32
  %2149 = zext i32 %2148 to i64
  store i64 %2149, i64* %rax
  store volatile i64 26398, i64* @assembly_address
  %2150 = load i64* %rax
  store i64 %2150, i64* @global_var_267540
  store volatile i64 26405, i64* @assembly_address
  br label %block_68ee

block_672a:                                       ; preds = %block_5f5f
  store volatile i64 26410, i64* @assembly_address
  %2151 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2151, i64* %rax
  store volatile i64 26414, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 26419, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_118e4 to i64), i64* %rsi
  store volatile i64 26426, i64* @assembly_address
  %2152 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2152, i64* %rdi
  store volatile i64 26429, i64* @assembly_address
  %2153 = load i64* %rdi
  %2154 = inttoptr i64 %2153 to i64*
  %2155 = load i64* %rsi
  %2156 = inttoptr i64 %2155 to i64*
  %2157 = load i64* %rdx
  %2158 = trunc i64 %2157 to i32
  %2159 = call i32 @memcmp(i64* %2154, i64* %2156, i32 %2158)
  %2160 = sext i32 %2159 to i64
  store i64 %2160, i64* %rax
  %2161 = sext i32 %2159 to i64
  store i64 %2161, i64* %rax
  store volatile i64 26434, i64* @assembly_address
  %2162 = load i64* %rax
  %2163 = trunc i64 %2162 to i32
  %2164 = load i64* %rax
  %2165 = trunc i64 %2164 to i32
  %2166 = and i32 %2163, %2165
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2167 = icmp eq i32 %2166, 0
  store i1 %2167, i1* %zf
  %2168 = icmp slt i32 %2166, 0
  store i1 %2168, i1* %sf
  %2169 = trunc i32 %2166 to i8
  %2170 = call i8 @llvm.ctpop.i8(i8 %2169)
  %2171 = and i8 %2170, 1
  %2172 = icmp eq i8 %2171, 0
  store i1 %2172, i1* %pf
  store volatile i64 26436, i64* @assembly_address
  %2173 = load i1* %zf
  %2174 = icmp eq i1 %2173, false
  br i1 %2174, label %block_67ac, label %block_6746

block_6746:                                       ; preds = %block_672a
  store volatile i64 26438, i64* @assembly_address
  %2175 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %2176 = zext i32 %2175 to i64
  store i64 %2176, i64* %rax
  store volatile i64 26444, i64* @assembly_address
  %2177 = load i64* %rax
  %2178 = trunc i64 %2177 to i32
  %2179 = sub i32 %2178, 2
  %2180 = and i32 %2178, 15
  %2181 = sub i32 %2180, 2
  %2182 = icmp ugt i32 %2181, 15
  %2183 = icmp ult i32 %2178, 2
  %2184 = xor i32 %2178, 2
  %2185 = xor i32 %2178, %2179
  %2186 = and i32 %2184, %2185
  %2187 = icmp slt i32 %2186, 0
  store i1 %2182, i1* %az
  store i1 %2183, i1* %cf
  store i1 %2187, i1* %of
  %2188 = icmp eq i32 %2179, 0
  store i1 %2188, i1* %zf
  %2189 = icmp slt i32 %2179, 0
  store i1 %2189, i1* %sf
  %2190 = trunc i32 %2179 to i8
  %2191 = call i8 @llvm.ctpop.i8(i8 %2190)
  %2192 = and i8 %2191, 1
  %2193 = icmp eq i8 %2192, 0
  store i1 %2193, i1* %pf
  store volatile i64 26447, i64* @assembly_address
  %2194 = load i1* %zf
  %2195 = icmp eq i1 %2194, false
  br i1 %2195, label %block_67ac, label %block_6751

block_6751:                                       ; preds = %block_6746
  store volatile i64 26449, i64* @assembly_address
  store i64 4, i64* %rdx
  store volatile i64 26454, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_118e4 to i64), i64* %rsi
  store volatile i64 26461, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rdi
  store volatile i64 26468, i64* @assembly_address
  %2196 = load i64* %rdi
  %2197 = inttoptr i64 %2196 to i64*
  %2198 = load i64* %rsi
  %2199 = inttoptr i64 %2198 to i64*
  %2200 = load i64* %rdx
  %2201 = trunc i64 %2200 to i32
  %2202 = call i32 @memcmp(i64* %2197, i64* %2199, i32 %2201)
  %2203 = sext i32 %2202 to i64
  store i64 %2203, i64* %rax
  %2204 = sext i32 %2202 to i64
  store i64 %2204, i64* %rax
  store volatile i64 26473, i64* @assembly_address
  %2205 = load i64* %rax
  %2206 = trunc i64 %2205 to i32
  %2207 = load i64* %rax
  %2208 = trunc i64 %2207 to i32
  %2209 = and i32 %2206, %2208
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2210 = icmp eq i32 %2209, 0
  store i1 %2210, i1* %zf
  %2211 = icmp slt i32 %2209, 0
  store i1 %2211, i1* %sf
  %2212 = trunc i32 %2209 to i8
  %2213 = call i8 @llvm.ctpop.i8(i8 %2212)
  %2214 = and i8 %2213, 1
  %2215 = icmp eq i8 %2214, 0
  store i1 %2215, i1* %pf
  store volatile i64 26475, i64* @assembly_address
  %2216 = load i1* %zf
  %2217 = icmp eq i1 %2216, false
  br i1 %2217, label %block_67ac, label %block_676d

block_676d:                                       ; preds = %block_6751
  store volatile i64 26477, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 26487, i64* @assembly_address
  store i64 54102, i64* %rax
  store volatile i64 26494, i64* @assembly_address
  %2218 = load i64* %rax
  store i64 %2218, i64* @global_var_2160d0
  store volatile i64 26501, i64* @assembly_address
  %2219 = load i32* %stack_var_-92
  %2220 = zext i32 %2219 to i64
  store i64 %2220, i64* %rax
  store volatile i64 26504, i64* @assembly_address
  %2221 = load i64* %rax
  %2222 = trunc i64 %2221 to i32
  %2223 = zext i32 %2222 to i64
  store i64 %2223, i64* %rdi
  store volatile i64 26506, i64* @assembly_address
  %2224 = load i64* %rdi
  %2225 = trunc i64 %2224 to i32
  %2226 = call i64 @check_zipfile(i32 %2225)
  store i64 %2226, i64* %rax
  store i64 %2226, i64* %rax
  store volatile i64 26511, i64* @assembly_address
  %2227 = load i64* %rax
  %2228 = trunc i64 %2227 to i32
  %2229 = load i64* %rax
  %2230 = trunc i64 %2229 to i32
  %2231 = and i32 %2228, %2230
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2232 = icmp eq i32 %2231, 0
  store i1 %2232, i1* %zf
  %2233 = icmp slt i32 %2231, 0
  store i1 %2233, i1* %sf
  %2234 = trunc i32 %2231 to i8
  %2235 = call i8 @llvm.ctpop.i8(i8 %2234)
  %2236 = and i8 %2235, 1
  %2237 = icmp eq i8 %2236, 0
  store i1 %2237, i1* %pf
  store volatile i64 26513, i64* @assembly_address
  %2238 = load i1* %zf
  br i1 %2238, label %block_679d, label %block_6793

block_6793:                                       ; preds = %block_676d
  store volatile i64 26515, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 26520, i64* @assembly_address
  br label %block_6a4d

block_679d:                                       ; preds = %block_676d
  store volatile i64 26525, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21661c to i32*)
  store volatile i64 26535, i64* @assembly_address
  br label %block_68ee

block_67ac:                                       ; preds = %block_6751, %block_6746, %block_672a
  store volatile i64 26540, i64* @assembly_address
  %2239 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2239, i64* %rax
  store volatile i64 26544, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 26549, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_118e9 to i64), i64* %rsi
  store volatile i64 26556, i64* @assembly_address
  %2240 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2240, i64* %rdi
  store volatile i64 26559, i64* @assembly_address
  %2241 = load i64* %rdi
  %2242 = inttoptr i64 %2241 to i64*
  %2243 = load i64* %rsi
  %2244 = inttoptr i64 %2243 to i64*
  %2245 = load i64* %rdx
  %2246 = trunc i64 %2245 to i32
  %2247 = call i32 @memcmp(i64* %2242, i64* %2244, i32 %2246)
  %2248 = sext i32 %2247 to i64
  store i64 %2248, i64* %rax
  %2249 = sext i32 %2247 to i64
  store i64 %2249, i64* %rax
  store volatile i64 26564, i64* @assembly_address
  %2250 = load i64* %rax
  %2251 = trunc i64 %2250 to i32
  %2252 = load i64* %rax
  %2253 = trunc i64 %2252 to i32
  %2254 = and i32 %2251, %2253
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2255 = icmp eq i32 %2254, 0
  store i1 %2255, i1* %zf
  %2256 = icmp slt i32 %2254, 0
  store i1 %2256, i1* %sf
  %2257 = trunc i32 %2254 to i8
  %2258 = call i8 @llvm.ctpop.i8(i8 %2257)
  %2259 = and i8 %2258, 1
  %2260 = icmp eq i8 %2259, 0
  store i1 %2260, i1* %pf
  store volatile i64 26566, i64* @assembly_address
  %2261 = load i1* %zf
  %2262 = icmp eq i1 %2261, false
  br i1 %2262, label %block_67e5, label %block_67c8

block_67c8:                                       ; preds = %block_67ac
  store volatile i64 26568, i64* @assembly_address
  store i64 52977, i64* %rax
  store volatile i64 26575, i64* @assembly_address
  %2263 = load i64* %rax
  store i64 %2263, i64* @global_var_2160d0
  store volatile i64 26582, i64* @assembly_address
  store i32 2, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 26592, i64* @assembly_address
  br label %block_68ee

block_67e5:                                       ; preds = %block_67ac
  store volatile i64 26597, i64* @assembly_address
  %2264 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2264, i64* %rax
  store volatile i64 26601, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 26606, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_118ec to i64), i64* %rsi
  store volatile i64 26613, i64* @assembly_address
  %2265 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2265, i64* %rdi
  store volatile i64 26616, i64* @assembly_address
  %2266 = load i64* %rdi
  %2267 = inttoptr i64 %2266 to i64*
  %2268 = load i64* %rsi
  %2269 = inttoptr i64 %2268 to i64*
  %2270 = load i64* %rdx
  %2271 = trunc i64 %2270 to i32
  %2272 = call i32 @memcmp(i64* %2267, i64* %2269, i32 %2271)
  %2273 = sext i32 %2272 to i64
  store i64 %2273, i64* %rax
  %2274 = sext i32 %2272 to i64
  store i64 %2274, i64* %rax
  store volatile i64 26621, i64* @assembly_address
  %2275 = load i64* %rax
  %2276 = trunc i64 %2275 to i32
  %2277 = load i64* %rax
  %2278 = trunc i64 %2277 to i32
  %2279 = and i32 %2276, %2278
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2280 = icmp eq i32 %2279, 0
  store i1 %2280, i1* %zf
  %2281 = icmp slt i32 %2279, 0
  store i1 %2281, i1* %sf
  %2282 = trunc i32 %2279 to i8
  %2283 = call i8 @llvm.ctpop.i8(i8 %2282)
  %2284 = and i8 %2283, 1
  %2285 = icmp eq i8 %2284, 0
  store i1 %2285, i1* %pf
  store volatile i64 26623, i64* @assembly_address
  %2286 = load i1* %zf
  %2287 = icmp eq i1 %2286, false
  br i1 %2287, label %block_6828, label %block_6801

block_6801:                                       ; preds = %block_67e5
  store volatile i64 26625, i64* @assembly_address
  store i64 49898, i64* %rax
  store volatile i64 26632, i64* @assembly_address
  %2288 = load i64* %rax
  store i64 %2288, i64* @global_var_2160d0
  store volatile i64 26639, i64* @assembly_address
  store i32 1, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 26649, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21661c to i32*)
  store volatile i64 26659, i64* @assembly_address
  br label %block_68ee

block_6828:                                       ; preds = %block_67e5
  store volatile i64 26664, i64* @assembly_address
  %2289 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2289, i64* %rax
  store volatile i64 26668, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 26673, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_118ef to i64), i64* %rsi
  store volatile i64 26680, i64* @assembly_address
  %2290 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2290, i64* %rdi
  store volatile i64 26683, i64* @assembly_address
  %2291 = load i64* %rdi
  %2292 = inttoptr i64 %2291 to i64*
  %2293 = load i64* %rsi
  %2294 = inttoptr i64 %2293 to i64*
  %2295 = load i64* %rdx
  %2296 = trunc i64 %2295 to i32
  %2297 = call i32 @memcmp(i64* %2292, i64* %2294, i32 %2296)
  %2298 = sext i32 %2297 to i64
  store i64 %2298, i64* %rax
  %2299 = sext i32 %2297 to i64
  store i64 %2299, i64* %rax
  store volatile i64 26688, i64* @assembly_address
  %2300 = load i64* %rax
  %2301 = trunc i64 %2300 to i32
  %2302 = load i64* %rax
  %2303 = trunc i64 %2302 to i32
  %2304 = and i32 %2301, %2303
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2305 = icmp eq i32 %2304, 0
  store i1 %2305, i1* %zf
  %2306 = icmp slt i32 %2304, 0
  store i1 %2306, i1* %sf
  %2307 = trunc i32 %2304 to i8
  %2308 = call i8 @llvm.ctpop.i8(i8 %2307)
  %2309 = and i8 %2308, 1
  %2310 = icmp eq i8 %2309, 0
  store i1 %2310, i1* %pf
  store volatile i64 26690, i64* @assembly_address
  %2311 = load i1* %zf
  %2312 = icmp eq i1 %2311, false
  br i1 %2312, label %block_686b, label %block_6844

block_6844:                                       ; preds = %block_6828
  store volatile i64 26692, i64* @assembly_address
  store i64 49786, i64* %rax
  store volatile i64 26699, i64* @assembly_address
  %2313 = load i64* %rax
  store i64 %2313, i64* @global_var_2160d0
  store volatile i64 26706, i64* @assembly_address
  store i32 3, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 26716, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21661c to i32*)
  store volatile i64 26726, i64* @assembly_address
  br label %block_68ee

block_686b:                                       ; preds = %block_6828
  store volatile i64 26731, i64* @assembly_address
  %2314 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %2315 = zext i32 %2314 to i64
  store i64 %2315, i64* %rax
  store volatile i64 26737, i64* @assembly_address
  %2316 = load i64* %rax
  %2317 = trunc i64 %2316 to i32
  %2318 = load i64* %rax
  %2319 = trunc i64 %2318 to i32
  %2320 = and i32 %2317, %2319
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2321 = icmp eq i32 %2320, 0
  store i1 %2321, i1* %zf
  %2322 = icmp slt i32 %2320, 0
  store i1 %2322, i1* %sf
  %2323 = trunc i32 %2320 to i8
  %2324 = call i8 @llvm.ctpop.i8(i8 %2323)
  %2325 = and i8 %2324, 1
  %2326 = icmp eq i8 %2325, 0
  store i1 %2326, i1* %pf
  store volatile i64 26739, i64* @assembly_address
  %2327 = load i1* %zf
  br i1 %2327, label %block_68ee, label %block_6875

block_6875:                                       ; preds = %block_686b
  store volatile i64 26741, i64* @assembly_address
  %2328 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %2329 = zext i32 %2328 to i64
  store i64 %2329, i64* %rax
  store volatile i64 26747, i64* @assembly_address
  %2330 = load i64* %rax
  %2331 = trunc i64 %2330 to i32
  %2332 = load i64* %rax
  %2333 = trunc i64 %2332 to i32
  %2334 = and i32 %2331, %2333
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2335 = icmp eq i32 %2334, 0
  store i1 %2335, i1* %zf
  %2336 = icmp slt i32 %2334, 0
  store i1 %2336, i1* %sf
  %2337 = trunc i32 %2334 to i8
  %2338 = call i8 @llvm.ctpop.i8(i8 %2337)
  %2339 = and i8 %2338, 1
  %2340 = icmp eq i8 %2339, 0
  store i1 %2340, i1* %pf
  store volatile i64 26749, i64* @assembly_address
  %2341 = load i1* %zf
  br i1 %2341, label %block_68ee, label %block_687f

block_687f:                                       ; preds = %block_6875
  store volatile i64 26751, i64* @assembly_address
  %2342 = load i32* bitcast (i64* @global_var_216610 to i32*)
  %2343 = zext i32 %2342 to i64
  store i64 %2343, i64* %rax
  store volatile i64 26757, i64* @assembly_address
  %2344 = load i64* %rax
  %2345 = trunc i64 %2344 to i32
  %2346 = load i64* %rax
  %2347 = trunc i64 %2346 to i32
  %2348 = and i32 %2345, %2347
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2349 = icmp eq i32 %2348, 0
  store i1 %2349, i1* %zf
  %2350 = icmp slt i32 %2348, 0
  store i1 %2350, i1* %sf
  %2351 = trunc i32 %2348 to i8
  %2352 = call i8 @llvm.ctpop.i8(i8 %2351)
  %2353 = and i8 %2352, 1
  %2354 = icmp eq i8 %2353, 0
  store i1 %2354, i1* %pf
  store volatile i64 26759, i64* @assembly_address
  %2355 = load i1* %zf
  %2356 = icmp eq i1 %2355, false
  br i1 %2356, label %block_68ee, label %block_6889

block_6889:                                       ; preds = %block_687f
  store volatile i64 26761, i64* @assembly_address
  store i32 0, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 26771, i64* @assembly_address
  store i64 55791, i64* %rax
  store volatile i64 26778, i64* @assembly_address
  %2357 = load i64* %rax
  store i64 %2357, i64* @global_var_2160d0
  store volatile i64 26785, i64* @assembly_address
  %2358 = load i32* %stack_var_-76
  %2359 = sub i32 %2358, -1
  %2360 = and i32 %2358, 15
  %2361 = sub i32 %2360, 15
  %2362 = icmp ugt i32 %2361, 15
  %2363 = icmp ult i32 %2358, -1
  %2364 = xor i32 %2358, -1
  %2365 = xor i32 %2358, %2359
  %2366 = and i32 %2364, %2365
  %2367 = icmp slt i32 %2366, 0
  store i1 %2362, i1* %az
  store i1 %2363, i1* %cf
  store i1 %2367, i1* %of
  %2368 = icmp eq i32 %2359, 0
  store i1 %2368, i1* %zf
  %2369 = icmp slt i32 %2359, 0
  store i1 %2369, i1* %sf
  %2370 = trunc i32 %2359 to i8
  %2371 = call i8 @llvm.ctpop.i8(i8 %2370)
  %2372 = and i8 %2371, 1
  %2373 = icmp eq i8 %2372, 0
  store i1 %2373, i1* %pf
  store volatile i64 26789, i64* @assembly_address
  %2374 = load i1* %zf
  br i1 %2374, label %block_68b6, label %block_68a7

block_68a7:                                       ; preds = %block_6889
  store volatile i64 26791, i64* @assembly_address
  %2375 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %2376 = zext i32 %2375 to i64
  store i64 %2376, i64* %rax
  store volatile i64 26797, i64* @assembly_address
  %2377 = load i64* %rax
  %2378 = trunc i64 %2377 to i32
  %2379 = sub i32 %2378, 1
  %2380 = and i32 %2378, 15
  %2381 = sub i32 %2380, 1
  %2382 = icmp ugt i32 %2381, 15
  %2383 = icmp ult i32 %2378, 1
  %2384 = xor i32 %2378, 1
  %2385 = xor i32 %2378, %2379
  %2386 = and i32 %2384, %2385
  %2387 = icmp slt i32 %2386, 0
  store i1 %2382, i1* %az
  store i1 %2383, i1* %cf
  store i1 %2387, i1* %of
  %2388 = icmp eq i32 %2379, 0
  store i1 %2388, i1* %zf
  %2389 = icmp slt i32 %2379, 0
  store i1 %2389, i1* %sf
  %2390 = trunc i32 %2379 to i8
  %2391 = call i8 @llvm.ctpop.i8(i8 %2390)
  %2392 = and i8 %2391, 1
  %2393 = icmp eq i8 %2392, 0
  store i1 %2393, i1* %pf
  %2394 = zext i32 %2379 to i64
  store i64 %2394, i64* %rax
  store volatile i64 26800, i64* @assembly_address
  %2395 = load i64* %rax
  %2396 = trunc i64 %2395 to i32
  store i32 %2396, i32* bitcast (i64* @global_var_24a884 to i32*)
  br label %block_68b6

block_68b6:                                       ; preds = %block_68a7, %block_6889
  store volatile i64 26806, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21661c to i32*)
  store volatile i64 26816, i64* @assembly_address
  %2397 = load i32* %stack_var_-80
  %2398 = sub i32 %2397, -1
  %2399 = and i32 %2397, 15
  %2400 = sub i32 %2399, 15
  %2401 = icmp ugt i32 %2400, 15
  %2402 = icmp ult i32 %2397, -1
  %2403 = xor i32 %2397, -1
  %2404 = xor i32 %2397, %2398
  %2405 = and i32 %2403, %2404
  %2406 = icmp slt i32 %2405, 0
  store i1 %2401, i1* %az
  store i1 %2402, i1* %cf
  store i1 %2406, i1* %of
  %2407 = icmp eq i32 %2398, 0
  store i1 %2407, i1* %zf
  %2408 = icmp slt i32 %2398, 0
  store i1 %2408, i1* %sf
  %2409 = trunc i32 %2398 to i8
  %2410 = call i8 @llvm.ctpop.i8(i8 %2409)
  %2411 = and i8 %2410, 1
  %2412 = icmp eq i8 %2411, 0
  store i1 %2412, i1* %pf
  store volatile i64 26820, i64* @assembly_address
  %2413 = load i1* %zf
  br i1 %2413, label %block_68ee, label %block_68c6

block_68c6:                                       ; preds = %block_68b6
  store volatile i64 26822, i64* @assembly_address
  %2414 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2414, i64* %rax
  store volatile i64 26826, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 26831, i64* @assembly_address
  %2415 = ptrtoint i8* %stack_var_-26 to i64
  store i64 %2415, i64* %rsi
  store volatile i64 26834, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 26839, i64* @assembly_address
  %2416 = load i64* %rdi
  %2417 = load i64* %rsi
  %2418 = inttoptr i64 %2417 to i8*
  %2419 = load i64* %rdx
  %2420 = trunc i64 %2416 to i32
  %2421 = call i64 @write_buf(i32 %2420, i8* %2418, i64 %2419)
  store i64 %2421, i64* %rax
  store i64 %2421, i64* %rax
  store volatile i64 26844, i64* @assembly_address
  %2422 = load i64* @global_var_25f4c0
  store i64 %2422, i64* %rax
  store volatile i64 26851, i64* @assembly_address
  %2423 = load i64* %rax
  %2424 = add i64 %2423, 1
  %2425 = and i64 %2423, 15
  %2426 = add i64 %2425, 1
  %2427 = icmp ugt i64 %2426, 15
  %2428 = icmp ult i64 %2424, %2423
  %2429 = xor i64 %2423, %2424
  %2430 = xor i64 1, %2424
  %2431 = and i64 %2429, %2430
  %2432 = icmp slt i64 %2431, 0
  store i1 %2427, i1* %az
  store i1 %2428, i1* %cf
  store i1 %2432, i1* %of
  %2433 = icmp eq i64 %2424, 0
  store i1 %2433, i1* %zf
  %2434 = icmp slt i64 %2424, 0
  store i1 %2434, i1* %sf
  %2435 = trunc i64 %2424 to i8
  %2436 = call i8 @llvm.ctpop.i8(i8 %2435)
  %2437 = and i8 %2436, 1
  %2438 = icmp eq i8 %2437, 0
  store i1 %2438, i1* %pf
  store i64 %2424, i64* %rax
  store volatile i64 26855, i64* @assembly_address
  %2439 = load i64* %rax
  store i64 %2439, i64* @global_var_25f4c0
  br label %block_68ee

block_68ee:                                       ; preds = %block_68c6, %block_68b6, %block_687f, %block_6875, %block_686b, %block_6844, %block_6801, %block_67c8, %block_679d, %block_6713, %block_6704
  store volatile i64 26862, i64* @assembly_address
  %2440 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %2441 = zext i32 %2440 to i64
  store i64 %2441, i64* %rax
  store volatile i64 26868, i64* @assembly_address
  %2442 = load i64* %rax
  %2443 = trunc i64 %2442 to i32
  %2444 = load i64* %rax
  %2445 = trunc i64 %2444 to i32
  %2446 = and i32 %2443, %2445
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2447 = icmp eq i32 %2446, 0
  store i1 %2447, i1* %zf
  %2448 = icmp slt i32 %2446, 0
  store i1 %2448, i1* %sf
  %2449 = trunc i32 %2446 to i8
  %2450 = call i8 @llvm.ctpop.i8(i8 %2449)
  %2451 = and i8 %2450, 1
  %2452 = icmp eq i8 %2451, 0
  store i1 %2452, i1* %pf
  store volatile i64 26870, i64* @assembly_address
  %2453 = load i1* %sf
  br i1 %2453, label %block_6903, label %block_68f8

block_68f8:                                       ; preds = %block_68ee
  store volatile i64 26872, i64* @assembly_address
  %2454 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %2455 = zext i32 %2454 to i64
  store i64 %2455, i64* %rax
  store volatile i64 26878, i64* @assembly_address
  br label %block_6a4d

block_6903:                                       ; preds = %block_68ee
  store volatile i64 26883, i64* @assembly_address
  %2456 = load i32* bitcast (i64* @global_var_216620 to i32*)
  %2457 = zext i32 %2456 to i64
  store i64 %2457, i64* %rax
  store volatile i64 26889, i64* @assembly_address
  %2458 = load i64* %rax
  %2459 = trunc i64 %2458 to i32
  %2460 = sub i32 %2459, 1
  %2461 = and i32 %2459, 15
  %2462 = sub i32 %2461, 1
  %2463 = icmp ugt i32 %2462, 15
  %2464 = icmp ult i32 %2459, 1
  %2465 = xor i32 %2459, 1
  %2466 = xor i32 %2459, %2460
  %2467 = and i32 %2465, %2466
  %2468 = icmp slt i32 %2467, 0
  store i1 %2463, i1* %az
  store i1 %2464, i1* %cf
  store i1 %2468, i1* %of
  %2469 = icmp eq i32 %2460, 0
  store i1 %2469, i1* %zf
  %2470 = icmp slt i32 %2460, 0
  store i1 %2470, i1* %sf
  %2471 = trunc i32 %2460 to i8
  %2472 = call i8 @llvm.ctpop.i8(i8 %2471)
  %2473 = and i8 %2472, 1
  %2474 = icmp eq i8 %2473, 0
  store i1 %2474, i1* %pf
  store volatile i64 26892, i64* @assembly_address
  %2475 = load i1* %zf
  %2476 = icmp eq i1 %2475, false
  br i1 %2476, label %block_694b, label %block_690e

block_690e:                                       ; preds = %block_6903
  store volatile i64 26894, i64* @assembly_address
  %2477 = load i64* @global_var_25f4c8
  store i64 %2477, i64* %rdx
  store volatile i64 26901, i64* @assembly_address
  %2478 = load i64* @global_var_216580
  store i64 %2478, i64* %rax
  store volatile i64 26908, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 26915, i64* @assembly_address
  store i64 ptrtoint ([29 x i8]* @global_var_118f2 to i64), i64* %rsi
  store volatile i64 26922, i64* @assembly_address
  %2479 = load i64* %rax
  store i64 %2479, i64* %rdi
  store volatile i64 26925, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 26930, i64* @assembly_address
  %2480 = load i64* %rdi
  %2481 = inttoptr i64 %2480 to %_IO_FILE*
  %2482 = load i64* %rsi
  %2483 = inttoptr i64 %2482 to i8*
  %2484 = load i64* %rdx
  %2485 = inttoptr i64 %2484 to i8*
  %2486 = load i64* %rcx
  %2487 = inttoptr i64 %2486 to i8*
  %2488 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %2481, i8* %2483, i8* %2485, i8* %2487)
  %2489 = sext i32 %2488 to i64
  store i64 %2489, i64* %rax
  %2490 = sext i32 %2488 to i64
  store i64 %2490, i64* %rax
  store volatile i64 26935, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 26945, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 26950, i64* @assembly_address
  br label %block_6a4d

block_694b:                                       ; preds = %block_6903
  store volatile i64 26955, i64* @assembly_address
  %2491 = load i8* %stack_var_-26
  %2492 = zext i8 %2491 to i64
  store i64 %2492, i64* %rax
  store volatile i64 26959, i64* @assembly_address
  %2493 = load i64* %rax
  %2494 = trunc i64 %2493 to i8
  %2495 = load i64* %rax
  %2496 = trunc i64 %2495 to i8
  %2497 = and i8 %2494, %2496
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2498 = icmp eq i8 %2497, 0
  store i1 %2498, i1* %zf
  %2499 = icmp slt i8 %2497, 0
  store i1 %2499, i1* %sf
  %2500 = call i8 @llvm.ctpop.i8(i8 %2497)
  %2501 = and i8 %2500, 1
  %2502 = icmp eq i8 %2501, 0
  store i1 %2502, i1* %pf
  store volatile i64 26961, i64* @assembly_address
  %2503 = load i1* %zf
  %2504 = icmp eq i1 %2503, false
  br i1 %2504, label %block_6a01, label %block_6957

block_6957:                                       ; preds = %block_694b
  store volatile i64 26967, i64* @assembly_address
  %2505 = load i32* %stack_var_-76
  %2506 = zext i32 %2505 to i64
  store i64 %2506, i64* %rax
  store volatile i64 26970, i64* @assembly_address
  %2507 = load i64* %rax
  %2508 = trunc i64 %2507 to i32
  store i32 %2508, i32* %stack_var_-72
  store volatile i64 26973, i64* @assembly_address
  br label %block_699d

block_695f:                                       ; preds = %block_699d
  store volatile i64 26975, i64* @assembly_address
  %2509 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %2510 = zext i32 %2509 to i64
  store i64 %2510, i64* %rdx
  store volatile i64 26981, i64* @assembly_address
  %2511 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %2512 = zext i32 %2511 to i64
  store i64 %2512, i64* %rax
  store volatile i64 26987, i64* @assembly_address
  %2513 = load i64* %rdx
  %2514 = trunc i64 %2513 to i32
  %2515 = load i64* %rax
  %2516 = trunc i64 %2515 to i32
  %2517 = sub i32 %2514, %2516
  %2518 = and i32 %2514, 15
  %2519 = and i32 %2516, 15
  %2520 = sub i32 %2518, %2519
  %2521 = icmp ugt i32 %2520, 15
  %2522 = icmp ult i32 %2514, %2516
  %2523 = xor i32 %2514, %2516
  %2524 = xor i32 %2514, %2517
  %2525 = and i32 %2523, %2524
  %2526 = icmp slt i32 %2525, 0
  store i1 %2521, i1* %az
  store i1 %2522, i1* %cf
  store i1 %2526, i1* %of
  %2527 = icmp eq i32 %2517, 0
  store i1 %2527, i1* %zf
  %2528 = icmp slt i32 %2517, 0
  store i1 %2528, i1* %sf
  %2529 = trunc i32 %2517 to i8
  %2530 = call i8 @llvm.ctpop.i8(i8 %2529)
  %2531 = and i8 %2530, 1
  %2532 = icmp eq i8 %2531, 0
  store i1 %2532, i1* %pf
  store volatile i64 26989, i64* @assembly_address
  %2533 = load i1* %cf
  %2534 = icmp eq i1 %2533, false
  br i1 %2534, label %block_6990, label %block_696f

block_696f:                                       ; preds = %block_695f
  store volatile i64 26991, i64* @assembly_address
  %2535 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %2536 = zext i32 %2535 to i64
  store i64 %2536, i64* %rax
  store volatile i64 26997, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 27000, i64* @assembly_address
  %2537 = load i64* %rdx
  %2538 = trunc i64 %2537 to i32
  store i32 %2538, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 27006, i64* @assembly_address
  %2539 = load i64* %rax
  %2540 = trunc i64 %2539 to i32
  %2541 = zext i32 %2540 to i64
  store i64 %2541, i64* %rdx
  store volatile i64 27008, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 27015, i64* @assembly_address
  %2542 = load i64* %rdx
  %2543 = load i64* %rax
  %2544 = mul i64 %2543, 1
  %2545 = add i64 %2542, %2544
  %2546 = inttoptr i64 %2545 to i8*
  %2547 = load i8* %2546
  %2548 = zext i8 %2547 to i64
  store i64 %2548, i64* %rax
  store volatile i64 27019, i64* @assembly_address
  %2549 = load i64* %rax
  %2550 = trunc i64 %2549 to i8
  %2551 = zext i8 %2550 to i64
  store i64 %2551, i64* %rax
  store volatile i64 27022, i64* @assembly_address
  br label %block_699a

block_6990:                                       ; preds = %block_695f
  store volatile i64 27024, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 27029, i64* @assembly_address
  %2552 = load i64* %rdi
  %2553 = trunc i64 %2552 to i32
  %2554 = call i64 @fill_inbuf(i32 %2553)
  store i64 %2554, i64* %rax
  store i64 %2554, i64* %rax
  br label %block_699a

block_699a:                                       ; preds = %block_6990, %block_696f
  store volatile i64 27034, i64* @assembly_address
  %2555 = load i64* %rax
  %2556 = trunc i64 %2555 to i32
  store i32 %2556, i32* %stack_var_-72
  br label %block_699d

block_699d:                                       ; preds = %block_699a, %block_6957
  store volatile i64 27037, i64* @assembly_address
  %2557 = load i32* %stack_var_-72
  %2558 = and i32 %2557, 15
  %2559 = icmp ugt i32 %2558, 15
  %2560 = icmp ult i32 %2557, 0
  %2561 = xor i32 %2557, 0
  %2562 = and i32 %2561, 0
  %2563 = icmp slt i32 %2562, 0
  store i1 %2559, i1* %az
  store i1 %2560, i1* %cf
  store i1 %2563, i1* %of
  %2564 = icmp eq i32 %2557, 0
  store i1 %2564, i1* %zf
  %2565 = icmp slt i32 %2557, 0
  store i1 %2565, i1* %sf
  %2566 = trunc i32 %2557 to i8
  %2567 = call i8 @llvm.ctpop.i8(i8 %2566)
  %2568 = and i8 %2567, 1
  %2569 = icmp eq i8 %2568, 0
  store i1 %2569, i1* %pf
  store volatile i64 27041, i64* @assembly_address
  %2570 = load i1* %zf
  br i1 %2570, label %block_695f, label %block_69a3

block_69a3:                                       ; preds = %block_699d
  store volatile i64 27043, i64* @assembly_address
  %2571 = load i32* %stack_var_-72
  %2572 = sub i32 %2571, -1
  %2573 = and i32 %2571, 15
  %2574 = sub i32 %2573, 15
  %2575 = icmp ugt i32 %2574, 15
  %2576 = icmp ult i32 %2571, -1
  %2577 = xor i32 %2571, -1
  %2578 = xor i32 %2571, %2572
  %2579 = and i32 %2577, %2578
  %2580 = icmp slt i32 %2579, 0
  store i1 %2575, i1* %az
  store i1 %2576, i1* %cf
  store i1 %2580, i1* %of
  %2581 = icmp eq i32 %2572, 0
  store i1 %2581, i1* %zf
  %2582 = icmp slt i32 %2572, 0
  store i1 %2582, i1* %sf
  %2583 = trunc i32 %2572 to i8
  %2584 = call i8 @llvm.ctpop.i8(i8 %2583)
  %2585 = and i8 %2584, 1
  %2586 = icmp eq i8 %2585, 0
  store i1 %2586, i1* %pf
  store volatile i64 27047, i64* @assembly_address
  %2587 = load i1* %zf
  %2588 = icmp eq i1 %2587, false
  br i1 %2588, label %block_6a01, label %block_69a9

block_69a9:                                       ; preds = %block_69a3
  store volatile i64 27049, i64* @assembly_address
  %2589 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %2590 = zext i32 %2589 to i64
  store i64 %2590, i64* %rax
  store volatile i64 27055, i64* @assembly_address
  %2591 = load i64* %rax
  %2592 = trunc i64 %2591 to i32
  %2593 = load i64* %rax
  %2594 = trunc i64 %2593 to i32
  %2595 = and i32 %2592, %2594
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2596 = icmp eq i32 %2595, 0
  store i1 %2596, i1* %zf
  %2597 = icmp slt i32 %2595, 0
  store i1 %2597, i1* %sf
  %2598 = trunc i32 %2595 to i8
  %2599 = call i8 @llvm.ctpop.i8(i8 %2598)
  %2600 = and i8 %2599, 1
  %2601 = icmp eq i8 %2600, 0
  store i1 %2601, i1* %pf
  store volatile i64 27057, i64* @assembly_address
  %2602 = load i1* %zf
  br i1 %2602, label %block_69fa, label %block_69b3

block_69b3:                                       ; preds = %block_69a9
  store volatile i64 27059, i64* @assembly_address
  %2603 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %2604 = zext i32 %2603 to i64
  store i64 %2604, i64* %rax
  store volatile i64 27065, i64* @assembly_address
  %2605 = load i64* %rax
  %2606 = trunc i64 %2605 to i32
  %2607 = load i64* %rax
  %2608 = trunc i64 %2607 to i32
  %2609 = and i32 %2606, %2608
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2610 = icmp eq i32 %2609, 0
  store i1 %2610, i1* %zf
  %2611 = icmp slt i32 %2609, 0
  store i1 %2611, i1* %sf
  %2612 = trunc i32 %2609 to i8
  %2613 = call i8 @llvm.ctpop.i8(i8 %2612)
  %2614 = and i8 %2613, 1
  %2615 = icmp eq i8 %2614, 0
  store i1 %2615, i1* %pf
  store volatile i64 27067, i64* @assembly_address
  %2616 = load i1* %zf
  %2617 = icmp eq i1 %2616, false
  br i1 %2617, label %block_69e6, label %block_69bd

block_69bd:                                       ; preds = %block_69b3
  store volatile i64 27069, i64* @assembly_address
  %2618 = load i64* @global_var_25f4c8
  store i64 %2618, i64* %rdx
  store volatile i64 27076, i64* @assembly_address
  %2619 = load i64* @global_var_216580
  store i64 %2619, i64* %rax
  store volatile i64 27083, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 27090, i64* @assembly_address
  store i64 ptrtoint ([56 x i8]* @global_var_11910 to i64), i64* %rsi
  store volatile i64 27097, i64* @assembly_address
  %2620 = load i64* %rax
  store i64 %2620, i64* %rdi
  store volatile i64 27100, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 27105, i64* @assembly_address
  %2621 = load i64* %rdi
  %2622 = inttoptr i64 %2621 to %_IO_FILE*
  %2623 = load i64* %rsi
  %2624 = inttoptr i64 %2623 to i8*
  %2625 = load i64* %rdx
  %2626 = inttoptr i64 %2625 to i8*
  %2627 = load i64* %rcx
  %2628 = inttoptr i64 %2627 to i8*
  %2629 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %2622, i8* %2624, i8* %2626, i8* %2628)
  %2630 = sext i32 %2629 to i64
  store i64 %2630, i64* %rax
  %2631 = sext i32 %2629 to i64
  store i64 %2631, i64* %rax
  br label %block_69e6

block_69e6:                                       ; preds = %block_69bd, %block_69b3
  store volatile i64 27110, i64* @assembly_address
  %2632 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %2633 = zext i32 %2632 to i64
  store i64 %2633, i64* %rax
  store volatile i64 27116, i64* @assembly_address
  %2634 = load i64* %rax
  %2635 = trunc i64 %2634 to i32
  %2636 = load i64* %rax
  %2637 = trunc i64 %2636 to i32
  %2638 = and i32 %2635, %2637
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2639 = icmp eq i32 %2638, 0
  store i1 %2639, i1* %zf
  %2640 = icmp slt i32 %2638, 0
  store i1 %2640, i1* %sf
  %2641 = trunc i32 %2638 to i8
  %2642 = call i8 @llvm.ctpop.i8(i8 %2641)
  %2643 = and i8 %2642, 1
  %2644 = icmp eq i8 %2643, 0
  store i1 %2644, i1* %pf
  store volatile i64 27118, i64* @assembly_address
  %2645 = load i1* %zf
  %2646 = icmp eq i1 %2645, false
  br i1 %2646, label %block_69fa, label %block_69f0

block_69f0:                                       ; preds = %block_69e6
  store volatile i64 27120, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_69fa

block_69fa:                                       ; preds = %block_69f0, %block_69e6, %block_69a9
  store volatile i64 27130, i64* @assembly_address
  store i64 4294967293, i64* %rax
  store volatile i64 27135, i64* @assembly_address
  br label %block_6a4d

block_6a01:                                       ; preds = %block_69a3, %block_694b
  store volatile i64 27137, i64* @assembly_address
  %2647 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %2648 = zext i32 %2647 to i64
  store i64 %2648, i64* %rax
  store volatile i64 27143, i64* @assembly_address
  %2649 = load i64* %rax
  %2650 = trunc i64 %2649 to i32
  %2651 = load i64* %rax
  %2652 = trunc i64 %2651 to i32
  %2653 = and i32 %2650, %2652
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2654 = icmp eq i32 %2653, 0
  store i1 %2654, i1* %zf
  %2655 = icmp slt i32 %2653, 0
  store i1 %2655, i1* %sf
  %2656 = trunc i32 %2653 to i8
  %2657 = call i8 @llvm.ctpop.i8(i8 %2656)
  %2658 = and i8 %2657, 1
  %2659 = icmp eq i8 %2658, 0
  store i1 %2659, i1* %pf
  store volatile i64 27145, i64* @assembly_address
  %2660 = load i1* %zf
  %2661 = icmp eq i1 %2660, false
  br i1 %2661, label %block_6a34, label %block_6a0b

block_6a0b:                                       ; preds = %block_6a01
  store volatile i64 27147, i64* @assembly_address
  %2662 = load i64* @global_var_25f4c8
  store i64 %2662, i64* %rdx
  store volatile i64 27154, i64* @assembly_address
  %2663 = load i64* @global_var_216580
  store i64 %2663, i64* %rax
  store volatile i64 27161, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 27168, i64* @assembly_address
  store i64 ptrtoint ([53 x i8]* @global_var_11948 to i64), i64* %rsi
  store volatile i64 27175, i64* @assembly_address
  %2664 = load i64* %rax
  store i64 %2664, i64* %rdi
  store volatile i64 27178, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 27183, i64* @assembly_address
  %2665 = load i64* %rdi
  %2666 = inttoptr i64 %2665 to %_IO_FILE*
  %2667 = load i64* %rsi
  %2668 = inttoptr i64 %2667 to i8*
  %2669 = load i64* %rdx
  %2670 = inttoptr i64 %2669 to i8*
  %2671 = load i64* %rcx
  %2672 = inttoptr i64 %2671 to i8*
  %2673 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %2666, i8* %2668, i8* %2670, i8* %2672)
  %2674 = sext i32 %2673 to i64
  store i64 %2674, i64* %rax
  %2675 = sext i32 %2673 to i64
  store i64 %2675, i64* %rax
  br label %block_6a34

block_6a34:                                       ; preds = %block_6a0b, %block_6a01
  store volatile i64 27188, i64* @assembly_address
  %2676 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %2677 = zext i32 %2676 to i64
  store i64 %2677, i64* %rax
  store volatile i64 27194, i64* @assembly_address
  %2678 = load i64* %rax
  %2679 = trunc i64 %2678 to i32
  %2680 = load i64* %rax
  %2681 = trunc i64 %2680 to i32
  %2682 = and i32 %2679, %2681
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2683 = icmp eq i32 %2682, 0
  store i1 %2683, i1* %zf
  %2684 = icmp slt i32 %2682, 0
  store i1 %2684, i1* %sf
  %2685 = trunc i32 %2682 to i8
  %2686 = call i8 @llvm.ctpop.i8(i8 %2685)
  %2687 = and i8 %2686, 1
  %2688 = icmp eq i8 %2687, 0
  store i1 %2688, i1* %pf
  store volatile i64 27196, i64* @assembly_address
  %2689 = load i1* %zf
  %2690 = icmp eq i1 %2689, false
  br i1 %2690, label %block_6a48, label %block_6a3e

block_6a3e:                                       ; preds = %block_6a34
  store volatile i64 27198, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_6a48

block_6a48:                                       ; preds = %block_6a3e, %block_6a34
  store volatile i64 27208, i64* @assembly_address
  store i64 4294967294, i64* %rax
  br label %block_6a4d

block_6a4d:                                       ; preds = %block_6a48, %block_69fa, %block_690e, %block_68f8, %block_6793, %block_66fa, %block_60f4, %block_6065, %block_5fcb
  store volatile i64 27213, i64* @assembly_address
  %2691 = load i64* %stack_var_-16
  store i64 %2691, i64* %rcx
  store volatile i64 27217, i64* @assembly_address
  %2692 = load i64* %rcx
  %2693 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %2694 = xor i64 %2692, %2693
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2695 = icmp eq i64 %2694, 0
  store i1 %2695, i1* %zf
  %2696 = icmp slt i64 %2694, 0
  store i1 %2696, i1* %sf
  %2697 = trunc i64 %2694 to i8
  %2698 = call i8 @llvm.ctpop.i8(i8 %2697)
  %2699 = and i8 %2698, 1
  %2700 = icmp eq i8 %2699, 0
  store i1 %2700, i1* %pf
  store i64 %2694, i64* %rcx
  store volatile i64 27226, i64* @assembly_address
  %2701 = load i1* %zf
  br i1 %2701, label %block_6a61, label %block_6a5c

block_6a5c:                                       ; preds = %block_6a4d
  store volatile i64 27228, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_6a61:                                       ; preds = %block_6a4d
  store volatile i64 27233, i64* @assembly_address
  %2702 = load i64* %stack_var_-8
  store i64 %2702, i64* %rbp
  %2703 = ptrtoint i64* %stack_var_0 to i64
  store i64 %2703, i64* %rsp
  store volatile i64 27234, i64* @assembly_address
  %2704 = load i64* %rax
  ret i64 %2704
}

declare i64 @181(i64)

define i64 @do_list(i32 %arg1, i64 %arg2) {
block_6a63:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-17 = alloca i32
  %1 = alloca i8
  %stack_var_-18 = alloca i32
  %2 = alloca i8
  %stack_var_-19 = alloca i32
  %3 = alloca i8
  %stack_var_-20 = alloca i32
  %4 = alloca i8
  %stack_var_-21 = alloca i32
  %5 = alloca i8
  %stack_var_-22 = alloca i32
  %6 = alloca i8
  %stack_var_-23 = alloca i32
  %7 = alloca i8
  %stack_var_-24 = alloca i32
  %8 = alloca i64
  %stack_var_-40 = alloca i32
  %9 = alloca i64
  %stack_var_-88 = alloca i8*
  %10 = alloca i64
  %stack_var_-80 = alloca i64
  %stack_var_-44 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-64 = alloca i32
  %stack_var_-60 = alloca i32
  %stack_var_-72 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 27235, i64* @assembly_address
  %11 = load i64* %rbp
  store i64 %11, i64* %stack_var_-8
  %12 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %12, i64* %rsp
  store volatile i64 27236, i64* @assembly_address
  %13 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %13, i64* %rbp
  store volatile i64 27239, i64* @assembly_address
  %14 = load i64* %rsp
  %15 = sub i64 %14, 64
  %16 = and i64 %14, 15
  %17 = icmp ugt i64 %16, 15
  %18 = icmp ult i64 %14, 64
  %19 = xor i64 %14, 64
  %20 = xor i64 %14, %15
  %21 = and i64 %19, %20
  %22 = icmp slt i64 %21, 0
  store i1 %17, i1* %az
  store i1 %18, i1* %cf
  store i1 %22, i1* %of
  %23 = icmp eq i64 %15, 0
  store i1 %23, i1* %zf
  %24 = icmp slt i64 %15, 0
  store i1 %24, i1* %sf
  %25 = trunc i64 %15 to i8
  %26 = call i8 @llvm.ctpop.i8(i8 %25)
  %27 = and i8 %26, 1
  %28 = icmp eq i8 %27, 0
  store i1 %28, i1* %pf
  %29 = ptrtoint i64* %stack_var_-72 to i64
  store i64 %29, i64* %rsp
  store volatile i64 27243, i64* @assembly_address
  %30 = load i64* %rdi
  %31 = trunc i64 %30 to i32
  store i32 %31, i32* %stack_var_-60
  store volatile i64 27246, i64* @assembly_address
  %32 = load i64* %rsi
  %33 = trunc i64 %32 to i32
  store i32 %33, i32* %stack_var_-64
  store volatile i64 27249, i64* @assembly_address
  %34 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %34, i64* %rax
  store volatile i64 27258, i64* @assembly_address
  %35 = load i64* %rax
  store i64 %35, i64* %stack_var_-16
  store volatile i64 27262, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %36 = icmp eq i32 0, 0
  store i1 %36, i1* %zf
  %37 = icmp slt i32 0, 0
  store i1 %37, i1* %sf
  %38 = trunc i32 0 to i8
  %39 = call i8 @llvm.ctpop.i8(i8 %38)
  %40 = and i8 %39, 1
  %41 = icmp eq i8 %40, 0
  store i1 %41, i1* %pf
  %42 = zext i32 0 to i64
  store i64 %42, i64* %rax
  store volatile i64 27264, i64* @assembly_address
  store i32 19, i32* %stack_var_-44
  store volatile i64 27271, i64* @assembly_address
  %43 = load i32* bitcast (i64* @global_var_2160c8 to i32*)
  %44 = zext i32 %43 to i64
  store i64 %44, i64* %rax
  store volatile i64 27277, i64* @assembly_address
  %45 = load i64* %rax
  %46 = trunc i64 %45 to i32
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i32
  %49 = and i32 %46, %48
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %50 = icmp eq i32 %49, 0
  store i1 %50, i1* %zf
  %51 = icmp slt i32 %49, 0
  store i1 %51, i1* %sf
  %52 = trunc i32 %49 to i8
  %53 = call i8 @llvm.ctpop.i8(i8 %52)
  %54 = and i8 %53, 1
  %55 = icmp eq i8 %54, 0
  store i1 %55, i1* %pf
  store volatile i64 27279, i64* @assembly_address
  %56 = load i1* %zf
  br i1 %56, label %block_6b0b, label %block_6a91

block_6a91:                                       ; preds = %block_6a63
  store volatile i64 27281, i64* @assembly_address
  %57 = load i32* %stack_var_-64
  %58 = and i32 %57, 15
  %59 = icmp ugt i32 %58, 15
  %60 = icmp ult i32 %57, 0
  %61 = xor i32 %57, 0
  %62 = and i32 %61, 0
  %63 = icmp slt i32 %62, 0
  store i1 %59, i1* %az
  store i1 %60, i1* %cf
  store i1 %63, i1* %of
  %64 = icmp eq i32 %57, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i32 %57, 0
  store i1 %65, i1* %sf
  %66 = trunc i32 %57 to i8
  %67 = call i8 @llvm.ctpop.i8(i8 %66)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  store volatile i64 27285, i64* @assembly_address
  %70 = load i1* %sf
  br i1 %70, label %block_6b0b, label %block_6a97

block_6a97:                                       ; preds = %block_6a91
  store volatile i64 27287, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_2160c8 to i32*)
  store volatile i64 27297, i64* @assembly_address
  %71 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %72 = zext i32 %71 to i64
  store i64 %72, i64* %rax
  store volatile i64 27303, i64* @assembly_address
  %73 = load i64* %rax
  %74 = trunc i64 %73 to i32
  %75 = load i64* %rax
  %76 = trunc i64 %75 to i32
  %77 = and i32 %74, %76
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %78 = icmp eq i32 %77, 0
  store i1 %78, i1* %zf
  %79 = icmp slt i32 %77, 0
  store i1 %79, i1* %sf
  %80 = trunc i32 %77 to i8
  %81 = call i8 @llvm.ctpop.i8(i8 %80)
  %82 = and i8 %81, 1
  %83 = icmp eq i8 %82, 0
  store i1 %83, i1* %pf
  store volatile i64 27305, i64* @assembly_address
  %84 = load i1* %zf
  br i1 %84, label %block_6abc, label %block_6aab

block_6aab:                                       ; preds = %block_6a97
  store volatile i64 27307, i64* @assembly_address
  store i64 ptrtoint ([29 x i8]* @global_var_1197d to i64), i64* %rdi
  store volatile i64 27314, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 27319, i64* @assembly_address
  %85 = load i64* %rdi
  %86 = inttoptr i64 %85 to i8*
  %87 = call i32 (i8*, ...)* @printf(i8* %86)
  %88 = sext i32 %87 to i64
  store i64 %88, i64* %rax
  %89 = sext i32 %87 to i64
  store i64 %89, i64* %rax
  br label %block_6abc

block_6abc:                                       ; preds = %block_6aab, %block_6a97
  store volatile i64 27324, i64* @assembly_address
  %90 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %91 = zext i32 %90 to i64
  store i64 %91, i64* %rax
  store volatile i64 27330, i64* @assembly_address
  %92 = load i64* %rax
  %93 = trunc i64 %92 to i32
  %94 = load i64* %rax
  %95 = trunc i64 %94 to i32
  %96 = and i32 %93, %95
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %97 = icmp eq i32 %96, 0
  store i1 %97, i1* %zf
  %98 = icmp slt i32 %96, 0
  store i1 %98, i1* %sf
  %99 = trunc i32 %96 to i8
  %100 = call i8 @llvm.ctpop.i8(i8 %99)
  %101 = and i8 %100, 1
  %102 = icmp eq i8 %101, 0
  store i1 %102, i1* %pf
  store volatile i64 27332, i64* @assembly_address
  %103 = load i1* %zf
  %104 = icmp eq i1 %103, false
  br i1 %104, label %block_6bf8, label %block_6aca

block_6aca:                                       ; preds = %block_6abc
  store volatile i64 27338, i64* @assembly_address
  %105 = load i32* %stack_var_-44
  %106 = zext i32 %105 to i64
  store i64 %106, i64* %rdi
  store volatile i64 27341, i64* @assembly_address
  %107 = load i32* %stack_var_-44
  %108 = zext i32 %107 to i64
  store i64 %108, i64* %rsi
  store volatile i64 27344, i64* @assembly_address
  %109 = load i32* %stack_var_-44
  %110 = zext i32 %109 to i64
  store i64 %110, i64* %rdx
  store volatile i64 27347, i64* @assembly_address
  %111 = load i32* %stack_var_-44
  %112 = zext i32 %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 27350, i64* @assembly_address
  %113 = load i64* %rsp
  %114 = sub i64 %113, 8
  %115 = and i64 %113, 15
  %116 = sub i64 %115, 8
  %117 = icmp ugt i64 %116, 15
  %118 = icmp ult i64 %113, 8
  %119 = xor i64 %113, 8
  %120 = xor i64 %113, %114
  %121 = and i64 %119, %120
  %122 = icmp slt i64 %121, 0
  store i1 %117, i1* %az
  store i1 %118, i1* %cf
  store i1 %122, i1* %of
  %123 = icmp eq i64 %114, 0
  store i1 %123, i1* %zf
  %124 = icmp slt i64 %114, 0
  store i1 %124, i1* %sf
  %125 = trunc i64 %114 to i8
  %126 = call i8 @llvm.ctpop.i8(i8 %125)
  %127 = and i8 %126, 1
  %128 = icmp eq i8 %127, 0
  store i1 %128, i1* %pf
  %129 = ptrtoint i64* %stack_var_-80 to i64
  store i64 %129, i64* %rsp
  store volatile i64 27354, i64* @assembly_address
  store i64 ptrtoint ([13 x i8]* @global_var_119ce to i64), i64* %rcx
  store volatile i64 27361, i64* @assembly_address
  %130 = load i64* %rcx
  %131 = inttoptr i64 %130 to i8*
  store i8* %131, i8** %stack_var_-88
  %132 = ptrtoint i8** %stack_var_-88 to i64
  store i64 %132, i64* %rsp
  store volatile i64 27362, i64* @assembly_address
  %133 = load i64* %rdi
  %134 = trunc i64 %133 to i32
  %135 = zext i32 %134 to i64
  store i64 %135, i64* %r9
  store volatile i64 27365, i64* @assembly_address
  %136 = load i64* %rsi
  %137 = trunc i64 %136 to i32
  %138 = zext i32 %137 to i64
  store i64 %138, i64* %r8
  store volatile i64 27368, i64* @assembly_address
  store i64 ptrtoint ([11 x i8]* @global_var_1199a to i64), i64* %rcx
  store volatile i64 27375, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  %141 = zext i32 %140 to i64
  store i64 %141, i64* %rsi
  store volatile i64 27377, i64* @assembly_address
  store i64 ptrtoint ([38 x i8]* @global_var_119a8 to i64), i64* %rdi
  store volatile i64 27384, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 27389, i64* @assembly_address
  %142 = load i64* %rdi
  %143 = inttoptr i64 %142 to i8*
  %144 = load i64* %rsi
  %145 = load i64* %rdx
  %146 = load i64* %rcx
  %147 = inttoptr i64 %146 to i8*
  %148 = load i64* %r8
  %149 = load i64* %r9
  %150 = load i8** %stack_var_-88
  %151 = call i32 (i8*, ...)* @printf(i8* %143, i64 %144, i64 %145, i8* %147, i64 %148, i64 %149, i8* %150)
  %152 = sext i32 %151 to i64
  store i64 %152, i64* %rax
  %153 = sext i32 %151 to i64
  store i64 %153, i64* %rax
  store volatile i64 27394, i64* @assembly_address
  %154 = load i64* %rsp
  %155 = add i64 %154, 16
  %156 = and i64 %154, 15
  %157 = icmp ugt i64 %156, 15
  %158 = icmp ult i64 %155, %154
  %159 = xor i64 %154, %155
  %160 = xor i64 16, %155
  %161 = and i64 %159, %160
  %162 = icmp slt i64 %161, 0
  store i1 %157, i1* %az
  store i1 %158, i1* %cf
  store i1 %162, i1* %of
  %163 = icmp eq i64 %155, 0
  store i1 %163, i1* %zf
  %164 = icmp slt i64 %155, 0
  store i1 %164, i1* %sf
  %165 = trunc i64 %155 to i8
  %166 = call i8 @llvm.ctpop.i8(i8 %165)
  %167 = and i8 %166, 1
  %168 = icmp eq i8 %167, 0
  store i1 %168, i1* %pf
  %169 = ptrtoint i64* %stack_var_-72 to i64
  store i64 %169, i64* %rsp
  store volatile i64 27398, i64* @assembly_address
  br label %block_6bf8

block_6b0b:                                       ; preds = %block_6a91, %block_6a63
  store volatile i64 27403, i64* @assembly_address
  %170 = load i32* %stack_var_-64
  %171 = and i32 %170, 15
  %172 = icmp ugt i32 %171, 15
  %173 = icmp ult i32 %170, 0
  %174 = xor i32 %170, 0
  %175 = and i32 %174, 0
  %176 = icmp slt i32 %175, 0
  store i1 %172, i1* %az
  store i1 %173, i1* %cf
  store i1 %176, i1* %of
  %177 = icmp eq i32 %170, 0
  store i1 %177, i1* %zf
  %178 = icmp slt i32 %170, 0
  store i1 %178, i1* %sf
  %179 = trunc i32 %170 to i8
  %180 = call i8 @llvm.ctpop.i8(i8 %179)
  %181 = and i8 %180, 1
  %182 = icmp eq i8 %181, 0
  store i1 %182, i1* %pf
  store volatile i64 27407, i64* @assembly_address
  %183 = load i1* %sf
  %184 = icmp eq i1 %183, false
  br i1 %184, label %block_6bf8, label %block_6b15

block_6b15:                                       ; preds = %block_6b0b
  store volatile i64 27413, i64* @assembly_address
  %185 = load i64* @global_var_216ae8
  store i64 %185, i64* %rax
  store volatile i64 27420, i64* @assembly_address
  %186 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %187 = icmp eq i64 %186, 0
  store i1 %187, i1* %zf
  %188 = icmp slt i64 %186, 0
  store i1 %188, i1* %sf
  %189 = trunc i64 %186 to i8
  %190 = call i8 @llvm.ctpop.i8(i8 %189)
  %191 = and i8 %190, 1
  %192 = icmp eq i8 %191, 0
  store i1 %192, i1* %pf
  store volatile i64 27423, i64* @assembly_address
  %193 = load i1* %zf
  %194 = load i1* %sf
  %195 = load i1* %of
  %196 = icmp ne i1 %194, %195
  %197 = or i1 %193, %196
  br i1 %197, label %block_6f19, label %block_6b25

block_6b25:                                       ; preds = %block_6b15
  store volatile i64 27429, i64* @assembly_address
  %198 = load i64* @global_var_216af0
  store i64 %198, i64* %rax
  store volatile i64 27436, i64* @assembly_address
  %199 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %200 = icmp eq i64 %199, 0
  store i1 %200, i1* %zf
  %201 = icmp slt i64 %199, 0
  store i1 %201, i1* %sf
  %202 = trunc i64 %199 to i8
  %203 = call i8 @llvm.ctpop.i8(i8 %202)
  %204 = and i8 %203, 1
  %205 = icmp eq i8 %204, 0
  store i1 %205, i1* %pf
  store volatile i64 27439, i64* @assembly_address
  %206 = load i1* %zf
  %207 = load i1* %sf
  %208 = load i1* %of
  %209 = icmp ne i1 %207, %208
  %210 = or i1 %206, %209
  br i1 %210, label %block_6f19, label %block_6b35

block_6b35:                                       ; preds = %block_6b25
  store volatile i64 27445, i64* @assembly_address
  %211 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %212 = zext i32 %211 to i64
  store i64 %212, i64* %rax
  store volatile i64 27451, i64* @assembly_address
  %213 = load i64* %rax
  %214 = trunc i64 %213 to i32
  %215 = load i64* %rax
  %216 = trunc i64 %215 to i32
  %217 = and i32 %214, %216
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %218 = icmp eq i32 %217, 0
  store i1 %218, i1* %zf
  %219 = icmp slt i32 %217, 0
  store i1 %219, i1* %sf
  %220 = trunc i32 %217 to i8
  %221 = call i8 @llvm.ctpop.i8(i8 %220)
  %222 = and i8 %221, 1
  %223 = icmp eq i8 %222, 0
  store i1 %223, i1* %pf
  store volatile i64 27453, i64* @assembly_address
  %224 = load i1* %zf
  br i1 %224, label %block_6b50, label %block_6b3f

block_6b3f:                                       ; preds = %block_6b35
  store volatile i64 27455, i64* @assembly_address
  store i64 ptrtoint ([29 x i8]* @global_var_119db to i64), i64* %rdi
  store volatile i64 27462, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 27467, i64* @assembly_address
  %225 = load i64* %rdi
  %226 = inttoptr i64 %225 to i8*
  %227 = call i32 (i8*, ...)* @printf(i8* %226)
  %228 = sext i32 %227 to i64
  store i64 %228, i64* %rax
  %229 = sext i32 %227 to i64
  store i64 %229, i64* %rax
  br label %block_6b50

block_6b50:                                       ; preds = %block_6b3f, %block_6b35
  store volatile i64 27472, i64* @assembly_address
  %230 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %231 = zext i32 %230 to i64
  store i64 %231, i64* %rax
  store volatile i64 27478, i64* @assembly_address
  %232 = load i64* %rax
  %233 = trunc i64 %232 to i32
  %234 = load i64* %rax
  %235 = trunc i64 %234 to i32
  %236 = and i32 %233, %235
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %237 = icmp eq i32 %236, 0
  store i1 %237, i1* %zf
  %238 = icmp slt i32 %236, 0
  store i1 %238, i1* %sf
  %239 = trunc i32 %236 to i8
  %240 = call i8 @llvm.ctpop.i8(i8 %239)
  %241 = and i8 %240, 1
  %242 = icmp eq i8 %241, 0
  store i1 %242, i1* %pf
  store volatile i64 27480, i64* @assembly_address
  %243 = load i1* %zf
  %244 = icmp eq i1 %243, false
  br i1 %244, label %block_6b64, label %block_6b5a

block_6b5a:                                       ; preds = %block_6b50
  store volatile i64 27482, i64* @assembly_address
  %245 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %246 = zext i32 %245 to i64
  store i64 %246, i64* %rax
  store volatile i64 27488, i64* @assembly_address
  %247 = load i64* %rax
  %248 = trunc i64 %247 to i32
  %249 = load i64* %rax
  %250 = trunc i64 %249 to i32
  %251 = and i32 %248, %250
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %252 = icmp eq i32 %251, 0
  store i1 %252, i1* %zf
  %253 = icmp slt i32 %251, 0
  store i1 %253, i1* %sf
  %254 = trunc i32 %251 to i8
  %255 = call i8 @llvm.ctpop.i8(i8 %254)
  %256 = and i8 %255, 1
  %257 = icmp eq i8 %256, 0
  store i1 %257, i1* %pf
  store volatile i64 27490, i64* @assembly_address
  %258 = load i1* %zf
  %259 = icmp eq i1 %258, false
  br i1 %259, label %block_6bb0, label %block_6b64

block_6b64:                                       ; preds = %block_6b5a, %block_6b50
  store volatile i64 27492, i64* @assembly_address
  %260 = load i64* @global_var_216ae8
  store i64 %260, i64* %rcx
  store volatile i64 27499, i64* @assembly_address
  %261 = load i64* @global_var_216560
  store i64 %261, i64* %rax
  store volatile i64 27506, i64* @assembly_address
  %262 = load i32* %stack_var_-44
  %263 = zext i32 %262 to i64
  store i64 %263, i64* %rdx
  store volatile i64 27509, i64* @assembly_address
  %264 = load i64* %rcx
  store i64 %264, i64* %rsi
  store volatile i64 27512, i64* @assembly_address
  %265 = load i64* %rax
  store i64 %265, i64* %rdi
  store volatile i64 27515, i64* @assembly_address
  %266 = load i64* %rdi
  %267 = load i64* %rsi
  %268 = load i64* %rdx
  %269 = trunc i64 %268 to i32
  %270 = inttoptr i64 %266 to %_IO_FILE*
  %271 = call i64 @fprint_off(%_IO_FILE* %270, i64 %267, i32 %269)
  store i64 %271, i64* %rax
  store i64 %271, i64* %rax
  store volatile i64 27520, i64* @assembly_address
  store i64 32, i64* %rdi
  store volatile i64 27525, i64* @assembly_address
  %272 = load i64* %rdi
  %273 = trunc i64 %272 to i32
  %274 = call i32 @putchar(i32 %273)
  %275 = sext i32 %274 to i64
  store i64 %275, i64* %rax
  %276 = sext i32 %274 to i64
  store i64 %276, i64* %rax
  store volatile i64 27530, i64* @assembly_address
  %277 = load i64* @global_var_216af0
  store i64 %277, i64* %rcx
  store volatile i64 27537, i64* @assembly_address
  %278 = load i64* @global_var_216560
  store i64 %278, i64* %rax
  store volatile i64 27544, i64* @assembly_address
  %279 = load i32* %stack_var_-44
  %280 = zext i32 %279 to i64
  store i64 %280, i64* %rdx
  store volatile i64 27547, i64* @assembly_address
  %281 = load i64* %rcx
  store i64 %281, i64* %rsi
  store volatile i64 27550, i64* @assembly_address
  %282 = load i64* %rax
  store i64 %282, i64* %rdi
  store volatile i64 27553, i64* @assembly_address
  %283 = load i64* %rdi
  %284 = load i64* %rsi
  %285 = load i64* %rdx
  %286 = trunc i64 %285 to i32
  %287 = inttoptr i64 %283 to %_IO_FILE*
  %288 = call i64 @fprint_off(%_IO_FILE* %287, i64 %284, i32 %286)
  store i64 %288, i64* %rax
  store i64 %288, i64* %rax
  store volatile i64 27558, i64* @assembly_address
  store i64 32, i64* %rdi
  store volatile i64 27563, i64* @assembly_address
  %289 = load i64* %rdi
  %290 = trunc i64 %289 to i32
  %291 = call i32 @putchar(i32 %290)
  %292 = sext i32 %291 to i64
  store i64 %292, i64* %rax
  %293 = sext i32 %291 to i64
  store i64 %293, i64* %rax
  br label %block_6bb0

block_6bb0:                                       ; preds = %block_6b64, %block_6b5a
  store volatile i64 27568, i64* @assembly_address
  %294 = load i64* @global_var_216560
  store i64 %294, i64* %rdx
  store volatile i64 27575, i64* @assembly_address
  %295 = load i64* @global_var_216af0
  store i64 %295, i64* %rax
  store volatile i64 27582, i64* @assembly_address
  %296 = load i64* @global_var_216af0
  store i64 %296, i64* %rcx
  store volatile i64 27589, i64* @assembly_address
  %297 = load i64* @global_var_216ae8
  store i64 %297, i64* %rdi
  store volatile i64 27596, i64* @assembly_address
  %298 = load i64* @global_var_267540
  store i64 %298, i64* %rsi
  store volatile i64 27603, i64* @assembly_address
  %299 = load i64* %rdi
  %300 = load i64* %rsi
  %301 = sub i64 %299, %300
  %302 = and i64 %299, 15
  %303 = and i64 %300, 15
  %304 = sub i64 %302, %303
  %305 = icmp ugt i64 %304, 15
  %306 = icmp ult i64 %299, %300
  %307 = xor i64 %299, %300
  %308 = xor i64 %299, %301
  %309 = and i64 %307, %308
  %310 = icmp slt i64 %309, 0
  store i1 %305, i1* %az
  store i1 %306, i1* %cf
  store i1 %310, i1* %of
  %311 = icmp eq i64 %301, 0
  store i1 %311, i1* %zf
  %312 = icmp slt i64 %301, 0
  store i1 %312, i1* %sf
  %313 = trunc i64 %301 to i8
  %314 = call i8 @llvm.ctpop.i8(i8 %313)
  %315 = and i8 %314, 1
  %316 = icmp eq i8 %315, 0
  store i1 %316, i1* %pf
  store i64 %301, i64* %rdi
  store volatile i64 27606, i64* @assembly_address
  %317 = load i64* %rdi
  store i64 %317, i64* %rsi
  store volatile i64 27609, i64* @assembly_address
  %318 = load i64* %rcx
  %319 = load i64* %rsi
  %320 = sub i64 %318, %319
  %321 = and i64 %318, 15
  %322 = and i64 %319, 15
  %323 = sub i64 %321, %322
  %324 = icmp ugt i64 %323, 15
  %325 = icmp ult i64 %318, %319
  %326 = xor i64 %318, %319
  %327 = xor i64 %318, %320
  %328 = and i64 %326, %327
  %329 = icmp slt i64 %328, 0
  store i1 %324, i1* %az
  store i1 %325, i1* %cf
  store i1 %329, i1* %of
  %330 = icmp eq i64 %320, 0
  store i1 %330, i1* %zf
  %331 = icmp slt i64 %320, 0
  store i1 %331, i1* %sf
  %332 = trunc i64 %320 to i8
  %333 = call i8 @llvm.ctpop.i8(i8 %332)
  %334 = and i8 %333, 1
  %335 = icmp eq i8 %334, 0
  store i1 %335, i1* %pf
  store i64 %320, i64* %rcx
  store volatile i64 27612, i64* @assembly_address
  %336 = load i64* %rax
  store i64 %336, i64* %rsi
  store volatile i64 27615, i64* @assembly_address
  %337 = load i64* %rcx
  store i64 %337, i64* %rdi
  store volatile i64 27618, i64* @assembly_address
  %338 = load i64* %rdi
  %339 = load i64* %rsi
  %340 = load i64* %rdx
  %341 = inttoptr i64 %340 to %_IO_FILE*
  %342 = call i64 @display_ratio(i64 %338, i64 %339, %_IO_FILE* %341)
  store i64 %342, i64* %rax
  store i64 %342, i64* %rax
  store volatile i64 27623, i64* @assembly_address
  store i64 ptrtoint ([10 x i8]* @global_var_119f8 to i64), i64* %rdi
  store volatile i64 27630, i64* @assembly_address
  %343 = load i64* %rdi
  %344 = inttoptr i64 %343 to i8*
  %345 = call i32 @puts(i8* %344)
  %346 = sext i32 %345 to i64
  store i64 %346, i64* %rax
  %347 = sext i32 %345 to i64
  store i64 %347, i64* %rax
  store volatile i64 27635, i64* @assembly_address
  br label %block_6f1a

block_6bf8:                                       ; preds = %block_6b0b, %block_6aca, %block_6abc
  store volatile i64 27640, i64* @assembly_address
  %348 = trunc i64 -1 to i32
  store i32 %348, i32* %stack_var_-40
  store volatile i64 27648, i64* @assembly_address
  store i64 -1, i64* @global_var_25f4c0
  store volatile i64 27659, i64* @assembly_address
  %349 = load i64* @global_var_24a890
  store i64 %349, i64* %rax
  store volatile i64 27666, i64* @assembly_address
  %350 = load i64* %rax
  store i64 %350, i64* @global_var_21a860
  store volatile i64 27673, i64* @assembly_address
  %351 = load i32* %stack_var_-64
  %352 = sub i32 %351, 8
  %353 = and i32 %351, 15
  %354 = sub i32 %353, 8
  %355 = icmp ugt i32 %354, 15
  %356 = icmp ult i32 %351, 8
  %357 = xor i32 %351, 8
  %358 = xor i32 %351, %352
  %359 = and i32 %357, %358
  %360 = icmp slt i32 %359, 0
  store i1 %355, i1* %az
  store i1 %356, i1* %cf
  store i1 %360, i1* %of
  %361 = icmp eq i32 %352, 0
  store i1 %361, i1* %zf
  %362 = icmp slt i32 %352, 0
  store i1 %362, i1* %sf
  %363 = trunc i32 %352 to i8
  %364 = call i8 @llvm.ctpop.i8(i8 %363)
  %365 = and i8 %364, 1
  %366 = icmp eq i8 %365, 0
  store i1 %366, i1* %pf
  store volatile i64 27677, i64* @assembly_address
  %367 = load i1* %zf
  %368 = icmp eq i1 %367, false
  br i1 %368, label %block_6d01, label %block_6c23

block_6c23:                                       ; preds = %block_6bf8
  store volatile i64 27683, i64* @assembly_address
  %369 = load i32* bitcast (i64* @global_var_21661c to i32*)
  %370 = zext i32 %369 to i64
  store i64 %370, i64* %rax
  store volatile i64 27689, i64* @assembly_address
  %371 = load i64* %rax
  %372 = trunc i64 %371 to i32
  %373 = load i64* %rax
  %374 = trunc i64 %373 to i32
  %375 = and i32 %372, %374
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %376 = icmp eq i32 %375, 0
  store i1 %376, i1* %zf
  %377 = icmp slt i32 %375, 0
  store i1 %377, i1* %sf
  %378 = trunc i32 %375 to i8
  %379 = call i8 @llvm.ctpop.i8(i8 %378)
  %380 = and i8 %379, 1
  %381 = icmp eq i8 %380, 0
  store i1 %381, i1* %pf
  store volatile i64 27691, i64* @assembly_address
  %382 = load i1* %zf
  %383 = icmp eq i1 %382, false
  br i1 %383, label %block_6d01, label %block_6c31

block_6c31:                                       ; preds = %block_6c23
  store volatile i64 27697, i64* @assembly_address
  %384 = load i32* %stack_var_-60
  %385 = zext i32 %384 to i64
  store i64 %385, i64* %rax
  store volatile i64 27700, i64* @assembly_address
  store i64 2, i64* %rdx
  store volatile i64 27705, i64* @assembly_address
  store i64 -8, i64* %rsi
  store volatile i64 27712, i64* @assembly_address
  %386 = load i64* %rax
  %387 = trunc i64 %386 to i32
  %388 = zext i32 %387 to i64
  store i64 %388, i64* %rdi
  store volatile i64 27714, i64* @assembly_address
  %389 = load i64* %rdi
  %390 = trunc i64 %389 to i32
  %391 = load i64* %rsi
  %392 = trunc i64 %391 to i32
  %393 = load i64* %rdx
  %394 = trunc i64 %393 to i32
  %395 = call i32 @lseek(i32 %390, i32 %392, i32 %394)
  %396 = sext i32 %395 to i64
  store i64 %396, i64* %rax
  %397 = sext i32 %395 to i64
  store i64 %397, i64* %rax
  store volatile i64 27719, i64* @assembly_address
  %398 = load i64* %rax
  store i64 %398, i64* @global_var_21a860
  store volatile i64 27726, i64* @assembly_address
  %399 = load i64* @global_var_21a860
  store i64 %399, i64* %rax
  store volatile i64 27733, i64* @assembly_address
  %400 = load i64* %rax
  %401 = sub i64 %400, -1
  %402 = and i64 %400, 15
  %403 = sub i64 %402, 15
  %404 = icmp ugt i64 %403, 15
  %405 = icmp ult i64 %400, -1
  %406 = xor i64 %400, -1
  %407 = xor i64 %400, %401
  %408 = and i64 %406, %407
  %409 = icmp slt i64 %408, 0
  store i1 %404, i1* %az
  store i1 %405, i1* %cf
  store i1 %409, i1* %of
  %410 = icmp eq i64 %401, 0
  store i1 %410, i1* %zf
  %411 = icmp slt i64 %401, 0
  store i1 %411, i1* %sf
  %412 = trunc i64 %401 to i8
  %413 = call i8 @llvm.ctpop.i8(i8 %412)
  %414 = and i8 %413, 1
  %415 = icmp eq i8 %414, 0
  store i1 %415, i1* %pf
  store volatile i64 27737, i64* @assembly_address
  %416 = load i1* %zf
  br i1 %416, label %block_6d01, label %block_6c5f

block_6c5f:                                       ; preds = %block_6c31
  store volatile i64 27743, i64* @assembly_address
  %417 = load i64* @global_var_21a860
  store i64 %417, i64* %rax
  store volatile i64 27750, i64* @assembly_address
  %418 = load i64* %rax
  %419 = add i64 %418, 8
  %420 = and i64 %418, 15
  %421 = add i64 %420, 8
  %422 = icmp ugt i64 %421, 15
  %423 = icmp ult i64 %419, %418
  %424 = xor i64 %418, %419
  %425 = xor i64 8, %419
  %426 = and i64 %424, %425
  %427 = icmp slt i64 %426, 0
  store i1 %422, i1* %az
  store i1 %423, i1* %cf
  store i1 %427, i1* %of
  %428 = icmp eq i64 %419, 0
  store i1 %428, i1* %zf
  %429 = icmp slt i64 %419, 0
  store i1 %429, i1* %sf
  %430 = trunc i64 %419 to i8
  %431 = call i8 @llvm.ctpop.i8(i8 %430)
  %432 = and i8 %431, 1
  %433 = icmp eq i8 %432, 0
  store i1 %433, i1* %pf
  store i64 %419, i64* %rax
  store volatile i64 27754, i64* @assembly_address
  %434 = load i64* %rax
  store i64 %434, i64* @global_var_21a860
  store volatile i64 27761, i64* @assembly_address
  %435 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %435, i64* %rcx
  store volatile i64 27765, i64* @assembly_address
  %436 = load i32* %stack_var_-60
  %437 = zext i32 %436 to i64
  store i64 %437, i64* %rax
  store volatile i64 27768, i64* @assembly_address
  store i64 8, i64* %rdx
  store volatile i64 27773, i64* @assembly_address
  %438 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %438, i64* %rsi
  store volatile i64 27776, i64* @assembly_address
  %439 = load i64* %rax
  %440 = trunc i64 %439 to i32
  %441 = zext i32 %440 to i64
  store i64 %441, i64* %rdi
  store volatile i64 27778, i64* @assembly_address
  %442 = load i64* %rdi
  %443 = trunc i64 %442 to i32
  %444 = load i64* %rsi
  %445 = inttoptr i64 %444 to i64*
  %446 = load i64* %rdx
  %447 = trunc i64 %446 to i32
  %448 = call i32 @read(i32 %443, i64* %445, i32 %447)
  %449 = sext i32 %448 to i64
  store i64 %449, i64* %rax
  %450 = sext i32 %448 to i64
  store i64 %450, i64* %rax
  store volatile i64 27783, i64* @assembly_address
  %451 = load i64* %rax
  %452 = sub i64 %451, 8
  %453 = and i64 %451, 15
  %454 = sub i64 %453, 8
  %455 = icmp ugt i64 %454, 15
  %456 = icmp ult i64 %451, 8
  %457 = xor i64 %451, 8
  %458 = xor i64 %451, %452
  %459 = and i64 %457, %458
  %460 = icmp slt i64 %459, 0
  store i1 %455, i1* %az
  store i1 %456, i1* %cf
  store i1 %460, i1* %of
  %461 = icmp eq i64 %452, 0
  store i1 %461, i1* %zf
  %462 = icmp slt i64 %452, 0
  store i1 %462, i1* %sf
  %463 = trunc i64 %452 to i8
  %464 = call i8 @llvm.ctpop.i8(i8 %463)
  %465 = and i8 %464, 1
  %466 = icmp eq i8 %465, 0
  store i1 %466, i1* %pf
  store volatile i64 27787, i64* @assembly_address
  %467 = load i1* %zf
  br i1 %467, label %block_6c92, label %block_6c8d

block_6c8d:                                       ; preds = %block_6c5f
  store volatile i64 27789, i64* @assembly_address
  %468 = call i64 @read_error()
  store i64 %468, i64* %rax
  store i64 %468, i64* %rax
  store i64 %468, i64* %rax
  unreachable

block_6c92:                                       ; preds = %block_6c5f
  store volatile i64 27794, i64* @assembly_address
  %469 = load i32* %stack_var_-24
  %470 = sext i32 %469 to i64
  %471 = trunc i64 %470 to i8
  %472 = zext i8 %471 to i64
  store i64 %472, i64* %rax
  store volatile i64 27798, i64* @assembly_address
  %473 = load i64* %rax
  %474 = trunc i64 %473 to i8
  %475 = zext i8 %474 to i64
  store i64 %475, i64* %rax
  store volatile i64 27801, i64* @assembly_address
  %476 = load i32* %stack_var_-23
  %477 = trunc i32 %476 to i8
  %478 = zext i8 %477 to i64
  store i64 %478, i64* %rdx
  store volatile i64 27805, i64* @assembly_address
  %479 = load i64* %rdx
  %480 = trunc i64 %479 to i8
  %481 = zext i8 %480 to i64
  store i64 %481, i64* %rdx
  store volatile i64 27808, i64* @assembly_address
  %482 = load i64* %rdx
  %483 = trunc i64 %482 to i32
  %484 = load i1* %of
  %485 = shl i32 %483, 8
  %486 = icmp eq i32 %485, 0
  store i1 %486, i1* %zf
  %487 = icmp slt i32 %485, 0
  store i1 %487, i1* %sf
  %488 = trunc i32 %485 to i8
  %489 = call i8 @llvm.ctpop.i8(i8 %488)
  %490 = and i8 %489, 1
  %491 = icmp eq i8 %490, 0
  store i1 %491, i1* %pf
  %492 = zext i32 %485 to i64
  store i64 %492, i64* %rdx
  %493 = shl i32 %483, 7
  %494 = lshr i32 %493, 31
  %495 = trunc i32 %494 to i1
  store i1 %495, i1* %cf
  %496 = lshr i32 %485, 31
  %497 = icmp ne i32 %496, %494
  %498 = select i1 false, i1 %497, i1 %484
  store i1 %498, i1* %of
  store volatile i64 27811, i64* @assembly_address
  %499 = load i64* %rax
  %500 = trunc i64 %499 to i32
  %501 = load i64* %rdx
  %502 = trunc i64 %501 to i32
  %503 = or i32 %500, %502
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %504 = icmp eq i32 %503, 0
  store i1 %504, i1* %zf
  %505 = icmp slt i32 %503, 0
  store i1 %505, i1* %sf
  %506 = trunc i32 %503 to i8
  %507 = call i8 @llvm.ctpop.i8(i8 %506)
  %508 = and i8 %507, 1
  %509 = icmp eq i8 %508, 0
  store i1 %509, i1* %pf
  %510 = zext i32 %503 to i64
  store i64 %510, i64* %rax
  store volatile i64 27813, i64* @assembly_address
  %511 = load i64* %rax
  %512 = trunc i64 %511 to i32
  %513 = sext i32 %512 to i64
  store i64 %513, i64* %rax
  store volatile i64 27815, i64* @assembly_address
  %514 = load i32* %stack_var_-22
  %515 = trunc i32 %514 to i8
  %516 = zext i8 %515 to i64
  store i64 %516, i64* %rdx
  store volatile i64 27819, i64* @assembly_address
  %517 = load i64* %rdx
  %518 = trunc i64 %517 to i8
  %519 = zext i8 %518 to i64
  store i64 %519, i64* %rdx
  store volatile i64 27822, i64* @assembly_address
  %520 = load i32* %stack_var_-21
  %521 = trunc i32 %520 to i8
  %522 = zext i8 %521 to i64
  store i64 %522, i64* %rcx
  store volatile i64 27826, i64* @assembly_address
  %523 = load i64* %rcx
  %524 = trunc i64 %523 to i8
  %525 = zext i8 %524 to i64
  store i64 %525, i64* %rcx
  store volatile i64 27829, i64* @assembly_address
  %526 = load i64* %rcx
  %527 = trunc i64 %526 to i32
  %528 = load i1* %of
  %529 = shl i32 %527, 8
  %530 = icmp eq i32 %529, 0
  store i1 %530, i1* %zf
  %531 = icmp slt i32 %529, 0
  store i1 %531, i1* %sf
  %532 = trunc i32 %529 to i8
  %533 = call i8 @llvm.ctpop.i8(i8 %532)
  %534 = and i8 %533, 1
  %535 = icmp eq i8 %534, 0
  store i1 %535, i1* %pf
  %536 = zext i32 %529 to i64
  store i64 %536, i64* %rcx
  %537 = shl i32 %527, 7
  %538 = lshr i32 %537, 31
  %539 = trunc i32 %538 to i1
  store i1 %539, i1* %cf
  %540 = lshr i32 %529, 31
  %541 = icmp ne i32 %540, %538
  %542 = select i1 false, i1 %541, i1 %528
  store i1 %542, i1* %of
  store volatile i64 27832, i64* @assembly_address
  %543 = load i64* %rdx
  %544 = trunc i64 %543 to i32
  %545 = load i64* %rcx
  %546 = trunc i64 %545 to i32
  %547 = or i32 %544, %546
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %548 = icmp eq i32 %547, 0
  store i1 %548, i1* %zf
  %549 = icmp slt i32 %547, 0
  store i1 %549, i1* %sf
  %550 = trunc i32 %547 to i8
  %551 = call i8 @llvm.ctpop.i8(i8 %550)
  %552 = and i8 %551, 1
  %553 = icmp eq i8 %552, 0
  store i1 %553, i1* %pf
  %554 = zext i32 %547 to i64
  store i64 %554, i64* %rdx
  store volatile i64 27834, i64* @assembly_address
  %555 = load i64* %rdx
  %556 = trunc i64 %555 to i32
  %557 = sext i32 %556 to i64
  store i64 %557, i64* %rdx
  store volatile i64 27837, i64* @assembly_address
  %558 = load i64* %rdx
  %559 = load i1* %of
  %560 = shl i64 %558, 16
  %561 = icmp eq i64 %560, 0
  store i1 %561, i1* %zf
  %562 = icmp slt i64 %560, 0
  store i1 %562, i1* %sf
  %563 = trunc i64 %560 to i8
  %564 = call i8 @llvm.ctpop.i8(i8 %563)
  %565 = and i8 %564, 1
  %566 = icmp eq i8 %565, 0
  store i1 %566, i1* %pf
  store i64 %560, i64* %rdx
  %567 = shl i64 %558, 15
  %568 = lshr i64 %567, 63
  %569 = trunc i64 %568 to i1
  store i1 %569, i1* %cf
  %570 = lshr i64 %560, 63
  %571 = icmp ne i64 %570, %568
  %572 = select i1 false, i1 %571, i1 %559
  store i1 %572, i1* %of
  store volatile i64 27841, i64* @assembly_address
  %573 = load i64* %rax
  %574 = load i64* %rdx
  %575 = or i64 %573, %574
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %576 = icmp eq i64 %575, 0
  store i1 %576, i1* %zf
  %577 = icmp slt i64 %575, 0
  store i1 %577, i1* %sf
  %578 = trunc i64 %575 to i8
  %579 = call i8 @llvm.ctpop.i8(i8 %578)
  %580 = and i8 %579, 1
  %581 = icmp eq i8 %580, 0
  store i1 %581, i1* %pf
  store i64 %575, i64* %rax
  store volatile i64 27844, i64* @assembly_address
  %582 = load i64* %rax
  %583 = trunc i64 %582 to i32
  store i32 %583, i32* %stack_var_-40
  store volatile i64 27848, i64* @assembly_address
  %584 = load i32* %stack_var_-20
  %585 = trunc i32 %584 to i8
  %586 = zext i8 %585 to i64
  store i64 %586, i64* %rax
  store volatile i64 27852, i64* @assembly_address
  %587 = load i64* %rax
  %588 = trunc i64 %587 to i8
  %589 = zext i8 %588 to i64
  store i64 %589, i64* %rax
  store volatile i64 27855, i64* @assembly_address
  %590 = load i32* %stack_var_-19
  %591 = trunc i32 %590 to i8
  %592 = zext i8 %591 to i64
  store i64 %592, i64* %rdx
  store volatile i64 27859, i64* @assembly_address
  %593 = load i64* %rdx
  %594 = trunc i64 %593 to i8
  %595 = zext i8 %594 to i64
  store i64 %595, i64* %rdx
  store volatile i64 27862, i64* @assembly_address
  %596 = load i64* %rdx
  %597 = trunc i64 %596 to i32
  %598 = load i1* %of
  %599 = shl i32 %597, 8
  %600 = icmp eq i32 %599, 0
  store i1 %600, i1* %zf
  %601 = icmp slt i32 %599, 0
  store i1 %601, i1* %sf
  %602 = trunc i32 %599 to i8
  %603 = call i8 @llvm.ctpop.i8(i8 %602)
  %604 = and i8 %603, 1
  %605 = icmp eq i8 %604, 0
  store i1 %605, i1* %pf
  %606 = zext i32 %599 to i64
  store i64 %606, i64* %rdx
  %607 = shl i32 %597, 7
  %608 = lshr i32 %607, 31
  %609 = trunc i32 %608 to i1
  store i1 %609, i1* %cf
  %610 = lshr i32 %599, 31
  %611 = icmp ne i32 %610, %608
  %612 = select i1 false, i1 %611, i1 %598
  store i1 %612, i1* %of
  store volatile i64 27865, i64* @assembly_address
  %613 = load i64* %rax
  %614 = trunc i64 %613 to i32
  %615 = load i64* %rdx
  %616 = trunc i64 %615 to i32
  %617 = or i32 %614, %616
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %618 = icmp eq i32 %617, 0
  store i1 %618, i1* %zf
  %619 = icmp slt i32 %617, 0
  store i1 %619, i1* %sf
  %620 = trunc i32 %617 to i8
  %621 = call i8 @llvm.ctpop.i8(i8 %620)
  %622 = and i8 %621, 1
  %623 = icmp eq i8 %622, 0
  store i1 %623, i1* %pf
  %624 = zext i32 %617 to i64
  store i64 %624, i64* %rax
  store volatile i64 27867, i64* @assembly_address
  %625 = load i64* %rax
  %626 = trunc i64 %625 to i32
  %627 = sext i32 %626 to i64
  store i64 %627, i64* %rax
  store volatile i64 27869, i64* @assembly_address
  %628 = load i32* %stack_var_-18
  %629 = trunc i32 %628 to i8
  %630 = zext i8 %629 to i64
  store i64 %630, i64* %rdx
  store volatile i64 27873, i64* @assembly_address
  %631 = load i64* %rdx
  %632 = trunc i64 %631 to i8
  %633 = zext i8 %632 to i64
  store i64 %633, i64* %rdx
  store volatile i64 27876, i64* @assembly_address
  %634 = load i32* %stack_var_-17
  %635 = trunc i32 %634 to i8
  %636 = zext i8 %635 to i64
  store i64 %636, i64* %rcx
  store volatile i64 27880, i64* @assembly_address
  %637 = load i64* %rcx
  %638 = trunc i64 %637 to i8
  %639 = zext i8 %638 to i64
  store i64 %639, i64* %rcx
  store volatile i64 27883, i64* @assembly_address
  %640 = load i64* %rcx
  %641 = trunc i64 %640 to i32
  %642 = load i1* %of
  %643 = shl i32 %641, 8
  %644 = icmp eq i32 %643, 0
  store i1 %644, i1* %zf
  %645 = icmp slt i32 %643, 0
  store i1 %645, i1* %sf
  %646 = trunc i32 %643 to i8
  %647 = call i8 @llvm.ctpop.i8(i8 %646)
  %648 = and i8 %647, 1
  %649 = icmp eq i8 %648, 0
  store i1 %649, i1* %pf
  %650 = zext i32 %643 to i64
  store i64 %650, i64* %rcx
  %651 = shl i32 %641, 7
  %652 = lshr i32 %651, 31
  %653 = trunc i32 %652 to i1
  store i1 %653, i1* %cf
  %654 = lshr i32 %643, 31
  %655 = icmp ne i32 %654, %652
  %656 = select i1 false, i1 %655, i1 %642
  store i1 %656, i1* %of
  store volatile i64 27886, i64* @assembly_address
  %657 = load i64* %rdx
  %658 = trunc i64 %657 to i32
  %659 = load i64* %rcx
  %660 = trunc i64 %659 to i32
  %661 = or i32 %658, %660
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %662 = icmp eq i32 %661, 0
  store i1 %662, i1* %zf
  %663 = icmp slt i32 %661, 0
  store i1 %663, i1* %sf
  %664 = trunc i32 %661 to i8
  %665 = call i8 @llvm.ctpop.i8(i8 %664)
  %666 = and i8 %665, 1
  %667 = icmp eq i8 %666, 0
  store i1 %667, i1* %pf
  %668 = zext i32 %661 to i64
  store i64 %668, i64* %rdx
  store volatile i64 27888, i64* @assembly_address
  %669 = load i64* %rdx
  %670 = trunc i64 %669 to i32
  %671 = sext i32 %670 to i64
  store i64 %671, i64* %rdx
  store volatile i64 27891, i64* @assembly_address
  %672 = load i64* %rdx
  %673 = load i1* %of
  %674 = shl i64 %672, 16
  %675 = icmp eq i64 %674, 0
  store i1 %675, i1* %zf
  %676 = icmp slt i64 %674, 0
  store i1 %676, i1* %sf
  %677 = trunc i64 %674 to i8
  %678 = call i8 @llvm.ctpop.i8(i8 %677)
  %679 = and i8 %678, 1
  %680 = icmp eq i8 %679, 0
  store i1 %680, i1* %pf
  store i64 %674, i64* %rdx
  %681 = shl i64 %672, 15
  %682 = lshr i64 %681, 63
  %683 = trunc i64 %682 to i1
  store i1 %683, i1* %cf
  %684 = lshr i64 %674, 63
  %685 = icmp ne i64 %684, %682
  %686 = select i1 false, i1 %685, i1 %673
  store i1 %686, i1* %of
  store volatile i64 27895, i64* @assembly_address
  %687 = load i64* %rax
  %688 = load i64* %rdx
  %689 = or i64 %687, %688
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %690 = icmp eq i64 %689, 0
  store i1 %690, i1* %zf
  %691 = icmp slt i64 %689, 0
  store i1 %691, i1* %sf
  %692 = trunc i64 %689 to i8
  %693 = call i8 @llvm.ctpop.i8(i8 %692)
  %694 = and i8 %693, 1
  %695 = icmp eq i8 %694, 0
  store i1 %695, i1* %pf
  store i64 %689, i64* %rax
  store volatile i64 27898, i64* @assembly_address
  %696 = load i64* %rax
  store i64 %696, i64* @global_var_25f4c0
  br label %block_6d01

block_6d01:                                       ; preds = %block_6c92, %block_6c31, %block_6c23, %block_6bf8
  store volatile i64 27905, i64* @assembly_address
  %697 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %698 = zext i32 %697 to i64
  store i64 %698, i64* %rax
  store volatile i64 27911, i64* @assembly_address
  %699 = load i64* %rax
  %700 = trunc i64 %699 to i32
  %701 = load i64* %rax
  %702 = trunc i64 %701 to i32
  %703 = and i32 %700, %702
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %704 = icmp eq i32 %703, 0
  store i1 %704, i1* %zf
  %705 = icmp slt i32 %703, 0
  store i1 %705, i1* %sf
  %706 = trunc i32 %703 to i8
  %707 = call i8 @llvm.ctpop.i8(i8 %706)
  %708 = and i8 %707, 1
  %709 = icmp eq i8 %708, 0
  store i1 %709, i1* %pf
  store volatile i64 27913, i64* @assembly_address
  %710 = load i1* %zf
  br i1 %710, label %block_6db2, label %block_6d0f

block_6d0f:                                       ; preds = %block_6d01
  store volatile i64 27919, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4d0 to i64), i64* %rdi
  store volatile i64 27926, i64* @assembly_address
  %711 = load i64* %rdi
  %712 = inttoptr i64 %711 to i32*
  %713 = call %tm* @localtime(i32* %712)
  %714 = ptrtoint %tm* %713 to i64
  store i64 %714, i64* %rax
  %715 = ptrtoint %tm* %713 to i64
  store i64 %715, i64* %rax
  store volatile i64 27931, i64* @assembly_address
  %716 = load i64* %rax
  store i64 %716, i64* %stack_var_-32
  store volatile i64 27935, i64* @assembly_address
  %717 = load i32* %stack_var_-64
  %718 = zext i32 %717 to i64
  store i64 %718, i64* %rax
  store volatile i64 27938, i64* @assembly_address
  %719 = load i64* %rax
  %720 = trunc i64 %719 to i32
  %721 = sext i32 %720 to i64
  store i64 %721, i64* %rax
  store volatile i64 27940, i64* @assembly_address
  %722 = load i64* %rax
  %723 = mul i64 %722, 8
  store i64 %723, i64* %rdx
  store volatile i64 27948, i64* @assembly_address
  store i64 ptrtoint ([9 x i8*]* @global_var_215b00 to i64), i64* %rax
  store volatile i64 27955, i64* @assembly_address
  %724 = load i64* %rdx
  %725 = load i64* %rax
  %726 = mul i64 %725, 1
  %727 = add i64 %724, %726
  %728 = inttoptr i64 %727 to i64*
  %729 = load i64* %728
  store i64 %729, i64* %rax
  store volatile i64 27959, i64* @assembly_address
  %730 = load i32* %stack_var_-40
  %731 = sext i32 %730 to i64
  store i64 %731, i64* %rdx
  store volatile i64 27963, i64* @assembly_address
  %732 = load i64* %rax
  store i64 %732, i64* %rsi
  store volatile i64 27966, i64* @assembly_address
  store i64 ptrtoint ([11 x i8]* @global_var_11a02 to i64), i64* %rdi
  store volatile i64 27973, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 27978, i64* @assembly_address
  %733 = load i64* %rdi
  %734 = inttoptr i64 %733 to i8*
  %735 = load i64* %rsi
  %736 = inttoptr i64 %735 to i8*
  %737 = load i64* %rdx
  %738 = trunc i64 %737 to i32
  %739 = call i32 (i8*, ...)* @printf(i8* %734, i8* %736, i32 %738)
  %740 = sext i32 %739 to i64
  store i64 %740, i64* %rax
  %741 = sext i32 %739 to i64
  store i64 %741, i64* %rax
  store volatile i64 27983, i64* @assembly_address
  %742 = load i64* %stack_var_-32
  %743 = and i64 %742, 15
  %744 = icmp ugt i64 %743, 15
  %745 = icmp ult i64 %742, 0
  %746 = xor i64 %742, 0
  %747 = and i64 %746, 0
  %748 = icmp slt i64 %747, 0
  store i1 %744, i1* %az
  store i1 %745, i1* %cf
  store i1 %748, i1* %of
  %749 = icmp eq i64 %742, 0
  store i1 %749, i1* %zf
  %750 = icmp slt i64 %742, 0
  store i1 %750, i1* %sf
  %751 = trunc i64 %742 to i8
  %752 = call i8 @llvm.ctpop.i8(i8 %751)
  %753 = and i8 %752, 1
  %754 = icmp eq i8 %753, 0
  store i1 %754, i1* %pf
  store volatile i64 27988, i64* @assembly_address
  %755 = load i1* %zf
  br i1 %755, label %block_6da1, label %block_6d56

block_6d56:                                       ; preds = %block_6d0f
  store volatile i64 27990, i64* @assembly_address
  %756 = load i64* %stack_var_-32
  store i64 %756, i64* %rax
  store volatile i64 27994, i64* @assembly_address
  %757 = load i64* %rax
  %758 = add i64 %757, 4
  %759 = inttoptr i64 %758 to i32*
  %760 = load i32* %759
  %761 = zext i32 %760 to i64
  store i64 %761, i64* %rdi
  store volatile i64 27997, i64* @assembly_address
  %762 = load i64* %stack_var_-32
  store i64 %762, i64* %rax
  store volatile i64 28001, i64* @assembly_address
  %763 = load i64* %rax
  %764 = add i64 %763, 8
  %765 = inttoptr i64 %764 to i32*
  %766 = load i32* %765
  %767 = zext i32 %766 to i64
  store i64 %767, i64* %rdx
  store volatile i64 28004, i64* @assembly_address
  %768 = load i64* %stack_var_-32
  store i64 %768, i64* %rax
  store volatile i64 28008, i64* @assembly_address
  %769 = load i64* %rax
  %770 = add i64 %769, 12
  %771 = inttoptr i64 %770 to i32*
  %772 = load i32* %771
  %773 = zext i32 %772 to i64
  store i64 %773, i64* %rax
  store volatile i64 28011, i64* @assembly_address
  %774 = load i64* %stack_var_-32
  store i64 %774, i64* %rcx
  store volatile i64 28015, i64* @assembly_address
  %775 = load i64* %rcx
  %776 = add i64 %775, 16
  %777 = inttoptr i64 %776 to i32*
  %778 = load i32* %777
  %779 = zext i32 %778 to i64
  store i64 %779, i64* %rcx
  store volatile i64 28018, i64* @assembly_address
  %780 = load i64* %rcx
  %781 = trunc i64 %780 to i32
  %782 = sext i32 %781 to i64
  store i64 %782, i64* %rcx
  store volatile i64 28021, i64* @assembly_address
  %783 = load i64* %rcx
  %784 = mul i64 %783, 4
  store i64 %784, i64* %rsi
  store volatile i64 28029, i64* @assembly_address
  store i64 ptrtoint ([4 x i8]* @global_var_12060 to i64), i64* %rcx
  store volatile i64 28036, i64* @assembly_address
  %785 = load i64* %rsi
  %786 = load i64* %rcx
  %787 = add i64 %785, %786
  %788 = and i64 %785, 15
  %789 = and i64 %786, 15
  %790 = add i64 %788, %789
  %791 = icmp ugt i64 %790, 15
  %792 = icmp ult i64 %787, %785
  %793 = xor i64 %785, %787
  %794 = xor i64 %786, %787
  %795 = and i64 %793, %794
  %796 = icmp slt i64 %795, 0
  store i1 %791, i1* %az
  store i1 %792, i1* %cf
  store i1 %796, i1* %of
  %797 = icmp eq i64 %787, 0
  store i1 %797, i1* %zf
  %798 = icmp slt i64 %787, 0
  store i1 %798, i1* %sf
  %799 = trunc i64 %787 to i8
  %800 = call i8 @llvm.ctpop.i8(i8 %799)
  %801 = and i8 %800, 1
  %802 = icmp eq i8 %801, 0
  store i1 %802, i1* %pf
  store i64 %787, i64* %rsi
  store volatile i64 28039, i64* @assembly_address
  %803 = load i64* %rdi
  %804 = trunc i64 %803 to i32
  %805 = zext i32 %804 to i64
  store i64 %805, i64* %r8
  store volatile i64 28042, i64* @assembly_address
  %806 = load i64* %rdx
  %807 = trunc i64 %806 to i32
  %808 = zext i32 %807 to i64
  store i64 %808, i64* %rcx
  store volatile i64 28044, i64* @assembly_address
  %809 = load i64* %rax
  %810 = trunc i64 %809 to i32
  %811 = zext i32 %810 to i64
  store i64 %811, i64* %rdx
  store volatile i64 28046, i64* @assembly_address
  store i64 ptrtoint ([17 x i8]* @global_var_11a0d to i64), i64* %rdi
  store volatile i64 28053, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 28058, i64* @assembly_address
  %812 = load i64* %rdi
  %813 = inttoptr i64 %812 to i8*
  %814 = load i64* %rsi
  %815 = inttoptr i64 %814 to i8*
  %816 = load i64* %rdx
  %817 = load i64* %rcx
  %818 = load i64* %r8
  %819 = call i32 (i8*, ...)* @printf(i8* %813, i8* %815, i64 %816, i64 %817, i64 %818)
  %820 = sext i32 %819 to i64
  store i64 %820, i64* %rax
  %821 = sext i32 %819 to i64
  store i64 %821, i64* %rax
  store volatile i64 28063, i64* @assembly_address
  br label %block_6db2

block_6da1:                                       ; preds = %block_6d0f
  store volatile i64 28065, i64* @assembly_address
  store i64 ptrtoint ([14 x i8]* @global_var_11a1e to i64), i64* %rdi
  store volatile i64 28072, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 28077, i64* @assembly_address
  %822 = load i64* %rdi
  %823 = inttoptr i64 %822 to i8*
  %824 = call i32 (i8*, ...)* @printf(i8* %823)
  %825 = sext i32 %824 to i64
  store i64 %825, i64* %rax
  %826 = sext i32 %824 to i64
  store i64 %826, i64* %rax
  br label %block_6db2

block_6db2:                                       ; preds = %block_6da1, %block_6d56, %block_6d01
  store volatile i64 28082, i64* @assembly_address
  %827 = load i64* @global_var_21a860
  store i64 %827, i64* %rcx
  store volatile i64 28089, i64* @assembly_address
  %828 = load i64* @global_var_216560
  store i64 %828, i64* %rax
  store volatile i64 28096, i64* @assembly_address
  %829 = load i32* %stack_var_-44
  %830 = zext i32 %829 to i64
  store i64 %830, i64* %rdx
  store volatile i64 28099, i64* @assembly_address
  %831 = load i64* %rcx
  store i64 %831, i64* %rsi
  store volatile i64 28102, i64* @assembly_address
  %832 = load i64* %rax
  store i64 %832, i64* %rdi
  store volatile i64 28105, i64* @assembly_address
  %833 = load i64* %rdi
  %834 = load i64* %rsi
  %835 = load i64* %rdx
  %836 = trunc i64 %835 to i32
  %837 = inttoptr i64 %833 to %_IO_FILE*
  %838 = call i64 @fprint_off(%_IO_FILE* %837, i64 %834, i32 %836)
  store i64 %838, i64* %rax
  store i64 %838, i64* %rax
  store volatile i64 28110, i64* @assembly_address
  store i64 32, i64* %rdi
  store volatile i64 28115, i64* @assembly_address
  %839 = load i64* %rdi
  %840 = trunc i64 %839 to i32
  %841 = call i32 @putchar(i32 %840)
  %842 = sext i32 %841 to i64
  store i64 %842, i64* %rax
  %843 = sext i32 %841 to i64
  store i64 %843, i64* %rax
  store volatile i64 28120, i64* @assembly_address
  %844 = load i64* @global_var_25f4c0
  store i64 %844, i64* %rcx
  store volatile i64 28127, i64* @assembly_address
  %845 = load i64* @global_var_216560
  store i64 %845, i64* %rax
  store volatile i64 28134, i64* @assembly_address
  %846 = load i32* %stack_var_-44
  %847 = zext i32 %846 to i64
  store i64 %847, i64* %rdx
  store volatile i64 28137, i64* @assembly_address
  %848 = load i64* %rcx
  store i64 %848, i64* %rsi
  store volatile i64 28140, i64* @assembly_address
  %849 = load i64* %rax
  store i64 %849, i64* %rdi
  store volatile i64 28143, i64* @assembly_address
  %850 = load i64* %rdi
  %851 = load i64* %rsi
  %852 = load i64* %rdx
  %853 = trunc i64 %852 to i32
  %854 = inttoptr i64 %850 to %_IO_FILE*
  %855 = call i64 @fprint_off(%_IO_FILE* %854, i64 %851, i32 %853)
  store i64 %855, i64* %rax
  store i64 %855, i64* %rax
  store volatile i64 28148, i64* @assembly_address
  store i64 32, i64* %rdi
  store volatile i64 28153, i64* @assembly_address
  %856 = load i64* %rdi
  %857 = trunc i64 %856 to i32
  %858 = call i32 @putchar(i32 %857)
  %859 = sext i32 %858 to i64
  store i64 %859, i64* %rax
  %860 = sext i32 %858 to i64
  store i64 %860, i64* %rax
  store volatile i64 28158, i64* @assembly_address
  %861 = load i64* @global_var_21a860
  store i64 %861, i64* %rax
  store volatile i64 28165, i64* @assembly_address
  %862 = load i64* %rax
  %863 = sub i64 %862, -1
  %864 = and i64 %862, 15
  %865 = sub i64 %864, 15
  %866 = icmp ugt i64 %865, 15
  %867 = icmp ult i64 %862, -1
  %868 = xor i64 %862, -1
  %869 = xor i64 %862, %863
  %870 = and i64 %868, %869
  %871 = icmp slt i64 %870, 0
  store i1 %866, i1* %az
  store i1 %867, i1* %cf
  store i1 %871, i1* %of
  %872 = icmp eq i64 %863, 0
  store i1 %872, i1* %zf
  %873 = icmp slt i64 %863, 0
  store i1 %873, i1* %sf
  %874 = trunc i64 %863 to i8
  %875 = call i8 @llvm.ctpop.i8(i8 %874)
  %876 = and i8 %875, 1
  %877 = icmp eq i8 %876, 0
  store i1 %877, i1* %pf
  store volatile i64 28169, i64* @assembly_address
  %878 = load i1* %zf
  %879 = icmp eq i1 %878, false
  br i1 %879, label %block_6e3f, label %block_6e0b

block_6e0b:                                       ; preds = %block_6db2
  store volatile i64 28171, i64* @assembly_address
  store i64 -1, i64* @global_var_216ae8
  store volatile i64 28182, i64* @assembly_address
  store i64 0, i64* @global_var_267540
  store volatile i64 28193, i64* @assembly_address
  %880 = load i64* @global_var_267540
  store i64 %880, i64* %rax
  store volatile i64 28200, i64* @assembly_address
  %881 = load i64* %rax
  store i64 %881, i64* @global_var_25f4c0
  store volatile i64 28207, i64* @assembly_address
  %882 = load i64* @global_var_25f4c0
  store i64 %882, i64* %rax
  store volatile i64 28214, i64* @assembly_address
  %883 = load i64* %rax
  store i64 %883, i64* @global_var_21a860
  store volatile i64 28221, i64* @assembly_address
  br label %block_6e63

block_6e3f:                                       ; preds = %block_6db2
  store volatile i64 28223, i64* @assembly_address
  %884 = load i64* @global_var_216ae8
  store i64 %884, i64* %rax
  store volatile i64 28230, i64* @assembly_address
  %885 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %886 = icmp eq i64 %885, 0
  store i1 %886, i1* %zf
  %887 = icmp slt i64 %885, 0
  store i1 %887, i1* %sf
  %888 = trunc i64 %885 to i8
  %889 = call i8 @llvm.ctpop.i8(i8 %888)
  %890 = and i8 %889, 1
  %891 = icmp eq i8 %890, 0
  store i1 %891, i1* %pf
  store volatile i64 28233, i64* @assembly_address
  %892 = load i1* %sf
  br i1 %892, label %block_6e63, label %block_6e4b

block_6e4b:                                       ; preds = %block_6e3f
  store volatile i64 28235, i64* @assembly_address
  %893 = load i64* @global_var_216ae8
  store i64 %893, i64* %rdx
  store volatile i64 28242, i64* @assembly_address
  %894 = load i64* @global_var_21a860
  store i64 %894, i64* %rax
  store volatile i64 28249, i64* @assembly_address
  %895 = load i64* %rax
  %896 = load i64* %rdx
  %897 = add i64 %895, %896
  %898 = and i64 %895, 15
  %899 = and i64 %896, 15
  %900 = add i64 %898, %899
  %901 = icmp ugt i64 %900, 15
  %902 = icmp ult i64 %897, %895
  %903 = xor i64 %895, %897
  %904 = xor i64 %896, %897
  %905 = and i64 %903, %904
  %906 = icmp slt i64 %905, 0
  store i1 %901, i1* %az
  store i1 %902, i1* %cf
  store i1 %906, i1* %of
  %907 = icmp eq i64 %897, 0
  store i1 %907, i1* %zf
  %908 = icmp slt i64 %897, 0
  store i1 %908, i1* %sf
  %909 = trunc i64 %897 to i8
  %910 = call i8 @llvm.ctpop.i8(i8 %909)
  %911 = and i8 %910, 1
  %912 = icmp eq i8 %911, 0
  store i1 %912, i1* %pf
  store i64 %897, i64* %rax
  store volatile i64 28252, i64* @assembly_address
  %913 = load i64* %rax
  store i64 %913, i64* @global_var_216ae8
  br label %block_6e63

block_6e63:                                       ; preds = %block_6e4b, %block_6e3f, %block_6e0b
  store volatile i64 28259, i64* @assembly_address
  %914 = load i64* @global_var_25f4c0
  store i64 %914, i64* %rax
  store volatile i64 28266, i64* @assembly_address
  %915 = load i64* %rax
  %916 = sub i64 %915, -1
  %917 = and i64 %915, 15
  %918 = sub i64 %917, 15
  %919 = icmp ugt i64 %918, 15
  %920 = icmp ult i64 %915, -1
  %921 = xor i64 %915, -1
  %922 = xor i64 %915, %916
  %923 = and i64 %921, %922
  %924 = icmp slt i64 %923, 0
  store i1 %919, i1* %az
  store i1 %920, i1* %cf
  store i1 %924, i1* %of
  %925 = icmp eq i64 %916, 0
  store i1 %925, i1* %zf
  %926 = icmp slt i64 %916, 0
  store i1 %926, i1* %sf
  %927 = trunc i64 %916 to i8
  %928 = call i8 @llvm.ctpop.i8(i8 %927)
  %929 = and i8 %928, 1
  %930 = icmp eq i8 %929, 0
  store i1 %930, i1* %pf
  store volatile i64 28270, i64* @assembly_address
  %931 = load i1* %zf
  %932 = icmp eq i1 %931, false
  br i1 %932, label %block_6ea4, label %block_6e70

block_6e70:                                       ; preds = %block_6e63
  store volatile i64 28272, i64* @assembly_address
  store i64 -1, i64* @global_var_216af0
  store volatile i64 28283, i64* @assembly_address
  store i64 0, i64* @global_var_267540
  store volatile i64 28294, i64* @assembly_address
  %933 = load i64* @global_var_267540
  store i64 %933, i64* %rax
  store volatile i64 28301, i64* @assembly_address
  %934 = load i64* %rax
  store i64 %934, i64* @global_var_25f4c0
  store volatile i64 28308, i64* @assembly_address
  %935 = load i64* @global_var_25f4c0
  store i64 %935, i64* %rax
  store volatile i64 28315, i64* @assembly_address
  %936 = load i64* %rax
  store i64 %936, i64* @global_var_21a860
  store volatile i64 28322, i64* @assembly_address
  br label %block_6ec8

block_6ea4:                                       ; preds = %block_6e63
  store volatile i64 28324, i64* @assembly_address
  %937 = load i64* @global_var_216af0
  store i64 %937, i64* %rax
  store volatile i64 28331, i64* @assembly_address
  %938 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %939 = icmp eq i64 %938, 0
  store i1 %939, i1* %zf
  %940 = icmp slt i64 %938, 0
  store i1 %940, i1* %sf
  %941 = trunc i64 %938 to i8
  %942 = call i8 @llvm.ctpop.i8(i8 %941)
  %943 = and i8 %942, 1
  %944 = icmp eq i8 %943, 0
  store i1 %944, i1* %pf
  store volatile i64 28334, i64* @assembly_address
  %945 = load i1* %sf
  br i1 %945, label %block_6ec8, label %block_6eb0

block_6eb0:                                       ; preds = %block_6ea4
  store volatile i64 28336, i64* @assembly_address
  %946 = load i64* @global_var_216af0
  store i64 %946, i64* %rdx
  store volatile i64 28343, i64* @assembly_address
  %947 = load i64* @global_var_25f4c0
  store i64 %947, i64* %rax
  store volatile i64 28350, i64* @assembly_address
  %948 = load i64* %rax
  %949 = load i64* %rdx
  %950 = add i64 %948, %949
  %951 = and i64 %948, 15
  %952 = and i64 %949, 15
  %953 = add i64 %951, %952
  %954 = icmp ugt i64 %953, 15
  %955 = icmp ult i64 %950, %948
  %956 = xor i64 %948, %950
  %957 = xor i64 %949, %950
  %958 = and i64 %956, %957
  %959 = icmp slt i64 %958, 0
  store i1 %954, i1* %az
  store i1 %955, i1* %cf
  store i1 %959, i1* %of
  %960 = icmp eq i64 %950, 0
  store i1 %960, i1* %zf
  %961 = icmp slt i64 %950, 0
  store i1 %961, i1* %sf
  %962 = trunc i64 %950 to i8
  %963 = call i8 @llvm.ctpop.i8(i8 %962)
  %964 = and i8 %963, 1
  %965 = icmp eq i8 %964, 0
  store i1 %965, i1* %pf
  store i64 %950, i64* %rax
  store volatile i64 28353, i64* @assembly_address
  %966 = load i64* %rax
  store i64 %966, i64* @global_var_216af0
  br label %block_6ec8

block_6ec8:                                       ; preds = %block_6eb0, %block_6ea4, %block_6e70
  store volatile i64 28360, i64* @assembly_address
  %967 = load i64* @global_var_216560
  store i64 %967, i64* %rdx
  store volatile i64 28367, i64* @assembly_address
  %968 = load i64* @global_var_25f4c0
  store i64 %968, i64* %rax
  store volatile i64 28374, i64* @assembly_address
  %969 = load i64* @global_var_25f4c0
  store i64 %969, i64* %rcx
  store volatile i64 28381, i64* @assembly_address
  %970 = load i64* @global_var_21a860
  store i64 %970, i64* %rdi
  store volatile i64 28388, i64* @assembly_address
  %971 = load i64* @global_var_267540
  store i64 %971, i64* %rsi
  store volatile i64 28395, i64* @assembly_address
  %972 = load i64* %rdi
  %973 = load i64* %rsi
  %974 = sub i64 %972, %973
  %975 = and i64 %972, 15
  %976 = and i64 %973, 15
  %977 = sub i64 %975, %976
  %978 = icmp ugt i64 %977, 15
  %979 = icmp ult i64 %972, %973
  %980 = xor i64 %972, %973
  %981 = xor i64 %972, %974
  %982 = and i64 %980, %981
  %983 = icmp slt i64 %982, 0
  store i1 %978, i1* %az
  store i1 %979, i1* %cf
  store i1 %983, i1* %of
  %984 = icmp eq i64 %974, 0
  store i1 %984, i1* %zf
  %985 = icmp slt i64 %974, 0
  store i1 %985, i1* %sf
  %986 = trunc i64 %974 to i8
  %987 = call i8 @llvm.ctpop.i8(i8 %986)
  %988 = and i8 %987, 1
  %989 = icmp eq i8 %988, 0
  store i1 %989, i1* %pf
  store i64 %974, i64* %rdi
  store volatile i64 28398, i64* @assembly_address
  %990 = load i64* %rdi
  store i64 %990, i64* %rsi
  store volatile i64 28401, i64* @assembly_address
  %991 = load i64* %rcx
  %992 = load i64* %rsi
  %993 = sub i64 %991, %992
  %994 = and i64 %991, 15
  %995 = and i64 %992, 15
  %996 = sub i64 %994, %995
  %997 = icmp ugt i64 %996, 15
  %998 = icmp ult i64 %991, %992
  %999 = xor i64 %991, %992
  %1000 = xor i64 %991, %993
  %1001 = and i64 %999, %1000
  %1002 = icmp slt i64 %1001, 0
  store i1 %997, i1* %az
  store i1 %998, i1* %cf
  store i1 %1002, i1* %of
  %1003 = icmp eq i64 %993, 0
  store i1 %1003, i1* %zf
  %1004 = icmp slt i64 %993, 0
  store i1 %1004, i1* %sf
  %1005 = trunc i64 %993 to i8
  %1006 = call i8 @llvm.ctpop.i8(i8 %1005)
  %1007 = and i8 %1006, 1
  %1008 = icmp eq i8 %1007, 0
  store i1 %1008, i1* %pf
  store i64 %993, i64* %rcx
  store volatile i64 28404, i64* @assembly_address
  %1009 = load i64* %rax
  store i64 %1009, i64* %rsi
  store volatile i64 28407, i64* @assembly_address
  %1010 = load i64* %rcx
  store i64 %1010, i64* %rdi
  store volatile i64 28410, i64* @assembly_address
  %1011 = load i64* %rdi
  %1012 = load i64* %rsi
  %1013 = load i64* %rdx
  %1014 = inttoptr i64 %1013 to %_IO_FILE*
  %1015 = call i64 @display_ratio(i64 %1011, i64 %1012, %_IO_FILE* %1014)
  store i64 %1015, i64* %rax
  store i64 %1015, i64* %rax
  store volatile i64 28415, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rsi
  store volatile i64 28422, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11a2c to i64), i64* %rdi
  store volatile i64 28429, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 28434, i64* @assembly_address
  %1016 = load i64* %rdi
  %1017 = inttoptr i64 %1016 to i8*
  %1018 = load i64* %rsi
  %1019 = inttoptr i64 %1018 to i8*
  %1020 = call i32 (i8*, ...)* @printf(i8* %1017, i8* %1019)
  %1021 = sext i32 %1020 to i64
  store i64 %1021, i64* %rax
  %1022 = sext i32 %1020 to i64
  store i64 %1022, i64* %rax
  store volatile i64 28439, i64* @assembly_address
  br label %block_6f1a

block_6f19:                                       ; preds = %block_6b25, %block_6b15
  store volatile i64 28441, i64* @assembly_address
  br label %block_6f1a

block_6f1a:                                       ; preds = %block_6f19, %block_6ec8, %block_6bb0
  store volatile i64 28442, i64* @assembly_address
  %1023 = load i64* %stack_var_-16
  store i64 %1023, i64* %rax
  store volatile i64 28446, i64* @assembly_address
  %1024 = load i64* %rax
  %1025 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %1026 = xor i64 %1024, %1025
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1027 = icmp eq i64 %1026, 0
  store i1 %1027, i1* %zf
  %1028 = icmp slt i64 %1026, 0
  store i1 %1028, i1* %sf
  %1029 = trunc i64 %1026 to i8
  %1030 = call i8 @llvm.ctpop.i8(i8 %1029)
  %1031 = and i8 %1030, 1
  %1032 = icmp eq i8 %1031, 0
  store i1 %1032, i1* %pf
  store i64 %1026, i64* %rax
  store volatile i64 28455, i64* @assembly_address
  %1033 = load i1* %zf
  br i1 %1033, label %block_6f2e, label %block_6f29

block_6f29:                                       ; preds = %block_6f1a
  store volatile i64 28457, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_6f2e:                                       ; preds = %block_6f1a
  store volatile i64 28462, i64* @assembly_address
  %1034 = load i64* %stack_var_-8
  store i64 %1034, i64* %rbp
  %1035 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1035, i64* %rsp
  store volatile i64 28463, i64* @assembly_address
  %1036 = load i64* %rax
  ret i64 %1036
}

declare i64 @182(i64, i32)

declare i64 @183(i64, i64)

define i64 @shorten_name(i8* %arg1) {
block_6f30:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = bitcast i8* %arg1 to i64*
  %1 = ptrtoint i64* %0 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i8*
  %2 = alloca i32
  %stack_var_-16 = alloca i8*
  %3 = alloca i64
  %stack_var_-32 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-24 = alloca i8*
  %4 = alloca i64
  %stack_var_-48 = alloca i8*
  %5 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  %6 = alloca i8*
  %7 = alloca i32
  %8 = alloca i8*
  %9 = alloca i64
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i32
  %13 = alloca i32
  store volatile i64 28464, i64* @assembly_address
  %14 = load i64* %rbp
  store i64 %14, i64* %stack_var_-8
  %15 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %15, i64* %rsp
  store volatile i64 28465, i64* @assembly_address
  %16 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %16, i64* %rbp
  store volatile i64 28468, i64* @assembly_address
  %17 = load i64* %rsp
  %18 = sub i64 %17, 48
  %19 = and i64 %17, 15
  %20 = icmp ugt i64 %19, 15
  %21 = icmp ult i64 %17, 48
  %22 = xor i64 %17, 48
  %23 = xor i64 %17, %18
  %24 = and i64 %22, %23
  %25 = icmp slt i64 %24, 0
  store i1 %20, i1* %az
  store i1 %21, i1* %cf
  store i1 %25, i1* %of
  %26 = icmp eq i64 %18, 0
  store i1 %26, i1* %zf
  %27 = icmp slt i64 %18, 0
  store i1 %27, i1* %sf
  %28 = trunc i64 %18 to i8
  %29 = call i8 @llvm.ctpop.i8(i8 %28)
  %30 = and i8 %29, 1
  %31 = icmp eq i8 %30, 0
  store i1 %31, i1* %pf
  %32 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %32, i64* %rsp
  store volatile i64 28472, i64* @assembly_address
  %33 = load i64* %rdi
  %34 = inttoptr i64 %33 to i8*
  store i8* %34, i8** %stack_var_-48
  store volatile i64 28476, i64* @assembly_address
  %35 = inttoptr i64 0 to i8*
  store i8* %35, i8** %stack_var_-24
  store volatile i64 28484, i64* @assembly_address
  store i32 3, i32* %stack_var_-36
  store volatile i64 28491, i64* @assembly_address
  %36 = load i8** %stack_var_-48
  %37 = ptrtoint i8* %36 to i64
  store i64 %37, i64* %rax
  store volatile i64 28495, i64* @assembly_address
  %38 = load i64* %rax
  store i64 %38, i64* %rdi
  store volatile i64 28498, i64* @assembly_address
  %39 = load i64* %rdi
  %40 = inttoptr i64 %39 to i8*
  %41 = call i32 @strlen(i8* %40)
  %42 = sext i32 %41 to i64
  store i64 %42, i64* %rax
  %43 = sext i32 %41 to i64
  store i64 %43, i64* %rax
  store volatile i64 28503, i64* @assembly_address
  %44 = load i64* %rax
  %45 = trunc i64 %44 to i32
  store i32 %45, i32* %stack_var_-32
  store volatile i64 28506, i64* @assembly_address
  %46 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %47 = zext i32 %46 to i64
  store i64 %47, i64* %rax
  store volatile i64 28512, i64* @assembly_address
  %48 = load i64* %rax
  %49 = trunc i64 %48 to i32
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  %52 = and i32 %49, %51
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %53 = icmp eq i32 %52, 0
  store i1 %53, i1* %zf
  %54 = icmp slt i32 %52, 0
  store i1 %54, i1* %sf
  %55 = trunc i32 %52 to i8
  %56 = call i8 @llvm.ctpop.i8(i8 %55)
  %57 = and i8 %56, 1
  %58 = icmp eq i8 %57, 0
  store i1 %58, i1* %pf
  store volatile i64 28514, i64* @assembly_address
  %59 = load i1* %zf
  br i1 %59, label %block_6f8e, label %block_6f64

block_6f64:                                       ; preds = %block_6f30
  store volatile i64 28516, i64* @assembly_address
  %60 = load i32* %stack_var_-32
  store i32 %60, i32* %13
  store i32 1, i32* %12
  %61 = sub i32 %60, 1
  %62 = and i32 %60, 15
  %63 = sub i32 %62, 1
  %64 = icmp ugt i32 %63, 15
  %65 = icmp ult i32 %60, 1
  %66 = xor i32 %60, 1
  %67 = xor i32 %60, %61
  %68 = and i32 %66, %67
  %69 = icmp slt i32 %68, 0
  store i1 %64, i1* %az
  store i1 %65, i1* %cf
  store i1 %69, i1* %of
  %70 = icmp eq i32 %61, 0
  store i1 %70, i1* %zf
  %71 = icmp slt i32 %61, 0
  store i1 %71, i1* %sf
  %72 = trunc i32 %61 to i8
  %73 = call i8 @llvm.ctpop.i8(i8 %72)
  %74 = and i8 %73, 1
  %75 = icmp eq i8 %74, 0
  store i1 %75, i1* %pf
  store volatile i64 28520, i64* @assembly_address
  %76 = load i32* %13
  %77 = load i32* %12
  %78 = icmp sgt i32 %76, %77
  br i1 %78, label %block_6f76, label %block_6f6a

block_6f6a:                                       ; preds = %block_6f64
  store volatile i64 28522, i64* @assembly_address
  store i64 ptrtoint ([15 x i8]* @global_var_11a31 to i64), i64* %rdi
  store volatile i64 28529, i64* @assembly_address
  %79 = load i64* %rdi
  %80 = inttoptr i64 %79 to i8*
  %81 = call i64 @gzip_error(i8* %80)
  store i64 %81, i64* %rax
  store i64 %81, i64* %rax
  unreachable

block_6f76:                                       ; preds = %block_6f64
  store volatile i64 28534, i64* @assembly_address
  %82 = load i32* %stack_var_-32
  %83 = zext i32 %82 to i64
  store i64 %83, i64* %rax
  store volatile i64 28537, i64* @assembly_address
  %84 = load i64* %rax
  %85 = trunc i64 %84 to i32
  %86 = sext i32 %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 28539, i64* @assembly_address
  %87 = load i64* %rax
  %88 = add i64 %87, -1
  store i64 %88, i64* %rdx
  store volatile i64 28543, i64* @assembly_address
  %89 = load i8** %stack_var_-48
  %90 = ptrtoint i8* %89 to i64
  store i64 %90, i64* %rax
  store volatile i64 28547, i64* @assembly_address
  %91 = load i64* %rax
  %92 = load i64* %rdx
  %93 = add i64 %91, %92
  %94 = and i64 %91, 15
  %95 = and i64 %92, 15
  %96 = add i64 %94, %95
  %97 = icmp ugt i64 %96, 15
  %98 = icmp ult i64 %93, %91
  %99 = xor i64 %91, %93
  %100 = xor i64 %92, %93
  %101 = and i64 %99, %100
  %102 = icmp slt i64 %101, 0
  store i1 %97, i1* %az
  store i1 %98, i1* %cf
  store i1 %102, i1* %of
  %103 = icmp eq i64 %93, 0
  store i1 %103, i1* %zf
  %104 = icmp slt i64 %93, 0
  store i1 %104, i1* %sf
  %105 = trunc i64 %93 to i8
  %106 = call i8 @llvm.ctpop.i8(i8 %105)
  %107 = and i8 %106, 1
  %108 = icmp eq i8 %107, 0
  store i1 %108, i1* %pf
  store i64 %93, i64* %rax
  store volatile i64 28550, i64* @assembly_address
  %109 = load i64* %rax
  %110 = inttoptr i64 %109 to i8*
  store i8 0, i8* %110
  store volatile i64 28553, i64* @assembly_address
  br label %block_70f1

block_6f8e:                                       ; preds = %block_6f30
  store volatile i64 28558, i64* @assembly_address
  %111 = load i8** %stack_var_-48
  %112 = ptrtoint i8* %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 28562, i64* @assembly_address
  %113 = load i64* %rax
  store i64 %113, i64* %rdi
  store volatile i64 28565, i64* @assembly_address
  %114 = load i64* %rdi
  %115 = inttoptr i64 %114 to i64*
  %116 = bitcast i64* %115 to i8*
  %117 = call i64 @get_suffix(i8* %116)
  store i64 %117, i64* %rax
  store i64 %117, i64* %rax
  store volatile i64 28570, i64* @assembly_address
  %118 = load i64* %rax
  %119 = inttoptr i64 %118 to i8*
  store i8* %119, i8** %stack_var_-16
  store volatile i64 28574, i64* @assembly_address
  %120 = load i8** %stack_var_-16
  %121 = ptrtoint i8* %120 to i64
  %122 = and i64 %121, 15
  %123 = icmp ugt i64 %122, 15
  %124 = icmp ult i64 %121, 0
  %125 = xor i64 %121, 0
  %126 = and i64 %125, 0
  %127 = icmp slt i64 %126, 0
  store i1 %123, i1* %az
  store i1 %124, i1* %cf
  store i1 %127, i1* %of
  %128 = icmp eq i64 %121, 0
  store i1 %128, i1* %zf
  %129 = icmp slt i64 %121, 0
  store i1 %129, i1* %sf
  %130 = trunc i64 %121 to i8
  %131 = call i8 @llvm.ctpop.i8(i8 %130)
  %132 = and i8 %131, 1
  %133 = icmp eq i8 %132, 0
  store i1 %133, i1* %pf
  store volatile i64 28579, i64* @assembly_address
  %134 = load i1* %zf
  %135 = icmp eq i1 %134, false
  br i1 %135, label %block_6fb1, label %block_6fa5

block_6fa5:                                       ; preds = %block_6f8e
  store volatile i64 28581, i64* @assembly_address
  store i64 ptrtoint ([22 x i8]* @global_var_11a40 to i64), i64* %rdi
  store volatile i64 28588, i64* @assembly_address
  %136 = load i64* %rdi
  %137 = inttoptr i64 %136 to i8*
  %138 = call i64 @gzip_error(i8* %137)
  store i64 %138, i64* %rax
  store i64 %138, i64* %rax
  unreachable

block_6fb1:                                       ; preds = %block_6f8e
  store volatile i64 28593, i64* @assembly_address
  %139 = load i8** %stack_var_-16
  %140 = ptrtoint i8* %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 28597, i64* @assembly_address
  %141 = load i64* %rax
  %142 = inttoptr i64 %141 to i8*
  store i8 0, i8* %142
  store volatile i64 28600, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_24a888 to i32*)
  store volatile i64 28610, i64* @assembly_address
  %143 = load i32* %stack_var_-32
  store i32 %143, i32* %11
  store i32 4, i32* %10
  %144 = sub i32 %143, 4
  %145 = and i32 %143, 15
  %146 = sub i32 %145, 4
  %147 = icmp ugt i32 %146, 15
  %148 = icmp ult i32 %143, 4
  %149 = xor i32 %143, 4
  %150 = xor i32 %143, %144
  %151 = and i32 %149, %150
  %152 = icmp slt i32 %151, 0
  store i1 %147, i1* %az
  store i1 %148, i1* %cf
  store i1 %152, i1* %of
  %153 = icmp eq i32 %144, 0
  store i1 %153, i1* %zf
  %154 = icmp slt i32 %144, 0
  store i1 %154, i1* %sf
  %155 = trunc i32 %144 to i8
  %156 = call i8 @llvm.ctpop.i8(i8 %155)
  %157 = and i8 %156, 1
  %158 = icmp eq i8 %157, 0
  store i1 %158, i1* %pf
  store volatile i64 28614, i64* @assembly_address
  %159 = load i32* %11
  %160 = load i32* %10
  %161 = icmp sle i32 %159, %160
  br i1 %161, label %block_6ffa, label %block_6fc8

block_6fc8:                                       ; preds = %block_6fb1
  store volatile i64 28616, i64* @assembly_address
  %162 = load i8** %stack_var_-16
  %163 = ptrtoint i8* %162 to i64
  store i64 %163, i64* %rax
  store volatile i64 28620, i64* @assembly_address
  %164 = load i64* %rax
  %165 = sub i64 %164, 4
  %166 = and i64 %164, 15
  %167 = sub i64 %166, 4
  %168 = icmp ugt i64 %167, 15
  %169 = icmp ult i64 %164, 4
  %170 = xor i64 %164, 4
  %171 = xor i64 %164, %165
  %172 = and i64 %170, %171
  %173 = icmp slt i64 %172, 0
  store i1 %168, i1* %az
  store i1 %169, i1* %cf
  store i1 %173, i1* %of
  %174 = icmp eq i64 %165, 0
  store i1 %174, i1* %zf
  %175 = icmp slt i64 %165, 0
  store i1 %175, i1* %sf
  %176 = trunc i64 %165 to i8
  %177 = call i8 @llvm.ctpop.i8(i8 %176)
  %178 = and i8 %177, 1
  %179 = icmp eq i8 %178, 0
  store i1 %179, i1* %pf
  store i64 %165, i64* %rax
  store volatile i64 28624, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11a56 to i64), i64* %rsi
  store volatile i64 28631, i64* @assembly_address
  %180 = load i64* %rax
  store i64 %180, i64* %rdi
  store volatile i64 28634, i64* @assembly_address
  %181 = load i64* %rdi
  %182 = inttoptr i64 %181 to i8*
  %183 = load i64* %rsi
  %184 = inttoptr i64 %183 to i8*
  %185 = call i32 @strcmp(i8* %182, i8* %184)
  %186 = sext i32 %185 to i64
  store i64 %186, i64* %rax
  %187 = sext i32 %185 to i64
  store i64 %187, i64* %rax
  store volatile i64 28639, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  %190 = load i64* %rax
  %191 = trunc i64 %190 to i32
  %192 = and i32 %189, %191
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %193 = icmp eq i32 %192, 0
  store i1 %193, i1* %zf
  %194 = icmp slt i32 %192, 0
  store i1 %194, i1* %sf
  %195 = trunc i32 %192 to i8
  %196 = call i8 @llvm.ctpop.i8(i8 %195)
  %197 = and i8 %196, 1
  %198 = icmp eq i8 %197, 0
  store i1 %198, i1* %pf
  store volatile i64 28641, i64* @assembly_address
  %199 = load i1* %zf
  %200 = icmp eq i1 %199, false
  br i1 %200, label %block_6ffa, label %block_6fe3

block_6fe3:                                       ; preds = %block_6fc8
  store volatile i64 28643, i64* @assembly_address
  %201 = load i8** %stack_var_-16
  %202 = ptrtoint i8* %201 to i64
  store i64 %202, i64* %rax
  store volatile i64 28647, i64* @assembly_address
  %203 = load i64* %rax
  %204 = sub i64 %203, 4
  %205 = and i64 %203, 15
  %206 = sub i64 %205, 4
  %207 = icmp ugt i64 %206, 15
  %208 = icmp ult i64 %203, 4
  %209 = xor i64 %203, 4
  %210 = xor i64 %203, %204
  %211 = and i64 %209, %210
  %212 = icmp slt i64 %211, 0
  store i1 %207, i1* %az
  store i1 %208, i1* %cf
  store i1 %212, i1* %of
  %213 = icmp eq i64 %204, 0
  store i1 %213, i1* %zf
  %214 = icmp slt i64 %204, 0
  store i1 %214, i1* %sf
  %215 = trunc i64 %204 to i8
  %216 = call i8 @llvm.ctpop.i8(i8 %215)
  %217 = and i8 %216, 1
  %218 = icmp eq i8 %217, 0
  store i1 %218, i1* %pf
  store i64 %204, i64* %rax
  store volatile i64 28651, i64* @assembly_address
  %219 = load i64* %rax
  %220 = inttoptr i64 %219 to i32*
  store i32 2053600302, i32* %220
  store volatile i64 28657, i64* @assembly_address
  %221 = load i64* %rax
  %222 = add i64 %221, 4
  %223 = inttoptr i64 %222 to i8*
  store i8 0, i8* %223
  store volatile i64 28661, i64* @assembly_address
  br label %block_70f1

block_6ffa:                                       ; preds = %block_7061, %block_6fc8, %block_6fb1
  store volatile i64 28666, i64* @assembly_address
  %224 = load i8** %stack_var_-48
  %225 = ptrtoint i8* %224 to i64
  store i64 %225, i64* %rax
  store volatile i64 28670, i64* @assembly_address
  %226 = load i64* %rax
  store i64 %226, i64* %rdi
  store volatile i64 28673, i64* @assembly_address
  %227 = load i64* %rdi
  %228 = inttoptr i64 %227 to i64*
  %229 = bitcast i64* %228 to i8*
  %230 = call i64 @last_component(i8* %229)
  store i64 %230, i64* %rax
  store i64 %230, i64* %rax
  store volatile i64 28678, i64* @assembly_address
  %231 = load i64* %rax
  %232 = inttoptr i64 %231 to i8*
  store i8* %232, i8** %stack_var_-16
  store volatile i64 28682, i64* @assembly_address
  br label %block_704f

block_700c:                                       ; preds = %block_704f
  store volatile i64 28684, i64* @assembly_address
  %233 = load i8** %stack_var_-16
  %234 = ptrtoint i8* %233 to i64
  store i64 %234, i64* %rax
  store volatile i64 28688, i64* @assembly_address
  store i64 ptrtoint ([2 x i8]* @global_var_11a5b to i64), i64* %rsi
  store volatile i64 28695, i64* @assembly_address
  %235 = load i64* %rax
  store i64 %235, i64* %rdi
  store volatile i64 28698, i64* @assembly_address
  %236 = load i64* %rdi
  %237 = inttoptr i64 %236 to i8*
  %238 = load i64* %rsi
  %239 = inttoptr i64 %238 to i8*
  %240 = call i32 @strcspn(i8* %237, i8* %239)
  %241 = sext i32 %240 to i64
  store i64 %241, i64* %rax
  %242 = sext i32 %240 to i64
  store i64 %242, i64* %rax
  store volatile i64 28703, i64* @assembly_address
  %243 = load i64* %rax
  %244 = trunc i64 %243 to i32
  %245 = inttoptr i32 %244 to i8*
  store i8* %245, i8** %stack_var_-28
  store volatile i64 28706, i64* @assembly_address
  %246 = load i8** %stack_var_-28
  %247 = ptrtoint i8* %246 to i32
  %248 = zext i32 %247 to i64
  store i64 %248, i64* %rax
  store volatile i64 28709, i64* @assembly_address
  %249 = load i64* %rax
  %250 = trunc i64 %249 to i32
  %251 = sext i32 %250 to i64
  store i64 %251, i64* %rax
  store volatile i64 28711, i64* @assembly_address
  %252 = load i8** %stack_var_-16
  %253 = ptrtoint i8* %252 to i64
  %254 = load i64* %rax
  %255 = add i64 %253, %254
  %256 = and i64 %253, 15
  %257 = and i64 %254, 15
  %258 = add i64 %256, %257
  %259 = icmp ugt i64 %258, 15
  %260 = icmp ult i64 %255, %253
  %261 = xor i64 %253, %255
  %262 = xor i64 %254, %255
  %263 = and i64 %261, %262
  %264 = icmp slt i64 %263, 0
  store i1 %259, i1* %az
  store i1 %260, i1* %cf
  store i1 %264, i1* %of
  %265 = icmp eq i64 %255, 0
  store i1 %265, i1* %zf
  %266 = icmp slt i64 %255, 0
  store i1 %266, i1* %sf
  %267 = trunc i64 %255 to i8
  %268 = call i8 @llvm.ctpop.i8(i8 %267)
  %269 = and i8 %268, 1
  %270 = icmp eq i8 %269, 0
  store i1 %270, i1* %pf
  %271 = inttoptr i64 %255 to i8*
  store i8* %271, i8** %stack_var_-16
  store volatile i64 28715, i64* @assembly_address
  %272 = load i8** %stack_var_-28
  %273 = ptrtoint i8* %272 to i32
  %274 = zext i32 %273 to i64
  store i64 %274, i64* %rax
  store volatile i64 28718, i64* @assembly_address
  %275 = load i64* %rax
  %276 = trunc i64 %275 to i32
  %277 = load i32* %stack_var_-36
  %278 = inttoptr i64 %275 to i8*
  store i8* %278, i8** %8
  %279 = inttoptr i32 %277 to i8*
  store i8* %279, i8** %6
  %280 = sub i32 %276, %277
  %281 = and i32 %276, 15
  %282 = and i32 %277, 15
  %283 = sub i32 %281, %282
  %284 = icmp ugt i32 %283, 15
  %285 = icmp ult i32 %276, %277
  %286 = xor i32 %276, %277
  %287 = xor i32 %276, %280
  %288 = and i32 %286, %287
  %289 = icmp slt i32 %288, 0
  store i1 %284, i1* %az
  store i1 %285, i1* %cf
  store i1 %289, i1* %of
  %290 = icmp eq i32 %280, 0
  store i1 %290, i1* %zf
  %291 = icmp slt i32 %280, 0
  store i1 %291, i1* %sf
  %292 = trunc i32 %280 to i8
  %293 = call i8 @llvm.ctpop.i8(i8 %292)
  %294 = and i8 %293, 1
  %295 = icmp eq i8 %294, 0
  store i1 %295, i1* %pf
  store volatile i64 28721, i64* @assembly_address
  %296 = load i8** %8
  %297 = ptrtoint i8* %296 to i64
  %298 = load i8** %6
  %299 = ptrtoint i8* %298 to i32
  %300 = trunc i64 %297 to i32
  %301 = icmp sle i32 %300, %299
  br i1 %301, label %block_703f, label %block_7033

block_7033:                                       ; preds = %block_700c
  store volatile i64 28723, i64* @assembly_address
  %302 = load i8** %stack_var_-16
  %303 = ptrtoint i8* %302 to i64
  store i64 %303, i64* %rax
  store volatile i64 28727, i64* @assembly_address
  %304 = load i64* %rax
  %305 = sub i64 %304, 1
  %306 = and i64 %304, 15
  %307 = sub i64 %306, 1
  %308 = icmp ugt i64 %307, 15
  %309 = icmp ult i64 %304, 1
  %310 = xor i64 %304, 1
  %311 = xor i64 %304, %305
  %312 = and i64 %310, %311
  %313 = icmp slt i64 %312, 0
  store i1 %308, i1* %az
  store i1 %309, i1* %cf
  store i1 %313, i1* %of
  %314 = icmp eq i64 %305, 0
  store i1 %314, i1* %zf
  %315 = icmp slt i64 %305, 0
  store i1 %315, i1* %sf
  %316 = trunc i64 %305 to i8
  %317 = call i8 @llvm.ctpop.i8(i8 %316)
  %318 = and i8 %317, 1
  %319 = icmp eq i8 %318, 0
  store i1 %319, i1* %pf
  store i64 %305, i64* %rax
  store volatile i64 28731, i64* @assembly_address
  %320 = load i64* %rax
  %321 = inttoptr i64 %320 to i8*
  store i8* %321, i8** %stack_var_-24
  br label %block_703f

block_703f:                                       ; preds = %block_7033, %block_700c
  store volatile i64 28735, i64* @assembly_address
  %322 = load i8** %stack_var_-16
  %323 = ptrtoint i8* %322 to i64
  store i64 %323, i64* %rax
  store volatile i64 28739, i64* @assembly_address
  %324 = load i64* %rax
  %325 = inttoptr i64 %324 to i8*
  %326 = load i8* %325
  %327 = zext i8 %326 to i64
  store i64 %327, i64* %rax
  store volatile i64 28742, i64* @assembly_address
  %328 = load i64* %rax
  %329 = trunc i64 %328 to i8
  %330 = load i64* %rax
  %331 = trunc i64 %330 to i8
  %332 = and i8 %329, %331
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %333 = icmp eq i8 %332, 0
  store i1 %333, i1* %zf
  %334 = icmp slt i8 %332, 0
  store i1 %334, i1* %sf
  %335 = call i8 @llvm.ctpop.i8(i8 %332)
  %336 = and i8 %335, 1
  %337 = icmp eq i8 %336, 0
  store i1 %337, i1* %pf
  store volatile i64 28744, i64* @assembly_address
  %338 = load i1* %zf
  br i1 %338, label %block_704f, label %block_704a

block_704a:                                       ; preds = %block_703f
  store volatile i64 28746, i64* @assembly_address
  %339 = load i8** %stack_var_-16
  %340 = ptrtoint i8* %339 to i64
  %341 = add i64 %340, 1
  %342 = and i64 %340, 15
  %343 = add i64 %342, 1
  %344 = icmp ugt i64 %343, 15
  %345 = icmp ult i64 %341, %340
  %346 = xor i64 %340, %341
  %347 = xor i64 1, %341
  %348 = and i64 %346, %347
  %349 = icmp slt i64 %348, 0
  store i1 %344, i1* %az
  store i1 %345, i1* %cf
  store i1 %349, i1* %of
  %350 = icmp eq i64 %341, 0
  store i1 %350, i1* %zf
  %351 = icmp slt i64 %341, 0
  store i1 %351, i1* %sf
  %352 = trunc i64 %341 to i8
  %353 = call i8 @llvm.ctpop.i8(i8 %352)
  %354 = and i8 %353, 1
  %355 = icmp eq i8 %354, 0
  store i1 %355, i1* %pf
  %356 = inttoptr i64 %341 to i8*
  store i8* %356, i8** %stack_var_-16
  br label %block_704f

block_704f:                                       ; preds = %block_704a, %block_703f, %block_6ffa
  store volatile i64 28751, i64* @assembly_address
  %357 = load i8** %stack_var_-16
  %358 = ptrtoint i8* %357 to i64
  store i64 %358, i64* %rax
  store volatile i64 28755, i64* @assembly_address
  %359 = load i64* %rax
  %360 = inttoptr i64 %359 to i8*
  %361 = load i8* %360
  %362 = zext i8 %361 to i64
  store i64 %362, i64* %rax
  store volatile i64 28758, i64* @assembly_address
  %363 = load i64* %rax
  %364 = trunc i64 %363 to i8
  %365 = load i64* %rax
  %366 = trunc i64 %365 to i8
  %367 = and i8 %364, %366
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %368 = icmp eq i8 %367, 0
  store i1 %368, i1* %zf
  %369 = icmp slt i8 %367, 0
  store i1 %369, i1* %sf
  %370 = call i8 @llvm.ctpop.i8(i8 %367)
  %371 = and i8 %370, 1
  %372 = icmp eq i8 %371, 0
  store i1 %372, i1* %pf
  store volatile i64 28760, i64* @assembly_address
  %373 = load i1* %zf
  %374 = icmp eq i1 %373, false
  br i1 %374, label %block_700c, label %block_705a

block_705a:                                       ; preds = %block_704f
  store volatile i64 28762, i64* @assembly_address
  %375 = load i8** %stack_var_-24
  %376 = ptrtoint i8* %375 to i64
  %377 = and i64 %376, 15
  %378 = icmp ugt i64 %377, 15
  %379 = icmp ult i64 %376, 0
  %380 = xor i64 %376, 0
  %381 = and i64 %380, 0
  %382 = icmp slt i64 %381, 0
  store i1 %378, i1* %az
  store i1 %379, i1* %cf
  store i1 %382, i1* %of
  %383 = icmp eq i64 %376, 0
  store i1 %383, i1* %zf
  %384 = icmp slt i64 %376, 0
  store i1 %384, i1* %sf
  %385 = trunc i64 %376 to i8
  %386 = call i8 @llvm.ctpop.i8(i8 %385)
  %387 = and i8 %386, 1
  %388 = icmp eq i8 %387, 0
  store i1 %388, i1* %pf
  store volatile i64 28767, i64* @assembly_address
  %389 = load i1* %zf
  %390 = icmp eq i1 %389, false
  br i1 %390, label %block_706b, label %block_7061

block_7061:                                       ; preds = %block_705a
  store volatile i64 28769, i64* @assembly_address
  %391 = load i32* %stack_var_-36
  %392 = sub i32 %391, 1
  %393 = and i32 %391, 15
  %394 = sub i32 %393, 1
  %395 = icmp ugt i32 %394, 15
  %396 = icmp ult i32 %391, 1
  %397 = xor i32 %391, 1
  %398 = xor i32 %391, %392
  %399 = and i32 %397, %398
  %400 = icmp slt i32 %399, 0
  store i1 %395, i1* %az
  store i1 %396, i1* %cf
  store i1 %400, i1* %of
  %401 = icmp eq i32 %392, 0
  store i1 %401, i1* %zf
  %402 = icmp slt i32 %392, 0
  store i1 %402, i1* %sf
  %403 = trunc i32 %392 to i8
  %404 = call i8 @llvm.ctpop.i8(i8 %403)
  %405 = and i8 %404, 1
  %406 = icmp eq i8 %405, 0
  store i1 %406, i1* %pf
  store i32 %392, i32* %stack_var_-36
  store volatile i64 28773, i64* @assembly_address
  %407 = load i32* %stack_var_-36
  %408 = and i32 %407, 15
  %409 = icmp ugt i32 %408, 15
  %410 = icmp ult i32 %407, 0
  %411 = xor i32 %407, 0
  %412 = and i32 %411, 0
  %413 = icmp slt i32 %412, 0
  store i1 %409, i1* %az
  store i1 %410, i1* %cf
  store i1 %413, i1* %of
  %414 = icmp eq i32 %407, 0
  store i1 %414, i1* %zf
  %415 = icmp slt i32 %407, 0
  store i1 %415, i1* %sf
  %416 = trunc i32 %407 to i8
  %417 = call i8 @llvm.ctpop.i8(i8 %416)
  %418 = and i8 %417, 1
  %419 = icmp eq i8 %418, 0
  store i1 %419, i1* %pf
  store volatile i64 28777, i64* @assembly_address
  %420 = load i1* %zf
  %421 = icmp eq i1 %420, false
  br i1 %421, label %block_6ffa, label %block_706b

block_706b:                                       ; preds = %block_7061, %block_705a
  store volatile i64 28779, i64* @assembly_address
  %422 = load i8** %stack_var_-24
  %423 = ptrtoint i8* %422 to i64
  %424 = and i64 %423, 15
  %425 = icmp ugt i64 %424, 15
  %426 = icmp ult i64 %423, 0
  %427 = xor i64 %423, 0
  %428 = and i64 %427, 0
  %429 = icmp slt i64 %428, 0
  store i1 %425, i1* %az
  store i1 %426, i1* %cf
  store i1 %429, i1* %of
  %430 = icmp eq i64 %423, 0
  store i1 %430, i1* %zf
  %431 = icmp slt i64 %423, 0
  store i1 %431, i1* %sf
  %432 = trunc i64 %423 to i8
  %433 = call i8 @llvm.ctpop.i8(i8 %432)
  %434 = and i8 %433, 1
  %435 = icmp eq i8 %434, 0
  store i1 %435, i1* %pf
  store volatile i64 28784, i64* @assembly_address
  %436 = load i1* %zf
  br i1 %436, label %block_709a, label %block_7072

block_7072:                                       ; preds = %block_7072, %block_706b
  store volatile i64 28786, i64* @assembly_address
  %437 = load i8** %stack_var_-24
  %438 = ptrtoint i8* %437 to i64
  store i64 %438, i64* %rax
  store volatile i64 28790, i64* @assembly_address
  %439 = load i64* %rax
  %440 = add i64 %439, 1
  %441 = inttoptr i64 %440 to i8*
  %442 = load i8* %441
  %443 = zext i8 %442 to i64
  store i64 %443, i64* %rdx
  store volatile i64 28794, i64* @assembly_address
  %444 = load i8** %stack_var_-24
  %445 = ptrtoint i8* %444 to i64
  store i64 %445, i64* %rax
  store volatile i64 28798, i64* @assembly_address
  %446 = load i64* %rdx
  %447 = trunc i64 %446 to i8
  %448 = load i64* %rax
  %449 = inttoptr i64 %448 to i8*
  store i8 %447, i8* %449
  store volatile i64 28800, i64* @assembly_address
  %450 = load i8** %stack_var_-24
  %451 = ptrtoint i8* %450 to i64
  store i64 %451, i64* %rax
  store volatile i64 28804, i64* @assembly_address
  %452 = load i64* %rax
  %453 = add i64 %452, 1
  store i64 %453, i64* %rdx
  store volatile i64 28808, i64* @assembly_address
  %454 = load i64* %rdx
  %455 = inttoptr i64 %454 to i8*
  store i8* %455, i8** %stack_var_-24
  store volatile i64 28812, i64* @assembly_address
  %456 = load i64* %rax
  %457 = inttoptr i64 %456 to i8*
  %458 = load i8* %457
  %459 = zext i8 %458 to i64
  store i64 %459, i64* %rax
  store volatile i64 28815, i64* @assembly_address
  %460 = load i64* %rax
  %461 = trunc i64 %460 to i8
  %462 = load i64* %rax
  %463 = trunc i64 %462 to i8
  %464 = and i8 %461, %463
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %465 = icmp eq i8 %464, 0
  store i1 %465, i1* %zf
  %466 = icmp slt i8 %464, 0
  store i1 %466, i1* %sf
  %467 = call i8 @llvm.ctpop.i8(i8 %464)
  %468 = and i8 %467, 1
  %469 = icmp eq i8 %468, 0
  store i1 %469, i1* %pf
  store volatile i64 28817, i64* @assembly_address
  %470 = load i1* %zf
  %471 = icmp eq i1 %470, false
  br i1 %471, label %block_7072, label %block_7093

block_7093:                                       ; preds = %block_7072
  store volatile i64 28819, i64* @assembly_address
  %472 = load i8** %stack_var_-24
  %473 = ptrtoint i8* %472 to i64
  %474 = sub i64 %473, 1
  %475 = and i64 %473, 15
  %476 = sub i64 %475, 1
  %477 = icmp ugt i64 %476, 15
  %478 = icmp ult i64 %473, 1
  %479 = xor i64 %473, 1
  %480 = xor i64 %473, %474
  %481 = and i64 %479, %480
  %482 = icmp slt i64 %481, 0
  store i1 %477, i1* %az
  store i1 %478, i1* %cf
  store i1 %482, i1* %of
  %483 = icmp eq i64 %474, 0
  store i1 %483, i1* %zf
  %484 = icmp slt i64 %474, 0
  store i1 %484, i1* %sf
  %485 = trunc i64 %474 to i8
  %486 = call i8 @llvm.ctpop.i8(i8 %485)
  %487 = and i8 %486, 1
  %488 = icmp eq i8 %487, 0
  store i1 %488, i1* %pf
  %489 = inttoptr i64 %474 to i8*
  store i8* %489, i8** %stack_var_-24
  store volatile i64 28824, i64* @assembly_address
  br label %block_70db

block_709a:                                       ; preds = %block_706b
  store volatile i64 28826, i64* @assembly_address
  store i64 46, i64* %rax
  store volatile i64 28831, i64* @assembly_address
  %490 = load i64* %rax
  %491 = trunc i64 %490 to i8
  %492 = sext i8 %491 to i64
  store i64 %492, i64* %rdx
  store volatile i64 28834, i64* @assembly_address
  %493 = load i8** %stack_var_-48
  %494 = ptrtoint i8* %493 to i64
  store i64 %494, i64* %rax
  store volatile i64 28838, i64* @assembly_address
  %495 = load i64* %rdx
  %496 = trunc i64 %495 to i32
  %497 = zext i32 %496 to i64
  store i64 %497, i64* %rsi
  store volatile i64 28840, i64* @assembly_address
  %498 = load i64* %rax
  store i64 %498, i64* %rdi
  store volatile i64 28843, i64* @assembly_address
  %499 = load i64* %rdi
  %500 = inttoptr i64 %499 to i8*
  %501 = load i64* %rsi
  %502 = trunc i64 %501 to i32
  %503 = call i8* @strrchr(i8* %500, i32 %502)
  %504 = ptrtoint i8* %503 to i64
  store i64 %504, i64* %rax
  %505 = ptrtoint i8* %503 to i64
  store i64 %505, i64* %rax
  store volatile i64 28848, i64* @assembly_address
  %506 = load i64* %rax
  %507 = inttoptr i64 %506 to i8*
  store i8* %507, i8** %stack_var_-24
  store volatile i64 28852, i64* @assembly_address
  %508 = load i8** %stack_var_-24
  %509 = ptrtoint i8* %508 to i64
  %510 = and i64 %509, 15
  %511 = icmp ugt i64 %510, 15
  %512 = icmp ult i64 %509, 0
  %513 = xor i64 %509, 0
  %514 = and i64 %513, 0
  %515 = icmp slt i64 %514, 0
  store i1 %511, i1* %az
  store i1 %512, i1* %cf
  store i1 %515, i1* %of
  %516 = icmp eq i64 %509, 0
  store i1 %516, i1* %zf
  %517 = icmp slt i64 %509, 0
  store i1 %517, i1* %sf
  %518 = trunc i64 %509 to i8
  %519 = call i8 @llvm.ctpop.i8(i8 %518)
  %520 = and i8 %519, 1
  %521 = icmp eq i8 %520, 0
  store i1 %521, i1* %pf
  store volatile i64 28857, i64* @assembly_address
  %522 = load i1* %zf
  %523 = icmp eq i1 %522, false
  br i1 %523, label %block_70c7, label %block_70bb

block_70bb:                                       ; preds = %block_709a
  store volatile i64 28859, i64* @assembly_address
  store i64 ptrtoint ([31 x i8]* @global_var_11a60 to i64), i64* %rdi
  store volatile i64 28866, i64* @assembly_address
  %524 = load i64* %rdi
  %525 = inttoptr i64 %524 to i8*
  %526 = call i64 @gzip_error(i8* %525)
  store i64 %526, i64* %rax
  store i64 %526, i64* %rax
  unreachable

block_70c7:                                       ; preds = %block_709a
  store volatile i64 28871, i64* @assembly_address
  %527 = load i8** %stack_var_-24
  %528 = ptrtoint i8* %527 to i64
  store i64 %528, i64* %rax
  store volatile i64 28875, i64* @assembly_address
  %529 = load i64* %rax
  %530 = add i64 %529, 1
  %531 = and i64 %529, 15
  %532 = add i64 %531, 1
  %533 = icmp ugt i64 %532, 15
  %534 = icmp ult i64 %530, %529
  %535 = xor i64 %529, %530
  %536 = xor i64 1, %530
  %537 = and i64 %535, %536
  %538 = icmp slt i64 %537, 0
  store i1 %533, i1* %az
  store i1 %534, i1* %cf
  store i1 %538, i1* %of
  %539 = icmp eq i64 %530, 0
  store i1 %539, i1* %zf
  %540 = icmp slt i64 %530, 0
  store i1 %540, i1* %sf
  %541 = trunc i64 %530 to i8
  %542 = call i8 @llvm.ctpop.i8(i8 %541)
  %543 = and i8 %542, 1
  %544 = icmp eq i8 %543, 0
  store i1 %544, i1* %pf
  store i64 %530, i64* %rax
  store volatile i64 28879, i64* @assembly_address
  %545 = load i64* %rax
  %546 = inttoptr i64 %545 to i8*
  %547 = load i8* %546
  %548 = zext i8 %547 to i64
  store i64 %548, i64* %rax
  store volatile i64 28882, i64* @assembly_address
  %549 = load i64* %rax
  %550 = trunc i64 %549 to i8
  %551 = load i64* %rax
  %552 = trunc i64 %551 to i8
  %553 = and i8 %550, %552
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %554 = icmp eq i8 %553, 0
  store i1 %554, i1* %zf
  %555 = icmp slt i8 %553, 0
  store i1 %555, i1* %sf
  %556 = call i8 @llvm.ctpop.i8(i8 %553)
  %557 = and i8 %556, 1
  %558 = icmp eq i8 %557, 0
  store i1 %558, i1* %pf
  store volatile i64 28884, i64* @assembly_address
  %559 = load i1* %zf
  %560 = icmp eq i1 %559, false
  br i1 %560, label %block_70db, label %block_70d6

block_70d6:                                       ; preds = %block_70c7
  store volatile i64 28886, i64* @assembly_address
  %561 = load i8** %stack_var_-24
  %562 = ptrtoint i8* %561 to i64
  %563 = sub i64 %562, 1
  %564 = and i64 %562, 15
  %565 = sub i64 %564, 1
  %566 = icmp ugt i64 %565, 15
  %567 = icmp ult i64 %562, 1
  %568 = xor i64 %562, 1
  %569 = xor i64 %562, %563
  %570 = and i64 %568, %569
  %571 = icmp slt i64 %570, 0
  store i1 %566, i1* %az
  store i1 %567, i1* %cf
  store i1 %571, i1* %of
  %572 = icmp eq i64 %563, 0
  store i1 %572, i1* %zf
  %573 = icmp slt i64 %563, 0
  store i1 %573, i1* %sf
  %574 = trunc i64 %563 to i8
  %575 = call i8 @llvm.ctpop.i8(i8 %574)
  %576 = and i8 %575, 1
  %577 = icmp eq i8 %576, 0
  store i1 %577, i1* %pf
  %578 = inttoptr i64 %563 to i8*
  store i8* %578, i8** %stack_var_-24
  br label %block_70db

block_70db:                                       ; preds = %block_70d6, %block_70c7, %block_7093
  store volatile i64 28891, i64* @assembly_address
  %579 = load i64* @global_var_216630
  store i64 %579, i64* %rdx
  store volatile i64 28898, i64* @assembly_address
  %580 = load i8** %stack_var_-24
  %581 = ptrtoint i8* %580 to i64
  store i64 %581, i64* %rax
  store volatile i64 28902, i64* @assembly_address
  %582 = load i64* %rdx
  store i64 %582, i64* %rsi
  store volatile i64 28905, i64* @assembly_address
  %583 = load i64* %rax
  store i64 %583, i64* %rdi
  store volatile i64 28908, i64* @assembly_address
  %584 = load i64* %rdi
  %585 = inttoptr i64 %584 to i8*
  %586 = load i64* %rsi
  %587 = inttoptr i64 %586 to i8*
  %588 = call i8* @strcpy(i8* %585, i8* %587)
  %589 = ptrtoint i8* %588 to i64
  store i64 %589, i64* %rax
  %590 = ptrtoint i8* %588 to i64
  store i64 %590, i64* %rax
  br label %block_70f1

block_70f1:                                       ; preds = %block_70db, %block_6fe3, %block_6f76
  store volatile i64 28913, i64* @assembly_address
  %591 = load i64* %stack_var_-8
  store i64 %591, i64* %rbp
  %592 = ptrtoint i64* %stack_var_0 to i64
  store i64 %592, i64* %rsp
  store volatile i64 28914, i64* @assembly_address
  %593 = load i64* %rax
  ret i64 %593
}

declare i64 @184(i64*)

define i64 @check_ofname() {
block_70f3:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 28915, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 28916, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 28919, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 16
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 16
  %8 = xor i64 %3, 16
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %18, i64* %rsp
  store volatile i64 28923, i64* @assembly_address
  %19 = load i32* bitcast (i64* @global_var_216604 to i32*)
  %20 = zext i32 %19 to i64
  store i64 %20, i64* %rax
  store volatile i64 28929, i64* @assembly_address
  %21 = load i64* %rax
  %22 = trunc i64 %21 to i32
  %23 = load i64* %rax
  %24 = trunc i64 %23 to i32
  %25 = and i32 %22, %24
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %26 = icmp eq i32 %25, 0
  store i1 %26, i1* %zf
  %27 = icmp slt i32 %25, 0
  store i1 %27, i1* %sf
  %28 = trunc i32 %25 to i8
  %29 = call i8 @llvm.ctpop.i8(i8 %28)
  %30 = and i8 %29, 1
  %31 = icmp eq i8 %30, 0
  store i1 %31, i1* %pf
  store volatile i64 28931, i64* @assembly_address
  %32 = load i1* %zf
  %33 = icmp eq i1 %32, false
  br i1 %33, label %block_71d7, label %block_7109

block_7109:                                       ; preds = %block_70f3
  store volatile i64 28937, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 28944, i64* @assembly_address
  %34 = load i64* @global_var_25f4c8
  store i64 %34, i64* %rdx
  store volatile i64 28951, i64* @assembly_address
  %35 = load i64* @global_var_216580
  store i64 %35, i64* %rax
  store volatile i64 28958, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rcx
  store volatile i64 28965, i64* @assembly_address
  store i64 ptrtoint ([23 x i8]* @global_var_11a7f to i64), i64* %rsi
  store volatile i64 28972, i64* @assembly_address
  %36 = load i64* %rax
  store i64 %36, i64* %rdi
  store volatile i64 28975, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 28980, i64* @assembly_address
  %37 = load i64* %rdi
  %38 = inttoptr i64 %37 to %_IO_FILE*
  %39 = load i64* %rsi
  %40 = inttoptr i64 %39 to i8*
  %41 = load i64* %rdx
  %42 = inttoptr i64 %41 to i8*
  %43 = load i64* %rcx
  %44 = inttoptr i64 %43 to i8*
  %45 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %38, i8* %40, i8* %42, i8* %44)
  %46 = sext i32 %45 to i64
  store i64 %46, i64* %rax
  %47 = sext i32 %45 to i64
  store i64 %47, i64* %rax
  store volatile i64 28985, i64* @assembly_address
  %48 = load i32* bitcast (i64* @global_var_216618 to i32*)
  %49 = zext i32 %48 to i64
  store i64 %49, i64* %rax
  store volatile i64 28991, i64* @assembly_address
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  %52 = load i64* %rax
  %53 = trunc i64 %52 to i32
  %54 = and i32 %51, %53
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %55 = icmp eq i32 %54, 0
  store i1 %55, i1* %zf
  %56 = icmp slt i32 %54, 0
  store i1 %56, i1* %sf
  %57 = trunc i32 %54 to i8
  %58 = call i8 @llvm.ctpop.i8(i8 %57)
  %59 = and i8 %58, 1
  %60 = icmp eq i8 %59, 0
  store i1 %60, i1* %pf
  store volatile i64 28993, i64* @assembly_address
  %61 = load i1* %zf
  br i1 %61, label %block_7196, label %block_7143

block_7143:                                       ; preds = %block_7109
  store volatile i64 28995, i64* @assembly_address
  %62 = load i8* bitcast (i64* @global_var_2165f8 to i8*)
  %63 = zext i8 %62 to i64
  store i64 %63, i64* %rax
  store volatile i64 29002, i64* @assembly_address
  %64 = load i64* %rax
  %65 = trunc i64 %64 to i8
  %66 = load i64* %rax
  %67 = trunc i64 %66 to i8
  %68 = and i8 %65, %67
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i8 %68, 0
  store i1 %70, i1* %sf
  %71 = call i8 @llvm.ctpop.i8(i8 %68)
  %72 = and i8 %71, 1
  %73 = icmp eq i8 %72, 0
  store i1 %73, i1* %pf
  store volatile i64 29004, i64* @assembly_address
  %74 = load i1* %zf
  %75 = icmp eq i1 %74, false
  br i1 %75, label %block_715c, label %block_714e

block_714e:                                       ; preds = %block_7143
  store volatile i64 29006, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 29011, i64* @assembly_address
  %76 = load i64* %rdi
  %77 = trunc i64 %76 to i32
  %78 = call i32 @isatty(i32 %77)
  %79 = sext i32 %78 to i64
  store i64 %79, i64* %rax
  %80 = sext i32 %78 to i64
  store i64 %80, i64* %rax
  store volatile i64 29016, i64* @assembly_address
  %81 = load i64* %rax
  %82 = trunc i64 %81 to i32
  %83 = load i64* %rax
  %84 = trunc i64 %83 to i32
  %85 = and i32 %82, %84
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %86 = icmp eq i32 %85, 0
  store i1 %86, i1* %zf
  %87 = icmp slt i32 %85, 0
  store i1 %87, i1* %sf
  %88 = trunc i32 %85 to i8
  %89 = call i8 @llvm.ctpop.i8(i8 %88)
  %90 = and i8 %89, 1
  %91 = icmp eq i8 %90, 0
  store i1 %91, i1* %pf
  store volatile i64 29018, i64* @assembly_address
  %92 = load i1* %zf
  br i1 %92, label %block_7196, label %block_715c

block_715c:                                       ; preds = %block_714e, %block_7143
  store volatile i64 29020, i64* @assembly_address
  %93 = load i64* @global_var_216580
  store i64 %93, i64* %rax
  store volatile i64 29027, i64* @assembly_address
  %94 = load i64* %rax
  store i64 %94, i64* %rcx
  store volatile i64 29030, i64* @assembly_address
  store i64 36, i64* %rdx
  store volatile i64 29035, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 29040, i64* @assembly_address
  store i64 ptrtoint ([37 x i8]* @global_var_11a98 to i64), i64* %rdi
  store volatile i64 29047, i64* @assembly_address
  %95 = load i64* %rdi
  %96 = inttoptr i64 %95 to i64*
  %97 = load i64* %rsi
  %98 = trunc i64 %97 to i32
  %99 = load i64* %rdx
  %100 = trunc i64 %99 to i32
  %101 = load i64* %rcx
  %102 = inttoptr i64 %101 to %_IO_FILE*
  %103 = call i32 @fwrite(i64* %96, i32 %98, i32 %100, %_IO_FILE* %102)
  %104 = sext i32 %103 to i64
  store i64 %104, i64* %rax
  %105 = sext i32 %103 to i64
  store i64 %105, i64* %rax
  store volatile i64 29052, i64* @assembly_address
  %106 = load i64* @global_var_216580
  store i64 %106, i64* %rax
  store volatile i64 29059, i64* @assembly_address
  %107 = load i64* %rax
  store i64 %107, i64* %rdi
  store volatile i64 29062, i64* @assembly_address
  %108 = load i64* %rdi
  %109 = inttoptr i64 %108 to %_IO_FILE*
  %110 = call i64 @rpl_fflush(%_IO_FILE* %109)
  store i64 %110, i64* %rax
  store i64 %110, i64* %rax
  store volatile i64 29067, i64* @assembly_address
  %111 = call i64 @yesno()
  store i64 %111, i64* %rax
  store i64 %111, i64* %rax
  store i64 %111, i64* %rax
  store volatile i64 29072, i64* @assembly_address
  %112 = load i64* %rax
  %113 = trunc i64 %112 to i8
  %114 = zext i8 %113 to i64
  store i64 %114, i64* %rax
  store volatile i64 29075, i64* @assembly_address
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i32
  store i32 %116, i32* %stack_var_-12
  br label %block_7196

block_7196:                                       ; preds = %block_715c, %block_714e, %block_7109
  store volatile i64 29078, i64* @assembly_address
  %117 = load i32* %stack_var_-12
  %118 = and i32 %117, 15
  %119 = icmp ugt i32 %118, 15
  %120 = icmp ult i32 %117, 0
  %121 = xor i32 %117, 0
  %122 = and i32 %121, 0
  %123 = icmp slt i32 %122, 0
  store i1 %119, i1* %az
  store i1 %120, i1* %cf
  store i1 %123, i1* %of
  %124 = icmp eq i32 %117, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i32 %117, 0
  store i1 %125, i1* %sf
  %126 = trunc i32 %117 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  store volatile i64 29082, i64* @assembly_address
  %130 = load i1* %zf
  %131 = icmp eq i1 %130, false
  br i1 %131, label %block_71d7, label %block_719c

block_719c:                                       ; preds = %block_7196
  store volatile i64 29084, i64* @assembly_address
  %132 = load i64* @global_var_216580
  store i64 %132, i64* %rax
  store volatile i64 29091, i64* @assembly_address
  %133 = load i64* %rax
  store i64 %133, i64* %rcx
  store volatile i64 29094, i64* @assembly_address
  store i64 17, i64* %rdx
  store volatile i64 29099, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 29104, i64* @assembly_address
  store i64 ptrtoint ([18 x i8]* @global_var_11abd to i64), i64* %rdi
  store volatile i64 29111, i64* @assembly_address
  %134 = load i64* %rdi
  %135 = inttoptr i64 %134 to i64*
  %136 = load i64* %rsi
  %137 = trunc i64 %136 to i32
  %138 = load i64* %rdx
  %139 = trunc i64 %138 to i32
  %140 = load i64* %rcx
  %141 = inttoptr i64 %140 to %_IO_FILE*
  %142 = call i32 @fwrite(i64* %135, i32 %137, i32 %139, %_IO_FILE* %141)
  %143 = sext i32 %142 to i64
  store i64 %143, i64* %rax
  %144 = sext i32 %142 to i64
  store i64 %144, i64* %rax
  store volatile i64 29116, i64* @assembly_address
  %145 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %146 = zext i32 %145 to i64
  store i64 %146, i64* %rax
  store volatile i64 29122, i64* @assembly_address
  %147 = load i64* %rax
  %148 = trunc i64 %147 to i32
  %149 = load i64* %rax
  %150 = trunc i64 %149 to i32
  %151 = and i32 %148, %150
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %152 = icmp eq i32 %151, 0
  store i1 %152, i1* %zf
  %153 = icmp slt i32 %151, 0
  store i1 %153, i1* %sf
  %154 = trunc i32 %151 to i8
  %155 = call i8 @llvm.ctpop.i8(i8 %154)
  %156 = and i8 %155, 1
  %157 = icmp eq i8 %156, 0
  store i1 %157, i1* %pf
  store volatile i64 29124, i64* @assembly_address
  %158 = load i1* %zf
  %159 = icmp eq i1 %158, false
  br i1 %159, label %block_71d0, label %block_71c6

block_71c6:                                       ; preds = %block_719c
  store volatile i64 29126, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_71d0

block_71d0:                                       ; preds = %block_71c6, %block_719c
  store volatile i64 29136, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 29141, i64* @assembly_address
  br label %block_71ff

block_71d7:                                       ; preds = %block_7196, %block_70f3
  store volatile i64 29143, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 29150, i64* @assembly_address
  %160 = load i64* %rdi
  %161 = inttoptr i64 %160 to i64*
  %162 = bitcast i64* %161 to i8*
  %163 = call i64 @xunlink(i8* %162)
  store i64 %163, i64* %rax
  store i64 %163, i64* %rax
  store volatile i64 29155, i64* @assembly_address
  %164 = load i64* %rax
  %165 = trunc i64 %164 to i32
  %166 = load i64* %rax
  %167 = trunc i64 %166 to i32
  %168 = and i32 %165, %167
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %169 = icmp eq i32 %168, 0
  store i1 %169, i1* %zf
  %170 = icmp slt i32 %168, 0
  store i1 %170, i1* %sf
  %171 = trunc i32 %168 to i8
  %172 = call i8 @llvm.ctpop.i8(i8 %171)
  %173 = and i8 %172, 1
  %174 = icmp eq i8 %173, 0
  store i1 %174, i1* %pf
  store volatile i64 29157, i64* @assembly_address
  %175 = load i1* %zf
  br i1 %175, label %block_71fa, label %block_71e7

block_71e7:                                       ; preds = %block_71d7
  store volatile i64 29159, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 29166, i64* @assembly_address
  %176 = load i64* %rdi
  %177 = inttoptr i64 %176 to i8*
  %178 = call i64 @progerror(i8* %177)
  store i64 %178, i64* %rax
  store i64 %178, i64* %rax
  store volatile i64 29171, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 29176, i64* @assembly_address
  br label %block_71ff

block_71fa:                                       ; preds = %block_71d7
  store volatile i64 29178, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_71ff

block_71ff:                                       ; preds = %block_71fa, %block_71e7, %block_71d0
  store volatile i64 29183, i64* @assembly_address
  %179 = load i64* %stack_var_-8
  store i64 %179, i64* %rbp
  %180 = ptrtoint i64* %stack_var_0 to i64
  store i64 %180, i64* %rsp
  store volatile i64 29184, i64* @assembly_address
  %181 = load i64* %rax
  %182 = load i64* %rax
  ret i64 %182
}

define i64 @do_chown(i32 %arg1, i64* %arg2, i64 %arg3, i64 %arg4) {
block_7201:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg4, i64* %rcx
  store i64 %arg3, i64* %rdx
  %0 = ptrtoint i64* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-44 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 29185, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 29186, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 29189, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 48
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 48
  %10 = xor i64 %5, 48
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %20, i64* %rsp
  store volatile i64 29193, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = trunc i64 %21 to i32
  store i32 %22, i32* %stack_var_-28
  store volatile i64 29196, i64* @assembly_address
  %23 = load i64* %rsi
  store i64 %23, i64* %stack_var_-40
  store volatile i64 29200, i64* @assembly_address
  %24 = load i64* %rdx
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-32
  store volatile i64 29203, i64* @assembly_address
  %26 = load i64* %rcx
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-44
  store volatile i64 29206, i64* @assembly_address
  %28 = load i32* %stack_var_-44
  %29 = zext i32 %28 to i64
  store i64 %29, i64* %rdx
  store volatile i64 29209, i64* @assembly_address
  %30 = load i32* %stack_var_-32
  %31 = zext i32 %30 to i64
  store i64 %31, i64* %rcx
  store volatile i64 29212, i64* @assembly_address
  %32 = load i32* %stack_var_-28
  %33 = zext i32 %32 to i64
  store i64 %33, i64* %rax
  store volatile i64 29215, i64* @assembly_address
  %34 = load i64* %rcx
  %35 = trunc i64 %34 to i32
  %36 = zext i32 %35 to i64
  store i64 %36, i64* %rsi
  store volatile i64 29217, i64* @assembly_address
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i32
  %39 = zext i32 %38 to i64
  store i64 %39, i64* %rdi
  store volatile i64 29219, i64* @assembly_address
  %40 = load i64* %rdi
  %41 = trunc i64 %40 to i32
  %42 = load i64* %rsi
  %43 = trunc i64 %42 to i32
  %44 = load i64* %rdx
  %45 = trunc i64 %44 to i32
  %46 = call i32 @fchown(i32 %41, i32 %43, i32 %45)
  %47 = sext i32 %46 to i64
  store i64 %47, i64* %rax
  %48 = sext i32 %46 to i64
  store i64 %48, i64* %rax
  store volatile i64 29224, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  store i32 %50, i32* %stack_var_-12
  store volatile i64 29227, i64* @assembly_address
  store volatile i64 29228, i64* @assembly_address
  %51 = load i64* %stack_var_-8
  store i64 %51, i64* %rbp
  %52 = ptrtoint i64* %stack_var_0 to i64
  store i64 %52, i64* %rsp
  store volatile i64 29229, i64* @assembly_address
  %53 = load i64* %rax
  ret i64 %53
}

declare i64 @185(i64, i64*, i32, i64)

declare i64 @186(i64, i64*, i64, i32)

declare i64 @187(i64, i64*, i64, i64)

define i64 @copy_stat(i64* %arg1, i64 %arg2, i64 %arg3) {
block_722e:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-60 = alloca i32
  %stack_var_-64 = alloca i32
  %stack_var_-68 = alloca i32
  %stack_var_-73 = alloca i8
  %stack_var_-32 = alloca i64
  %stack_var_-40 = alloca i32*
  %1 = alloca i64
  %stack_var_-48 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-72 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-96 = alloca i64
  %stack_var_-104 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i64
  store volatile i64 29230, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 29231, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 29234, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 96
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 96
  %13 = xor i64 %8, 96
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-104 to i64
  store i64 %23, i64* %rsp
  store volatile i64 29238, i64* @assembly_address
  %24 = load i64* %rdi
  store i64 %24, i64* %stack_var_-96
  store volatile i64 29242, i64* @assembly_address
  %25 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %25, i64* %rax
  store volatile i64 29251, i64* @assembly_address
  %26 = load i64* %rax
  store i64 %26, i64* %stack_var_-16
  store volatile i64 29255, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %27 = icmp eq i32 0, 0
  store i1 %27, i1* %zf
  %28 = icmp slt i32 0, 0
  store i1 %28, i1* %sf
  %29 = trunc i32 0 to i8
  %30 = call i8 @llvm.ctpop.i8(i8 %29)
  %31 = and i8 %30, 1
  %32 = icmp eq i8 %31, 0
  store i1 %32, i1* %pf
  %33 = zext i32 0 to i64
  store i64 %33, i64* %rax
  store volatile i64 29257, i64* @assembly_address
  %34 = load i64* %stack_var_-96
  store i64 %34, i64* %rax
  store volatile i64 29261, i64* @assembly_address
  %35 = load i64* %rax
  %36 = add i64 %35, 24
  %37 = inttoptr i64 %36 to i32*
  %38 = load i32* %37
  %39 = zext i32 %38 to i64
  store i64 %39, i64* %rax
  store volatile i64 29264, i64* @assembly_address
  %40 = load i64* %rax
  %41 = trunc i64 %40 to i32
  %42 = and i32 %41, 511
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %43 = icmp eq i32 %42, 0
  store i1 %43, i1* %zf
  %44 = icmp slt i32 %42, 0
  store i1 %44, i1* %sf
  %45 = trunc i32 %42 to i8
  %46 = call i8 @llvm.ctpop.i8(i8 %45)
  %47 = and i8 %46, 1
  %48 = icmp eq i8 %47, 0
  store i1 %48, i1* %pf
  %49 = zext i32 %42 to i64
  store i64 %49, i64* %rax
  store volatile i64 29269, i64* @assembly_address
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  store i32 %51, i32* %stack_var_-72
  store volatile i64 29272, i64* @assembly_address
  %52 = load i64* %stack_var_-96
  store i64 %52, i64* %rax
  store volatile i64 29276, i64* @assembly_address
  %53 = load i64* %rax
  store i64 %53, i64* %rdi
  store volatile i64 29279, i64* @assembly_address
  %54 = load i64* %rdi
  %55 = call i64 @get_stat_atime(i64 %54)
  store i64 %55, i64* %rax
  store i64 %55, i64* %rax
  store volatile i64 29284, i64* @assembly_address
  %56 = load i64* %rax
  store i64 %56, i64* %stack_var_-56
  store volatile i64 29288, i64* @assembly_address
  %57 = load i64* %rdx
  store i64 %57, i64* %stack_var_-48
  store volatile i64 29292, i64* @assembly_address
  %58 = load i64* %stack_var_-96
  store i64 %58, i64* %rax
  store volatile i64 29296, i64* @assembly_address
  %59 = load i64* %rax
  store i64 %59, i64* %rdi
  store volatile i64 29299, i64* @assembly_address
  %60 = load i64* %rdi
  %61 = inttoptr i64 %60 to i64*
  %62 = call i64 @get_stat_mtime(i64* %61)
  store i64 %62, i64* %rax
  store i64 %62, i64* %rax
  store volatile i64 29304, i64* @assembly_address
  %63 = load i64* %rax
  %64 = inttoptr i64 %63 to i32*
  store i32* %64, i32** %stack_var_-40
  store volatile i64 29308, i64* @assembly_address
  %65 = load i64* %rdx
  store i64 %65, i64* %stack_var_-32
  store volatile i64 29312, i64* @assembly_address
  %66 = load i32* bitcast (i64* @global_var_216600 to i32*)
  %67 = zext i32 %66 to i64
  store i64 %67, i64* %rax
  store volatile i64 29318, i64* @assembly_address
  %68 = load i64* %rax
  %69 = trunc i64 %68 to i32
  %70 = load i64* %rax
  %71 = trunc i64 %70 to i32
  %72 = and i32 %69, %71
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %73 = icmp eq i32 %72, 0
  store i1 %73, i1* %zf
  %74 = icmp slt i32 %72, 0
  store i1 %74, i1* %sf
  %75 = trunc i32 %72 to i8
  %76 = call i8 @llvm.ctpop.i8(i8 %75)
  %77 = and i8 %76, 1
  %78 = icmp eq i8 %77, 0
  store i1 %78, i1* %pf
  store volatile i64 29320, i64* @assembly_address
  %79 = load i1* %zf
  br i1 %79, label %block_72bd, label %block_728a

block_728a:                                       ; preds = %block_722e
  store volatile i64 29322, i64* @assembly_address
  %80 = load i64* @global_var_25f4d8
  store i64 %80, i64* %rax
  store volatile i64 29329, i64* @assembly_address
  %81 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %82 = icmp eq i64 %81, 0
  store i1 %82, i1* %zf
  %83 = icmp slt i64 %81, 0
  store i1 %83, i1* %sf
  %84 = trunc i64 %81 to i8
  %85 = call i8 @llvm.ctpop.i8(i8 %84)
  %86 = and i8 %85, 1
  %87 = icmp eq i8 %86, 0
  store i1 %87, i1* %pf
  store volatile i64 29332, i64* @assembly_address
  %88 = load i1* %sf
  br i1 %88, label %block_72bd, label %block_7296

block_7296:                                       ; preds = %block_728a
  store volatile i64 29334, i64* @assembly_address
  %89 = load i32** %stack_var_-40
  %90 = ptrtoint i32* %89 to i64
  store i64 %90, i64* %rdx
  store volatile i64 29338, i64* @assembly_address
  %91 = load i64* @global_var_25f4d0
  store i64 %91, i64* %rax
  store volatile i64 29345, i64* @assembly_address
  %92 = load i64* %rdx
  %93 = load i64* %rax
  %94 = sub i64 %92, %93
  %95 = and i64 %92, 15
  %96 = and i64 %93, 15
  %97 = sub i64 %95, %96
  %98 = icmp ugt i64 %97, 15
  %99 = icmp ult i64 %92, %93
  %100 = xor i64 %92, %93
  %101 = xor i64 %92, %94
  %102 = and i64 %100, %101
  %103 = icmp slt i64 %102, 0
  store i1 %98, i1* %az
  store i1 %99, i1* %cf
  store i1 %103, i1* %of
  %104 = icmp eq i64 %94, 0
  store i1 %104, i1* %zf
  %105 = icmp slt i64 %94, 0
  store i1 %105, i1* %sf
  %106 = trunc i64 %94 to i8
  %107 = call i8 @llvm.ctpop.i8(i8 %106)
  %108 = and i8 %107, 1
  %109 = icmp eq i8 %108, 0
  store i1 %109, i1* %pf
  store volatile i64 29348, i64* @assembly_address
  %110 = load i1* %zf
  %111 = icmp eq i1 %110, false
  br i1 %111, label %block_72b6, label %block_72a6

block_72a6:                                       ; preds = %block_7296
  store volatile i64 29350, i64* @assembly_address
  %112 = load i64* %stack_var_-32
  store i64 %112, i64* %rdx
  store volatile i64 29354, i64* @assembly_address
  %113 = load i64* @global_var_25f4d8
  store i64 %113, i64* %rax
  store volatile i64 29361, i64* @assembly_address
  %114 = load i64* %rdx
  %115 = load i64* %rax
  %116 = sub i64 %114, %115
  %117 = and i64 %114, 15
  %118 = and i64 %115, 15
  %119 = sub i64 %117, %118
  %120 = icmp ugt i64 %119, 15
  %121 = icmp ult i64 %114, %115
  %122 = xor i64 %114, %115
  %123 = xor i64 %114, %116
  %124 = and i64 %122, %123
  %125 = icmp slt i64 %124, 0
  store i1 %120, i1* %az
  store i1 %121, i1* %cf
  store i1 %125, i1* %of
  %126 = icmp eq i64 %116, 0
  store i1 %126, i1* %zf
  %127 = icmp slt i64 %116, 0
  store i1 %127, i1* %sf
  %128 = trunc i64 %116 to i8
  %129 = call i8 @llvm.ctpop.i8(i8 %128)
  %130 = and i8 %129, 1
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %pf
  store volatile i64 29364, i64* @assembly_address
  %132 = load i1* %zf
  br i1 %132, label %block_72bd, label %block_72b6

block_72b6:                                       ; preds = %block_72a6, %block_7296
  store volatile i64 29366, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 29371, i64* @assembly_address
  br label %block_72c2

block_72bd:                                       ; preds = %block_72a6, %block_728a, %block_722e
  store volatile i64 29373, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_72c2

block_72c2:                                       ; preds = %block_72bd, %block_72b6
  store volatile i64 29378, i64* @assembly_address
  %133 = load i64* %rax
  %134 = trunc i64 %133 to i8
  store i8 %134, i8* %stack_var_-73
  store volatile i64 29381, i64* @assembly_address
  %135 = load i8* %stack_var_-73
  %136 = and i8 %135, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %137 = icmp eq i8 %136, 0
  store i1 %137, i1* %zf
  %138 = icmp slt i8 %136, 0
  store i1 %138, i1* %sf
  %139 = call i8 @llvm.ctpop.i8(i8 %136)
  %140 = and i8 %139, 1
  %141 = icmp eq i8 %140, 0
  store i1 %141, i1* %pf
  store i8 %136, i8* %stack_var_-73
  store volatile i64 29385, i64* @assembly_address
  %142 = load i8* %stack_var_-73
  %143 = and i8 %142, 15
  %144 = icmp ugt i8 %143, 15
  %145 = icmp ult i8 %142, 0
  %146 = xor i8 %142, 0
  %147 = and i8 %146, 0
  %148 = icmp slt i8 %147, 0
  store i1 %144, i1* %az
  store i1 %145, i1* %cf
  store i1 %148, i1* %of
  %149 = icmp eq i8 %142, 0
  store i1 %149, i1* %zf
  %150 = icmp slt i8 %142, 0
  store i1 %150, i1* %sf
  %151 = call i8 @llvm.ctpop.i8(i8 %142)
  %152 = and i8 %151, 1
  %153 = icmp eq i8 %152, 0
  store i1 %153, i1* %pf
  store volatile i64 29389, i64* @assembly_address
  %154 = load i1* %zf
  br i1 %154, label %block_72e5, label %block_72cf

block_72cf:                                       ; preds = %block_72c2
  store volatile i64 29391, i64* @assembly_address
  %155 = load i64* @global_var_25f4d0
  store i64 %155, i64* %rax
  store volatile i64 29398, i64* @assembly_address
  %156 = load i64* @global_var_25f4d8
  store i64 %156, i64* %rdx
  store volatile i64 29405, i64* @assembly_address
  %157 = load i64* %rax
  %158 = inttoptr i64 %157 to i32*
  store i32* %158, i32** %stack_var_-40
  store volatile i64 29409, i64* @assembly_address
  %159 = load i64* %rdx
  store i64 %159, i64* %stack_var_-32
  br label %block_72e5

block_72e5:                                       ; preds = %block_72cf, %block_72c2
  store volatile i64 29413, i64* @assembly_address
  %160 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %161 = zext i32 %160 to i64
  store i64 %161, i64* %rax
  store volatile i64 29419, i64* @assembly_address
  %162 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %162, i64* %rdx
  store volatile i64 29423, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rsi
  store volatile i64 29430, i64* @assembly_address
  %163 = load i64* %rax
  %164 = trunc i64 %163 to i32
  %165 = zext i32 %164 to i64
  store i64 %165, i64* %rdi
  store volatile i64 29432, i64* @assembly_address
  %166 = load i64* %rdi
  %167 = load i64* %rsi
  %168 = inttoptr i64 %167 to i64*
  %169 = load i64* %rdx
  %170 = inttoptr i64 %169 to i64*
  %171 = trunc i64 %166 to i32
  %172 = call i64 @fdutimens(i32 %171, i64* %168, i64* %170)
  store i64 %172, i64* %rax
  store i64 %172, i64* %rax
  store volatile i64 29437, i64* @assembly_address
  %173 = load i64* %rax
  %174 = trunc i64 %173 to i32
  %175 = load i64* %rax
  %176 = trunc i64 %175 to i32
  %177 = and i32 %174, %176
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %178 = icmp eq i32 %177, 0
  store i1 %178, i1* %zf
  %179 = icmp slt i32 %177, 0
  store i1 %179, i1* %sf
  %180 = trunc i32 %177 to i8
  %181 = call i8 @llvm.ctpop.i8(i8 %180)
  %182 = and i8 %181, 1
  %183 = icmp eq i8 %182, 0
  store i1 %183, i1* %pf
  store volatile i64 29439, i64* @assembly_address
  %184 = load i1* %zf
  %185 = icmp eq i1 %184, false
  br i1 %185, label %block_733e, label %block_7301

block_7301:                                       ; preds = %block_72e5
  store volatile i64 29441, i64* @assembly_address
  %186 = load i8* %stack_var_-73
  %187 = and i8 %186, 15
  %188 = icmp ugt i8 %187, 15
  %189 = icmp ult i8 %186, 0
  %190 = xor i8 %186, 0
  %191 = and i8 %190, 0
  %192 = icmp slt i8 %191, 0
  store i1 %188, i1* %az
  store i1 %189, i1* %cf
  store i1 %192, i1* %of
  %193 = icmp eq i8 %186, 0
  store i1 %193, i1* %zf
  %194 = icmp slt i8 %186, 0
  store i1 %194, i1* %sf
  %195 = call i8 @llvm.ctpop.i8(i8 %186)
  %196 = and i8 %195, 1
  %197 = icmp eq i8 %196, 0
  store i1 %197, i1* %pf
  store volatile i64 29445, i64* @assembly_address
  %198 = load i1* %zf
  br i1 %198, label %block_73ab, label %block_730b

block_730b:                                       ; preds = %block_7301
  store volatile i64 29451, i64* @assembly_address
  %199 = load i32* bitcast (i64* @global_var_2165e4 to i32*)
  %200 = zext i32 %199 to i64
  store i64 %200, i64* %rax
  store volatile i64 29457, i64* @assembly_address
  %201 = load i64* %rax
  %202 = trunc i64 %201 to i32
  %203 = trunc i64 %201 to i32
  store i32 %203, i32* %3
  store i32 1, i32* %2
  %204 = sub i32 %202, 1
  %205 = and i32 %202, 15
  %206 = sub i32 %205, 1
  %207 = icmp ugt i32 %206, 15
  %208 = icmp ult i32 %202, 1
  %209 = xor i32 %202, 1
  %210 = xor i32 %202, %204
  %211 = and i32 %209, %210
  %212 = icmp slt i32 %211, 0
  store i1 %207, i1* %az
  store i1 %208, i1* %cf
  store i1 %212, i1* %of
  %213 = icmp eq i32 %204, 0
  store i1 %213, i1* %zf
  %214 = icmp slt i32 %204, 0
  store i1 %214, i1* %sf
  %215 = trunc i32 %204 to i8
  %216 = call i8 @llvm.ctpop.i8(i8 %215)
  %217 = and i8 %216, 1
  %218 = icmp eq i8 %217, 0
  store i1 %218, i1* %pf
  store volatile i64 29460, i64* @assembly_address
  %219 = load i32* %3
  %220 = sext i32 %219 to i64
  %221 = load i32* %2
  %222 = trunc i64 %220 to i32
  %223 = icmp sle i32 %222, %221
  br i1 %223, label %block_73ab, label %block_731a

block_731a:                                       ; preds = %block_730b
  store volatile i64 29466, i64* @assembly_address
  %224 = load i64* @global_var_216580
  store i64 %224, i64* %rax
  store volatile i64 29473, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdx
  store volatile i64 29480, i64* @assembly_address
  store i64 ptrtoint ([24 x i8]* @global_var_11acf to i64), i64* %rsi
  store volatile i64 29487, i64* @assembly_address
  %225 = load i64* %rax
  store i64 %225, i64* %rdi
  store volatile i64 29490, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 29495, i64* @assembly_address
  %226 = load i64* %rdi
  %227 = inttoptr i64 %226 to %_IO_FILE*
  %228 = load i64* %rsi
  %229 = inttoptr i64 %228 to i8*
  %230 = load i64* %rdx
  %231 = inttoptr i64 %230 to i8*
  %232 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %227, i8* %229, i8* %231)
  %233 = sext i32 %232 to i64
  store i64 %233, i64* %rax
  %234 = sext i32 %232 to i64
  store i64 %234, i64* %rax
  store volatile i64 29500, i64* @assembly_address
  br label %block_73ab

block_733e:                                       ; preds = %block_72e5
  store volatile i64 29502, i64* @assembly_address
  %235 = call i32* @__errno_location()
  %236 = ptrtoint i32* %235 to i64
  store i64 %236, i64* %rax
  %237 = ptrtoint i32* %235 to i64
  store i64 %237, i64* %rax
  %238 = ptrtoint i32* %235 to i64
  store i64 %238, i64* %rax
  store volatile i64 29507, i64* @assembly_address
  %239 = load i64* %rax
  %240 = inttoptr i64 %239 to i32*
  %241 = load i32* %240
  %242 = zext i32 %241 to i64
  store i64 %242, i64* %rax
  store volatile i64 29509, i64* @assembly_address
  %243 = load i64* %rax
  %244 = trunc i64 %243 to i32
  store i32 %244, i32* %stack_var_-68
  store volatile i64 29512, i64* @assembly_address
  %245 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %246 = zext i32 %245 to i64
  store i64 %246, i64* %rax
  store volatile i64 29518, i64* @assembly_address
  %247 = load i64* %rax
  %248 = trunc i64 %247 to i32
  %249 = load i64* %rax
  %250 = trunc i64 %249 to i32
  %251 = and i32 %248, %250
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %252 = icmp eq i32 %251, 0
  store i1 %252, i1* %zf
  %253 = icmp slt i32 %251, 0
  store i1 %253, i1* %sf
  %254 = trunc i32 %251 to i8
  %255 = call i8 @llvm.ctpop.i8(i8 %254)
  %256 = and i8 %255, 1
  %257 = icmp eq i8 %256, 0
  store i1 %257, i1* %pf
  store volatile i64 29520, i64* @assembly_address
  %258 = load i1* %zf
  %259 = icmp eq i1 %258, false
  br i1 %259, label %block_7374, label %block_7352

block_7352:                                       ; preds = %block_733e
  store volatile i64 29522, i64* @assembly_address
  %260 = load i64* @global_var_25f4c8
  store i64 %260, i64* %rdx
  store volatile i64 29529, i64* @assembly_address
  %261 = load i64* @global_var_216580
  store i64 %261, i64* %rax
  store volatile i64 29536, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11035 to i64), i64* %rsi
  store volatile i64 29543, i64* @assembly_address
  %262 = load i64* %rax
  store i64 %262, i64* %rdi
  store volatile i64 29546, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 29551, i64* @assembly_address
  %263 = load i64* %rdi
  %264 = inttoptr i64 %263 to %_IO_FILE*
  %265 = load i64* %rsi
  %266 = inttoptr i64 %265 to i8*
  %267 = load i64* %rdx
  %268 = inttoptr i64 %267 to i8*
  %269 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %264, i8* %266, i8* %268)
  %270 = sext i32 %269 to i64
  store i64 %270, i64* %rax
  %271 = sext i32 %269 to i64
  store i64 %271, i64* %rax
  br label %block_7374

block_7374:                                       ; preds = %block_7352, %block_733e
  store volatile i64 29556, i64* @assembly_address
  %272 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %273 = zext i32 %272 to i64
  store i64 %273, i64* %rax
  store volatile i64 29562, i64* @assembly_address
  %274 = load i64* %rax
  %275 = trunc i64 %274 to i32
  %276 = load i64* %rax
  %277 = trunc i64 %276 to i32
  %278 = and i32 %275, %277
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %279 = icmp eq i32 %278, 0
  store i1 %279, i1* %zf
  %280 = icmp slt i32 %278, 0
  store i1 %280, i1* %sf
  %281 = trunc i32 %278 to i8
  %282 = call i8 @llvm.ctpop.i8(i8 %281)
  %283 = and i8 %282, 1
  %284 = icmp eq i8 %283, 0
  store i1 %284, i1* %pf
  store volatile i64 29564, i64* @assembly_address
  %285 = load i1* %zf
  %286 = icmp eq i1 %285, false
  br i1 %286, label %block_7388, label %block_737e

block_737e:                                       ; preds = %block_7374
  store volatile i64 29566, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_7388

block_7388:                                       ; preds = %block_737e, %block_7374
  store volatile i64 29576, i64* @assembly_address
  %287 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %288 = zext i32 %287 to i64
  store i64 %288, i64* %rax
  store volatile i64 29582, i64* @assembly_address
  %289 = load i64* %rax
  %290 = trunc i64 %289 to i32
  %291 = load i64* %rax
  %292 = trunc i64 %291 to i32
  %293 = and i32 %290, %292
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %294 = icmp eq i32 %293, 0
  store i1 %294, i1* %zf
  %295 = icmp slt i32 %293, 0
  store i1 %295, i1* %sf
  %296 = trunc i32 %293 to i8
  %297 = call i8 @llvm.ctpop.i8(i8 %296)
  %298 = and i8 %297, 1
  %299 = icmp eq i8 %298, 0
  store i1 %299, i1* %pf
  store volatile i64 29584, i64* @assembly_address
  %300 = load i1* %zf
  %301 = icmp eq i1 %300, false
  br i1 %301, label %block_73ab, label %block_7392

block_7392:                                       ; preds = %block_7388
  store volatile i64 29586, i64* @assembly_address
  %302 = call i32* @__errno_location()
  %303 = ptrtoint i32* %302 to i64
  store i64 %303, i64* %rax
  %304 = ptrtoint i32* %302 to i64
  store i64 %304, i64* %rax
  %305 = ptrtoint i32* %302 to i64
  store i64 %305, i64* %rax
  store volatile i64 29591, i64* @assembly_address
  %306 = load i64* %rax
  store i64 %306, i64* %rdx
  store volatile i64 29594, i64* @assembly_address
  %307 = load i32* %stack_var_-68
  %308 = zext i32 %307 to i64
  store i64 %308, i64* %rax
  store volatile i64 29597, i64* @assembly_address
  %309 = load i64* %rax
  %310 = trunc i64 %309 to i32
  %311 = load i64* %rdx
  %312 = inttoptr i64 %311 to i32*
  store i32 %310, i32* %312
  store volatile i64 29599, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 29606, i64* @assembly_address
  %313 = load i64* %rdi
  %314 = inttoptr i64 %313 to i8*
  call void @perror(i8* %314)
  br label %block_73ab

block_73ab:                                       ; preds = %block_7392, %block_7388, %block_731a, %block_730b, %block_7301
  store volatile i64 29611, i64* @assembly_address
  %315 = load i64* %stack_var_-96
  store i64 %315, i64* %rax
  store volatile i64 29615, i64* @assembly_address
  %316 = load i64* %rax
  %317 = add i64 %316, 32
  %318 = inttoptr i64 %317 to i32*
  %319 = load i32* %318
  %320 = zext i32 %319 to i64
  store i64 %320, i64* %rdx
  store volatile i64 29618, i64* @assembly_address
  %321 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %322 = zext i32 %321 to i64
  store i64 %322, i64* %rax
  store volatile i64 29624, i64* @assembly_address
  %323 = load i64* %rdx
  %324 = trunc i64 %323 to i32
  %325 = zext i32 %324 to i64
  store i64 %325, i64* %rcx
  store volatile i64 29626, i64* @assembly_address
  store i64 4294967295, i64* %rdx
  store volatile i64 29631, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rsi
  store volatile i64 29638, i64* @assembly_address
  %326 = load i64* %rax
  %327 = trunc i64 %326 to i32
  %328 = zext i32 %327 to i64
  store i64 %328, i64* %rdi
  store volatile i64 29640, i64* @assembly_address
  %329 = load i64* %rdi
  %330 = load i64* %rsi
  %331 = inttoptr i64 %330 to i64*
  %332 = load i64* %rdx
  %333 = load i64* %rcx
  %334 = trunc i64 %329 to i32
  %335 = call i64 @do_chown(i32 %334, i64* %331, i64 %332, i64 %333)
  store i64 %335, i64* %rax
  store i64 %335, i64* %rax
  store volatile i64 29645, i64* @assembly_address
  %336 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %337 = zext i32 %336 to i64
  store i64 %337, i64* %rax
  store volatile i64 29651, i64* @assembly_address
  %338 = load i32* %stack_var_-72
  %339 = zext i32 %338 to i64
  store i64 %339, i64* %rdx
  store volatile i64 29654, i64* @assembly_address
  %340 = load i64* %rdx
  %341 = trunc i64 %340 to i32
  %342 = zext i32 %341 to i64
  store i64 %342, i64* %rsi
  store volatile i64 29656, i64* @assembly_address
  %343 = load i64* %rax
  %344 = trunc i64 %343 to i32
  %345 = zext i32 %344 to i64
  store i64 %345, i64* %rdi
  store volatile i64 29658, i64* @assembly_address
  %346 = load i64* %rdi
  %347 = trunc i64 %346 to i32
  %348 = load i64* %rsi
  %349 = trunc i64 %348 to i32
  %350 = call i32 @fchmod(i32 %347, i32 %349)
  %351 = sext i32 %350 to i64
  store i64 %351, i64* %rax
  %352 = sext i32 %350 to i64
  store i64 %352, i64* %rax
  store volatile i64 29663, i64* @assembly_address
  %353 = load i64* %rax
  %354 = trunc i64 %353 to i32
  store i32 %354, i32* %stack_var_-64
  store volatile i64 29666, i64* @assembly_address
  %355 = load i32* %stack_var_-64
  %356 = and i32 %355, 15
  %357 = icmp ugt i32 %356, 15
  %358 = icmp ult i32 %355, 0
  %359 = xor i32 %355, 0
  %360 = and i32 %359, 0
  %361 = icmp slt i32 %360, 0
  store i1 %357, i1* %az
  store i1 %358, i1* %cf
  store i1 %361, i1* %of
  %362 = icmp eq i32 %355, 0
  store i1 %362, i1* %zf
  %363 = icmp slt i32 %355, 0
  store i1 %363, i1* %sf
  %364 = trunc i32 %355 to i8
  %365 = call i8 @llvm.ctpop.i8(i8 %364)
  %366 = and i8 %365, 1
  %367 = icmp eq i8 %366, 0
  store i1 %367, i1* %pf
  store volatile i64 29670, i64* @assembly_address
  %368 = load i1* %zf
  br i1 %368, label %block_7455, label %block_73e8

block_73e8:                                       ; preds = %block_73ab
  store volatile i64 29672, i64* @assembly_address
  %369 = call i32* @__errno_location()
  %370 = ptrtoint i32* %369 to i64
  store i64 %370, i64* %rax
  %371 = ptrtoint i32* %369 to i64
  store i64 %371, i64* %rax
  %372 = ptrtoint i32* %369 to i64
  store i64 %372, i64* %rax
  store volatile i64 29677, i64* @assembly_address
  %373 = load i64* %rax
  %374 = inttoptr i64 %373 to i32*
  %375 = load i32* %374
  %376 = zext i32 %375 to i64
  store i64 %376, i64* %rax
  store volatile i64 29679, i64* @assembly_address
  %377 = load i64* %rax
  %378 = trunc i64 %377 to i32
  store i32 %378, i32* %stack_var_-60
  store volatile i64 29682, i64* @assembly_address
  %379 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %380 = zext i32 %379 to i64
  store i64 %380, i64* %rax
  store volatile i64 29688, i64* @assembly_address
  %381 = load i64* %rax
  %382 = trunc i64 %381 to i32
  %383 = load i64* %rax
  %384 = trunc i64 %383 to i32
  %385 = and i32 %382, %384
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %386 = icmp eq i32 %385, 0
  store i1 %386, i1* %zf
  %387 = icmp slt i32 %385, 0
  store i1 %387, i1* %sf
  %388 = trunc i32 %385 to i8
  %389 = call i8 @llvm.ctpop.i8(i8 %388)
  %390 = and i8 %389, 1
  %391 = icmp eq i8 %390, 0
  store i1 %391, i1* %pf
  store volatile i64 29690, i64* @assembly_address
  %392 = load i1* %zf
  %393 = icmp eq i1 %392, false
  br i1 %393, label %block_741e, label %block_73fc

block_73fc:                                       ; preds = %block_73e8
  store volatile i64 29692, i64* @assembly_address
  %394 = load i64* @global_var_25f4c8
  store i64 %394, i64* %rdx
  store volatile i64 29699, i64* @assembly_address
  %395 = load i64* @global_var_216580
  store i64 %395, i64* %rax
  store volatile i64 29706, i64* @assembly_address
  store i64 ptrtoint ([5 x i8]* @global_var_11035 to i64), i64* %rsi
  store volatile i64 29713, i64* @assembly_address
  %396 = load i64* %rax
  store i64 %396, i64* %rdi
  store volatile i64 29716, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 29721, i64* @assembly_address
  %397 = load i64* %rdi
  %398 = inttoptr i64 %397 to %_IO_FILE*
  %399 = load i64* %rsi
  %400 = inttoptr i64 %399 to i8*
  %401 = load i64* %rdx
  %402 = inttoptr i64 %401 to i8*
  %403 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %398, i8* %400, i8* %402)
  %404 = sext i32 %403 to i64
  store i64 %404, i64* %rax
  %405 = sext i32 %403 to i64
  store i64 %405, i64* %rax
  br label %block_741e

block_741e:                                       ; preds = %block_73fc, %block_73e8
  store volatile i64 29726, i64* @assembly_address
  %406 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %407 = zext i32 %406 to i64
  store i64 %407, i64* %rax
  store volatile i64 29732, i64* @assembly_address
  %408 = load i64* %rax
  %409 = trunc i64 %408 to i32
  %410 = load i64* %rax
  %411 = trunc i64 %410 to i32
  %412 = and i32 %409, %411
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %413 = icmp eq i32 %412, 0
  store i1 %413, i1* %zf
  %414 = icmp slt i32 %412, 0
  store i1 %414, i1* %sf
  %415 = trunc i32 %412 to i8
  %416 = call i8 @llvm.ctpop.i8(i8 %415)
  %417 = and i8 %416, 1
  %418 = icmp eq i8 %417, 0
  store i1 %418, i1* %pf
  store volatile i64 29734, i64* @assembly_address
  %419 = load i1* %zf
  %420 = icmp eq i1 %419, false
  br i1 %420, label %block_7432, label %block_7428

block_7428:                                       ; preds = %block_741e
  store volatile i64 29736, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_7432

block_7432:                                       ; preds = %block_7428, %block_741e
  store volatile i64 29746, i64* @assembly_address
  %421 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %422 = zext i32 %421 to i64
  store i64 %422, i64* %rax
  store volatile i64 29752, i64* @assembly_address
  %423 = load i64* %rax
  %424 = trunc i64 %423 to i32
  %425 = load i64* %rax
  %426 = trunc i64 %425 to i32
  %427 = and i32 %424, %426
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %428 = icmp eq i32 %427, 0
  store i1 %428, i1* %zf
  %429 = icmp slt i32 %427, 0
  store i1 %429, i1* %sf
  %430 = trunc i32 %427 to i8
  %431 = call i8 @llvm.ctpop.i8(i8 %430)
  %432 = and i8 %431, 1
  %433 = icmp eq i8 %432, 0
  store i1 %433, i1* %pf
  store volatile i64 29754, i64* @assembly_address
  %434 = load i1* %zf
  %435 = icmp eq i1 %434, false
  br i1 %435, label %block_7455, label %block_743c

block_743c:                                       ; preds = %block_7432
  store volatile i64 29756, i64* @assembly_address
  %436 = call i32* @__errno_location()
  %437 = ptrtoint i32* %436 to i64
  store i64 %437, i64* %rax
  %438 = ptrtoint i32* %436 to i64
  store i64 %438, i64* %rax
  %439 = ptrtoint i32* %436 to i64
  store i64 %439, i64* %rax
  store volatile i64 29761, i64* @assembly_address
  %440 = load i64* %rax
  store i64 %440, i64* %rdx
  store volatile i64 29764, i64* @assembly_address
  %441 = load i32* %stack_var_-60
  %442 = zext i32 %441 to i64
  store i64 %442, i64* %rax
  store volatile i64 29767, i64* @assembly_address
  %443 = load i64* %rax
  %444 = trunc i64 %443 to i32
  %445 = load i64* %rdx
  %446 = inttoptr i64 %445 to i32*
  store i32 %444, i32* %446
  store volatile i64 29769, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 29776, i64* @assembly_address
  %447 = load i64* %rdi
  %448 = inttoptr i64 %447 to i8*
  call void @perror(i8* %448)
  br label %block_7455

block_7455:                                       ; preds = %block_743c, %block_7432, %block_73ab
  store volatile i64 29781, i64* @assembly_address
  %449 = load i64* %stack_var_-96
  store i64 %449, i64* %rax
  store volatile i64 29785, i64* @assembly_address
  %450 = load i64* %rax
  %451 = add i64 %450, 28
  %452 = inttoptr i64 %451 to i32*
  %453 = load i32* %452
  %454 = zext i32 %453 to i64
  store i64 %454, i64* %rdx
  store volatile i64 29788, i64* @assembly_address
  %455 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %456 = zext i32 %455 to i64
  store i64 %456, i64* %rax
  store volatile i64 29794, i64* @assembly_address
  store i64 4294967295, i64* %rcx
  store volatile i64 29799, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rsi
  store volatile i64 29806, i64* @assembly_address
  %457 = load i64* %rax
  %458 = trunc i64 %457 to i32
  %459 = zext i32 %458 to i64
  store i64 %459, i64* %rdi
  store volatile i64 29808, i64* @assembly_address
  %460 = load i64* %rdi
  %461 = load i64* %rsi
  %462 = inttoptr i64 %461 to i64*
  %463 = load i64* %rdx
  %464 = load i64* %rcx
  %465 = trunc i64 %460 to i32
  %466 = call i64 @do_chown(i32 %465, i64* %462, i64 %463, i64 %464)
  store i64 %466, i64* %rax
  store i64 %466, i64* %rax
  store volatile i64 29813, i64* @assembly_address
  store volatile i64 29814, i64* @assembly_address
  %467 = load i64* %stack_var_-16
  store i64 %467, i64* %rax
  store volatile i64 29818, i64* @assembly_address
  %468 = load i64* %rax
  %469 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %470 = xor i64 %468, %469
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %471 = icmp eq i64 %470, 0
  store i1 %471, i1* %zf
  %472 = icmp slt i64 %470, 0
  store i1 %472, i1* %sf
  %473 = trunc i64 %470 to i8
  %474 = call i8 @llvm.ctpop.i8(i8 %473)
  %475 = and i8 %474, 1
  %476 = icmp eq i8 %475, 0
  store i1 %476, i1* %pf
  store i64 %470, i64* %rax
  store volatile i64 29827, i64* @assembly_address
  %477 = load i1* %zf
  br i1 %477, label %block_748a, label %block_7485

block_7485:                                       ; preds = %block_7455
  store volatile i64 29829, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_748a:                                       ; preds = %block_7455
  store volatile i64 29834, i64* @assembly_address
  %478 = load i64* %stack_var_-8
  store i64 %478, i64* %rbp
  %479 = ptrtoint i64* %stack_var_0 to i64
  store i64 %479, i64* %rsp
  store volatile i64 29835, i64* @assembly_address
  %480 = load i64* %rax
  ret i64 %480
}

define i64 @treat_dir(i32 %arg1, i64 %arg2) {
block_748c:
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-1048 = alloca i64
  %stack_var_-1056 = alloca i8*
  %1 = alloca i64
  %stack_var_-1080 = alloca i8*
  %2 = alloca i64
  %stack_var_-1088 = alloca i8*
  %3 = alloca i64
  %stack_var_-1064 = alloca i64
  %stack_var_-1072 = alloca %__dirstream*
  %4 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-1100 = alloca i32
  %stack_var_-1112 = alloca i8*
  %5 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 29836, i64* @assembly_address
  %6 = load i64* %rbp
  store i64 %6, i64* %stack_var_-8
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rsp
  store volatile i64 29837, i64* @assembly_address
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rbp
  store volatile i64 29840, i64* @assembly_address
  %9 = load i64* %rsp
  %10 = sub i64 %9, 1104
  %11 = and i64 %9, 15
  %12 = icmp ugt i64 %11, 15
  %13 = icmp ult i64 %9, 1104
  %14 = xor i64 %9, 1104
  %15 = xor i64 %9, %10
  %16 = and i64 %14, %15
  %17 = icmp slt i64 %16, 0
  store i1 %12, i1* %az
  store i1 %13, i1* %cf
  store i1 %17, i1* %of
  %18 = icmp eq i64 %10, 0
  store i1 %18, i1* %zf
  %19 = icmp slt i64 %10, 0
  store i1 %19, i1* %sf
  %20 = trunc i64 %10 to i8
  %21 = call i8 @llvm.ctpop.i8(i8 %20)
  %22 = and i8 %21, 1
  %23 = icmp eq i8 %22, 0
  store i1 %23, i1* %pf
  %24 = ptrtoint i8** %stack_var_-1112 to i64
  store i64 %24, i64* %rsp
  store volatile i64 29847, i64* @assembly_address
  %25 = load i64* %rdi
  %26 = trunc i64 %25 to i32
  store i32 %26, i32* %stack_var_-1100
  store volatile i64 29853, i64* @assembly_address
  %27 = load i64* %rsi
  %28 = inttoptr i64 %27 to i8*
  store i8* %28, i8** %stack_var_-1112
  store volatile i64 29860, i64* @assembly_address
  %29 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %29, i64* %rax
  store volatile i64 29869, i64* @assembly_address
  %30 = load i64* %rax
  store i64 %30, i64* %stack_var_-16
  store volatile i64 29873, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %31 = icmp eq i32 0, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i32 0, 0
  store i1 %32, i1* %sf
  %33 = trunc i32 0 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  %37 = zext i32 0 to i64
  store i64 %37, i64* %rax
  store volatile i64 29875, i64* @assembly_address
  %38 = load i32* %stack_var_-1100
  %39 = zext i32 %38 to i64
  store i64 %39, i64* %rax
  store volatile i64 29881, i64* @assembly_address
  %40 = load i64* %rax
  %41 = trunc i64 %40 to i32
  %42 = zext i32 %41 to i64
  store i64 %42, i64* %rdi
  store volatile i64 29883, i64* @assembly_address
  %43 = load i64* %rdi
  %44 = trunc i64 %43 to i32
  %45 = call %__dirstream* @fdopendir(i32 %44)
  %46 = ptrtoint %__dirstream* %45 to i64
  store i64 %46, i64* %rax
  %47 = ptrtoint %__dirstream* %45 to i64
  store i64 %47, i64* %rax
  store volatile i64 29888, i64* @assembly_address
  %48 = load i64* %rax
  %49 = inttoptr i64 %48 to %__dirstream*
  store %__dirstream* %49, %__dirstream** %stack_var_-1072
  store volatile i64 29895, i64* @assembly_address
  %50 = load %__dirstream** %stack_var_-1072
  %51 = ptrtoint %__dirstream* %50 to i64
  %52 = and i64 %51, 15
  %53 = icmp ugt i64 %52, 15
  %54 = icmp ult i64 %51, 0
  %55 = xor i64 %51, 0
  %56 = and i64 %55, 0
  %57 = icmp slt i64 %56, 0
  store i1 %53, i1* %az
  store i1 %54, i1* %cf
  store i1 %57, i1* %of
  %58 = icmp eq i64 %51, 0
  store i1 %58, i1* %zf
  %59 = icmp slt i64 %51, 0
  store i1 %59, i1* %sf
  %60 = trunc i64 %51 to i8
  %61 = call i8 @llvm.ctpop.i8(i8 %60)
  %62 = and i8 %61, 1
  %63 = icmp eq i8 %62, 0
  store i1 %63, i1* %pf
  store volatile i64 29903, i64* @assembly_address
  %64 = load i1* %zf
  %65 = icmp eq i1 %64, false
  br i1 %65, label %block_74f2, label %block_74d1

block_74d1:                                       ; preds = %block_748c
  store volatile i64 29905, i64* @assembly_address
  %66 = load i8** %stack_var_-1112
  %67 = ptrtoint i8* %66 to i64
  store i64 %67, i64* %rax
  store volatile i64 29912, i64* @assembly_address
  %68 = load i64* %rax
  store i64 %68, i64* %rdi
  store volatile i64 29915, i64* @assembly_address
  %69 = load i64* %rdi
  %70 = inttoptr i64 %69 to i8*
  %71 = call i64 @progerror(i8* %70)
  store i64 %71, i64* %rax
  store i64 %71, i64* %rax
  store volatile i64 29920, i64* @assembly_address
  %72 = load i32* %stack_var_-1100
  %73 = zext i32 %72 to i64
  store i64 %73, i64* %rax
  store volatile i64 29926, i64* @assembly_address
  %74 = load i64* %rax
  %75 = trunc i64 %74 to i32
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rdi
  store volatile i64 29928, i64* @assembly_address
  %77 = load i64* %rdi
  %78 = trunc i64 %77 to i32
  %79 = call i32 @close(i32 %78)
  %80 = sext i32 %79 to i64
  store i64 %80, i64* %rax
  %81 = sext i32 %79 to i64
  store i64 %81, i64* %rax
  store volatile i64 29933, i64* @assembly_address
  br label %block_76f8

block_74f2:                                       ; preds = %block_748c
  store volatile i64 29938, i64* @assembly_address
  %82 = load %__dirstream** %stack_var_-1072
  %83 = ptrtoint %__dirstream* %82 to i64
  store i64 %83, i64* %rax
  store volatile i64 29945, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 29950, i64* @assembly_address
  %84 = load i64* %rax
  store i64 %84, i64* %rdi
  store volatile i64 29953, i64* @assembly_address
  %85 = load i64* %rdi
  %86 = load i64* %rsi
  %87 = inttoptr i64 %85 to %__dirstream*
  %88 = call i64 @streamsavedir(%__dirstream* %87, i64 %86)
  store i64 %88, i64* %rax
  store i64 %88, i64* %rax
  store volatile i64 29958, i64* @assembly_address
  %89 = load i64* %rax
  store i64 %89, i64* %stack_var_-1064
  store volatile i64 29965, i64* @assembly_address
  %90 = load i64* %stack_var_-1064
  %91 = and i64 %90, 15
  %92 = icmp ugt i64 %91, 15
  %93 = icmp ult i64 %90, 0
  %94 = xor i64 %90, 0
  %95 = and i64 %94, 0
  %96 = icmp slt i64 %95, 0
  store i1 %92, i1* %az
  store i1 %93, i1* %cf
  store i1 %96, i1* %of
  %97 = icmp eq i64 %90, 0
  store i1 %97, i1* %zf
  %98 = icmp slt i64 %90, 0
  store i1 %98, i1* %sf
  %99 = trunc i64 %90 to i8
  %100 = call i8 @llvm.ctpop.i8(i8 %99)
  %101 = and i8 %100, 1
  %102 = icmp eq i8 %101, 0
  store i1 %102, i1* %pf
  store volatile i64 29973, i64* @assembly_address
  %103 = load i1* %zf
  %104 = icmp eq i1 %103, false
  br i1 %104, label %block_7526, label %block_7517

block_7517:                                       ; preds = %block_74f2
  store volatile i64 29975, i64* @assembly_address
  %105 = load i8** %stack_var_-1112
  %106 = ptrtoint i8* %105 to i64
  store i64 %106, i64* %rax
  store volatile i64 29982, i64* @assembly_address
  %107 = load i64* %rax
  store i64 %107, i64* %rdi
  store volatile i64 29985, i64* @assembly_address
  %108 = load i64* %rdi
  %109 = inttoptr i64 %108 to i8*
  %110 = call i64 @progerror(i8* %109)
  store i64 %110, i64* %rax
  store i64 %110, i64* %rax
  br label %block_7526

block_7526:                                       ; preds = %block_7517, %block_74f2
  store volatile i64 29990, i64* @assembly_address
  %111 = load %__dirstream** %stack_var_-1072
  %112 = ptrtoint %__dirstream* %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 29997, i64* @assembly_address
  %113 = load i64* %rax
  store i64 %113, i64* %rdi
  store volatile i64 30000, i64* @assembly_address
  %114 = load i64* %rdi
  %115 = inttoptr i64 %114 to %__dirstream*
  %116 = call i32 @closedir(%__dirstream* %115)
  %117 = sext i32 %116 to i64
  store i64 %117, i64* %rax
  %118 = sext i32 %116 to i64
  store i64 %118, i64* %rax
  store volatile i64 30005, i64* @assembly_address
  %119 = load i64* %rax
  %120 = trunc i64 %119 to i32
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i32
  %123 = and i32 %120, %122
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %124 = icmp eq i32 %123, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i32 %123, 0
  store i1 %125, i1* %sf
  %126 = trunc i32 %123 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  store volatile i64 30007, i64* @assembly_address
  %130 = load i1* %zf
  br i1 %130, label %block_7548, label %block_7539

block_7539:                                       ; preds = %block_7526
  store volatile i64 30009, i64* @assembly_address
  %131 = load i8** %stack_var_-1112
  %132 = ptrtoint i8* %131 to i64
  store i64 %132, i64* %rax
  store volatile i64 30016, i64* @assembly_address
  %133 = load i64* %rax
  store i64 %133, i64* %rdi
  store volatile i64 30019, i64* @assembly_address
  %134 = load i64* %rdi
  %135 = inttoptr i64 %134 to i8*
  %136 = call i64 @progerror(i8* %135)
  store i64 %136, i64* %rax
  store i64 %136, i64* %rax
  br label %block_7548

block_7548:                                       ; preds = %block_7539, %block_7526
  store volatile i64 30024, i64* @assembly_address
  %137 = load i64* %stack_var_-1064
  %138 = and i64 %137, 15
  %139 = icmp ugt i64 %138, 15
  %140 = icmp ult i64 %137, 0
  %141 = xor i64 %137, 0
  %142 = and i64 %141, 0
  %143 = icmp slt i64 %142, 0
  store i1 %139, i1* %az
  store i1 %140, i1* %cf
  store i1 %143, i1* %of
  %144 = icmp eq i64 %137, 0
  store i1 %144, i1* %zf
  %145 = icmp slt i64 %137, 0
  store i1 %145, i1* %sf
  %146 = trunc i64 %137 to i8
  %147 = call i8 @llvm.ctpop.i8(i8 %146)
  %148 = and i8 %147, 1
  %149 = icmp eq i8 %148, 0
  store i1 %149, i1* %pf
  store volatile i64 30032, i64* @assembly_address
  %150 = load i1* %zf
  br i1 %150, label %block_76f7, label %block_7556

block_7556:                                       ; preds = %block_7548
  store volatile i64 30038, i64* @assembly_address
  %151 = load i64* %stack_var_-1064
  store i64 %151, i64* %rax
  store volatile i64 30045, i64* @assembly_address
  %152 = load i64* %rax
  %153 = inttoptr i64 %152 to i8*
  store i8* %153, i8** %stack_var_-1088
  store volatile i64 30052, i64* @assembly_address
  br label %block_76d4

block_7569:                                       ; preds = %block_76d4
  store volatile i64 30057, i64* @assembly_address
  %154 = load i8** %stack_var_-1112
  %155 = ptrtoint i8* %154 to i64
  store i64 %155, i64* %rax
  store volatile i64 30064, i64* @assembly_address
  %156 = load i64* %rax
  store i64 %156, i64* %rdi
  store volatile i64 30067, i64* @assembly_address
  %157 = load i64* %rdi
  %158 = inttoptr i64 %157 to i8*
  %159 = call i32 @strlen(i8* %158)
  %160 = sext i32 %159 to i64
  store i64 %160, i64* %rax
  %161 = sext i32 %159 to i64
  store i64 %161, i64* %rax
  store volatile i64 30072, i64* @assembly_address
  %162 = load i64* %rax
  %163 = inttoptr i64 %162 to i8*
  store i8* %163, i8** %stack_var_-1080
  store volatile i64 30079, i64* @assembly_address
  %164 = load i8** %stack_var_-1088
  %165 = ptrtoint i8* %164 to i64
  store i64 %165, i64* %rax
  store volatile i64 30086, i64* @assembly_address
  %166 = load i64* %rax
  store i64 %166, i64* %rdi
  store volatile i64 30089, i64* @assembly_address
  %167 = load i64* %rdi
  %168 = inttoptr i64 %167 to i8*
  %169 = call i32 @strlen(i8* %168)
  %170 = sext i32 %169 to i64
  store i64 %170, i64* %rax
  %171 = sext i32 %169 to i64
  store i64 %171, i64* %rax
  store volatile i64 30094, i64* @assembly_address
  %172 = load i64* %rax
  %173 = inttoptr i64 %172 to i8*
  store i8* %173, i8** %stack_var_-1056
  store volatile i64 30101, i64* @assembly_address
  %174 = load i8** %stack_var_-1088
  %175 = ptrtoint i8* %174 to i64
  store i64 %175, i64* %rax
  store volatile i64 30108, i64* @assembly_address
  store i64 ptrtoint ([2 x i8]* @global_var_11a5b to i64), i64* %rsi
  store volatile i64 30115, i64* @assembly_address
  %176 = load i64* %rax
  store i64 %176, i64* %rdi
  store volatile i64 30118, i64* @assembly_address
  %177 = load i64* %rdi
  %178 = inttoptr i64 %177 to i8*
  %179 = load i64* %rsi
  %180 = inttoptr i64 %179 to i8*
  %181 = call i32 @strcmp(i8* %178, i8* %180)
  %182 = sext i32 %181 to i64
  store i64 %182, i64* %rax
  %183 = sext i32 %181 to i64
  store i64 %183, i64* %rax
  store volatile i64 30123, i64* @assembly_address
  %184 = load i64* %rax
  %185 = trunc i64 %184 to i32
  %186 = load i64* %rax
  %187 = trunc i64 %186 to i32
  %188 = and i32 %185, %187
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %189 = icmp eq i32 %188, 0
  store i1 %189, i1* %zf
  %190 = icmp slt i32 %188, 0
  store i1 %190, i1* %sf
  %191 = trunc i32 %188 to i8
  %192 = call i8 @llvm.ctpop.i8(i8 %191)
  %193 = and i8 %192, 1
  %194 = icmp eq i8 %193, 0
  store i1 %194, i1* %pf
  store volatile i64 30125, i64* @assembly_address
  %195 = load i1* %zf
  br i1 %195, label %block_76c1, label %block_75b3

block_75b3:                                       ; preds = %block_7569
  store volatile i64 30131, i64* @assembly_address
  %196 = load i8** %stack_var_-1088
  %197 = ptrtoint i8* %196 to i64
  store i64 %197, i64* %rax
  store volatile i64 30138, i64* @assembly_address
  store i64 ptrtoint ([3 x i8]* @global_var_11ae7 to i64), i64* %rsi
  store volatile i64 30145, i64* @assembly_address
  %198 = load i64* %rax
  store i64 %198, i64* %rdi
  store volatile i64 30148, i64* @assembly_address
  %199 = load i64* %rdi
  %200 = inttoptr i64 %199 to i8*
  %201 = load i64* %rsi
  %202 = inttoptr i64 %201 to i8*
  %203 = call i32 @strcmp(i8* %200, i8* %202)
  %204 = sext i32 %203 to i64
  store i64 %204, i64* %rax
  %205 = sext i32 %203 to i64
  store i64 %205, i64* %rax
  store volatile i64 30153, i64* @assembly_address
  %206 = load i64* %rax
  %207 = trunc i64 %206 to i32
  %208 = load i64* %rax
  %209 = trunc i64 %208 to i32
  %210 = and i32 %207, %209
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %211 = icmp eq i32 %210, 0
  store i1 %211, i1* %zf
  %212 = icmp slt i32 %210, 0
  store i1 %212, i1* %sf
  %213 = trunc i32 %210 to i8
  %214 = call i8 @llvm.ctpop.i8(i8 %213)
  %215 = and i8 %214, 1
  %216 = icmp eq i8 %215, 0
  store i1 %216, i1* %pf
  store volatile i64 30155, i64* @assembly_address
  %217 = load i1* %zf
  br i1 %217, label %block_76c1, label %block_75d1

block_75d1:                                       ; preds = %block_75b3
  store volatile i64 30161, i64* @assembly_address
  %218 = load i8** %stack_var_-1080
  %219 = ptrtoint i8* %218 to i64
  store i64 %219, i64* %rdx
  store volatile i64 30168, i64* @assembly_address
  %220 = load i8** %stack_var_-1056
  %221 = ptrtoint i8* %220 to i64
  store i64 %221, i64* %rax
  store volatile i64 30175, i64* @assembly_address
  %222 = load i64* %rax
  %223 = load i64* %rdx
  %224 = add i64 %222, %223
  %225 = and i64 %222, 15
  %226 = and i64 %223, 15
  %227 = add i64 %225, %226
  %228 = icmp ugt i64 %227, 15
  %229 = icmp ult i64 %224, %222
  %230 = xor i64 %222, %224
  %231 = xor i64 %223, %224
  %232 = and i64 %230, %231
  %233 = icmp slt i64 %232, 0
  store i1 %228, i1* %az
  store i1 %229, i1* %cf
  store i1 %233, i1* %of
  %234 = icmp eq i64 %224, 0
  store i1 %234, i1* %zf
  %235 = icmp slt i64 %224, 0
  store i1 %235, i1* %sf
  %236 = trunc i64 %224 to i8
  %237 = call i8 @llvm.ctpop.i8(i8 %236)
  %238 = and i8 %237, 1
  %239 = icmp eq i8 %238, 0
  store i1 %239, i1* %pf
  store i64 %224, i64* %rax
  store volatile i64 30178, i64* @assembly_address
  %240 = load i64* %rax
  %241 = sub i64 %240, 1021
  %242 = and i64 %240, 15
  %243 = sub i64 %242, 13
  %244 = icmp ugt i64 %243, 15
  %245 = icmp ult i64 %240, 1021
  %246 = xor i64 %240, 1021
  %247 = xor i64 %240, %241
  %248 = and i64 %246, %247
  %249 = icmp slt i64 %248, 0
  store i1 %244, i1* %az
  store i1 %245, i1* %cf
  store i1 %249, i1* %of
  %250 = icmp eq i64 %241, 0
  store i1 %250, i1* %zf
  %251 = icmp slt i64 %241, 0
  store i1 %251, i1* %sf
  %252 = trunc i64 %241 to i8
  %253 = call i8 @llvm.ctpop.i8(i8 %252)
  %254 = and i8 %253, 1
  %255 = icmp eq i8 %254, 0
  store i1 %255, i1* %pf
  store volatile i64 30184, i64* @assembly_address
  %256 = load i1* %cf
  %257 = load i1* %zf
  %258 = or i1 %256, %257
  %259 = icmp ne i1 %258, true
  br i1 %259, label %block_7682, label %block_75ee

block_75ee:                                       ; preds = %block_75d1
  store volatile i64 30190, i64* @assembly_address
  %260 = load i8** %stack_var_-1112
  %261 = ptrtoint i8* %260 to i64
  store i64 %261, i64* %rdx
  store volatile i64 30197, i64* @assembly_address
  %262 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %262, i64* %rax
  store volatile i64 30204, i64* @assembly_address
  %263 = load i64* %rdx
  store i64 %263, i64* %rsi
  store volatile i64 30207, i64* @assembly_address
  %264 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %264, i64* %rdi
  store volatile i64 30210, i64* @assembly_address
  %265 = load i64* %rdi
  %266 = inttoptr i64 %265 to i8*
  %267 = load i64* %rsi
  %268 = inttoptr i64 %267 to i8*
  %269 = call i8* @strcpy(i8* %266, i8* %268)
  %270 = ptrtoint i8* %269 to i64
  store i64 %270, i64* %rax
  %271 = ptrtoint i8* %269 to i64
  store i64 %271, i64* %rax
  store volatile i64 30215, i64* @assembly_address
  %272 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %272, i64* %rax
  store volatile i64 30222, i64* @assembly_address
  %273 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %273, i64* %rdi
  store volatile i64 30225, i64* @assembly_address
  %274 = load i64* %rdi
  %275 = inttoptr i64 %274 to i64*
  %276 = bitcast i64* %275 to i8*
  %277 = call i64 @last_component(i8* %276)
  store i64 %277, i64* %rax
  store i64 %277, i64* %rax
  store volatile i64 30230, i64* @assembly_address
  %278 = load i64* %rax
  %279 = inttoptr i64 %278 to i8*
  %280 = load i8* %279
  %281 = zext i8 %280 to i64
  store i64 %281, i64* %rax
  store volatile i64 30233, i64* @assembly_address
  %282 = load i64* %rax
  %283 = trunc i64 %282 to i8
  %284 = load i64* %rax
  %285 = trunc i64 %284 to i8
  %286 = and i8 %283, %285
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %287 = icmp eq i8 %286, 0
  store i1 %287, i1* %zf
  %288 = icmp slt i8 %286, 0
  store i1 %288, i1* %sf
  %289 = call i8 @llvm.ctpop.i8(i8 %286)
  %290 = and i8 %289, 1
  %291 = icmp eq i8 %290, 0
  store i1 %291, i1* %pf
  store volatile i64 30235, i64* @assembly_address
  %292 = load i1* %zf
  br i1 %292, label %block_764e, label %block_761d

block_761d:                                       ; preds = %block_75ee
  store volatile i64 30237, i64* @assembly_address
  %293 = load i8** %stack_var_-1080
  %294 = ptrtoint i8* %293 to i64
  store i64 %294, i64* %rax
  store volatile i64 30244, i64* @assembly_address
  %295 = load i64* %rax
  %296 = sub i64 %295, 1
  %297 = and i64 %295, 15
  %298 = sub i64 %297, 1
  %299 = icmp ugt i64 %298, 15
  %300 = icmp ult i64 %295, 1
  %301 = xor i64 %295, 1
  %302 = xor i64 %295, %296
  %303 = and i64 %301, %302
  %304 = icmp slt i64 %303, 0
  store i1 %299, i1* %az
  store i1 %300, i1* %cf
  store i1 %304, i1* %of
  %305 = icmp eq i64 %296, 0
  store i1 %305, i1* %zf
  %306 = icmp slt i64 %296, 0
  store i1 %306, i1* %sf
  %307 = trunc i64 %296 to i8
  %308 = call i8 @llvm.ctpop.i8(i8 %307)
  %309 = and i8 %308, 1
  %310 = icmp eq i8 %309, 0
  store i1 %310, i1* %pf
  store i64 %296, i64* %rax
  store volatile i64 30248, i64* @assembly_address
  %311 = load i64* %rbp
  %312 = load i64* %rax
  %313 = mul i64 %312, 1
  %314 = add i64 %311, -1040
  %315 = add i64 %314, %313
  %316 = inttoptr i64 %315 to i8*
  %317 = load i8* %316
  %318 = zext i8 %317 to i64
  store i64 %318, i64* %rax
  store volatile i64 30256, i64* @assembly_address
  %319 = load i64* %rax
  %320 = trunc i64 %319 to i8
  %321 = sub i8 %320, 47
  %322 = and i8 %320, 15
  %323 = sub i8 %322, 15
  %324 = icmp ugt i8 %323, 15
  %325 = icmp ult i8 %320, 47
  %326 = xor i8 %320, 47
  %327 = xor i8 %320, %321
  %328 = and i8 %326, %327
  %329 = icmp slt i8 %328, 0
  store i1 %324, i1* %az
  store i1 %325, i1* %cf
  store i1 %329, i1* %of
  %330 = icmp eq i8 %321, 0
  store i1 %330, i1* %zf
  %331 = icmp slt i8 %321, 0
  store i1 %331, i1* %sf
  %332 = call i8 @llvm.ctpop.i8(i8 %321)
  %333 = and i8 %332, 1
  %334 = icmp eq i8 %333, 0
  store i1 %334, i1* %pf
  store volatile i64 30258, i64* @assembly_address
  %335 = load i1* %zf
  br i1 %335, label %block_764e, label %block_7634

block_7634:                                       ; preds = %block_761d
  store volatile i64 30260, i64* @assembly_address
  %336 = load i8** %stack_var_-1080
  %337 = ptrtoint i8* %336 to i64
  store i64 %337, i64* %rax
  store volatile i64 30267, i64* @assembly_address
  %338 = load i64* %rax
  %339 = add i64 %338, 1
  store i64 %339, i64* %rdx
  store volatile i64 30271, i64* @assembly_address
  %340 = load i64* %rdx
  %341 = inttoptr i64 %340 to i8*
  store i8* %341, i8** %stack_var_-1080
  store volatile i64 30278, i64* @assembly_address
  %342 = load i64* %rbp
  %343 = load i64* %rax
  %344 = mul i64 %343, 1
  %345 = add i64 %342, -1040
  %346 = add i64 %345, %344
  %347 = inttoptr i64 %346 to i8*
  store i8 47, i8* %347
  br label %block_764e

block_764e:                                       ; preds = %block_7634, %block_761d, %block_75ee
  store volatile i64 30286, i64* @assembly_address
  %348 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %348, i64* %rdx
  store volatile i64 30293, i64* @assembly_address
  %349 = load i8** %stack_var_-1080
  %350 = ptrtoint i8* %349 to i64
  store i64 %350, i64* %rax
  store volatile i64 30300, i64* @assembly_address
  %351 = load i64* %rdx
  %352 = load i64* %rax
  %353 = add i64 %351, %352
  %354 = and i64 %351, 15
  %355 = and i64 %352, 15
  %356 = add i64 %354, %355
  %357 = icmp ugt i64 %356, 15
  %358 = icmp ult i64 %353, %351
  %359 = xor i64 %351, %353
  %360 = xor i64 %352, %353
  %361 = and i64 %359, %360
  %362 = icmp slt i64 %361, 0
  store i1 %357, i1* %az
  store i1 %358, i1* %cf
  store i1 %362, i1* %of
  %363 = icmp eq i64 %353, 0
  store i1 %363, i1* %zf
  %364 = icmp slt i64 %353, 0
  store i1 %364, i1* %sf
  %365 = trunc i64 %353 to i8
  %366 = call i8 @llvm.ctpop.i8(i8 %365)
  %367 = and i8 %366, 1
  %368 = icmp eq i8 %367, 0
  store i1 %368, i1* %pf
  store i64 %353, i64* %rdx
  store volatile i64 30303, i64* @assembly_address
  %369 = load i8** %stack_var_-1088
  %370 = ptrtoint i8* %369 to i64
  store i64 %370, i64* %rax
  store volatile i64 30310, i64* @assembly_address
  %371 = load i64* %rax
  store i64 %371, i64* %rsi
  store volatile i64 30313, i64* @assembly_address
  %372 = load i64* %rdx
  store i64 %372, i64* %rdi
  store volatile i64 30316, i64* @assembly_address
  %373 = load i64* %rdi
  %374 = inttoptr i64 %373 to i8*
  %375 = load i64* %rsi
  %376 = inttoptr i64 %375 to i8*
  %377 = call i8* @strcpy(i8* %374, i8* %376)
  %378 = ptrtoint i8* %377 to i64
  store i64 %378, i64* %rax
  %379 = ptrtoint i8* %377 to i64
  store i64 %379, i64* %rax
  store volatile i64 30321, i64* @assembly_address
  %380 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %380, i64* %rax
  store volatile i64 30328, i64* @assembly_address
  %381 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %381, i64* %rdi
  store volatile i64 30331, i64* @assembly_address
  %382 = load i64* %rdi
  %383 = inttoptr i64 %382 to i8*
  %384 = call i64 @treat_file(i8* %383)
  store i64 %384, i64* %rax
  store i64 %384, i64* %rax
  store volatile i64 30336, i64* @assembly_address
  br label %block_76c2

block_7682:                                       ; preds = %block_75d1
  store volatile i64 30338, i64* @assembly_address
  %385 = load i64* @global_var_25f4c8
  store i64 %385, i64* %rdx
  store volatile i64 30345, i64* @assembly_address
  %386 = load i64* @global_var_216580
  store i64 %386, i64* %rax
  store volatile i64 30352, i64* @assembly_address
  %387 = load i8** %stack_var_-1088
  %388 = ptrtoint i8* %387 to i64
  store i64 %388, i64* %rsi
  store volatile i64 30359, i64* @assembly_address
  %389 = load i8** %stack_var_-1112
  %390 = ptrtoint i8* %389 to i64
  store i64 %390, i64* %rcx
  store volatile i64 30366, i64* @assembly_address
  %391 = load i64* %rsi
  store i64 %391, i64* %r8
  store volatile i64 30369, i64* @assembly_address
  store i64 ptrtoint ([30 x i8]* @global_var_11aea to i64), i64* %rsi
  store volatile i64 30376, i64* @assembly_address
  %392 = load i64* %rax
  store i64 %392, i64* %rdi
  store volatile i64 30379, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 30384, i64* @assembly_address
  %393 = load i64* %rdi
  %394 = inttoptr i64 %393 to %_IO_FILE*
  %395 = load i64* %rsi
  %396 = inttoptr i64 %395 to i8*
  %397 = load i64* %rdx
  %398 = inttoptr i64 %397 to i8*
  %399 = load i64* %rcx
  %400 = inttoptr i64 %399 to i8*
  %401 = load i64* %r8
  %402 = inttoptr i64 %401 to i8*
  %403 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %394, i8* %396, i8* %398, i8* %400, i8* %402)
  %404 = sext i32 %403 to i64
  store i64 %404, i64* %rax
  %405 = sext i32 %403 to i64
  store i64 %405, i64* %rax
  store volatile i64 30389, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 30399, i64* @assembly_address
  br label %block_76c2

block_76c1:                                       ; preds = %block_75b3, %block_7569
  store volatile i64 30401, i64* @assembly_address
  br label %block_76c2

block_76c2:                                       ; preds = %block_76c1, %block_7682, %block_764e
  store volatile i64 30402, i64* @assembly_address
  %406 = load i8** %stack_var_-1056
  %407 = ptrtoint i8* %406 to i64
  store i64 %407, i64* %rax
  store volatile i64 30409, i64* @assembly_address
  %408 = load i64* %rax
  %409 = add i64 %408, 1
  %410 = and i64 %408, 15
  %411 = add i64 %410, 1
  %412 = icmp ugt i64 %411, 15
  %413 = icmp ult i64 %409, %408
  %414 = xor i64 %408, %409
  %415 = xor i64 1, %409
  %416 = and i64 %414, %415
  %417 = icmp slt i64 %416, 0
  store i1 %412, i1* %az
  store i1 %413, i1* %cf
  store i1 %417, i1* %of
  %418 = icmp eq i64 %409, 0
  store i1 %418, i1* %zf
  %419 = icmp slt i64 %409, 0
  store i1 %419, i1* %sf
  %420 = trunc i64 %409 to i8
  %421 = call i8 @llvm.ctpop.i8(i8 %420)
  %422 = and i8 %421, 1
  %423 = icmp eq i8 %422, 0
  store i1 %423, i1* %pf
  store i64 %409, i64* %rax
  store volatile i64 30413, i64* @assembly_address
  %424 = load i8** %stack_var_-1088
  %425 = ptrtoint i8* %424 to i64
  %426 = load i64* %rax
  %427 = add i64 %425, %426
  %428 = and i64 %425, 15
  %429 = and i64 %426, 15
  %430 = add i64 %428, %429
  %431 = icmp ugt i64 %430, 15
  %432 = icmp ult i64 %427, %425
  %433 = xor i64 %425, %427
  %434 = xor i64 %426, %427
  %435 = and i64 %433, %434
  %436 = icmp slt i64 %435, 0
  store i1 %431, i1* %az
  store i1 %432, i1* %cf
  store i1 %436, i1* %of
  %437 = icmp eq i64 %427, 0
  store i1 %437, i1* %zf
  %438 = icmp slt i64 %427, 0
  store i1 %438, i1* %sf
  %439 = trunc i64 %427 to i8
  %440 = call i8 @llvm.ctpop.i8(i8 %439)
  %441 = and i8 %440, 1
  %442 = icmp eq i8 %441, 0
  store i1 %442, i1* %pf
  %443 = inttoptr i64 %427 to i8*
  store i8* %443, i8** %stack_var_-1088
  br label %block_76d4

block_76d4:                                       ; preds = %block_76c2, %block_7556
  store volatile i64 30420, i64* @assembly_address
  %444 = load i8** %stack_var_-1088
  %445 = ptrtoint i8* %444 to i64
  store i64 %445, i64* %rax
  store volatile i64 30427, i64* @assembly_address
  %446 = load i64* %rax
  %447 = inttoptr i64 %446 to i8*
  %448 = load i8* %447
  %449 = zext i8 %448 to i64
  store i64 %449, i64* %rax
  store volatile i64 30430, i64* @assembly_address
  %450 = load i64* %rax
  %451 = trunc i64 %450 to i8
  %452 = load i64* %rax
  %453 = trunc i64 %452 to i8
  %454 = and i8 %451, %453
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %455 = icmp eq i8 %454, 0
  store i1 %455, i1* %zf
  %456 = icmp slt i8 %454, 0
  store i1 %456, i1* %sf
  %457 = call i8 @llvm.ctpop.i8(i8 %454)
  %458 = and i8 %457, 1
  %459 = icmp eq i8 %458, 0
  store i1 %459, i1* %pf
  store volatile i64 30432, i64* @assembly_address
  %460 = load i1* %zf
  %461 = icmp eq i1 %460, false
  br i1 %461, label %block_7569, label %block_76e6

block_76e6:                                       ; preds = %block_76d4
  store volatile i64 30438, i64* @assembly_address
  %462 = load i64* %stack_var_-1064
  store i64 %462, i64* %rax
  store volatile i64 30445, i64* @assembly_address
  %463 = load i64* %rax
  store i64 %463, i64* %rdi
  store volatile i64 30448, i64* @assembly_address
  %464 = load i64* %rdi
  %465 = inttoptr i64 %464 to i64*
  call void @free(i64* %465)
  store volatile i64 30453, i64* @assembly_address
  br label %block_76f8

block_76f7:                                       ; preds = %block_7548
  store volatile i64 30455, i64* @assembly_address
  br label %block_76f8

block_76f8:                                       ; preds = %block_76f7, %block_76e6, %block_74d1
  store volatile i64 30456, i64* @assembly_address
  %466 = load i64* %stack_var_-16
  store i64 %466, i64* %rax
  store volatile i64 30460, i64* @assembly_address
  %467 = load i64* %rax
  %468 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %469 = xor i64 %467, %468
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %470 = icmp eq i64 %469, 0
  store i1 %470, i1* %zf
  %471 = icmp slt i64 %469, 0
  store i1 %471, i1* %sf
  %472 = trunc i64 %469 to i8
  %473 = call i8 @llvm.ctpop.i8(i8 %472)
  %474 = and i8 %473, 1
  %475 = icmp eq i8 %474, 0
  store i1 %475, i1* %pf
  store i64 %469, i64* %rax
  store volatile i64 30469, i64* @assembly_address
  %476 = load i1* %zf
  br i1 %476, label %block_770c, label %block_7707

block_7707:                                       ; preds = %block_76f8
  store volatile i64 30471, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_770c:                                       ; preds = %block_76f8
  store volatile i64 30476, i64* @assembly_address
  %477 = load i64* %stack_var_-8
  store i64 %477, i64* %rbp
  %478 = ptrtoint i64* %stack_var_0 to i64
  store i64 %478, i64* %rsp
  store volatile i64 30477, i64* @assembly_address
  %479 = load i64* %rax
  ret i64 %479
}

declare i64 @188(i64, i8*)

declare i64 @189(i64, i64)

define i64 @install_signal_handlers() {
block_770e:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-48 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-64 = alloca i64
  %stack_var_-72 = alloca i64
  %stack_var_-80 = alloca i64
  %stack_var_-88 = alloca i64
  %stack_var_-96 = alloca i64
  %stack_var_-104 = alloca i64
  %stack_var_-112 = alloca i64
  %stack_var_-120 = alloca i64
  %stack_var_-128 = alloca i64
  %stack_var_-136 = alloca i64
  %stack_var_-144 = alloca i64
  %stack_var_-152 = alloca i64
  %stack_var_-160 = alloca %_TYPEDEF_sigset_t*
  %0 = alloca i64
  %stack_var_-168 = alloca i64
  %stack_var_-176 = alloca i32
  %stack_var_-172 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-184 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i32
  %3 = alloca i64
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i64
  store volatile i64 30478, i64* @assembly_address
  %7 = load i64* %rbp
  store i64 %7, i64* %stack_var_-8
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rsp
  store volatile i64 30479, i64* @assembly_address
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rbp
  store volatile i64 30482, i64* @assembly_address
  %10 = load i64* %rsp
  %11 = sub i64 %10, 176
  %12 = and i64 %10, 15
  %13 = icmp ugt i64 %12, 15
  %14 = icmp ult i64 %10, 176
  %15 = xor i64 %10, 176
  %16 = xor i64 %10, %11
  %17 = and i64 %15, %16
  %18 = icmp slt i64 %17, 0
  store i1 %13, i1* %az
  store i1 %14, i1* %cf
  store i1 %18, i1* %of
  %19 = icmp eq i64 %11, 0
  store i1 %19, i1* %zf
  %20 = icmp slt i64 %11, 0
  store i1 %20, i1* %sf
  %21 = trunc i64 %11 to i8
  %22 = call i8 @llvm.ctpop.i8(i8 %21)
  %23 = and i8 %22, 1
  %24 = icmp eq i8 %23, 0
  store i1 %24, i1* %pf
  %25 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %25, i64* %rsp
  store volatile i64 30489, i64* @assembly_address
  %26 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %26, i64* %rax
  store volatile i64 30498, i64* @assembly_address
  %27 = load i64* %rax
  store i64 %27, i64* %stack_var_-16
  store volatile i64 30502, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %28 = icmp eq i32 0, 0
  store i1 %28, i1* %zf
  %29 = icmp slt i32 0, 0
  store i1 %29, i1* %sf
  %30 = trunc i32 0 to i8
  %31 = call i8 @llvm.ctpop.i8(i8 %30)
  %32 = and i8 %31, 1
  %33 = icmp eq i8 %32, 0
  store i1 %33, i1* %pf
  %34 = zext i32 0 to i64
  store i64 %34, i64* %rax
  store volatile i64 30504, i64* @assembly_address
  store i32 6, i32* %stack_var_-172
  store volatile i64 30514, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216640 to i64), i64* %rdi
  store volatile i64 30521, i64* @assembly_address
  %35 = load i64* %rdi
  %36 = inttoptr i64 %35 to %_TYPEDEF_sigset_t*
  %37 = call i32 @sigemptyset(%_TYPEDEF_sigset_t* %36)
  %38 = sext i32 %37 to i64
  store i64 %38, i64* %rax
  %39 = sext i32 %37 to i64
  store i64 %39, i64* %rax
  store volatile i64 30526, i64* @assembly_address
  store i32 0, i32* %stack_var_-176
  store volatile i64 30536, i64* @assembly_address
  br label %block_77b3

block_774a:                                       ; preds = %block_77b3
  store volatile i64 30538, i64* @assembly_address
  %40 = load i32* %stack_var_-176
  %41 = zext i32 %40 to i64
  store i64 %41, i64* %rax
  store volatile i64 30544, i64* @assembly_address
  %42 = load i64* %rax
  %43 = trunc i64 %42 to i32
  %44 = sext i32 %43 to i64
  store i64 %44, i64* %rax
  store volatile i64 30546, i64* @assembly_address
  %45 = load i64* %rax
  %46 = mul i64 %45, 4
  store i64 %46, i64* %rdx
  store volatile i64 30554, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2160b0 to i64), i64* %rax
  store volatile i64 30561, i64* @assembly_address
  %47 = load i64* %rdx
  %48 = load i64* %rax
  %49 = mul i64 %48, 1
  %50 = add i64 %47, %49
  %51 = inttoptr i64 %50 to i32*
  %52 = load i32* %51
  %53 = zext i32 %52 to i64
  store i64 %53, i64* %rax
  store volatile i64 30564, i64* @assembly_address
  %54 = ptrtoint i64* %stack_var_-168 to i64
  store i64 %54, i64* %rdx
  store volatile i64 30571, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 30576, i64* @assembly_address
  %55 = load i64* %rax
  %56 = trunc i64 %55 to i32
  %57 = zext i32 %56 to i64
  store i64 %57, i64* %rdi
  store volatile i64 30578, i64* @assembly_address
  %58 = load i64* %rdi
  %59 = trunc i64 %58 to i32
  %60 = load i64* %rsi
  %61 = inttoptr i64 %60 to %sigaction*
  %62 = load i64* %rdx
  %63 = inttoptr i64 %62 to %sigaction*
  %64 = call i32 @sigaction(i32 %59, %sigaction* %61, %sigaction* %63)
  %65 = sext i32 %64 to i64
  store i64 %65, i64* %rax
  %66 = sext i32 %64 to i64
  store i64 %66, i64* %rax
  store volatile i64 30583, i64* @assembly_address
  %67 = load i64* %stack_var_-168
  store i64 %67, i64* %rax
  store volatile i64 30590, i64* @assembly_address
  %68 = load i64* %rax
  %69 = sub i64 %68, 1
  %70 = and i64 %68, 15
  %71 = sub i64 %70, 1
  %72 = icmp ugt i64 %71, 15
  %73 = icmp ult i64 %68, 1
  %74 = xor i64 %68, 1
  %75 = xor i64 %68, %69
  %76 = and i64 %74, %75
  %77 = icmp slt i64 %76, 0
  store i1 %72, i1* %az
  store i1 %73, i1* %cf
  store i1 %77, i1* %of
  %78 = icmp eq i64 %69, 0
  store i1 %78, i1* %zf
  %79 = icmp slt i64 %69, 0
  store i1 %79, i1* %sf
  %80 = trunc i64 %69 to i8
  %81 = call i8 @llvm.ctpop.i8(i8 %80)
  %82 = and i8 %81, 1
  %83 = icmp eq i8 %82, 0
  store i1 %83, i1* %pf
  store volatile i64 30594, i64* @assembly_address
  %84 = load i1* %zf
  br i1 %84, label %block_77ac, label %block_7784

block_7784:                                       ; preds = %block_774a
  store volatile i64 30596, i64* @assembly_address
  %85 = load i32* %stack_var_-176
  %86 = zext i32 %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 30602, i64* @assembly_address
  %87 = load i64* %rax
  %88 = trunc i64 %87 to i32
  %89 = sext i32 %88 to i64
  store i64 %89, i64* %rax
  store volatile i64 30604, i64* @assembly_address
  %90 = load i64* %rax
  %91 = mul i64 %90, 4
  store i64 %91, i64* %rdx
  store volatile i64 30612, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2160b0 to i64), i64* %rax
  store volatile i64 30619, i64* @assembly_address
  %92 = load i64* %rdx
  %93 = load i64* %rax
  %94 = mul i64 %93, 1
  %95 = add i64 %92, %94
  %96 = inttoptr i64 %95 to i32*
  %97 = load i32* %96
  %98 = zext i32 %97 to i64
  store i64 %98, i64* %rax
  store volatile i64 30622, i64* @assembly_address
  %99 = load i64* %rax
  %100 = trunc i64 %99 to i32
  %101 = zext i32 %100 to i64
  store i64 %101, i64* %rsi
  store volatile i64 30624, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216640 to i64), i64* %rdi
  store volatile i64 30631, i64* @assembly_address
  %102 = load i64* %rdi
  %103 = inttoptr i64 %102 to %_TYPEDEF_sigset_t*
  %104 = load i64* %rsi
  %105 = trunc i64 %104 to i32
  %106 = call i32 @sigaddset(%_TYPEDEF_sigset_t* %103, i32 %105)
  %107 = sext i32 %106 to i64
  store i64 %107, i64* %rax
  %108 = sext i32 %106 to i64
  store i64 %108, i64* %rax
  br label %block_77ac

block_77ac:                                       ; preds = %block_7784, %block_774a
  store volatile i64 30636, i64* @assembly_address
  %109 = load i32* %stack_var_-176
  %110 = add i32 %109, 1
  %111 = and i32 %109, 15
  %112 = add i32 %111, 1
  %113 = icmp ugt i32 %112, 15
  %114 = icmp ult i32 %110, %109
  %115 = xor i32 %109, %110
  %116 = xor i32 1, %110
  %117 = and i32 %115, %116
  %118 = icmp slt i32 %117, 0
  store i1 %113, i1* %az
  store i1 %114, i1* %cf
  store i1 %118, i1* %of
  %119 = icmp eq i32 %110, 0
  store i1 %119, i1* %zf
  %120 = icmp slt i32 %110, 0
  store i1 %120, i1* %sf
  %121 = trunc i32 %110 to i8
  %122 = call i8 @llvm.ctpop.i8(i8 %121)
  %123 = and i8 %122, 1
  %124 = icmp eq i8 %123, 0
  store i1 %124, i1* %pf
  store i32 %110, i32* %stack_var_-176
  br label %block_77b3

block_77b3:                                       ; preds = %block_77ac, %block_770e
  store volatile i64 30643, i64* @assembly_address
  %125 = load i32* %stack_var_-176
  %126 = zext i32 %125 to i64
  store i64 %126, i64* %rax
  store volatile i64 30649, i64* @assembly_address
  %127 = load i64* %rax
  %128 = trunc i64 %127 to i32
  %129 = load i32* %stack_var_-172
  %130 = trunc i64 %127 to i32
  store i32 %130, i32* %5
  store i32 %129, i32* %4
  %131 = sub i32 %128, %129
  %132 = and i32 %128, 15
  %133 = and i32 %129, 15
  %134 = sub i32 %132, %133
  %135 = icmp ugt i32 %134, 15
  %136 = icmp ult i32 %128, %129
  %137 = xor i32 %128, %129
  %138 = xor i32 %128, %131
  %139 = and i32 %137, %138
  %140 = icmp slt i32 %139, 0
  store i1 %135, i1* %az
  store i1 %136, i1* %cf
  store i1 %140, i1* %of
  %141 = icmp eq i32 %131, 0
  store i1 %141, i1* %zf
  %142 = icmp slt i32 %131, 0
  store i1 %142, i1* %sf
  %143 = trunc i32 %131 to i8
  %144 = call i8 @llvm.ctpop.i8(i8 %143)
  %145 = and i8 %144, 1
  %146 = icmp eq i8 %145, 0
  store i1 %146, i1* %pf
  store volatile i64 30655, i64* @assembly_address
  %147 = load i32* %5
  %148 = sext i32 %147 to i64
  %149 = load i32* %4
  %150 = trunc i64 %148 to i32
  %151 = icmp slt i32 %150, %149
  br i1 %151, label %block_774a, label %block_77c1

block_77c1:                                       ; preds = %block_77b3
  store volatile i64 30657, i64* @assembly_address
  store i64 31391, i64* %rax
  store volatile i64 30664, i64* @assembly_address
  %152 = load i64* %rax
  store i64 %152, i64* %stack_var_-168
  store volatile i64 30671, i64* @assembly_address
  %153 = load i64* @global_var_216640
  store i64 %153, i64* %rax
  store volatile i64 30678, i64* @assembly_address
  %154 = load i64* @global_var_216648
  store i64 %154, i64* %rdx
  store volatile i64 30685, i64* @assembly_address
  %155 = load i64* %rax
  %156 = inttoptr i64 %155 to %_TYPEDEF_sigset_t*
  store %_TYPEDEF_sigset_t* %156, %_TYPEDEF_sigset_t** %stack_var_-160
  store volatile i64 30692, i64* @assembly_address
  %157 = load i64* %rdx
  store i64 %157, i64* %stack_var_-152
  store volatile i64 30699, i64* @assembly_address
  %158 = load i64* @global_var_216650
  store i64 %158, i64* %rax
  store volatile i64 30706, i64* @assembly_address
  %159 = load i64* @global_var_216658
  store i64 %159, i64* %rdx
  store volatile i64 30713, i64* @assembly_address
  %160 = load i64* %rax
  store i64 %160, i64* %stack_var_-144
  store volatile i64 30720, i64* @assembly_address
  %161 = load i64* %rdx
  store i64 %161, i64* %stack_var_-136
  store volatile i64 30724, i64* @assembly_address
  %162 = load i64* @global_var_216660
  store i64 %162, i64* %rax
  store volatile i64 30731, i64* @assembly_address
  %163 = load i64* @global_var_216668
  store i64 %163, i64* %rdx
  store volatile i64 30738, i64* @assembly_address
  %164 = load i64* %rax
  store i64 %164, i64* %stack_var_-128
  store volatile i64 30742, i64* @assembly_address
  %165 = load i64* %rdx
  store i64 %165, i64* %stack_var_-120
  store volatile i64 30746, i64* @assembly_address
  %166 = load i64* @global_var_216670
  store i64 %166, i64* %rax
  store volatile i64 30753, i64* @assembly_address
  %167 = load i64* @global_var_216678
  store i64 %167, i64* %rdx
  store volatile i64 30760, i64* @assembly_address
  %168 = load i64* %rax
  store i64 %168, i64* %stack_var_-112
  store volatile i64 30764, i64* @assembly_address
  %169 = load i64* %rdx
  store i64 %169, i64* %stack_var_-104
  store volatile i64 30768, i64* @assembly_address
  %170 = load i64* @global_var_216680
  store i64 %170, i64* %rax
  store volatile i64 30775, i64* @assembly_address
  %171 = load i64* @global_var_216688
  store i64 %171, i64* %rdx
  store volatile i64 30782, i64* @assembly_address
  %172 = load i64* %rax
  store i64 %172, i64* %stack_var_-96
  store volatile i64 30786, i64* @assembly_address
  %173 = load i64* %rdx
  store i64 %173, i64* %stack_var_-88
  store volatile i64 30790, i64* @assembly_address
  %174 = load i64* @global_var_216690
  store i64 %174, i64* %rax
  store volatile i64 30797, i64* @assembly_address
  %175 = load i64* @global_var_216698
  store i64 %175, i64* %rdx
  store volatile i64 30804, i64* @assembly_address
  %176 = load i64* %rax
  store i64 %176, i64* %stack_var_-80
  store volatile i64 30808, i64* @assembly_address
  %177 = load i64* %rdx
  store i64 %177, i64* %stack_var_-72
  store volatile i64 30812, i64* @assembly_address
  %178 = load i64* @global_var_2166a0
  store i64 %178, i64* %rax
  store volatile i64 30819, i64* @assembly_address
  %179 = load i64* @global_var_2166a8
  store i64 %179, i64* %rdx
  store volatile i64 30826, i64* @assembly_address
  %180 = load i64* %rax
  store i64 %180, i64* %stack_var_-64
  store volatile i64 30830, i64* @assembly_address
  %181 = load i64* %rdx
  store i64 %181, i64* %stack_var_-56
  store volatile i64 30834, i64* @assembly_address
  %182 = load i64* @global_var_2166b0
  store i64 %182, i64* %rax
  store volatile i64 30841, i64* @assembly_address
  %183 = load i64* @global_var_2166b8
  store i64 %183, i64* %rdx
  store volatile i64 30848, i64* @assembly_address
  %184 = load i64* %rax
  store i64 %184, i64* %stack_var_-48
  store volatile i64 30852, i64* @assembly_address
  %185 = load i64* %rdx
  store i64 %185, i64* %stack_var_-40
  store volatile i64 30856, i64* @assembly_address
  store i32 0, i32* %stack_var_-32
  store volatile i64 30863, i64* @assembly_address
  store i32 0, i32* %stack_var_-176
  store volatile i64 30873, i64* @assembly_address
  br label %block_7911

block_789b:                                       ; preds = %block_7911
  store volatile i64 30875, i64* @assembly_address
  %186 = load i32* %stack_var_-176
  %187 = zext i32 %186 to i64
  store i64 %187, i64* %rax
  store volatile i64 30881, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  %190 = sext i32 %189 to i64
  store i64 %190, i64* %rax
  store volatile i64 30883, i64* @assembly_address
  %191 = load i64* %rax
  %192 = mul i64 %191, 4
  store i64 %192, i64* %rdx
  store volatile i64 30891, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2160b0 to i64), i64* %rax
  store volatile i64 30898, i64* @assembly_address
  %193 = load i64* %rdx
  %194 = load i64* %rax
  %195 = mul i64 %194, 1
  %196 = add i64 %193, %195
  %197 = inttoptr i64 %196 to i32*
  %198 = load i32* %197
  %199 = zext i32 %198 to i64
  store i64 %199, i64* %rax
  store volatile i64 30901, i64* @assembly_address
  %200 = load i64* %rax
  %201 = trunc i64 %200 to i32
  %202 = zext i32 %201 to i64
  store i64 %202, i64* %rsi
  store volatile i64 30903, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216640 to i64), i64* %rdi
  store volatile i64 30910, i64* @assembly_address
  %203 = load i64* %rdi
  %204 = inttoptr i64 %203 to %_TYPEDEF_sigset_t*
  %205 = load i64* %rsi
  %206 = trunc i64 %205 to i32
  %207 = call i32 @sigismember(%_TYPEDEF_sigset_t* %204, i32 %206)
  %208 = sext i32 %207 to i64
  store i64 %208, i64* %rax
  %209 = sext i32 %207 to i64
  store i64 %209, i64* %rax
  store volatile i64 30915, i64* @assembly_address
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  %212 = load i64* %rax
  %213 = trunc i64 %212 to i32
  %214 = and i32 %211, %213
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %215 = icmp eq i32 %214, 0
  store i1 %215, i1* %zf
  %216 = icmp slt i32 %214, 0
  store i1 %216, i1* %sf
  %217 = trunc i32 %214 to i8
  %218 = call i8 @llvm.ctpop.i8(i8 %217)
  %219 = and i8 %218, 1
  %220 = icmp eq i8 %219, 0
  store i1 %220, i1* %pf
  store volatile i64 30917, i64* @assembly_address
  %221 = load i1* %zf
  br i1 %221, label %block_790a, label %block_78c7

block_78c7:                                       ; preds = %block_789b
  store volatile i64 30919, i64* @assembly_address
  %222 = load i32* %stack_var_-176
  %223 = and i32 %222, 15
  %224 = icmp ugt i32 %223, 15
  %225 = icmp ult i32 %222, 0
  %226 = xor i32 %222, 0
  %227 = and i32 %226, 0
  %228 = icmp slt i32 %227, 0
  store i1 %224, i1* %az
  store i1 %225, i1* %cf
  store i1 %228, i1* %of
  %229 = icmp eq i32 %222, 0
  store i1 %229, i1* %zf
  %230 = icmp slt i32 %222, 0
  store i1 %230, i1* %sf
  %231 = trunc i32 %222 to i8
  %232 = call i8 @llvm.ctpop.i8(i8 %231)
  %233 = and i8 %232, 1
  %234 = icmp eq i8 %233, 0
  store i1 %234, i1* %pf
  store volatile i64 30926, i64* @assembly_address
  %235 = load i1* %zf
  %236 = icmp eq i1 %235, false
  br i1 %236, label %block_78da, label %block_78d0

block_78d0:                                       ; preds = %block_78c7
  store volatile i64 30928, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216618 to i32*)
  br label %block_78da

block_78da:                                       ; preds = %block_78d0, %block_78c7
  store volatile i64 30938, i64* @assembly_address
  %237 = load i32* %stack_var_-176
  %238 = zext i32 %237 to i64
  store i64 %238, i64* %rax
  store volatile i64 30944, i64* @assembly_address
  %239 = load i64* %rax
  %240 = trunc i64 %239 to i32
  %241 = sext i32 %240 to i64
  store i64 %241, i64* %rax
  store volatile i64 30946, i64* @assembly_address
  %242 = load i64* %rax
  %243 = mul i64 %242, 4
  store i64 %243, i64* %rdx
  store volatile i64 30954, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2160b0 to i64), i64* %rax
  store volatile i64 30961, i64* @assembly_address
  %244 = load i64* %rdx
  %245 = load i64* %rax
  %246 = mul i64 %245, 1
  %247 = add i64 %244, %246
  %248 = inttoptr i64 %247 to i32*
  %249 = load i32* %248
  %250 = zext i32 %249 to i64
  store i64 %250, i64* %rax
  store volatile i64 30964, i64* @assembly_address
  %251 = ptrtoint i64* %stack_var_-168 to i64
  store i64 %251, i64* %rcx
  store volatile i64 30971, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 30976, i64* @assembly_address
  %252 = ptrtoint i64* %stack_var_-168 to i64
  store i64 %252, i64* %rsi
  store volatile i64 30979, i64* @assembly_address
  %253 = load i64* %rax
  %254 = trunc i64 %253 to i32
  %255 = zext i32 %254 to i64
  store i64 %255, i64* %rdi
  store volatile i64 30981, i64* @assembly_address
  %256 = load i64* %rdi
  %257 = trunc i64 %256 to i32
  %258 = load i64* %rsi
  %259 = inttoptr i64 %258 to %sigaction*
  %260 = load i64* %rdx
  %261 = inttoptr i64 %260 to %sigaction*
  %262 = call i32 @sigaction(i32 %257, %sigaction* %259, %sigaction* %261)
  %263 = sext i32 %262 to i64
  store i64 %263, i64* %rax
  %264 = sext i32 %262 to i64
  store i64 %264, i64* %rax
  br label %block_790a

block_790a:                                       ; preds = %block_78da, %block_789b
  store volatile i64 30986, i64* @assembly_address
  %265 = load i32* %stack_var_-176
  %266 = add i32 %265, 1
  %267 = and i32 %265, 15
  %268 = add i32 %267, 1
  %269 = icmp ugt i32 %268, 15
  %270 = icmp ult i32 %266, %265
  %271 = xor i32 %265, %266
  %272 = xor i32 1, %266
  %273 = and i32 %271, %272
  %274 = icmp slt i32 %273, 0
  store i1 %269, i1* %az
  store i1 %270, i1* %cf
  store i1 %274, i1* %of
  %275 = icmp eq i32 %266, 0
  store i1 %275, i1* %zf
  %276 = icmp slt i32 %266, 0
  store i1 %276, i1* %sf
  %277 = trunc i32 %266 to i8
  %278 = call i8 @llvm.ctpop.i8(i8 %277)
  %279 = and i8 %278, 1
  %280 = icmp eq i8 %279, 0
  store i1 %280, i1* %pf
  store i32 %266, i32* %stack_var_-176
  br label %block_7911

block_7911:                                       ; preds = %block_790a, %block_77c1
  store volatile i64 30993, i64* @assembly_address
  %281 = load i32* %stack_var_-176
  %282 = zext i32 %281 to i64
  store i64 %282, i64* %rax
  store volatile i64 30999, i64* @assembly_address
  %283 = load i64* %rax
  %284 = trunc i64 %283 to i32
  %285 = load i32* %stack_var_-172
  %286 = trunc i64 %283 to i32
  store i32 %286, i32* %2
  store i32 %285, i32* %1
  %287 = sub i32 %284, %285
  %288 = and i32 %284, 15
  %289 = and i32 %285, 15
  %290 = sub i32 %288, %289
  %291 = icmp ugt i32 %290, 15
  %292 = icmp ult i32 %284, %285
  %293 = xor i32 %284, %285
  %294 = xor i32 %284, %287
  %295 = and i32 %293, %294
  %296 = icmp slt i32 %295, 0
  store i1 %291, i1* %az
  store i1 %292, i1* %cf
  store i1 %296, i1* %of
  %297 = icmp eq i32 %287, 0
  store i1 %297, i1* %zf
  %298 = icmp slt i32 %287, 0
  store i1 %298, i1* %sf
  %299 = trunc i32 %287 to i8
  %300 = call i8 @llvm.ctpop.i8(i8 %299)
  %301 = and i8 %300, 1
  %302 = icmp eq i8 %301, 0
  store i1 %302, i1* %pf
  store volatile i64 31005, i64* @assembly_address
  %303 = load i32* %2
  %304 = sext i32 %303 to i64
  %305 = load i32* %1
  %306 = trunc i64 %304 to i32
  %307 = icmp slt i32 %306, %305
  br i1 %307, label %block_789b, label %block_7923

block_7923:                                       ; preds = %block_7911
  store volatile i64 31011, i64* @assembly_address
  store volatile i64 31012, i64* @assembly_address
  %308 = load i64* %stack_var_-16
  store i64 %308, i64* %rax
  store volatile i64 31016, i64* @assembly_address
  %309 = load i64* %rax
  %310 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %311 = xor i64 %309, %310
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %312 = icmp eq i64 %311, 0
  store i1 %312, i1* %zf
  %313 = icmp slt i64 %311, 0
  store i1 %313, i1* %sf
  %314 = trunc i64 %311 to i8
  %315 = call i8 @llvm.ctpop.i8(i8 %314)
  %316 = and i8 %315, 1
  %317 = icmp eq i8 %316, 0
  store i1 %317, i1* %pf
  store i64 %311, i64* %rax
  store volatile i64 31025, i64* @assembly_address
  %318 = load i1* %zf
  br i1 %318, label %block_7938, label %block_7933

block_7933:                                       ; preds = %block_7923
  store volatile i64 31027, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_7938:                                       ; preds = %block_7923
  store volatile i64 31032, i64* @assembly_address
  %319 = load i64* %stack_var_-8
  store i64 %319, i64* %rbp
  %320 = ptrtoint i64* %stack_var_0 to i64
  store i64 %320, i64* %rsp
  store volatile i64 31033, i64* @assembly_address
  %321 = load i64* %rax
  %322 = load i64* %rax
  ret i64 %322
}

define i64 @do_exit(i32 %arg1) {
block_793a:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 31034, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 31035, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 31038, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 31042, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-12
  store volatile i64 31045, i64* @assembly_address
  %22 = load i32* bitcast (i64* @global_var_216f90 to i32*)
  %23 = zext i32 %22 to i64
  store i64 %23, i64* %rax
  store volatile i64 31051, i64* @assembly_address
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i32
  %26 = load i64* %rax
  %27 = trunc i64 %26 to i32
  %28 = and i32 %25, %27
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %29 = icmp eq i32 %28, 0
  store i1 %29, i1* %zf
  %30 = icmp slt i32 %28, 0
  store i1 %30, i1* %sf
  %31 = trunc i32 %28 to i8
  %32 = call i8 @llvm.ctpop.i8(i8 %31)
  %33 = and i8 %32, 1
  %34 = icmp eq i8 %33, 0
  store i1 %34, i1* %pf
  store volatile i64 31053, i64* @assembly_address
  %35 = load i1* %zf
  br i1 %35, label %block_7959, label %block_794f

block_794f:                                       ; preds = %block_793a
  store volatile i64 31055, i64* @assembly_address
  %36 = load i32* %stack_var_-12
  %37 = zext i32 %36 to i64
  store i64 %37, i64* %rax
  store volatile i64 31058, i64* @assembly_address
  %38 = load i64* %rax
  %39 = trunc i64 %38 to i32
  %40 = zext i32 %39 to i64
  store i64 %40, i64* %rdi
  store volatile i64 31060, i64* @assembly_address
  %41 = load i64* %rdi
  %42 = trunc i64 %41 to i32
  call void @exit(i32 %42)
  unreachable

block_7959:                                       ; preds = %block_793a
  store volatile i64 31065, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216f90 to i32*)
  store volatile i64 31075, i64* @assembly_address
  %43 = load i64* @global_var_216628
  store i64 %43, i64* %rax
  store volatile i64 31082, i64* @assembly_address
  %44 = load i64* %rax
  store i64 %44, i64* %rdi
  store volatile i64 31085, i64* @assembly_address
  %45 = load i64* %rdi
  %46 = inttoptr i64 %45 to i64*
  call void @free(i64* %46)
  store volatile i64 31090, i64* @assembly_address
  store i64 0, i64* @global_var_216628
  store volatile i64 31101, i64* @assembly_address
  %47 = load i32* %stack_var_-12
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rax
  store volatile i64 31104, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  %51 = zext i32 %50 to i64
  store i64 %51, i64* %rdi
  store volatile i64 31106, i64* @assembly_address
  %52 = load i64* %rdi
  %53 = trunc i64 %52 to i32
  call void @exit(i32 %53)
  %54 = load i64* %rax
  ret i64 %54
}

declare i64 @190(i64)

define i64 @finish_out() {
block_7987:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_-8 = alloca i64
  store volatile i64 31111, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 31112, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 31115, i64* @assembly_address
  %3 = load i64* @global_var_216560
  store i64 %3, i64* %rax
  store volatile i64 31122, i64* @assembly_address
  %4 = load i64* %rax
  store i64 %4, i64* %rdi
  store volatile i64 31125, i64* @assembly_address
  %5 = load i64* %rdi
  %6 = inttoptr i64 %5 to %_IO_FILE*
  %7 = call i64 @rpl_fclose(%_IO_FILE* %6)
  store i64 %7, i64* %rax
  store i64 %7, i64* %rax
  store volatile i64 31130, i64* @assembly_address
  %8 = load i64* %rax
  %9 = trunc i64 %8 to i32
  %10 = load i64* %rax
  %11 = trunc i64 %10 to i32
  %12 = and i32 %9, %11
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %13 = icmp eq i32 %12, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i32 %12, 0
  store i1 %14, i1* %sf
  %15 = trunc i32 %12 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  store volatile i64 31132, i64* @assembly_address
  %19 = load i1* %zf
  br i1 %19, label %block_79a3, label %block_799e

block_799e:                                       ; preds = %block_7987
  store volatile i64 31134, i64* @assembly_address
  %20 = call i64 @write_error()
  store i64 %20, i64* %rax
  store i64 %20, i64* %rax
  store i64 %20, i64* %rax
  unreachable

block_79a3:                                       ; preds = %block_7987
  store volatile i64 31139, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 31144, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = trunc i64 %21 to i32
  %23 = call i64 @do_exit(i32 %22)
  store i64 %23, i64* %rax
  store i64 %23, i64* %rax
  %24 = load i64* %rax
  %25 = load i64* %rax
  ret i64 %25
}

define i64 @remove_output_file(i32 %arg1) {
block_79ad:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-1048 = alloca i64
  %stack_var_-1180 = alloca i32
  %stack_var_-1176 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-1196 = alloca i32
  %1 = alloca i8
  %stack_var_-1208 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 31149, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 31150, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 31153, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 1200
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 1200
  %10 = xor i64 %5, 1200
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-1208 to i64
  store i64 %20, i64* %rsp
  store volatile i64 31160, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = trunc i64 %21 to i32
  %23 = zext i32 %22 to i64
  store i64 %23, i64* %rax
  store volatile i64 31162, i64* @assembly_address
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i8
  %26 = sext i8 %25 to i32
  store i32 %26, i32* %stack_var_-1196
  store volatile i64 31168, i64* @assembly_address
  %27 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %27, i64* %rax
  store volatile i64 31177, i64* @assembly_address
  %28 = load i64* %rax
  store i64 %28, i64* %stack_var_-16
  store volatile i64 31181, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %29 = icmp eq i32 0, 0
  store i1 %29, i1* %zf
  %30 = icmp slt i32 0, 0
  store i1 %30, i1* %sf
  %31 = trunc i32 0 to i8
  %32 = call i8 @llvm.ctpop.i8(i8 %31)
  %33 = and i8 %32, 1
  %34 = icmp eq i8 %33, 0
  store i1 %34, i1* %pf
  %35 = zext i32 0 to i64
  store i64 %35, i64* %rax
  store volatile i64 31183, i64* @assembly_address
  %36 = load i32* %stack_var_-1196
  %37 = trunc i32 %36 to i8
  %38 = zext i8 %37 to i64
  store i64 %38, i64* %rax
  store volatile i64 31190, i64* @assembly_address
  %39 = load i64* %rax
  %40 = trunc i64 %39 to i32
  %41 = xor i32 %40, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %42 = icmp eq i32 %41, 0
  store i1 %42, i1* %zf
  %43 = icmp slt i32 %41, 0
  store i1 %43, i1* %sf
  %44 = trunc i32 %41 to i8
  %45 = call i8 @llvm.ctpop.i8(i8 %44)
  %46 = and i8 %45, 1
  %47 = icmp eq i8 %46, 0
  store i1 %47, i1* %pf
  %48 = zext i32 %41 to i64
  store i64 %48, i64* %rax
  store volatile i64 31193, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i8
  %51 = load i64* %rax
  %52 = trunc i64 %51 to i8
  %53 = and i8 %50, %52
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %54 = icmp eq i8 %53, 0
  store i1 %54, i1* %zf
  %55 = icmp slt i8 %53, 0
  store i1 %55, i1* %sf
  %56 = call i8 @llvm.ctpop.i8(i8 %53)
  %57 = and i8 %56, 1
  %58 = icmp eq i8 %57, 0
  store i1 %58, i1* %pf
  store volatile i64 31195, i64* @assembly_address
  %59 = load i1* %zf
  br i1 %59, label %block_79f8, label %block_79dd

block_79dd:                                       ; preds = %block_79ad
  store volatile i64 31197, i64* @assembly_address
  %60 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %60, i64* %rax
  store volatile i64 31204, i64* @assembly_address
  %61 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %61, i64* %rdx
  store volatile i64 31207, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216640 to i64), i64* %rsi
  store volatile i64 31214, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 31219, i64* @assembly_address
  %62 = load i64* %rdi
  %63 = trunc i64 %62 to i32
  %64 = load i64* %rsi
  %65 = inttoptr i64 %64 to %_TYPEDEF_sigset_t*
  %66 = load i64* %rdx
  %67 = inttoptr i64 %66 to %_TYPEDEF_sigset_t*
  %68 = call i32 @sigprocmask(i32 %63, %_TYPEDEF_sigset_t* %65, %_TYPEDEF_sigset_t* %67)
  %69 = sext i32 %68 to i64
  store i64 %69, i64* %rax
  %70 = sext i32 %68 to i64
  store i64 %70, i64* %rax
  br label %block_79f8

block_79f8:                                       ; preds = %block_79dd, %block_79ad
  store volatile i64 31224, i64* @assembly_address
  %71 = load i32* bitcast (i64* @global_var_2160a4 to i32*)
  %72 = zext i32 %71 to i64
  store i64 %72, i64* %rax
  store volatile i64 31230, i64* @assembly_address
  %73 = load i64* %rax
  %74 = trunc i64 %73 to i32
  store i32 %74, i32* %stack_var_-1180
  store volatile i64 31236, i64* @assembly_address
  %75 = load i32* %stack_var_-1180
  %76 = and i32 %75, 15
  %77 = icmp ugt i32 %76, 15
  %78 = icmp ult i32 %75, 0
  %79 = xor i32 %75, 0
  %80 = and i32 %79, 0
  %81 = icmp slt i32 %80, 0
  store i1 %77, i1* %az
  store i1 %78, i1* %cf
  store i1 %81, i1* %of
  %82 = icmp eq i32 %75, 0
  store i1 %82, i1* %zf
  %83 = icmp slt i32 %75, 0
  store i1 %83, i1* %sf
  %84 = trunc i32 %75 to i8
  %85 = call i8 @llvm.ctpop.i8(i8 %84)
  %86 = and i8 %85, 1
  %87 = icmp eq i8 %86, 0
  store i1 %87, i1* %pf
  store volatile i64 31243, i64* @assembly_address
  %88 = load i1* %sf
  br i1 %88, label %block_7a49, label %block_7a0d

block_7a0d:                                       ; preds = %block_79f8
  store volatile i64 31245, i64* @assembly_address
  store i32 -1, i32* bitcast (i64* @global_var_2160a4 to i32*)
  store volatile i64 31255, i64* @assembly_address
  %89 = load i32* %stack_var_-1180
  %90 = zext i32 %89 to i64
  store i64 %90, i64* %rax
  store volatile i64 31261, i64* @assembly_address
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i32
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rdi
  store volatile i64 31263, i64* @assembly_address
  %94 = load i64* %rdi
  %95 = trunc i64 %94 to i32
  %96 = call i32 @close(i32 %95)
  %97 = sext i32 %96 to i64
  store i64 %97, i64* %rax
  %98 = sext i32 %96 to i64
  store i64 %98, i64* %rax
  store volatile i64 31268, i64* @assembly_address
  %99 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %99, i64* %rax
  store volatile i64 31275, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2166e0 to i64), i64* %rsi
  store volatile i64 31282, i64* @assembly_address
  %100 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %100, i64* %rdi
  store volatile i64 31285, i64* @assembly_address
  %101 = load i64* %rdi
  %102 = inttoptr i64 %101 to i64*
  %103 = load i64* %rsi
  %104 = inttoptr i64 %103 to i64*
  %105 = bitcast i64* %102 to i8*
  %106 = call i64 @volatile_strcpy(i8* %105, i64* %104)
  store i64 %106, i64* %rax
  store i64 %106, i64* %rax
  store volatile i64 31290, i64* @assembly_address
  %107 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %107, i64* %rax
  store volatile i64 31297, i64* @assembly_address
  %108 = ptrtoint i64* %stack_var_-1048 to i64
  store i64 %108, i64* %rdi
  store volatile i64 31300, i64* @assembly_address
  %109 = load i64* %rdi
  %110 = inttoptr i64 %109 to i64*
  %111 = bitcast i64* %110 to i8*
  %112 = call i64 @xunlink(i8* %111)
  store i64 %112, i64* %rax
  store i64 %112, i64* %rax
  br label %block_7a49

block_7a49:                                       ; preds = %block_7a0d, %block_79f8
  store volatile i64 31305, i64* @assembly_address
  %113 = load i32* %stack_var_-1196
  %114 = trunc i32 %113 to i8
  %115 = zext i8 %114 to i64
  store i64 %115, i64* %rax
  store volatile i64 31312, i64* @assembly_address
  %116 = load i64* %rax
  %117 = trunc i64 %116 to i32
  %118 = xor i32 %117, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %119 = icmp eq i32 %118, 0
  store i1 %119, i1* %zf
  %120 = icmp slt i32 %118, 0
  store i1 %120, i1* %sf
  %121 = trunc i32 %118 to i8
  %122 = call i8 @llvm.ctpop.i8(i8 %121)
  %123 = and i8 %122, 1
  %124 = icmp eq i8 %123, 0
  store i1 %124, i1* %pf
  %125 = zext i32 %118 to i64
  store i64 %125, i64* %rax
  store volatile i64 31315, i64* @assembly_address
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i8
  %128 = load i64* %rax
  %129 = trunc i64 %128 to i8
  %130 = and i8 %127, %129
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %zf
  %132 = icmp slt i8 %130, 0
  store i1 %132, i1* %sf
  %133 = call i8 @llvm.ctpop.i8(i8 %130)
  %134 = and i8 %133, 1
  %135 = icmp eq i8 %134, 0
  store i1 %135, i1* %pf
  store volatile i64 31317, i64* @assembly_address
  %136 = load i1* %zf
  br i1 %136, label %block_7a70, label %block_7a57

block_7a57:                                       ; preds = %block_7a49
  store volatile i64 31319, i64* @assembly_address
  %137 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %137, i64* %rax
  store volatile i64 31326, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 31331, i64* @assembly_address
  %138 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %138, i64* %rsi
  store volatile i64 31334, i64* @assembly_address
  store i64 2, i64* %rdi
  store volatile i64 31339, i64* @assembly_address
  %139 = load i64* %rdi
  %140 = trunc i64 %139 to i32
  %141 = load i64* %rsi
  %142 = inttoptr i64 %141 to %_TYPEDEF_sigset_t*
  %143 = load i64* %rdx
  %144 = inttoptr i64 %143 to %_TYPEDEF_sigset_t*
  %145 = call i32 @sigprocmask(i32 %140, %_TYPEDEF_sigset_t* %142, %_TYPEDEF_sigset_t* %144)
  %146 = sext i32 %145 to i64
  store i64 %146, i64* %rax
  %147 = sext i32 %145 to i64
  store i64 %147, i64* %rax
  br label %block_7a70

block_7a70:                                       ; preds = %block_7a57, %block_7a49
  store volatile i64 31344, i64* @assembly_address
  store volatile i64 31345, i64* @assembly_address
  %148 = load i64* %stack_var_-16
  store i64 %148, i64* %rax
  store volatile i64 31349, i64* @assembly_address
  %149 = load i64* %rax
  %150 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %151 = xor i64 %149, %150
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %152 = icmp eq i64 %151, 0
  store i1 %152, i1* %zf
  %153 = icmp slt i64 %151, 0
  store i1 %153, i1* %sf
  %154 = trunc i64 %151 to i8
  %155 = call i8 @llvm.ctpop.i8(i8 %154)
  %156 = and i8 %155, 1
  %157 = icmp eq i8 %156, 0
  store i1 %157, i1* %pf
  store i64 %151, i64* %rax
  store volatile i64 31358, i64* @assembly_address
  %158 = load i1* %zf
  br i1 %158, label %block_7a85, label %block_7a80

block_7a80:                                       ; preds = %block_7a70
  store volatile i64 31360, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_7a85:                                       ; preds = %block_7a70
  store volatile i64 31365, i64* @assembly_address
  %159 = load i64* %stack_var_-8
  store i64 %159, i64* %rbp
  %160 = ptrtoint i64* %stack_var_0 to i64
  store i64 %160, i64* %rsp
  store volatile i64 31366, i64* @assembly_address
  %161 = load i64* %rax
  ret i64 %161
}

declare i64 @191(i64)

define i64 @abort_gzip() {
block_7a87:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 31367, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 31368, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 31371, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 31376, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = call i64 @remove_output_file(i32 %4)
  store i64 %5, i64* %rax
  store i64 %5, i64* %rax
  store volatile i64 31381, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 31386, i64* @assembly_address
  %6 = load i64* %rdi
  %7 = trunc i64 %6 to i32
  %8 = call i64 @do_exit(i32 %7)
  store i64 %8, i64* %rax
  store i64 %8, i64* %rax
  %9 = load i64* %rax
  %10 = load i64* %rax
  ret i64 %10
}

define i64 @abort_gzip_signal(i32 %arg1) {
block_7a9f:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 31391, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 31392, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 31395, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 31399, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-12
  store volatile i64 31402, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 31407, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = trunc i64 %22 to i32
  %24 = call i64 @remove_output_file(i32 %23)
  store i64 %24, i64* %rax
  store i64 %24, i64* %rax
  store volatile i64 31412, i64* @assembly_address
  %25 = load i32* bitcast (i64* @global_var_2166c0 to i32*)
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rax
  store volatile i64 31418, i64* @assembly_address
  %27 = load i32* %stack_var_-12
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  %30 = sub i32 %27, %29
  %31 = and i32 %27, 15
  %32 = and i32 %29, 15
  %33 = sub i32 %31, %32
  %34 = icmp ugt i32 %33, 15
  %35 = icmp ult i32 %27, %29
  %36 = xor i32 %27, %29
  %37 = xor i32 %27, %30
  %38 = and i32 %36, %37
  %39 = icmp slt i32 %38, 0
  store i1 %34, i1* %az
  store i1 %35, i1* %cf
  store i1 %39, i1* %of
  %40 = icmp eq i32 %30, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i32 %30, 0
  store i1 %41, i1* %sf
  %42 = trunc i32 %30 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  store volatile i64 31421, i64* @assembly_address
  %46 = load i1* %zf
  %47 = icmp eq i1 %46, false
  br i1 %47, label %block_7ac9, label %block_7abf

block_7abf:                                       ; preds = %block_7a9f
  store volatile i64 31423, i64* @assembly_address
  store i64 2, i64* %rdi
  store volatile i64 31428, i64* @assembly_address
  %48 = load i64* %rdi
  %49 = trunc i64 %48 to i32
  call void @_exit(i32 %49)
  unreachable

block_7ac9:                                       ; preds = %block_7a9f
  store volatile i64 31433, i64* @assembly_address
  %50 = load i32* %stack_var_-12
  %51 = zext i32 %50 to i64
  store i64 %51, i64* %rax
  store volatile i64 31436, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 31441, i64* @assembly_address
  %52 = load i64* %rax
  %53 = trunc i64 %52 to i32
  %54 = zext i32 %53 to i64
  store i64 %54, i64* %rdi
  store volatile i64 31443, i64* @assembly_address
  %55 = load i64* %rdi
  %56 = trunc i64 %55 to i32
  %57 = load i64* %rsi
  %58 = inttoptr i64 %57 to void (i32)*
  %59 = call void (i32)* (i32, void (i32)*)* @signal(i32 %56, void (i32)* %58)
  %60 = ptrtoint void (i32)* %59 to i64
  store i64 %60, i64* %rax
  %61 = ptrtoint void (i32)* %59 to i64
  store i64 %61, i64* %rax
  store volatile i64 31448, i64* @assembly_address
  %62 = load i32* %stack_var_-12
  %63 = zext i32 %62 to i64
  store i64 %63, i64* %rax
  store volatile i64 31451, i64* @assembly_address
  %64 = load i64* %rax
  %65 = trunc i64 %64 to i32
  %66 = zext i32 %65 to i64
  store i64 %66, i64* %rdi
  store volatile i64 31453, i64* @assembly_address
  %67 = load i64* %rdi
  %68 = trunc i64 %67 to i32
  %69 = call i32 @raise(i32 %68)
  %70 = sext i32 %69 to i64
  store i64 %70, i64* %rax
  %71 = sext i32 %69 to i64
  store i64 %71, i64* %rax
  store volatile i64 31458, i64* @assembly_address
  store volatile i64 31459, i64* @assembly_address
  %72 = load i64* %stack_var_-8
  store i64 %72, i64* %rbp
  %73 = ptrtoint i64* %stack_var_0 to i64
  store i64 %73, i64* %rsp
  store volatile i64 31460, i64* @assembly_address
  %74 = load i64* %rax
  ret i64 %74
}

declare i64 @192(i64)

define i64 @huft_build(i32* %arg1, i64 %arg2, i64 %arg3, i64* %arg4, i64* %arg5, i64* %arg6, i32* %arg7) {
block_7ae5:
  %r15 = alloca i64
  %r14 = alloca i64
  %r13 = alloca i64
  %r12 = alloca i64
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg6 to i64
  store i64 %0, i64* %r9
  %1 = ptrtoint i64* %arg5 to i64
  store i64 %1, i64* %r8
  %2 = ptrtoint i64* %arg4 to i64
  store i64 %2, i64* %rcx
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %3 = bitcast i32* %arg1 to i64*
  %4 = ptrtoint i64* %3 to i64
  store i64 %4, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-1520 = alloca i64
  %stack_var_-1528 = alloca i8
  %stack_var_-1527 = alloca i8
  %stack_var_-1560 = alloca i32
  %stack_var_-1564 = alloca i32
  %stack_var_-1544 = alloca i32
  %stack_var_-1352 = alloca i64
  %stack_var_-1556 = alloca i32
  %stack_var_-1224 = alloca i64
  %stack_var_-1536 = alloca i32*
  %5 = alloca i64
  %stack_var_-1424 = alloca i64
  %stack_var_-1432 = alloca i64
  %stack_var_-1508 = alloca i64
  %stack_var_-1428 = alloca i32
  %stack_var_-1548 = alloca i32
  %stack_var_-1540 = alloca i32
  %stack_var_-1628 = alloca i32
  %stack_var_-1552 = alloca i32
  %stack_var_-1512 = alloca i32
  %6 = alloca i64
  %stack_var_-64 = alloca i64
  %stack_var_-1624 = alloca i32*
  %7 = alloca i64
  %stack_var_8 = alloca i32*
  %8 = alloca i64
  store i32* %arg7, i32** %stack_var_8
  %stack_var_-1616 = alloca i64
  %stack_var_-1608 = alloca i16*
  %9 = alloca i64
  %stack_var_-1600 = alloca i16*
  %10 = alloca i64
  %stack_var_-1592 = alloca i32
  %stack_var_-1588 = alloca i32
  %stack_var_-1584 = alloca i32*
  %11 = alloca i64
  %stack_var_-1640 = alloca i64
  %stack_var_-48 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %12 = alloca i32
  %13 = alloca i32
  %14 = alloca i64
  %15 = alloca i32
  %16 = alloca i64
  %17 = alloca i32
  store volatile i64 31461, i64* @assembly_address
  %18 = load i64* %rbp
  store i64 %18, i64* %stack_var_-8
  %19 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %19, i64* %rsp
  store volatile i64 31462, i64* @assembly_address
  %20 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %20, i64* %rbp
  store volatile i64 31465, i64* @assembly_address
  %21 = load i64* %r15
  store i64 %21, i64* %stack_var_-16
  %22 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %22, i64* %rsp
  store volatile i64 31467, i64* @assembly_address
  %23 = load i64* %r14
  store i64 %23, i64* %stack_var_-24
  %24 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %24, i64* %rsp
  store volatile i64 31469, i64* @assembly_address
  %25 = load i64* %r13
  store i64 %25, i64* %stack_var_-32
  %26 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %26, i64* %rsp
  store volatile i64 31471, i64* @assembly_address
  %27 = load i64* %r12
  store i64 %27, i64* %stack_var_-40
  %28 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %28, i64* %rsp
  store volatile i64 31473, i64* @assembly_address
  %29 = load i64* %rbx
  store i64 %29, i64* %stack_var_-48
  %30 = ptrtoint i64* %stack_var_-48 to i64
  store i64 %30, i64* %rsp
  store volatile i64 31474, i64* @assembly_address
  %31 = load i64* %rsp
  %32 = sub i64 %31, 1592
  %33 = and i64 %31, 15
  %34 = sub i64 %33, 8
  %35 = icmp ugt i64 %34, 15
  %36 = icmp ult i64 %31, 1592
  %37 = xor i64 %31, 1592
  %38 = xor i64 %31, %32
  %39 = and i64 %37, %38
  %40 = icmp slt i64 %39, 0
  store i1 %35, i1* %az
  store i1 %36, i1* %cf
  store i1 %40, i1* %of
  %41 = icmp eq i64 %32, 0
  store i1 %41, i1* %zf
  %42 = icmp slt i64 %32, 0
  store i1 %42, i1* %sf
  %43 = trunc i64 %32 to i8
  %44 = call i8 @llvm.ctpop.i8(i8 %43)
  %45 = and i8 %44, 1
  %46 = icmp eq i8 %45, 0
  store i1 %46, i1* %pf
  %47 = ptrtoint i64* %stack_var_-1640 to i64
  store i64 %47, i64* %rsp
  store volatile i64 31481, i64* @assembly_address
  %48 = load i64* %rdi
  %49 = inttoptr i64 %48 to i32*
  store i32* %49, i32** %stack_var_-1584
  store volatile i64 31488, i64* @assembly_address
  %50 = load i64* %rsi
  %51 = trunc i64 %50 to i32
  store i32 %51, i32* %stack_var_-1588
  store volatile i64 31494, i64* @assembly_address
  %52 = load i64* %rdx
  %53 = trunc i64 %52 to i32
  store i32 %53, i32* %stack_var_-1592
  store volatile i64 31500, i64* @assembly_address
  %54 = load i64* %rcx
  %55 = inttoptr i64 %54 to i16*
  store i16* %55, i16** %stack_var_-1600
  store volatile i64 31507, i64* @assembly_address
  %56 = load i64* %r8
  %57 = inttoptr i64 %56 to i16*
  store i16* %57, i16** %stack_var_-1608
  store volatile i64 31514, i64* @assembly_address
  %58 = load i64* %r9
  store i64 %58, i64* %stack_var_-1616
  store volatile i64 31521, i64* @assembly_address
  %59 = load i32** %stack_var_8
  %60 = ptrtoint i32* %59 to i64
  store i64 %60, i64* %rax
  store volatile i64 31525, i64* @assembly_address
  %61 = load i64* %rax
  %62 = inttoptr i64 %61 to i32*
  store i32* %62, i32** %stack_var_-1624
  store volatile i64 31532, i64* @assembly_address
  %63 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %63, i64* %rax
  store volatile i64 31541, i64* @assembly_address
  %64 = load i64* %rax
  store i64 %64, i64* %stack_var_-64
  store volatile i64 31545, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %65 = icmp eq i32 0, 0
  store i1 %65, i1* %zf
  %66 = icmp slt i32 0, 0
  store i1 %66, i1* %sf
  %67 = trunc i32 0 to i8
  %68 = call i8 @llvm.ctpop.i8(i8 %67)
  %69 = and i8 %68, 1
  %70 = icmp eq i8 %69, 0
  store i1 %70, i1* %pf
  %71 = zext i32 0 to i64
  store i64 %71, i64* %rax
  store volatile i64 31547, i64* @assembly_address
  %72 = ptrtoint i32* %stack_var_-1512 to i64
  store i64 %72, i64* %rax
  store volatile i64 31554, i64* @assembly_address
  store i64 68, i64* %rdx
  store volatile i64 31559, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 31564, i64* @assembly_address
  %73 = ptrtoint i32* %stack_var_-1512 to i64
  store i64 %73, i64* %rdi
  store volatile i64 31567, i64* @assembly_address
  %74 = load i64* %rdi
  %75 = inttoptr i64 %74 to i64*
  %76 = load i64* %rsi
  %77 = trunc i64 %76 to i32
  %78 = load i64* %rdx
  %79 = trunc i64 %78 to i32
  %80 = call i64* @memset(i64* %75, i32 %77, i32 %79)
  %81 = ptrtoint i64* %80 to i64
  store i64 %81, i64* %rax
  %82 = ptrtoint i64* %80 to i64
  store i64 %82, i64* %rax
  store volatile i64 31572, i64* @assembly_address
  %83 = load i32** %stack_var_-1584
  %84 = ptrtoint i32* %83 to i64
  store i64 %84, i64* %r13
  store volatile i64 31579, i64* @assembly_address
  %85 = load i32* %stack_var_-1588
  %86 = zext i32 %85 to i64
  store i64 %86, i64* %r12
  br label %block_7b62

block_7b62:                                       ; preds = %block_7b62, %block_7ae5
  store volatile i64 31586, i64* @assembly_address
  %87 = load i64* %r13
  %88 = inttoptr i64 %87 to i32*
  %89 = load i32* %88
  %90 = zext i32 %89 to i64
  store i64 %90, i64* %rax
  store volatile i64 31590, i64* @assembly_address
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i32
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rdx
  store volatile i64 31592, i64* @assembly_address
  %94 = load i64* %rbp
  %95 = load i64* %rdx
  %96 = mul i64 %95, 4
  %97 = add i64 %94, -1504
  %98 = add i64 %97, %96
  %99 = inttoptr i64 %98 to i32*
  %100 = load i32* %99
  %101 = zext i32 %100 to i64
  store i64 %101, i64* %rdx
  store volatile i64 31599, i64* @assembly_address
  %102 = load i64* %rdx
  %103 = trunc i64 %102 to i32
  %104 = add i32 %103, 1
  %105 = and i32 %103, 15
  %106 = add i32 %105, 1
  %107 = icmp ugt i32 %106, 15
  %108 = icmp ult i32 %104, %103
  %109 = xor i32 %103, %104
  %110 = xor i32 1, %104
  %111 = and i32 %109, %110
  %112 = icmp slt i32 %111, 0
  store i1 %107, i1* %az
  store i1 %108, i1* %cf
  store i1 %112, i1* %of
  %113 = icmp eq i32 %104, 0
  store i1 %113, i1* %zf
  %114 = icmp slt i32 %104, 0
  store i1 %114, i1* %sf
  %115 = trunc i32 %104 to i8
  %116 = call i8 @llvm.ctpop.i8(i8 %115)
  %117 = and i8 %116, 1
  %118 = icmp eq i8 %117, 0
  store i1 %118, i1* %pf
  %119 = zext i32 %104 to i64
  store i64 %119, i64* %rdx
  store volatile i64 31602, i64* @assembly_address
  %120 = load i64* %rax
  %121 = trunc i64 %120 to i32
  %122 = zext i32 %121 to i64
  store i64 %122, i64* %rax
  store volatile i64 31604, i64* @assembly_address
  %123 = load i64* %rdx
  %124 = trunc i64 %123 to i32
  %125 = load i64* %rbp
  %126 = load i64* %rax
  %127 = mul i64 %126, 4
  %128 = add i64 %125, -1504
  %129 = add i64 %128, %127
  %130 = inttoptr i64 %129 to i32*
  store i32 %124, i32* %130
  store volatile i64 31611, i64* @assembly_address
  %131 = load i64* %r13
  %132 = add i64 %131, 4
  %133 = and i64 %131, 15
  %134 = add i64 %133, 4
  %135 = icmp ugt i64 %134, 15
  %136 = icmp ult i64 %132, %131
  %137 = xor i64 %131, %132
  %138 = xor i64 4, %132
  %139 = and i64 %137, %138
  %140 = icmp slt i64 %139, 0
  store i1 %135, i1* %az
  store i1 %136, i1* %cf
  store i1 %140, i1* %of
  %141 = icmp eq i64 %132, 0
  store i1 %141, i1* %zf
  %142 = icmp slt i64 %132, 0
  store i1 %142, i1* %sf
  %143 = trunc i64 %132 to i8
  %144 = call i8 @llvm.ctpop.i8(i8 %143)
  %145 = and i8 %144, 1
  %146 = icmp eq i8 %145, 0
  store i1 %146, i1* %pf
  store i64 %132, i64* %r13
  store volatile i64 31615, i64* @assembly_address
  %147 = load i64* %r12
  %148 = trunc i64 %147 to i32
  %149 = sub i32 %148, 1
  %150 = and i32 %148, 15
  %151 = sub i32 %150, 1
  %152 = icmp ugt i32 %151, 15
  %153 = icmp ult i32 %148, 1
  %154 = xor i32 %148, 1
  %155 = xor i32 %148, %149
  %156 = and i32 %154, %155
  %157 = icmp slt i32 %156, 0
  store i1 %152, i1* %az
  store i1 %153, i1* %cf
  store i1 %157, i1* %of
  %158 = icmp eq i32 %149, 0
  store i1 %158, i1* %zf
  %159 = icmp slt i32 %149, 0
  store i1 %159, i1* %sf
  %160 = trunc i32 %149 to i8
  %161 = call i8 @llvm.ctpop.i8(i8 %160)
  %162 = and i8 %161, 1
  %163 = icmp eq i8 %162, 0
  store i1 %163, i1* %pf
  %164 = zext i32 %149 to i64
  store i64 %164, i64* %r12
  store volatile i64 31619, i64* @assembly_address
  %165 = load i64* %r12
  %166 = trunc i64 %165 to i32
  %167 = load i64* %r12
  %168 = trunc i64 %167 to i32
  %169 = and i32 %166, %168
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %170 = icmp eq i32 %169, 0
  store i1 %170, i1* %zf
  %171 = icmp slt i32 %169, 0
  store i1 %171, i1* %sf
  %172 = trunc i32 %169 to i8
  %173 = call i8 @llvm.ctpop.i8(i8 %172)
  %174 = and i8 %173, 1
  %175 = icmp eq i8 %174, 0
  store i1 %175, i1* %pf
  store volatile i64 31622, i64* @assembly_address
  %176 = load i1* %zf
  %177 = icmp eq i1 %176, false
  br i1 %177, label %block_7b62, label %block_7b88

block_7b88:                                       ; preds = %block_7b62
  store volatile i64 31624, i64* @assembly_address
  %178 = load i32* %stack_var_-1512
  %179 = sext i32 %178 to i64
  %180 = trunc i64 %179 to i32
  %181 = zext i32 %180 to i64
  store i64 %181, i64* %rax
  store volatile i64 31630, i64* @assembly_address
  %182 = load i32* %stack_var_-1588
  %183 = load i64* %rax
  %184 = trunc i64 %183 to i32
  %185 = sub i32 %182, %184
  %186 = and i32 %182, 15
  %187 = and i32 %184, 15
  %188 = sub i32 %186, %187
  %189 = icmp ugt i32 %188, 15
  %190 = icmp ult i32 %182, %184
  %191 = xor i32 %182, %184
  %192 = xor i32 %182, %185
  %193 = and i32 %191, %192
  %194 = icmp slt i32 %193, 0
  store i1 %189, i1* %az
  store i1 %190, i1* %cf
  store i1 %194, i1* %of
  %195 = icmp eq i32 %185, 0
  store i1 %195, i1* %zf
  %196 = icmp slt i32 %185, 0
  store i1 %196, i1* %sf
  %197 = trunc i32 %185 to i8
  %198 = call i8 @llvm.ctpop.i8(i8 %197)
  %199 = and i8 %198, 1
  %200 = icmp eq i8 %199, 0
  store i1 %200, i1* %pf
  store volatile i64 31636, i64* @assembly_address
  %201 = load i1* %zf
  %202 = icmp eq i1 %201, false
  br i1 %202, label %block_7c0c, label %block_7b96

block_7b96:                                       ; preds = %block_7b88
  store volatile i64 31638, i64* @assembly_address
  store i64 48, i64* %rdi
  store volatile i64 31643, i64* @assembly_address
  %203 = load i64* %rdi
  %204 = trunc i64 %203 to i32
  %205 = call i64* @malloc(i32 %204)
  %206 = ptrtoint i64* %205 to i64
  store i64 %206, i64* %rax
  %207 = ptrtoint i64* %205 to i64
  store i64 %207, i64* %rax
  store volatile i64 31648, i64* @assembly_address
  %208 = load i64* %rax
  store i64 %208, i64* %r14
  store volatile i64 31651, i64* @assembly_address
  %209 = load i64* %r14
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %210 = icmp eq i64 %209, 0
  store i1 %210, i1* %zf
  %211 = icmp slt i64 %209, 0
  store i1 %211, i1* %sf
  %212 = trunc i64 %209 to i8
  %213 = call i8 @llvm.ctpop.i8(i8 %212)
  %214 = and i8 %213, 1
  %215 = icmp eq i8 %214, 0
  store i1 %215, i1* %pf
  store volatile i64 31654, i64* @assembly_address
  %216 = load i1* %zf
  %217 = icmp eq i1 %216, false
  br i1 %217, label %block_7bb2, label %block_7ba8

block_7ba8:                                       ; preds = %block_7b96
  store volatile i64 31656, i64* @assembly_address
  store i64 3, i64* %rax
  store volatile i64 31661, i64* @assembly_address
  br label %block_821b

block_7bb2:                                       ; preds = %block_7b96
  store volatile i64 31666, i64* @assembly_address
  %218 = load i32* bitcast (i64* @global_var_216fa4 to i32*)
  %219 = zext i32 %218 to i64
  store i64 %219, i64* %rax
  store volatile i64 31672, i64* @assembly_address
  %220 = load i64* %rax
  %221 = trunc i64 %220 to i32
  %222 = add i32 %221, 3
  %223 = and i32 %221, 15
  %224 = add i32 %223, 3
  %225 = icmp ugt i32 %224, 15
  %226 = icmp ult i32 %222, %221
  %227 = xor i32 %221, %222
  %228 = xor i32 3, %222
  %229 = and i32 %227, %228
  %230 = icmp slt i32 %229, 0
  store i1 %225, i1* %az
  store i1 %226, i1* %cf
  store i1 %230, i1* %of
  %231 = icmp eq i32 %222, 0
  store i1 %231, i1* %zf
  %232 = icmp slt i32 %222, 0
  store i1 %232, i1* %sf
  %233 = trunc i32 %222 to i8
  %234 = call i8 @llvm.ctpop.i8(i8 %233)
  %235 = and i8 %234, 1
  %236 = icmp eq i8 %235, 0
  store i1 %236, i1* %pf
  store i64 ptrtoint (i64* @global_var_216fa7 to i64), i64* %rax
  store volatile i64 31675, i64* @assembly_address
  %237 = load i64* %rax
  %238 = trunc i64 %237 to i32
  store i32 %238, i32* bitcast (i64* @global_var_216fa4 to i32*)
  store volatile i64 31681, i64* @assembly_address
  %239 = load i64* %r14
  %240 = add i64 %239, 8
  %241 = inttoptr i64 %240 to i64*
  store i64 0, i64* %241
  store volatile i64 31689, i64* @assembly_address
  %242 = load i64* %r14
  %243 = add i64 %242, 16
  store i64 %243, i64* %rax
  store volatile i64 31693, i64* @assembly_address
  %244 = load i64* %rax
  %245 = inttoptr i64 %244 to i8*
  store i8 99, i8* %245
  store volatile i64 31696, i64* @assembly_address
  %246 = load i64* %r14
  %247 = add i64 %246, 16
  store i64 %247, i64* %rax
  store volatile i64 31700, i64* @assembly_address
  %248 = load i64* %rax
  %249 = add i64 %248, 1
  %250 = inttoptr i64 %249 to i8*
  store i8 1, i8* %250
  store volatile i64 31704, i64* @assembly_address
  %251 = load i64* %r14
  %252 = add i64 %251, 32
  store i64 %252, i64* %rax
  store volatile i64 31708, i64* @assembly_address
  %253 = load i64* %rax
  %254 = inttoptr i64 %253 to i8*
  store i8 99, i8* %254
  store volatile i64 31711, i64* @assembly_address
  %255 = load i64* %r14
  %256 = add i64 %255, 32
  store i64 %256, i64* %rax
  store volatile i64 31715, i64* @assembly_address
  %257 = load i64* %rax
  %258 = add i64 %257, 1
  %259 = inttoptr i64 %258 to i8*
  store i8 1, i8* %259
  store volatile i64 31719, i64* @assembly_address
  %260 = load i64* %r14
  %261 = add i64 %260, 16
  store i64 %261, i64* %rdx
  store volatile i64 31723, i64* @assembly_address
  %262 = load i64* %stack_var_-1616
  store i64 %262, i64* %rax
  store volatile i64 31730, i64* @assembly_address
  %263 = load i64* %rdx
  %264 = load i64* %rax
  %265 = inttoptr i64 %264 to i64*
  store i64 %263, i64* %265
  store volatile i64 31733, i64* @assembly_address
  %266 = load i32** %stack_var_-1624
  %267 = ptrtoint i32* %266 to i64
  store i64 %267, i64* %rax
  store volatile i64 31740, i64* @assembly_address
  %268 = load i64* %rax
  %269 = inttoptr i64 %268 to i32*
  store i32 1, i32* %269
  store volatile i64 31746, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 31751, i64* @assembly_address
  br label %block_821b

block_7c0c:                                       ; preds = %block_7b88
  store volatile i64 31756, i64* @assembly_address
  %270 = load i32** %stack_var_-1624
  %271 = ptrtoint i32* %270 to i64
  store i64 %271, i64* %rax
  store volatile i64 31763, i64* @assembly_address
  %272 = load i64* %rax
  %273 = inttoptr i64 %272 to i32*
  %274 = load i32* %273
  %275 = zext i32 %274 to i64
  store i64 %275, i64* %rax
  store volatile i64 31765, i64* @assembly_address
  %276 = load i64* %rax
  %277 = trunc i64 %276 to i32
  store i32 %277, i32* %stack_var_-1552
  store volatile i64 31771, i64* @assembly_address
  store i64 1, i64* %rbx
  store volatile i64 31776, i64* @assembly_address
  br label %block_7c32

block_7c22:                                       ; preds = %block_7c32
  store volatile i64 31778, i64* @assembly_address
  %278 = load i64* %rbx
  %279 = trunc i64 %278 to i32
  %280 = zext i32 %279 to i64
  store i64 %280, i64* %rax
  store volatile i64 31780, i64* @assembly_address
  %281 = load i64* %rbp
  %282 = load i64* %rax
  %283 = mul i64 %282, 4
  %284 = add i64 %281, -1504
  %285 = add i64 %284, %283
  %286 = inttoptr i64 %285 to i32*
  %287 = load i32* %286
  %288 = zext i32 %287 to i64
  store i64 %288, i64* %rax
  store volatile i64 31787, i64* @assembly_address
  %289 = load i64* %rax
  %290 = trunc i64 %289 to i32
  %291 = load i64* %rax
  %292 = trunc i64 %291 to i32
  %293 = and i32 %290, %292
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %294 = icmp eq i32 %293, 0
  store i1 %294, i1* %zf
  %295 = icmp slt i32 %293, 0
  store i1 %295, i1* %sf
  %296 = trunc i32 %293 to i8
  %297 = call i8 @llvm.ctpop.i8(i8 %296)
  %298 = and i8 %297, 1
  %299 = icmp eq i8 %298, 0
  store i1 %299, i1* %pf
  store volatile i64 31789, i64* @assembly_address
  %300 = load i1* %zf
  %301 = icmp eq i1 %300, false
  br i1 %301, label %block_7c39, label %block_7c2f

block_7c2f:                                       ; preds = %block_7c22
  store volatile i64 31791, i64* @assembly_address
  %302 = load i64* %rbx
  %303 = trunc i64 %302 to i32
  %304 = add i32 %303, 1
  %305 = and i32 %303, 15
  %306 = add i32 %305, 1
  %307 = icmp ugt i32 %306, 15
  %308 = icmp ult i32 %304, %303
  %309 = xor i32 %303, %304
  %310 = xor i32 1, %304
  %311 = and i32 %309, %310
  %312 = icmp slt i32 %311, 0
  store i1 %307, i1* %az
  store i1 %308, i1* %cf
  store i1 %312, i1* %of
  %313 = icmp eq i32 %304, 0
  store i1 %313, i1* %zf
  %314 = icmp slt i32 %304, 0
  store i1 %314, i1* %sf
  %315 = trunc i32 %304 to i8
  %316 = call i8 @llvm.ctpop.i8(i8 %315)
  %317 = and i8 %316, 1
  %318 = icmp eq i8 %317, 0
  store i1 %318, i1* %pf
  %319 = zext i32 %304 to i64
  store i64 %319, i64* %rbx
  br label %block_7c32

block_7c32:                                       ; preds = %block_7c2f, %block_7c0c
  store volatile i64 31794, i64* @assembly_address
  %320 = load i64* %rbx
  %321 = trunc i64 %320 to i32
  %322 = sub i32 %321, 16
  %323 = and i32 %321, 15
  %324 = icmp ugt i32 %323, 15
  %325 = icmp ult i32 %321, 16
  %326 = xor i32 %321, 16
  %327 = xor i32 %321, %322
  %328 = and i32 %326, %327
  %329 = icmp slt i32 %328, 0
  store i1 %324, i1* %az
  store i1 %325, i1* %cf
  store i1 %329, i1* %of
  %330 = icmp eq i32 %322, 0
  store i1 %330, i1* %zf
  %331 = icmp slt i32 %322, 0
  store i1 %331, i1* %sf
  %332 = trunc i32 %322 to i8
  %333 = call i8 @llvm.ctpop.i8(i8 %332)
  %334 = and i8 %333, 1
  %335 = icmp eq i8 %334, 0
  store i1 %335, i1* %pf
  store volatile i64 31797, i64* @assembly_address
  %336 = load i1* %cf
  %337 = load i1* %zf
  %338 = or i1 %336, %337
  br i1 %338, label %block_7c22, label %block_7c37

block_7c37:                                       ; preds = %block_7c32
  store volatile i64 31799, i64* @assembly_address
  br label %block_7c3a

block_7c39:                                       ; preds = %block_7c22
  store volatile i64 31801, i64* @assembly_address
  br label %block_7c3a

block_7c3a:                                       ; preds = %block_7c39, %block_7c37
  store volatile i64 31802, i64* @assembly_address
  %339 = load i64* %rbx
  %340 = trunc i64 %339 to i32
  store i32 %340, i32* %stack_var_-1628
  store volatile i64 31808, i64* @assembly_address
  %341 = load i32* %stack_var_-1552
  %342 = zext i32 %341 to i64
  store i64 %342, i64* %rax
  store volatile i64 31814, i64* @assembly_address
  %343 = load i64* %rbx
  %344 = trunc i64 %343 to i32
  %345 = load i64* %rax
  %346 = trunc i64 %345 to i32
  %347 = sub i32 %344, %346
  %348 = and i32 %344, 15
  %349 = and i32 %346, 15
  %350 = sub i32 %348, %349
  %351 = icmp ugt i32 %350, 15
  %352 = icmp ult i32 %344, %346
  %353 = xor i32 %344, %346
  %354 = xor i32 %344, %347
  %355 = and i32 %353, %354
  %356 = icmp slt i32 %355, 0
  store i1 %351, i1* %az
  store i1 %352, i1* %cf
  store i1 %356, i1* %of
  %357 = icmp eq i32 %347, 0
  store i1 %357, i1* %zf
  %358 = icmp slt i32 %347, 0
  store i1 %358, i1* %sf
  %359 = trunc i32 %347 to i8
  %360 = call i8 @llvm.ctpop.i8(i8 %359)
  %361 = and i8 %360, 1
  %362 = icmp eq i8 %361, 0
  store i1 %362, i1* %pf
  store volatile i64 31816, i64* @assembly_address
  %363 = load i1* %cf
  %364 = load i1* %zf
  %365 = or i1 %363, %364
  br i1 %365, label %block_7c50, label %block_7c4a

block_7c4a:                                       ; preds = %block_7c3a
  store volatile i64 31818, i64* @assembly_address
  %366 = load i64* %rbx
  %367 = trunc i64 %366 to i32
  store i32 %367, i32* %stack_var_-1552
  br label %block_7c50

block_7c50:                                       ; preds = %block_7c4a, %block_7c3a
  store volatile i64 31824, i64* @assembly_address
  store i64 16, i64* %r12
  store volatile i64 31830, i64* @assembly_address
  br label %block_7c6a

block_7c58:                                       ; preds = %block_7c6a
  store volatile i64 31832, i64* @assembly_address
  %368 = load i64* %r12
  %369 = trunc i64 %368 to i32
  %370 = zext i32 %369 to i64
  store i64 %370, i64* %rax
  store volatile i64 31835, i64* @assembly_address
  %371 = load i64* %rbp
  %372 = load i64* %rax
  %373 = mul i64 %372, 4
  %374 = add i64 %371, -1504
  %375 = add i64 %374, %373
  %376 = inttoptr i64 %375 to i32*
  %377 = load i32* %376
  %378 = zext i32 %377 to i64
  store i64 %378, i64* %rax
  store volatile i64 31842, i64* @assembly_address
  %379 = load i64* %rax
  %380 = trunc i64 %379 to i32
  %381 = load i64* %rax
  %382 = trunc i64 %381 to i32
  %383 = and i32 %380, %382
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %384 = icmp eq i32 %383, 0
  store i1 %384, i1* %zf
  %385 = icmp slt i32 %383, 0
  store i1 %385, i1* %sf
  %386 = trunc i32 %383 to i8
  %387 = call i8 @llvm.ctpop.i8(i8 %386)
  %388 = and i8 %387, 1
  %389 = icmp eq i8 %388, 0
  store i1 %389, i1* %pf
  store volatile i64 31844, i64* @assembly_address
  %390 = load i1* %zf
  %391 = icmp eq i1 %390, false
  br i1 %391, label %block_7c71, label %block_7c66

block_7c66:                                       ; preds = %block_7c58
  store volatile i64 31846, i64* @assembly_address
  %392 = load i64* %r12
  %393 = trunc i64 %392 to i32
  %394 = sub i32 %393, 1
  %395 = and i32 %393, 15
  %396 = sub i32 %395, 1
  %397 = icmp ugt i32 %396, 15
  %398 = icmp ult i32 %393, 1
  %399 = xor i32 %393, 1
  %400 = xor i32 %393, %394
  %401 = and i32 %399, %400
  %402 = icmp slt i32 %401, 0
  store i1 %397, i1* %az
  store i1 %398, i1* %cf
  store i1 %402, i1* %of
  %403 = icmp eq i32 %394, 0
  store i1 %403, i1* %zf
  %404 = icmp slt i32 %394, 0
  store i1 %404, i1* %sf
  %405 = trunc i32 %394 to i8
  %406 = call i8 @llvm.ctpop.i8(i8 %405)
  %407 = and i8 %406, 1
  %408 = icmp eq i8 %407, 0
  store i1 %408, i1* %pf
  %409 = zext i32 %394 to i64
  store i64 %409, i64* %r12
  br label %block_7c6a

block_7c6a:                                       ; preds = %block_7c66, %block_7c50
  store volatile i64 31850, i64* @assembly_address
  %410 = load i64* %r12
  %411 = trunc i64 %410 to i32
  %412 = load i64* %r12
  %413 = trunc i64 %412 to i32
  %414 = and i32 %411, %413
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %415 = icmp eq i32 %414, 0
  store i1 %415, i1* %zf
  %416 = icmp slt i32 %414, 0
  store i1 %416, i1* %sf
  %417 = trunc i32 %414 to i8
  %418 = call i8 @llvm.ctpop.i8(i8 %417)
  %419 = and i8 %418, 1
  %420 = icmp eq i8 %419, 0
  store i1 %420, i1* %pf
  store volatile i64 31853, i64* @assembly_address
  %421 = load i1* %zf
  %422 = icmp eq i1 %421, false
  br i1 %422, label %block_7c58, label %block_7c6f

block_7c6f:                                       ; preds = %block_7c6a
  store volatile i64 31855, i64* @assembly_address
  br label %block_7c72

block_7c71:                                       ; preds = %block_7c58
  store volatile i64 31857, i64* @assembly_address
  br label %block_7c72

block_7c72:                                       ; preds = %block_7c71, %block_7c6f
  store volatile i64 31858, i64* @assembly_address
  %423 = load i64* %r12
  %424 = trunc i64 %423 to i32
  store i32 %424, i32* %stack_var_-1540
  store volatile i64 31865, i64* @assembly_address
  %425 = load i32* %stack_var_-1552
  %426 = zext i32 %425 to i64
  store i64 %426, i64* %rax
  store volatile i64 31871, i64* @assembly_address
  %427 = load i64* %r12
  %428 = trunc i64 %427 to i32
  %429 = load i64* %rax
  %430 = trunc i64 %429 to i32
  %431 = sub i32 %428, %430
  %432 = and i32 %428, 15
  %433 = and i32 %430, 15
  %434 = sub i32 %432, %433
  %435 = icmp ugt i32 %434, 15
  %436 = icmp ult i32 %428, %430
  %437 = xor i32 %428, %430
  %438 = xor i32 %428, %431
  %439 = and i32 %437, %438
  %440 = icmp slt i32 %439, 0
  store i1 %435, i1* %az
  store i1 %436, i1* %cf
  store i1 %440, i1* %of
  %441 = icmp eq i32 %431, 0
  store i1 %441, i1* %zf
  %442 = icmp slt i32 %431, 0
  store i1 %442, i1* %sf
  %443 = trunc i32 %431 to i8
  %444 = call i8 @llvm.ctpop.i8(i8 %443)
  %445 = and i8 %444, 1
  %446 = icmp eq i8 %445, 0
  store i1 %446, i1* %pf
  store volatile i64 31874, i64* @assembly_address
  %447 = load i1* %cf
  %448 = icmp eq i1 %447, false
  br i1 %448, label %block_7c8b, label %block_7c84

block_7c84:                                       ; preds = %block_7c72
  store volatile i64 31876, i64* @assembly_address
  %449 = load i64* %r12
  %450 = trunc i64 %449 to i32
  store i32 %450, i32* %stack_var_-1552
  br label %block_7c8b

block_7c8b:                                       ; preds = %block_7c84, %block_7c72
  store volatile i64 31883, i64* @assembly_address
  %451 = load i32** %stack_var_-1624
  %452 = ptrtoint i32* %451 to i64
  store i64 %452, i64* %rax
  store volatile i64 31890, i64* @assembly_address
  %453 = load i32* %stack_var_-1552
  %454 = zext i32 %453 to i64
  store i64 %454, i64* %rdx
  store volatile i64 31896, i64* @assembly_address
  %455 = load i64* %rdx
  %456 = trunc i64 %455 to i32
  %457 = load i64* %rax
  %458 = inttoptr i64 %457 to i32*
  store i32 %456, i32* %458
  store volatile i64 31898, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 31903, i64* @assembly_address
  %459 = load i64* %rbx
  %460 = trunc i64 %459 to i32
  %461 = zext i32 %460 to i64
  store i64 %461, i64* %rcx
  store volatile i64 31905, i64* @assembly_address
  %462 = load i64* %rax
  %463 = trunc i64 %462 to i32
  %464 = load i64* %rcx
  %465 = trunc i64 %464 to i8
  %466 = zext i8 %465 to i32
  %467 = and i32 %466, 31
  %468 = load i1* %of
  %469 = icmp eq i32 %467, 0
  br i1 %469, label %487, label %470

; <label>:470                                     ; preds = %block_7c8b
  %471 = shl i32 %463, %467
  %472 = icmp eq i32 %471, 0
  store i1 %472, i1* %zf
  %473 = icmp slt i32 %471, 0
  store i1 %473, i1* %sf
  %474 = trunc i32 %471 to i8
  %475 = call i8 @llvm.ctpop.i8(i8 %474)
  %476 = and i8 %475, 1
  %477 = icmp eq i8 %476, 0
  store i1 %477, i1* %pf
  %478 = zext i32 %471 to i64
  store i64 %478, i64* %rax
  %479 = sub i32 %467, 1
  %480 = shl i32 %463, %479
  %481 = lshr i32 %480, 31
  %482 = trunc i32 %481 to i1
  store i1 %482, i1* %cf
  %483 = lshr i32 %471, 31
  %484 = icmp ne i32 %483, %481
  %485 = icmp eq i32 %467, 1
  %486 = select i1 %485, i1 %484, i1 %468
  store i1 %486, i1* %of
  br label %487

; <label>:487                                     ; preds = %block_7c8b, %470
  store volatile i64 31907, i64* @assembly_address
  %488 = load i64* %rax
  %489 = trunc i64 %488 to i32
  store i32 %489, i32* %stack_var_-1548
  store volatile i64 31913, i64* @assembly_address
  br label %block_7ce0

block_7cab:                                       ; preds = %block_7ce0
  store volatile i64 31915, i64* @assembly_address
  %490 = load i32* %stack_var_-1548
  %491 = zext i32 %490 to i64
  store i64 %491, i64* %rdx
  store volatile i64 31921, i64* @assembly_address
  %492 = load i64* %rbx
  %493 = trunc i64 %492 to i32
  %494 = zext i32 %493 to i64
  store i64 %494, i64* %rax
  store volatile i64 31923, i64* @assembly_address
  %495 = load i64* %rbp
  %496 = load i64* %rax
  %497 = mul i64 %496, 4
  %498 = add i64 %495, -1504
  %499 = add i64 %498, %497
  %500 = inttoptr i64 %499 to i32*
  %501 = load i32* %500
  %502 = zext i32 %501 to i64
  store i64 %502, i64* %rax
  store volatile i64 31930, i64* @assembly_address
  %503 = load i64* %rdx
  %504 = trunc i64 %503 to i32
  %505 = load i64* %rax
  %506 = trunc i64 %505 to i32
  %507 = sub i32 %504, %506
  %508 = and i32 %504, 15
  %509 = and i32 %506, 15
  %510 = sub i32 %508, %509
  %511 = icmp ugt i32 %510, 15
  %512 = icmp ult i32 %504, %506
  %513 = xor i32 %504, %506
  %514 = xor i32 %504, %507
  %515 = and i32 %513, %514
  %516 = icmp slt i32 %515, 0
  store i1 %511, i1* %az
  store i1 %512, i1* %cf
  store i1 %516, i1* %of
  %517 = icmp eq i32 %507, 0
  store i1 %517, i1* %zf
  %518 = icmp slt i32 %507, 0
  store i1 %518, i1* %sf
  %519 = trunc i32 %507 to i8
  %520 = call i8 @llvm.ctpop.i8(i8 %519)
  %521 = and i8 %520, 1
  %522 = icmp eq i8 %521, 0
  store i1 %522, i1* %pf
  %523 = zext i32 %507 to i64
  store i64 %523, i64* %rdx
  store volatile i64 31932, i64* @assembly_address
  %524 = load i64* %rdx
  %525 = trunc i64 %524 to i32
  %526 = zext i32 %525 to i64
  store i64 %526, i64* %rax
  store volatile i64 31934, i64* @assembly_address
  %527 = load i64* %rax
  %528 = trunc i64 %527 to i32
  store i32 %528, i32* %stack_var_-1548
  store volatile i64 31940, i64* @assembly_address
  %529 = load i32* %stack_var_-1548
  %530 = and i32 %529, 15
  %531 = icmp ugt i32 %530, 15
  %532 = icmp ult i32 %529, 0
  %533 = xor i32 %529, 0
  %534 = and i32 %533, 0
  %535 = icmp slt i32 %534, 0
  store i1 %531, i1* %az
  store i1 %532, i1* %cf
  store i1 %535, i1* %of
  %536 = icmp eq i32 %529, 0
  store i1 %536, i1* %zf
  %537 = icmp slt i32 %529, 0
  store i1 %537, i1* %sf
  %538 = trunc i32 %529 to i8
  %539 = call i8 @llvm.ctpop.i8(i8 %538)
  %540 = and i8 %539, 1
  %541 = icmp eq i8 %540, 0
  store i1 %541, i1* %pf
  store volatile i64 31947, i64* @assembly_address
  %542 = load i1* %sf
  %543 = icmp eq i1 %542, false
  br i1 %543, label %block_7cd7, label %block_7ccd

block_7ccd:                                       ; preds = %block_7cab
  store volatile i64 31949, i64* @assembly_address
  store i64 2, i64* %rax
  store volatile i64 31954, i64* @assembly_address
  br label %block_821b

block_7cd7:                                       ; preds = %block_7cab
  store volatile i64 31959, i64* @assembly_address
  %544 = load i64* %rbx
  %545 = trunc i64 %544 to i32
  %546 = add i32 %545, 1
  %547 = and i32 %545, 15
  %548 = add i32 %547, 1
  %549 = icmp ugt i32 %548, 15
  %550 = icmp ult i32 %546, %545
  %551 = xor i32 %545, %546
  %552 = xor i32 1, %546
  %553 = and i32 %551, %552
  %554 = icmp slt i32 %553, 0
  store i1 %549, i1* %az
  store i1 %550, i1* %cf
  store i1 %554, i1* %of
  %555 = icmp eq i32 %546, 0
  store i1 %555, i1* %zf
  %556 = icmp slt i32 %546, 0
  store i1 %556, i1* %sf
  %557 = trunc i32 %546 to i8
  %558 = call i8 @llvm.ctpop.i8(i8 %557)
  %559 = and i8 %558, 1
  %560 = icmp eq i8 %559, 0
  store i1 %560, i1* %pf
  %561 = zext i32 %546 to i64
  store i64 %561, i64* %rbx
  store volatile i64 31962, i64* @assembly_address
  %562 = load i32* %stack_var_-1548
  %563 = load i1* %of
  %564 = shl i32 %562, 1
  %565 = icmp eq i32 %564, 0
  store i1 %565, i1* %zf
  %566 = icmp slt i32 %564, 0
  store i1 %566, i1* %sf
  %567 = trunc i32 %564 to i8
  %568 = call i8 @llvm.ctpop.i8(i8 %567)
  %569 = and i8 %568, 1
  %570 = icmp eq i8 %569, 0
  store i1 %570, i1* %pf
  store i32 %564, i32* %stack_var_-1548
  %571 = shl i32 %562, 0
  %572 = lshr i32 %571, 31
  %573 = trunc i32 %572 to i1
  store i1 %573, i1* %cf
  %574 = lshr i32 %564, 31
  %575 = icmp ne i32 %574, %572
  %576 = select i1 true, i1 %575, i1 %563
  store i1 %576, i1* %of
  br label %block_7ce0

block_7ce0:                                       ; preds = %block_7cd7, %487
  store volatile i64 31968, i64* @assembly_address
  %577 = load i64* %rbx
  %578 = trunc i64 %577 to i32
  %579 = load i64* %r12
  %580 = trunc i64 %579 to i32
  %581 = sub i32 %578, %580
  %582 = and i32 %578, 15
  %583 = and i32 %580, 15
  %584 = sub i32 %582, %583
  %585 = icmp ugt i32 %584, 15
  %586 = icmp ult i32 %578, %580
  %587 = xor i32 %578, %580
  %588 = xor i32 %578, %581
  %589 = and i32 %587, %588
  %590 = icmp slt i32 %589, 0
  store i1 %585, i1* %az
  store i1 %586, i1* %cf
  store i1 %590, i1* %of
  %591 = icmp eq i32 %581, 0
  store i1 %591, i1* %zf
  %592 = icmp slt i32 %581, 0
  store i1 %592, i1* %sf
  %593 = trunc i32 %581 to i8
  %594 = call i8 @llvm.ctpop.i8(i8 %593)
  %595 = and i8 %594, 1
  %596 = icmp eq i8 %595, 0
  store i1 %596, i1* %pf
  store volatile i64 31971, i64* @assembly_address
  %597 = load i1* %cf
  br i1 %597, label %block_7cab, label %block_7ce5

block_7ce5:                                       ; preds = %block_7ce0
  store volatile i64 31973, i64* @assembly_address
  %598 = load i32* %stack_var_-1548
  %599 = zext i32 %598 to i64
  store i64 %599, i64* %rdx
  store volatile i64 31979, i64* @assembly_address
  %600 = load i64* %r12
  %601 = trunc i64 %600 to i32
  %602 = zext i32 %601 to i64
  store i64 %602, i64* %rax
  store volatile i64 31982, i64* @assembly_address
  %603 = load i64* %rbp
  %604 = load i64* %rax
  %605 = mul i64 %604, 4
  %606 = add i64 %603, -1504
  %607 = add i64 %606, %605
  %608 = inttoptr i64 %607 to i32*
  %609 = load i32* %608
  %610 = zext i32 %609 to i64
  store i64 %610, i64* %rax
  store volatile i64 31989, i64* @assembly_address
  %611 = load i64* %rdx
  %612 = trunc i64 %611 to i32
  %613 = load i64* %rax
  %614 = trunc i64 %613 to i32
  %615 = sub i32 %612, %614
  %616 = and i32 %612, 15
  %617 = and i32 %614, 15
  %618 = sub i32 %616, %617
  %619 = icmp ugt i32 %618, 15
  %620 = icmp ult i32 %612, %614
  %621 = xor i32 %612, %614
  %622 = xor i32 %612, %615
  %623 = and i32 %621, %622
  %624 = icmp slt i32 %623, 0
  store i1 %619, i1* %az
  store i1 %620, i1* %cf
  store i1 %624, i1* %of
  %625 = icmp eq i32 %615, 0
  store i1 %625, i1* %zf
  %626 = icmp slt i32 %615, 0
  store i1 %626, i1* %sf
  %627 = trunc i32 %615 to i8
  %628 = call i8 @llvm.ctpop.i8(i8 %627)
  %629 = and i8 %628, 1
  %630 = icmp eq i8 %629, 0
  store i1 %630, i1* %pf
  %631 = zext i32 %615 to i64
  store i64 %631, i64* %rdx
  store volatile i64 31991, i64* @assembly_address
  %632 = load i64* %rdx
  %633 = trunc i64 %632 to i32
  %634 = zext i32 %633 to i64
  store i64 %634, i64* %rax
  store volatile i64 31993, i64* @assembly_address
  %635 = load i64* %rax
  %636 = trunc i64 %635 to i32
  store i32 %636, i32* %stack_var_-1548
  store volatile i64 31999, i64* @assembly_address
  %637 = load i32* %stack_var_-1548
  %638 = and i32 %637, 15
  %639 = icmp ugt i32 %638, 15
  %640 = icmp ult i32 %637, 0
  %641 = xor i32 %637, 0
  %642 = and i32 %641, 0
  %643 = icmp slt i32 %642, 0
  store i1 %639, i1* %az
  store i1 %640, i1* %cf
  store i1 %643, i1* %of
  %644 = icmp eq i32 %637, 0
  store i1 %644, i1* %zf
  %645 = icmp slt i32 %637, 0
  store i1 %645, i1* %sf
  %646 = trunc i32 %637 to i8
  %647 = call i8 @llvm.ctpop.i8(i8 %646)
  %648 = and i8 %647, 1
  %649 = icmp eq i8 %648, 0
  store i1 %649, i1* %pf
  store volatile i64 32006, i64* @assembly_address
  %650 = load i1* %sf
  %651 = icmp eq i1 %650, false
  br i1 %651, label %block_7d12, label %block_7d08

block_7d08:                                       ; preds = %block_7ce5
  store volatile i64 32008, i64* @assembly_address
  store i64 2, i64* %rax
  store volatile i64 32013, i64* @assembly_address
  br label %block_821b

block_7d12:                                       ; preds = %block_7ce5
  store volatile i64 32018, i64* @assembly_address
  %652 = load i64* %r12
  %653 = trunc i64 %652 to i32
  %654 = zext i32 %653 to i64
  store i64 %654, i64* %rax
  store volatile i64 32021, i64* @assembly_address
  %655 = load i64* %rbp
  %656 = load i64* %rax
  %657 = mul i64 %656, 4
  %658 = add i64 %655, -1504
  %659 = add i64 %658, %657
  %660 = inttoptr i64 %659 to i32*
  %661 = load i32* %660
  %662 = zext i32 %661 to i64
  store i64 %662, i64* %rdx
  store volatile i64 32028, i64* @assembly_address
  %663 = load i32* %stack_var_-1548
  %664 = zext i32 %663 to i64
  store i64 %664, i64* %rax
  store volatile i64 32034, i64* @assembly_address
  %665 = load i64* %rdx
  %666 = trunc i64 %665 to i32
  %667 = load i64* %rax
  %668 = trunc i64 %667 to i32
  %669 = add i32 %666, %668
  %670 = and i32 %666, 15
  %671 = and i32 %668, 15
  %672 = add i32 %670, %671
  %673 = icmp ugt i32 %672, 15
  %674 = icmp ult i32 %669, %666
  %675 = xor i32 %666, %669
  %676 = xor i32 %668, %669
  %677 = and i32 %675, %676
  %678 = icmp slt i32 %677, 0
  store i1 %673, i1* %az
  store i1 %674, i1* %cf
  store i1 %678, i1* %of
  %679 = icmp eq i32 %669, 0
  store i1 %679, i1* %zf
  %680 = icmp slt i32 %669, 0
  store i1 %680, i1* %sf
  %681 = trunc i32 %669 to i8
  %682 = call i8 @llvm.ctpop.i8(i8 %681)
  %683 = and i8 %682, 1
  %684 = icmp eq i8 %683, 0
  store i1 %684, i1* %pf
  %685 = zext i32 %669 to i64
  store i64 %685, i64* %rdx
  store volatile i64 32036, i64* @assembly_address
  %686 = load i64* %r12
  %687 = trunc i64 %686 to i32
  %688 = zext i32 %687 to i64
  store i64 %688, i64* %rax
  store volatile i64 32039, i64* @assembly_address
  %689 = load i64* %rdx
  %690 = trunc i64 %689 to i32
  %691 = load i64* %rbp
  %692 = load i64* %rax
  %693 = mul i64 %692, 4
  %694 = add i64 %691, -1504
  %695 = add i64 %694, %693
  %696 = inttoptr i64 %695 to i32*
  store i32 %690, i32* %696
  store volatile i64 32046, i64* @assembly_address
  store i64 0, i64* %rbx
  store volatile i64 32051, i64* @assembly_address
  %697 = load i64* %rbx
  %698 = trunc i64 %697 to i32
  store i32 %698, i32* %stack_var_-1428
  store volatile i64 32057, i64* @assembly_address
  %699 = ptrtoint i32* %stack_var_-1512 to i64
  store i64 %699, i64* %r13
  store volatile i64 32064, i64* @assembly_address
  %700 = load i64* %r13
  %701 = add i64 %700, 4
  %702 = and i64 %700, 15
  %703 = add i64 %702, 4
  %704 = icmp ugt i64 %703, 15
  %705 = icmp ult i64 %701, %700
  %706 = xor i64 %700, %701
  %707 = xor i64 4, %701
  %708 = and i64 %706, %707
  %709 = icmp slt i64 %708, 0
  store i1 %704, i1* %az
  store i1 %705, i1* %cf
  store i1 %709, i1* %of
  %710 = icmp eq i64 %701, 0
  store i1 %710, i1* %zf
  %711 = icmp slt i64 %701, 0
  store i1 %711, i1* %sf
  %712 = trunc i64 %701 to i8
  %713 = call i8 @llvm.ctpop.i8(i8 %712)
  %714 = and i8 %713, 1
  %715 = icmp eq i8 %714, 0
  store i1 %715, i1* %pf
  %716 = ptrtoint i64* %stack_var_-1508 to i64
  store i64 %716, i64* %r13
  store volatile i64 32068, i64* @assembly_address
  %717 = ptrtoint i64* %stack_var_-1432 to i64
  store i64 %717, i64* %rax
  store volatile i64 32075, i64* @assembly_address
  %718 = load i64* %rax
  %719 = add i64 %718, 8
  %720 = and i64 %718, 15
  %721 = add i64 %720, 8
  %722 = icmp ugt i64 %721, 15
  %723 = icmp ult i64 %719, %718
  %724 = xor i64 %718, %719
  %725 = xor i64 8, %719
  %726 = and i64 %724, %725
  %727 = icmp slt i64 %726, 0
  store i1 %722, i1* %az
  store i1 %723, i1* %cf
  store i1 %727, i1* %of
  %728 = icmp eq i64 %719, 0
  store i1 %728, i1* %zf
  %729 = icmp slt i64 %719, 0
  store i1 %729, i1* %sf
  %730 = trunc i64 %719 to i8
  %731 = call i8 @llvm.ctpop.i8(i8 %730)
  %732 = and i8 %731, 1
  %733 = icmp eq i8 %732, 0
  store i1 %733, i1* %pf
  %734 = ptrtoint i64* %stack_var_-1424 to i64
  store i64 %734, i64* %rax
  store volatile i64 32079, i64* @assembly_address
  %735 = bitcast i64* %stack_var_-1424 to i32*
  store i32* %735, i32** %stack_var_-1536
  store volatile i64 32086, i64* @assembly_address
  br label %block_7d77

block_7d58:                                       ; preds = %block_7d77
  store volatile i64 32088, i64* @assembly_address
  %736 = load i64* %r13
  store i64 %736, i64* %rax
  store volatile i64 32091, i64* @assembly_address
  %737 = load i64* %rax
  %738 = add i64 %737, 4
  store i64 %738, i64* %r13
  store volatile i64 32095, i64* @assembly_address
  %739 = load i64* %rax
  %740 = inttoptr i64 %739 to i32*
  %741 = load i32* %740
  %742 = zext i32 %741 to i64
  store i64 %742, i64* %rax
  store volatile i64 32097, i64* @assembly_address
  %743 = load i64* %rbx
  %744 = trunc i64 %743 to i32
  %745 = load i64* %rax
  %746 = trunc i64 %745 to i32
  %747 = add i32 %744, %746
  %748 = and i32 %744, 15
  %749 = and i32 %746, 15
  %750 = add i32 %748, %749
  %751 = icmp ugt i32 %750, 15
  %752 = icmp ult i32 %747, %744
  %753 = xor i32 %744, %747
  %754 = xor i32 %746, %747
  %755 = and i32 %753, %754
  %756 = icmp slt i32 %755, 0
  store i1 %751, i1* %az
  store i1 %752, i1* %cf
  store i1 %756, i1* %of
  %757 = icmp eq i32 %747, 0
  store i1 %757, i1* %zf
  %758 = icmp slt i32 %747, 0
  store i1 %758, i1* %sf
  %759 = trunc i32 %747 to i8
  %760 = call i8 @llvm.ctpop.i8(i8 %759)
  %761 = and i8 %760, 1
  %762 = icmp eq i8 %761, 0
  store i1 %762, i1* %pf
  %763 = zext i32 %747 to i64
  store i64 %763, i64* %rbx
  store volatile i64 32099, i64* @assembly_address
  %764 = load i32** %stack_var_-1536
  %765 = ptrtoint i32* %764 to i64
  store i64 %765, i64* %rax
  store volatile i64 32106, i64* @assembly_address
  %766 = load i64* %rax
  %767 = add i64 %766, 4
  store i64 %767, i64* %rdx
  store volatile i64 32110, i64* @assembly_address
  %768 = load i64* %rdx
  %769 = inttoptr i64 %768 to i32*
  store i32* %769, i32** %stack_var_-1536
  store volatile i64 32117, i64* @assembly_address
  %770 = load i64* %rbx
  %771 = trunc i64 %770 to i32
  %772 = load i64* %rax
  %773 = inttoptr i64 %772 to i32*
  store i32 %771, i32* %773
  br label %block_7d77

block_7d77:                                       ; preds = %block_7d58, %block_7d12
  store volatile i64 32119, i64* @assembly_address
  %774 = load i64* %r12
  %775 = trunc i64 %774 to i32
  %776 = sub i32 %775, 1
  %777 = and i32 %775, 15
  %778 = sub i32 %777, 1
  %779 = icmp ugt i32 %778, 15
  %780 = icmp ult i32 %775, 1
  %781 = xor i32 %775, 1
  %782 = xor i32 %775, %776
  %783 = and i32 %781, %782
  %784 = icmp slt i32 %783, 0
  store i1 %779, i1* %az
  store i1 %780, i1* %cf
  store i1 %784, i1* %of
  %785 = icmp eq i32 %776, 0
  store i1 %785, i1* %zf
  %786 = icmp slt i32 %776, 0
  store i1 %786, i1* %sf
  %787 = trunc i32 %776 to i8
  %788 = call i8 @llvm.ctpop.i8(i8 %787)
  %789 = and i8 %788, 1
  %790 = icmp eq i8 %789, 0
  store i1 %790, i1* %pf
  %791 = zext i32 %776 to i64
  store i64 %791, i64* %r12
  store volatile i64 32123, i64* @assembly_address
  %792 = load i64* %r12
  %793 = trunc i64 %792 to i32
  %794 = load i64* %r12
  %795 = trunc i64 %794 to i32
  %796 = and i32 %793, %795
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %797 = icmp eq i32 %796, 0
  store i1 %797, i1* %zf
  %798 = icmp slt i32 %796, 0
  store i1 %798, i1* %sf
  %799 = trunc i32 %796 to i8
  %800 = call i8 @llvm.ctpop.i8(i8 %799)
  %801 = and i8 %800, 1
  %802 = icmp eq i8 %801, 0
  store i1 %802, i1* %pf
  store volatile i64 32126, i64* @assembly_address
  %803 = load i1* %zf
  %804 = icmp eq i1 %803, false
  br i1 %804, label %block_7d58, label %block_7d80

block_7d80:                                       ; preds = %block_7d77
  store volatile i64 32128, i64* @assembly_address
  %805 = load i32** %stack_var_-1584
  %806 = ptrtoint i32* %805 to i64
  store i64 %806, i64* %r13
  store volatile i64 32135, i64* @assembly_address
  store i64 0, i64* %r12
  br label %block_7d8d

block_7d8d:                                       ; preds = %block_7db9, %block_7d80
  store volatile i64 32141, i64* @assembly_address
  %807 = load i64* %r13
  store i64 %807, i64* %rax
  store volatile i64 32144, i64* @assembly_address
  %808 = load i64* %rax
  %809 = add i64 %808, 4
  store i64 %809, i64* %r13
  store volatile i64 32148, i64* @assembly_address
  %810 = load i64* %rax
  %811 = inttoptr i64 %810 to i32*
  %812 = load i32* %811
  %813 = zext i32 %812 to i64
  store i64 %813, i64* %rbx
  store volatile i64 32150, i64* @assembly_address
  %814 = load i64* %rbx
  %815 = trunc i64 %814 to i32
  %816 = load i64* %rbx
  %817 = trunc i64 %816 to i32
  %818 = and i32 %815, %817
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %819 = icmp eq i32 %818, 0
  store i1 %819, i1* %zf
  %820 = icmp slt i32 %818, 0
  store i1 %820, i1* %sf
  %821 = trunc i32 %818 to i8
  %822 = call i8 @llvm.ctpop.i8(i8 %821)
  %823 = and i8 %822, 1
  %824 = icmp eq i8 %823, 0
  store i1 %824, i1* %pf
  store volatile i64 32152, i64* @assembly_address
  %825 = load i1* %zf
  br i1 %825, label %block_7db9, label %block_7d9a

block_7d9a:                                       ; preds = %block_7d8d
  store volatile i64 32154, i64* @assembly_address
  %826 = load i64* %rbx
  %827 = trunc i64 %826 to i32
  %828 = zext i32 %827 to i64
  store i64 %828, i64* %rax
  store volatile i64 32156, i64* @assembly_address
  %829 = load i64* %rbp
  %830 = load i64* %rax
  %831 = mul i64 %830, 4
  %832 = add i64 %829, -1424
  %833 = add i64 %832, %831
  %834 = inttoptr i64 %833 to i32*
  %835 = load i32* %834
  %836 = zext i32 %835 to i64
  store i64 %836, i64* %rax
  store volatile i64 32163, i64* @assembly_address
  %837 = load i64* %rax
  %838 = add i64 %837, 1
  %839 = trunc i64 %838 to i32
  %840 = zext i32 %839 to i64
  store i64 %840, i64* %rdx
  store volatile i64 32166, i64* @assembly_address
  %841 = load i64* %rbx
  %842 = trunc i64 %841 to i32
  %843 = zext i32 %842 to i64
  store i64 %843, i64* %rcx
  store volatile i64 32168, i64* @assembly_address
  %844 = load i64* %rdx
  %845 = trunc i64 %844 to i32
  %846 = load i64* %rbp
  %847 = load i64* %rcx
  %848 = mul i64 %847, 4
  %849 = add i64 %846, -1424
  %850 = add i64 %849, %848
  %851 = inttoptr i64 %850 to i32*
  store i32 %845, i32* %851
  store volatile i64 32175, i64* @assembly_address
  %852 = load i64* %rax
  %853 = trunc i64 %852 to i32
  %854 = zext i32 %853 to i64
  store i64 %854, i64* %rax
  store volatile i64 32177, i64* @assembly_address
  %855 = load i64* %r12
  %856 = trunc i64 %855 to i32
  %857 = load i64* %rbp
  %858 = load i64* %rax
  %859 = mul i64 %858, 4
  %860 = add i64 %857, -1216
  %861 = add i64 %860, %859
  %862 = inttoptr i64 %861 to i32*
  store i32 %856, i32* %862
  br label %block_7db9

block_7db9:                                       ; preds = %block_7d9a, %block_7d8d
  store volatile i64 32185, i64* @assembly_address
  %863 = load i64* %r12
  %864 = trunc i64 %863 to i32
  %865 = add i32 %864, 1
  %866 = and i32 %864, 15
  %867 = add i32 %866, 1
  %868 = icmp ugt i32 %867, 15
  %869 = icmp ult i32 %865, %864
  %870 = xor i32 %864, %865
  %871 = xor i32 1, %865
  %872 = and i32 %870, %871
  %873 = icmp slt i32 %872, 0
  store i1 %868, i1* %az
  store i1 %869, i1* %cf
  store i1 %873, i1* %of
  %874 = icmp eq i32 %865, 0
  store i1 %874, i1* %zf
  %875 = icmp slt i32 %865, 0
  store i1 %875, i1* %sf
  %876 = trunc i32 %865 to i8
  %877 = call i8 @llvm.ctpop.i8(i8 %876)
  %878 = and i8 %877, 1
  %879 = icmp eq i8 %878, 0
  store i1 %879, i1* %pf
  %880 = zext i32 %865 to i64
  store i64 %880, i64* %r12
  store volatile i64 32189, i64* @assembly_address
  %881 = load i64* %r12
  %882 = trunc i64 %881 to i32
  %883 = load i32* %stack_var_-1588
  %884 = sub i32 %882, %883
  %885 = and i32 %882, 15
  %886 = and i32 %883, 15
  %887 = sub i32 %885, %886
  %888 = icmp ugt i32 %887, 15
  %889 = icmp ult i32 %882, %883
  %890 = xor i32 %882, %883
  %891 = xor i32 %882, %884
  %892 = and i32 %890, %891
  %893 = icmp slt i32 %892, 0
  store i1 %888, i1* %az
  store i1 %889, i1* %cf
  store i1 %893, i1* %of
  %894 = icmp eq i32 %884, 0
  store i1 %894, i1* %zf
  %895 = icmp slt i32 %884, 0
  store i1 %895, i1* %sf
  %896 = trunc i32 %884 to i8
  %897 = call i8 @llvm.ctpop.i8(i8 %896)
  %898 = and i8 %897, 1
  %899 = icmp eq i8 %898, 0
  store i1 %899, i1* %pf
  store volatile i64 32196, i64* @assembly_address
  %900 = load i1* %cf
  br i1 %900, label %block_7d8d, label %block_7dc6

block_7dc6:                                       ; preds = %block_7db9
  store volatile i64 32198, i64* @assembly_address
  %901 = load i32* %stack_var_-1540
  %902 = zext i32 %901 to i64
  store i64 %902, i64* %rax
  store volatile i64 32204, i64* @assembly_address
  %903 = load i64* %rax
  %904 = trunc i64 %903 to i32
  %905 = sext i32 %904 to i64
  store i64 %905, i64* %rax
  store volatile i64 32206, i64* @assembly_address
  %906 = load i64* %rbp
  %907 = load i64* %rax
  %908 = mul i64 %907, 4
  %909 = add i64 %906, -1424
  %910 = add i64 %909, %908
  %911 = inttoptr i64 %910 to i32*
  %912 = load i32* %911
  %913 = zext i32 %912 to i64
  store i64 %913, i64* %rax
  store volatile i64 32213, i64* @assembly_address
  %914 = load i64* %rax
  %915 = trunc i64 %914 to i32
  store i32 %915, i32* %stack_var_-1588
  store volatile i64 32219, i64* @assembly_address
  store i64 0, i64* %r12
  store volatile i64 32225, i64* @assembly_address
  %916 = load i64* %r12
  %917 = trunc i64 %916 to i32
  %918 = sext i32 %917 to i64
  store i64 %918, i64* %stack_var_-1432
  store volatile i64 32232, i64* @assembly_address
  %919 = ptrtoint i64* %stack_var_-1224 to i64
  store i64 %919, i64* %r13
  store volatile i64 32239, i64* @assembly_address
  store i32 -1, i32* %stack_var_-1556
  store volatile i64 32249, i64* @assembly_address
  %920 = load i32* %stack_var_-1552
  %921 = zext i32 %920 to i64
  store i64 %921, i64* %rax
  store volatile i64 32255, i64* @assembly_address
  %922 = load i64* %rax
  %923 = trunc i64 %922 to i32
  %924 = sub i32 0, %923
  %925 = and i32 %923, 15
  %926 = sub i32 0, %925
  %927 = icmp ugt i32 %926, 15
  %928 = icmp ne i32 %923, 0
  store i1 %927, i1* %az
  store i1 %928, i1* %cf
  store i1 false, i1* %of
  %929 = icmp eq i32 %924, 0
  store i1 %929, i1* %zf
  %930 = icmp slt i32 %924, 0
  store i1 %930, i1* %sf
  %931 = trunc i32 %924 to i8
  %932 = call i8 @llvm.ctpop.i8(i8 %931)
  %933 = and i8 %932, 1
  %934 = icmp eq i8 %933, 0
  store i1 %934, i1* %pf
  %935 = zext i32 %924 to i64
  store i64 %935, i64* %rax
  store volatile i64 32257, i64* @assembly_address
  %936 = load i64* %rax
  %937 = trunc i64 %936 to i32
  %938 = zext i32 %937 to i64
  store i64 %938, i64* %r15
  store volatile i64 32260, i64* @assembly_address
  store i64 0, i64* %stack_var_-1352
  store volatile i64 32271, i64* @assembly_address
  store i64 0, i64* %r14
  store volatile i64 32277, i64* @assembly_address
  store i32 0, i32* %stack_var_-1544
  store volatile i64 32287, i64* @assembly_address
  br label %block_81eb

block_7e24:                                       ; preds = %block_81eb
  store volatile i64 32292, i64* @assembly_address
  %939 = load i32* %stack_var_-1628
  %940 = sext i32 %939 to i64
  store i64 %940, i64* %rax
  store volatile i64 32299, i64* @assembly_address
  %941 = load i64* %rbp
  %942 = load i64* %rax
  %943 = mul i64 %942, 4
  %944 = add i64 %941, -1504
  %945 = add i64 %944, %943
  %946 = inttoptr i64 %945 to i32*
  %947 = load i32* %946
  %948 = zext i32 %947 to i64
  store i64 %948, i64* %rax
  store volatile i64 32306, i64* @assembly_address
  %949 = load i64* %rax
  %950 = trunc i64 %949 to i32
  store i32 %950, i32* %stack_var_-1564
  store volatile i64 32312, i64* @assembly_address
  br label %block_81cd

block_7e3d:                                       ; preds = %block_8042
  store volatile i64 32317, i64* @assembly_address
  %951 = load i32* %stack_var_-1556
  %952 = add i32 %951, 1
  %953 = and i32 %951, 15
  %954 = add i32 %953, 1
  %955 = icmp ugt i32 %954, 15
  %956 = icmp ult i32 %952, %951
  %957 = xor i32 %951, %952
  %958 = xor i32 1, %952
  %959 = and i32 %957, %958
  %960 = icmp slt i32 %959, 0
  store i1 %955, i1* %az
  store i1 %956, i1* %cf
  store i1 %960, i1* %of
  %961 = icmp eq i32 %952, 0
  store i1 %961, i1* %zf
  %962 = icmp slt i32 %952, 0
  store i1 %962, i1* %sf
  %963 = trunc i32 %952 to i8
  %964 = call i8 @llvm.ctpop.i8(i8 %963)
  %965 = and i8 %964, 1
  %966 = icmp eq i8 %965, 0
  store i1 %966, i1* %pf
  store i32 %952, i32* %stack_var_-1556
  store volatile i64 32324, i64* @assembly_address
  %967 = load i32* %stack_var_-1552
  %968 = zext i32 %967 to i64
  store i64 %968, i64* %rax
  store volatile i64 32330, i64* @assembly_address
  %969 = load i64* %rax
  %970 = trunc i64 %969 to i32
  %971 = load i64* %r15
  %972 = trunc i64 %971 to i32
  %973 = add i32 %970, %972
  %974 = and i32 %970, 15
  %975 = and i32 %972, 15
  %976 = add i32 %974, %975
  %977 = icmp ugt i32 %976, 15
  %978 = icmp ult i32 %973, %970
  %979 = xor i32 %970, %973
  %980 = xor i32 %972, %973
  %981 = and i32 %979, %980
  %982 = icmp slt i32 %981, 0
  store i1 %977, i1* %az
  store i1 %978, i1* %cf
  store i1 %982, i1* %of
  %983 = icmp eq i32 %973, 0
  store i1 %983, i1* %zf
  %984 = icmp slt i32 %973, 0
  store i1 %984, i1* %sf
  %985 = trunc i32 %973 to i8
  %986 = call i8 @llvm.ctpop.i8(i8 %985)
  %987 = and i8 %986, 1
  %988 = icmp eq i8 %987, 0
  store i1 %988, i1* %pf
  %989 = zext i32 %973 to i64
  store i64 %989, i64* %rax
  store volatile i64 32333, i64* @assembly_address
  %990 = load i64* %rax
  %991 = trunc i64 %990 to i32
  %992 = zext i32 %991 to i64
  store i64 %992, i64* %r15
  store volatile i64 32336, i64* @assembly_address
  %993 = load i32* %stack_var_-1540
  %994 = zext i32 %993 to i64
  store i64 %994, i64* %rax
  store volatile i64 32342, i64* @assembly_address
  %995 = load i64* %rax
  %996 = trunc i64 %995 to i32
  %997 = load i64* %r15
  %998 = trunc i64 %997 to i32
  %999 = sub i32 %996, %998
  %1000 = and i32 %996, 15
  %1001 = and i32 %998, 15
  %1002 = sub i32 %1000, %1001
  %1003 = icmp ugt i32 %1002, 15
  %1004 = icmp ult i32 %996, %998
  %1005 = xor i32 %996, %998
  %1006 = xor i32 %996, %999
  %1007 = and i32 %1005, %1006
  %1008 = icmp slt i32 %1007, 0
  store i1 %1003, i1* %az
  store i1 %1004, i1* %cf
  store i1 %1008, i1* %of
  %1009 = icmp eq i32 %999, 0
  store i1 %1009, i1* %zf
  %1010 = icmp slt i32 %999, 0
  store i1 %1010, i1* %sf
  %1011 = trunc i32 %999 to i8
  %1012 = call i8 @llvm.ctpop.i8(i8 %1011)
  %1013 = and i8 %1012, 1
  %1014 = icmp eq i8 %1013, 0
  store i1 %1014, i1* %pf
  %1015 = zext i32 %999 to i64
  store i64 %1015, i64* %rax
  store volatile i64 32345, i64* @assembly_address
  %1016 = load i64* %rax
  %1017 = trunc i64 %1016 to i32
  store i32 %1017, i32* %stack_var_-1544
  store volatile i64 32351, i64* @assembly_address
  %1018 = load i32* %stack_var_-1552
  %1019 = zext i32 %1018 to i64
  store i64 %1019, i64* %rax
  store volatile i64 32357, i64* @assembly_address
  %1020 = load i32* %stack_var_-1544
  %1021 = load i64* %rax
  %1022 = trunc i64 %1021 to i32
  %1023 = sub i32 %1020, %1022
  %1024 = and i32 %1020, 15
  %1025 = and i32 %1022, 15
  %1026 = sub i32 %1024, %1025
  %1027 = icmp ugt i32 %1026, 15
  %1028 = icmp ult i32 %1020, %1022
  %1029 = xor i32 %1020, %1022
  %1030 = xor i32 %1020, %1023
  %1031 = and i32 %1029, %1030
  %1032 = icmp slt i32 %1031, 0
  store i1 %1027, i1* %az
  store i1 %1028, i1* %cf
  store i1 %1032, i1* %of
  %1033 = icmp eq i32 %1023, 0
  store i1 %1033, i1* %zf
  %1034 = icmp slt i32 %1023, 0
  store i1 %1034, i1* %sf
  %1035 = trunc i32 %1023 to i8
  %1036 = call i8 @llvm.ctpop.i8(i8 %1035)
  %1037 = and i8 %1036, 1
  %1038 = icmp eq i8 %1037, 0
  store i1 %1038, i1* %pf
  store volatile i64 32363, i64* @assembly_address
  %1039 = load i1* %cf
  %1040 = load i1* %zf
  %1041 = or i1 %1039, %1040
  br i1 %1041, label %block_7e75, label %block_7e6d

block_7e6d:                                       ; preds = %block_7e3d
  store volatile i64 32365, i64* @assembly_address
  %1042 = load i32* %stack_var_-1552
  %1043 = zext i32 %1042 to i64
  store i64 %1043, i64* %rax
  store volatile i64 32371, i64* @assembly_address
  br label %block_7e7b

block_7e75:                                       ; preds = %block_7e3d
  store volatile i64 32373, i64* @assembly_address
  %1044 = load i32* %stack_var_-1544
  %1045 = zext i32 %1044 to i64
  store i64 %1045, i64* %rax
  br label %block_7e7b

block_7e7b:                                       ; preds = %block_7e75, %block_7e6d
  store volatile i64 32379, i64* @assembly_address
  %1046 = load i64* %rax
  %1047 = trunc i64 %1046 to i32
  store i32 %1047, i32* %stack_var_-1544
  store volatile i64 32385, i64* @assembly_address
  %1048 = load i32* %stack_var_-1628
  %1049 = zext i32 %1048 to i64
  store i64 %1049, i64* %rax
  store volatile i64 32391, i64* @assembly_address
  %1050 = load i64* %rax
  %1051 = trunc i64 %1050 to i32
  %1052 = load i64* %r15
  %1053 = trunc i64 %1052 to i32
  %1054 = sub i32 %1051, %1053
  %1055 = and i32 %1051, 15
  %1056 = and i32 %1053, 15
  %1057 = sub i32 %1055, %1056
  %1058 = icmp ugt i32 %1057, 15
  %1059 = icmp ult i32 %1051, %1053
  %1060 = xor i32 %1051, %1053
  %1061 = xor i32 %1051, %1054
  %1062 = and i32 %1060, %1061
  %1063 = icmp slt i32 %1062, 0
  store i1 %1058, i1* %az
  store i1 %1059, i1* %cf
  store i1 %1063, i1* %of
  %1064 = icmp eq i32 %1054, 0
  store i1 %1064, i1* %zf
  %1065 = icmp slt i32 %1054, 0
  store i1 %1065, i1* %sf
  %1066 = trunc i32 %1054 to i8
  %1067 = call i8 @llvm.ctpop.i8(i8 %1066)
  %1068 = and i8 %1067, 1
  %1069 = icmp eq i8 %1068, 0
  store i1 %1069, i1* %pf
  %1070 = zext i32 %1054 to i64
  store i64 %1070, i64* %rax
  store volatile i64 32394, i64* @assembly_address
  %1071 = load i64* %rax
  %1072 = trunc i64 %1071 to i32
  %1073 = zext i32 %1072 to i64
  store i64 %1073, i64* %rbx
  store volatile i64 32396, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 32401, i64* @assembly_address
  %1074 = load i64* %rbx
  %1075 = trunc i64 %1074 to i32
  %1076 = zext i32 %1075 to i64
  store i64 %1076, i64* %rcx
  store volatile i64 32403, i64* @assembly_address
  %1077 = load i64* %rax
  %1078 = trunc i64 %1077 to i32
  %1079 = load i64* %rcx
  %1080 = trunc i64 %1079 to i8
  %1081 = zext i8 %1080 to i32
  %1082 = and i32 %1081, 31
  %1083 = load i1* %of
  %1084 = icmp eq i32 %1082, 0
  br i1 %1084, label %1102, label %1085

; <label>:1085                                    ; preds = %block_7e7b
  %1086 = shl i32 %1078, %1082
  %1087 = icmp eq i32 %1086, 0
  store i1 %1087, i1* %zf
  %1088 = icmp slt i32 %1086, 0
  store i1 %1088, i1* %sf
  %1089 = trunc i32 %1086 to i8
  %1090 = call i8 @llvm.ctpop.i8(i8 %1089)
  %1091 = and i8 %1090, 1
  %1092 = icmp eq i8 %1091, 0
  store i1 %1092, i1* %pf
  %1093 = zext i32 %1086 to i64
  store i64 %1093, i64* %rax
  %1094 = sub i32 %1082, 1
  %1095 = shl i32 %1078, %1094
  %1096 = lshr i32 %1095, 31
  %1097 = trunc i32 %1096 to i1
  store i1 %1097, i1* %cf
  %1098 = lshr i32 %1086, 31
  %1099 = icmp ne i32 %1098, %1096
  %1100 = icmp eq i32 %1082, 1
  %1101 = select i1 %1100, i1 %1099, i1 %1083
  store i1 %1101, i1* %of
  br label %1102

; <label>:1102                                    ; preds = %block_7e7b, %1085
  store volatile i64 32405, i64* @assembly_address
  %1103 = load i64* %rax
  %1104 = trunc i64 %1103 to i32
  store i32 %1104, i32* %stack_var_-1560
  store volatile i64 32411, i64* @assembly_address
  %1105 = load i32* %stack_var_-1564
  %1106 = zext i32 %1105 to i64
  store i64 %1106, i64* %rax
  store volatile i64 32417, i64* @assembly_address
  %1107 = load i64* %rax
  %1108 = trunc i64 %1107 to i32
  %1109 = add i32 %1108, 1
  %1110 = and i32 %1108, 15
  %1111 = add i32 %1110, 1
  %1112 = icmp ugt i32 %1111, 15
  %1113 = icmp ult i32 %1109, %1108
  %1114 = xor i32 %1108, %1109
  %1115 = xor i32 1, %1109
  %1116 = and i32 %1114, %1115
  %1117 = icmp slt i32 %1116, 0
  store i1 %1112, i1* %az
  store i1 %1113, i1* %cf
  store i1 %1117, i1* %of
  %1118 = icmp eq i32 %1109, 0
  store i1 %1118, i1* %zf
  %1119 = icmp slt i32 %1109, 0
  store i1 %1119, i1* %sf
  %1120 = trunc i32 %1109 to i8
  %1121 = call i8 @llvm.ctpop.i8(i8 %1120)
  %1122 = and i8 %1121, 1
  %1123 = icmp eq i8 %1122, 0
  store i1 %1123, i1* %pf
  %1124 = zext i32 %1109 to i64
  store i64 %1124, i64* %rax
  store volatile i64 32420, i64* @assembly_address
  %1125 = load i32* %stack_var_-1560
  %1126 = load i64* %rax
  %1127 = trunc i64 %1126 to i32
  %1128 = sub i32 %1125, %1127
  %1129 = and i32 %1125, 15
  %1130 = and i32 %1127, 15
  %1131 = sub i32 %1129, %1130
  %1132 = icmp ugt i32 %1131, 15
  %1133 = icmp ult i32 %1125, %1127
  %1134 = xor i32 %1125, %1127
  %1135 = xor i32 %1125, %1128
  %1136 = and i32 %1134, %1135
  %1137 = icmp slt i32 %1136, 0
  store i1 %1132, i1* %az
  store i1 %1133, i1* %cf
  store i1 %1137, i1* %of
  %1138 = icmp eq i32 %1128, 0
  store i1 %1138, i1* %zf
  %1139 = icmp slt i32 %1128, 0
  store i1 %1139, i1* %sf
  %1140 = trunc i32 %1128 to i8
  %1141 = call i8 @llvm.ctpop.i8(i8 %1140)
  %1142 = and i8 %1141, 1
  %1143 = icmp eq i8 %1142, 0
  store i1 %1143, i1* %pf
  store volatile i64 32426, i64* @assembly_address
  %1144 = load i1* %cf
  %1145 = load i1* %zf
  %1146 = or i1 %1144, %1145
  br i1 %1146, label %block_7f27, label %block_7eac

block_7eac:                                       ; preds = %1102
  store volatile i64 32428, i64* @assembly_address
  %1147 = load i32* %stack_var_-1560
  %1148 = zext i32 %1147 to i64
  store i64 %1148, i64* %rax
  store volatile i64 32434, i64* @assembly_address
  %1149 = load i64* %rax
  %1150 = trunc i64 %1149 to i32
  %1151 = load i32* %stack_var_-1564
  %1152 = sub i32 %1150, %1151
  %1153 = and i32 %1150, 15
  %1154 = and i32 %1151, 15
  %1155 = sub i32 %1153, %1154
  %1156 = icmp ugt i32 %1155, 15
  %1157 = icmp ult i32 %1150, %1151
  %1158 = xor i32 %1150, %1151
  %1159 = xor i32 %1150, %1152
  %1160 = and i32 %1158, %1159
  %1161 = icmp slt i32 %1160, 0
  store i1 %1156, i1* %az
  store i1 %1157, i1* %cf
  store i1 %1161, i1* %of
  %1162 = icmp eq i32 %1152, 0
  store i1 %1162, i1* %zf
  %1163 = icmp slt i32 %1152, 0
  store i1 %1163, i1* %sf
  %1164 = trunc i32 %1152 to i8
  %1165 = call i8 @llvm.ctpop.i8(i8 %1164)
  %1166 = and i8 %1165, 1
  %1167 = icmp eq i8 %1166, 0
  store i1 %1167, i1* %pf
  %1168 = zext i32 %1152 to i64
  store i64 %1168, i64* %rax
  store volatile i64 32440, i64* @assembly_address
  %1169 = load i64* %rax
  %1170 = trunc i64 %1169 to i32
  %1171 = sub i32 %1170, 1
  %1172 = and i32 %1170, 15
  %1173 = sub i32 %1172, 1
  %1174 = icmp ugt i32 %1173, 15
  %1175 = icmp ult i32 %1170, 1
  %1176 = xor i32 %1170, 1
  %1177 = xor i32 %1170, %1171
  %1178 = and i32 %1176, %1177
  %1179 = icmp slt i32 %1178, 0
  store i1 %1174, i1* %az
  store i1 %1175, i1* %cf
  store i1 %1179, i1* %of
  %1180 = icmp eq i32 %1171, 0
  store i1 %1180, i1* %zf
  %1181 = icmp slt i32 %1171, 0
  store i1 %1181, i1* %sf
  %1182 = trunc i32 %1171 to i8
  %1183 = call i8 @llvm.ctpop.i8(i8 %1182)
  %1184 = and i8 %1183, 1
  %1185 = icmp eq i8 %1184, 0
  store i1 %1185, i1* %pf
  %1186 = zext i32 %1171 to i64
  store i64 %1186, i64* %rax
  store volatile i64 32443, i64* @assembly_address
  %1187 = load i64* %rax
  %1188 = trunc i64 %1187 to i32
  store i32 %1188, i32* %stack_var_-1560
  store volatile i64 32449, i64* @assembly_address
  %1189 = load i32* %stack_var_-1628
  %1190 = sext i32 %1189 to i64
  store i64 %1190, i64* %rax
  store volatile i64 32456, i64* @assembly_address
  %1191 = load i64* %rax
  %1192 = mul i64 %1191, 4
  store i64 %1192, i64* %rdx
  store volatile i64 32464, i64* @assembly_address
  %1193 = ptrtoint i32* %stack_var_-1512 to i64
  store i64 %1193, i64* %rax
  store volatile i64 32471, i64* @assembly_address
  %1194 = load i64* %rax
  %1195 = load i64* %rdx
  %1196 = add i64 %1194, %1195
  %1197 = and i64 %1194, 15
  %1198 = and i64 %1195, 15
  %1199 = add i64 %1197, %1198
  %1200 = icmp ugt i64 %1199, 15
  %1201 = icmp ult i64 %1196, %1194
  %1202 = xor i64 %1194, %1196
  %1203 = xor i64 %1195, %1196
  %1204 = and i64 %1202, %1203
  %1205 = icmp slt i64 %1204, 0
  store i1 %1200, i1* %az
  store i1 %1201, i1* %cf
  store i1 %1205, i1* %of
  %1206 = icmp eq i64 %1196, 0
  store i1 %1206, i1* %zf
  %1207 = icmp slt i64 %1196, 0
  store i1 %1207, i1* %sf
  %1208 = trunc i64 %1196 to i8
  %1209 = call i8 @llvm.ctpop.i8(i8 %1208)
  %1210 = and i8 %1209, 1
  %1211 = icmp eq i8 %1210, 0
  store i1 %1211, i1* %pf
  store i64 %1196, i64* %rax
  store volatile i64 32474, i64* @assembly_address
  %1212 = load i64* %rax
  %1213 = inttoptr i64 %1212 to i32*
  store i32* %1213, i32** %stack_var_-1536
  store volatile i64 32481, i64* @assembly_address
  %1214 = load i64* %rbx
  %1215 = trunc i64 %1214 to i32
  %1216 = load i32* %stack_var_-1544
  %1217 = sub i32 %1215, %1216
  %1218 = and i32 %1215, 15
  %1219 = and i32 %1216, 15
  %1220 = sub i32 %1218, %1219
  %1221 = icmp ugt i32 %1220, 15
  %1222 = icmp ult i32 %1215, %1216
  %1223 = xor i32 %1215, %1216
  %1224 = xor i32 %1215, %1217
  %1225 = and i32 %1223, %1224
  %1226 = icmp slt i32 %1225, 0
  store i1 %1221, i1* %az
  store i1 %1222, i1* %cf
  store i1 %1226, i1* %of
  %1227 = icmp eq i32 %1217, 0
  store i1 %1227, i1* %zf
  %1228 = icmp slt i32 %1217, 0
  store i1 %1228, i1* %sf
  %1229 = trunc i32 %1217 to i8
  %1230 = call i8 @llvm.ctpop.i8(i8 %1229)
  %1231 = and i8 %1230, 1
  %1232 = icmp eq i8 %1231, 0
  store i1 %1232, i1* %pf
  store volatile i64 32487, i64* @assembly_address
  %1233 = load i1* %cf
  %1234 = icmp eq i1 %1233, false
  br i1 %1234, label %block_7f27, label %block_7ee9

block_7ee9:                                       ; preds = %block_7eac
  store volatile i64 32489, i64* @assembly_address
  br label %block_7f19

block_7eeb:                                       ; preds = %block_7f19
  store volatile i64 32491, i64* @assembly_address
  %1235 = load i32* %stack_var_-1560
  %1236 = load i1* %of
  %1237 = shl i32 %1235, 1
  %1238 = icmp eq i32 %1237, 0
  store i1 %1238, i1* %zf
  %1239 = icmp slt i32 %1237, 0
  store i1 %1239, i1* %sf
  %1240 = trunc i32 %1237 to i8
  %1241 = call i8 @llvm.ctpop.i8(i8 %1240)
  %1242 = and i8 %1241, 1
  %1243 = icmp eq i8 %1242, 0
  store i1 %1243, i1* %pf
  store i32 %1237, i32* %stack_var_-1560
  %1244 = shl i32 %1235, 0
  %1245 = lshr i32 %1244, 31
  %1246 = trunc i32 %1245 to i1
  store i1 %1246, i1* %cf
  %1247 = lshr i32 %1237, 31
  %1248 = icmp ne i32 %1247, %1245
  %1249 = select i1 true, i1 %1248, i1 %1236
  store i1 %1249, i1* %of
  store volatile i64 32497, i64* @assembly_address
  %1250 = load i32** %stack_var_-1536
  %1251 = ptrtoint i32* %1250 to i64
  %1252 = add i64 %1251, 4
  %1253 = and i64 %1251, 15
  %1254 = add i64 %1253, 4
  %1255 = icmp ugt i64 %1254, 15
  %1256 = icmp ult i64 %1252, %1251
  %1257 = xor i64 %1251, %1252
  %1258 = xor i64 4, %1252
  %1259 = and i64 %1257, %1258
  %1260 = icmp slt i64 %1259, 0
  store i1 %1255, i1* %az
  store i1 %1256, i1* %cf
  store i1 %1260, i1* %of
  %1261 = icmp eq i64 %1252, 0
  store i1 %1261, i1* %zf
  %1262 = icmp slt i64 %1252, 0
  store i1 %1262, i1* %sf
  %1263 = trunc i64 %1252 to i8
  %1264 = call i8 @llvm.ctpop.i8(i8 %1263)
  %1265 = and i8 %1264, 1
  %1266 = icmp eq i8 %1265, 0
  store i1 %1266, i1* %pf
  %1267 = inttoptr i64 %1252 to i32*
  store i32* %1267, i32** %stack_var_-1536
  store volatile i64 32505, i64* @assembly_address
  %1268 = load i32** %stack_var_-1536
  %1269 = ptrtoint i32* %1268 to i64
  store i64 %1269, i64* %rax
  store volatile i64 32512, i64* @assembly_address
  %1270 = load i64* %rax
  %1271 = inttoptr i64 %1270 to i32*
  %1272 = load i32* %1271
  %1273 = zext i32 %1272 to i64
  store i64 %1273, i64* %rax
  store volatile i64 32514, i64* @assembly_address
  %1274 = load i32* %stack_var_-1560
  %1275 = load i64* %rax
  %1276 = trunc i64 %1275 to i32
  %1277 = sub i32 %1274, %1276
  %1278 = and i32 %1274, 15
  %1279 = and i32 %1276, 15
  %1280 = sub i32 %1278, %1279
  %1281 = icmp ugt i32 %1280, 15
  %1282 = icmp ult i32 %1274, %1276
  %1283 = xor i32 %1274, %1276
  %1284 = xor i32 %1274, %1277
  %1285 = and i32 %1283, %1284
  %1286 = icmp slt i32 %1285, 0
  store i1 %1281, i1* %az
  store i1 %1282, i1* %cf
  store i1 %1286, i1* %of
  %1287 = icmp eq i32 %1277, 0
  store i1 %1287, i1* %zf
  %1288 = icmp slt i32 %1277, 0
  store i1 %1288, i1* %sf
  %1289 = trunc i32 %1277 to i8
  %1290 = call i8 @llvm.ctpop.i8(i8 %1289)
  %1291 = and i8 %1290, 1
  %1292 = icmp eq i8 %1291, 0
  store i1 %1292, i1* %pf
  store volatile i64 32520, i64* @assembly_address
  %1293 = load i1* %cf
  %1294 = load i1* %zf
  %1295 = or i1 %1293, %1294
  br i1 %1295, label %block_7f26, label %block_7f0a

block_7f0a:                                       ; preds = %block_7eeb
  store volatile i64 32522, i64* @assembly_address
  %1296 = load i32** %stack_var_-1536
  %1297 = ptrtoint i32* %1296 to i64
  store i64 %1297, i64* %rax
  store volatile i64 32529, i64* @assembly_address
  %1298 = load i64* %rax
  %1299 = inttoptr i64 %1298 to i32*
  %1300 = load i32* %1299
  %1301 = zext i32 %1300 to i64
  store i64 %1301, i64* %rax
  store volatile i64 32531, i64* @assembly_address
  %1302 = load i32* %stack_var_-1560
  %1303 = load i64* %rax
  %1304 = trunc i64 %1303 to i32
  %1305 = sub i32 %1302, %1304
  %1306 = and i32 %1302, 15
  %1307 = and i32 %1304, 15
  %1308 = sub i32 %1306, %1307
  %1309 = icmp ugt i32 %1308, 15
  %1310 = icmp ult i32 %1302, %1304
  %1311 = xor i32 %1302, %1304
  %1312 = xor i32 %1302, %1305
  %1313 = and i32 %1311, %1312
  %1314 = icmp slt i32 %1313, 0
  store i1 %1309, i1* %az
  store i1 %1310, i1* %cf
  store i1 %1314, i1* %of
  %1315 = icmp eq i32 %1305, 0
  store i1 %1315, i1* %zf
  %1316 = icmp slt i32 %1305, 0
  store i1 %1316, i1* %sf
  %1317 = trunc i32 %1305 to i8
  %1318 = call i8 @llvm.ctpop.i8(i8 %1317)
  %1319 = and i8 %1318, 1
  %1320 = icmp eq i8 %1319, 0
  store i1 %1320, i1* %pf
  store i32 %1305, i32* %stack_var_-1560
  br label %block_7f19

block_7f19:                                       ; preds = %block_7f0a, %block_7ee9
  store volatile i64 32537, i64* @assembly_address
  %1321 = load i64* %rbx
  %1322 = trunc i64 %1321 to i32
  %1323 = add i32 %1322, 1
  %1324 = and i32 %1322, 15
  %1325 = add i32 %1324, 1
  %1326 = icmp ugt i32 %1325, 15
  %1327 = icmp ult i32 %1323, %1322
  %1328 = xor i32 %1322, %1323
  %1329 = xor i32 1, %1323
  %1330 = and i32 %1328, %1329
  %1331 = icmp slt i32 %1330, 0
  store i1 %1326, i1* %az
  store i1 %1327, i1* %cf
  store i1 %1331, i1* %of
  %1332 = icmp eq i32 %1323, 0
  store i1 %1332, i1* %zf
  %1333 = icmp slt i32 %1323, 0
  store i1 %1333, i1* %sf
  %1334 = trunc i32 %1323 to i8
  %1335 = call i8 @llvm.ctpop.i8(i8 %1334)
  %1336 = and i8 %1335, 1
  %1337 = icmp eq i8 %1336, 0
  store i1 %1337, i1* %pf
  %1338 = zext i32 %1323 to i64
  store i64 %1338, i64* %rbx
  store volatile i64 32540, i64* @assembly_address
  %1339 = load i64* %rbx
  %1340 = trunc i64 %1339 to i32
  %1341 = load i32* %stack_var_-1544
  %1342 = sub i32 %1340, %1341
  %1343 = and i32 %1340, 15
  %1344 = and i32 %1341, 15
  %1345 = sub i32 %1343, %1344
  %1346 = icmp ugt i32 %1345, 15
  %1347 = icmp ult i32 %1340, %1341
  %1348 = xor i32 %1340, %1341
  %1349 = xor i32 %1340, %1342
  %1350 = and i32 %1348, %1349
  %1351 = icmp slt i32 %1350, 0
  store i1 %1346, i1* %az
  store i1 %1347, i1* %cf
  store i1 %1351, i1* %of
  %1352 = icmp eq i32 %1342, 0
  store i1 %1352, i1* %zf
  %1353 = icmp slt i32 %1342, 0
  store i1 %1353, i1* %sf
  %1354 = trunc i32 %1342 to i8
  %1355 = call i8 @llvm.ctpop.i8(i8 %1354)
  %1356 = and i8 %1355, 1
  %1357 = icmp eq i8 %1356, 0
  store i1 %1357, i1* %pf
  store volatile i64 32546, i64* @assembly_address
  %1358 = load i1* %cf
  br i1 %1358, label %block_7eeb, label %block_7f24

block_7f24:                                       ; preds = %block_7f19
  store volatile i64 32548, i64* @assembly_address
  br label %block_7f27

block_7f26:                                       ; preds = %block_7eeb
  store volatile i64 32550, i64* @assembly_address
  br label %block_7f27

block_7f27:                                       ; preds = %block_7f26, %block_7f24, %block_7eac, %1102
  store volatile i64 32551, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 32556, i64* @assembly_address
  %1359 = load i64* %rbx
  %1360 = trunc i64 %1359 to i32
  %1361 = zext i32 %1360 to i64
  store i64 %1361, i64* %rcx
  store volatile i64 32558, i64* @assembly_address
  %1362 = load i64* %rax
  %1363 = trunc i64 %1362 to i32
  %1364 = load i64* %rcx
  %1365 = trunc i64 %1364 to i8
  %1366 = zext i8 %1365 to i32
  %1367 = and i32 %1366, 31
  %1368 = load i1* %of
  %1369 = icmp eq i32 %1367, 0
  br i1 %1369, label %1387, label %1370

; <label>:1370                                    ; preds = %block_7f27
  %1371 = shl i32 %1363, %1367
  %1372 = icmp eq i32 %1371, 0
  store i1 %1372, i1* %zf
  %1373 = icmp slt i32 %1371, 0
  store i1 %1373, i1* %sf
  %1374 = trunc i32 %1371 to i8
  %1375 = call i8 @llvm.ctpop.i8(i8 %1374)
  %1376 = and i8 %1375, 1
  %1377 = icmp eq i8 %1376, 0
  store i1 %1377, i1* %pf
  %1378 = zext i32 %1371 to i64
  store i64 %1378, i64* %rax
  %1379 = sub i32 %1367, 1
  %1380 = shl i32 %1363, %1379
  %1381 = lshr i32 %1380, 31
  %1382 = trunc i32 %1381 to i1
  store i1 %1382, i1* %cf
  %1383 = lshr i32 %1371, 31
  %1384 = icmp ne i32 %1383, %1381
  %1385 = icmp eq i32 %1367, 1
  %1386 = select i1 %1385, i1 %1384, i1 %1368
  store i1 %1386, i1* %of
  br label %1387

; <label>:1387                                    ; preds = %block_7f27, %1370
  store volatile i64 32560, i64* @assembly_address
  %1388 = load i64* %rax
  %1389 = trunc i64 %1388 to i32
  store i32 %1389, i32* %stack_var_-1544
  store volatile i64 32566, i64* @assembly_address
  %1390 = load i32* %stack_var_-1544
  %1391 = zext i32 %1390 to i64
  store i64 %1391, i64* %rax
  store volatile i64 32572, i64* @assembly_address
  %1392 = load i64* %rax
  %1393 = trunc i64 %1392 to i32
  %1394 = add i32 %1393, 1
  %1395 = and i32 %1393, 15
  %1396 = add i32 %1395, 1
  %1397 = icmp ugt i32 %1396, 15
  %1398 = icmp ult i32 %1394, %1393
  %1399 = xor i32 %1393, %1394
  %1400 = xor i32 1, %1394
  %1401 = and i32 %1399, %1400
  %1402 = icmp slt i32 %1401, 0
  store i1 %1397, i1* %az
  store i1 %1398, i1* %cf
  store i1 %1402, i1* %of
  %1403 = icmp eq i32 %1394, 0
  store i1 %1403, i1* %zf
  %1404 = icmp slt i32 %1394, 0
  store i1 %1404, i1* %sf
  %1405 = trunc i32 %1394 to i8
  %1406 = call i8 @llvm.ctpop.i8(i8 %1405)
  %1407 = and i8 %1406, 1
  %1408 = icmp eq i8 %1407, 0
  store i1 %1408, i1* %pf
  %1409 = zext i32 %1394 to i64
  store i64 %1409, i64* %rax
  store volatile i64 32575, i64* @assembly_address
  %1410 = load i64* %rax
  %1411 = trunc i64 %1410 to i32
  %1412 = zext i32 %1411 to i64
  store i64 %1412, i64* %rax
  store volatile i64 32577, i64* @assembly_address
  %1413 = load i64* %rax
  %1414 = load i1* %of
  %1415 = shl i64 %1413, 4
  %1416 = icmp eq i64 %1415, 0
  store i1 %1416, i1* %zf
  %1417 = icmp slt i64 %1415, 0
  store i1 %1417, i1* %sf
  %1418 = trunc i64 %1415 to i8
  %1419 = call i8 @llvm.ctpop.i8(i8 %1418)
  %1420 = and i8 %1419, 1
  %1421 = icmp eq i8 %1420, 0
  store i1 %1421, i1* %pf
  store i64 %1415, i64* %rax
  %1422 = shl i64 %1413, 3
  %1423 = lshr i64 %1422, 63
  %1424 = trunc i64 %1423 to i1
  store i1 %1424, i1* %cf
  %1425 = lshr i64 %1415, 63
  %1426 = icmp ne i64 %1425, %1423
  %1427 = select i1 false, i1 %1426, i1 %1414
  store i1 %1427, i1* %of
  store volatile i64 32581, i64* @assembly_address
  %1428 = load i64* %rax
  store i64 %1428, i64* %rdi
  store volatile i64 32584, i64* @assembly_address
  %1429 = load i64* %rdi
  %1430 = trunc i64 %1429 to i32
  %1431 = call i64* @malloc(i32 %1430)
  %1432 = ptrtoint i64* %1431 to i64
  store i64 %1432, i64* %rax
  %1433 = ptrtoint i64* %1431 to i64
  store i64 %1433, i64* %rax
  store volatile i64 32589, i64* @assembly_address
  %1434 = load i64* %rax
  store i64 %1434, i64* %r14
  store volatile i64 32592, i64* @assembly_address
  %1435 = load i64* %r14
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1436 = icmp eq i64 %1435, 0
  store i1 %1436, i1* %zf
  %1437 = icmp slt i64 %1435, 0
  store i1 %1437, i1* %sf
  %1438 = trunc i64 %1435 to i8
  %1439 = call i8 @llvm.ctpop.i8(i8 %1438)
  %1440 = and i8 %1439, 1
  %1441 = icmp eq i8 %1440, 0
  store i1 %1441, i1* %pf
  store volatile i64 32595, i64* @assembly_address
  %1442 = load i1* %zf
  %1443 = icmp eq i1 %1442, false
  br i1 %1443, label %block_7f77, label %block_7f55

block_7f55:                                       ; preds = %1387
  store volatile i64 32597, i64* @assembly_address
  %1444 = load i32* %stack_var_-1556
  %1445 = and i32 %1444, 15
  %1446 = icmp ugt i32 %1445, 15
  %1447 = icmp ult i32 %1444, 0
  %1448 = xor i32 %1444, 0
  %1449 = and i32 %1448, 0
  %1450 = icmp slt i32 %1449, 0
  store i1 %1446, i1* %az
  store i1 %1447, i1* %cf
  store i1 %1450, i1* %of
  %1451 = icmp eq i32 %1444, 0
  store i1 %1451, i1* %zf
  %1452 = icmp slt i32 %1444, 0
  store i1 %1452, i1* %sf
  %1453 = trunc i32 %1444 to i8
  %1454 = call i8 @llvm.ctpop.i8(i8 %1453)
  %1455 = and i8 %1454, 1
  %1456 = icmp eq i8 %1455, 0
  store i1 %1456, i1* %pf
  store volatile i64 32604, i64* @assembly_address
  %1457 = load i1* %zf
  br i1 %1457, label %block_7f6d, label %block_7f5e

block_7f5e:                                       ; preds = %block_7f55
  store volatile i64 32606, i64* @assembly_address
  %1458 = load i64* %stack_var_-1352
  store i64 %1458, i64* %rax
  store volatile i64 32613, i64* @assembly_address
  %1459 = load i64* %rax
  store i64 %1459, i64* %rdi
  store volatile i64 32616, i64* @assembly_address
  %1460 = load i64* %rdi
  %1461 = inttoptr i64 %1460 to i64*
  %1462 = call i64 @huft_free(i64* %1461)
  store i64 %1462, i64* %rax
  store i64 %1462, i64* %rax
  br label %block_7f6d

block_7f6d:                                       ; preds = %block_7f5e, %block_7f55
  store volatile i64 32621, i64* @assembly_address
  store i64 3, i64* %rax
  store volatile i64 32626, i64* @assembly_address
  br label %block_821b

block_7f77:                                       ; preds = %1387
  store volatile i64 32631, i64* @assembly_address
  %1463 = load i32* bitcast (i64* @global_var_216fa4 to i32*)
  %1464 = zext i32 %1463 to i64
  store i64 %1464, i64* %rdx
  store volatile i64 32637, i64* @assembly_address
  %1465 = load i32* %stack_var_-1544
  %1466 = zext i32 %1465 to i64
  store i64 %1466, i64* %rax
  store volatile i64 32643, i64* @assembly_address
  %1467 = load i64* %rax
  %1468 = trunc i64 %1467 to i32
  %1469 = load i64* %rdx
  %1470 = trunc i64 %1469 to i32
  %1471 = add i32 %1468, %1470
  %1472 = and i32 %1468, 15
  %1473 = and i32 %1470, 15
  %1474 = add i32 %1472, %1473
  %1475 = icmp ugt i32 %1474, 15
  %1476 = icmp ult i32 %1471, %1468
  %1477 = xor i32 %1468, %1471
  %1478 = xor i32 %1470, %1471
  %1479 = and i32 %1477, %1478
  %1480 = icmp slt i32 %1479, 0
  store i1 %1475, i1* %az
  store i1 %1476, i1* %cf
  store i1 %1480, i1* %of
  %1481 = icmp eq i32 %1471, 0
  store i1 %1481, i1* %zf
  %1482 = icmp slt i32 %1471, 0
  store i1 %1482, i1* %sf
  %1483 = trunc i32 %1471 to i8
  %1484 = call i8 @llvm.ctpop.i8(i8 %1483)
  %1485 = and i8 %1484, 1
  %1486 = icmp eq i8 %1485, 0
  store i1 %1486, i1* %pf
  %1487 = zext i32 %1471 to i64
  store i64 %1487, i64* %rax
  store volatile i64 32645, i64* @assembly_address
  %1488 = load i64* %rax
  %1489 = trunc i64 %1488 to i32
  %1490 = add i32 %1489, 1
  %1491 = and i32 %1489, 15
  %1492 = add i32 %1491, 1
  %1493 = icmp ugt i32 %1492, 15
  %1494 = icmp ult i32 %1490, %1489
  %1495 = xor i32 %1489, %1490
  %1496 = xor i32 1, %1490
  %1497 = and i32 %1495, %1496
  %1498 = icmp slt i32 %1497, 0
  store i1 %1493, i1* %az
  store i1 %1494, i1* %cf
  store i1 %1498, i1* %of
  %1499 = icmp eq i32 %1490, 0
  store i1 %1499, i1* %zf
  %1500 = icmp slt i32 %1490, 0
  store i1 %1500, i1* %sf
  %1501 = trunc i32 %1490 to i8
  %1502 = call i8 @llvm.ctpop.i8(i8 %1501)
  %1503 = and i8 %1502, 1
  %1504 = icmp eq i8 %1503, 0
  store i1 %1504, i1* %pf
  %1505 = zext i32 %1490 to i64
  store i64 %1505, i64* %rax
  store volatile i64 32648, i64* @assembly_address
  %1506 = load i64* %rax
  %1507 = trunc i64 %1506 to i32
  store i32 %1507, i32* bitcast (i64* @global_var_216fa4 to i32*)
  store volatile i64 32654, i64* @assembly_address
  %1508 = load i64* %r14
  %1509 = add i64 %1508, 16
  store i64 %1509, i64* %rdx
  store volatile i64 32658, i64* @assembly_address
  %1510 = load i64* %stack_var_-1616
  store i64 %1510, i64* %rax
  store volatile i64 32665, i64* @assembly_address
  %1511 = load i64* %rdx
  %1512 = load i64* %rax
  %1513 = inttoptr i64 %1512 to i64*
  store i64 %1511, i64* %1513
  store volatile i64 32668, i64* @assembly_address
  %1514 = load i64* %r14
  %1515 = add i64 %1514, 8
  store i64 %1515, i64* %rax
  store volatile i64 32672, i64* @assembly_address
  %1516 = load i64* %rax
  store i64 %1516, i64* %stack_var_-1616
  store volatile i64 32679, i64* @assembly_address
  %1517 = load i64* %stack_var_-1616
  store i64 %1517, i64* %rax
  store volatile i64 32686, i64* @assembly_address
  %1518 = load i64* %rax
  %1519 = inttoptr i64 %1518 to i64*
  store i64 0, i64* %1519
  store volatile i64 32693, i64* @assembly_address
  %1520 = load i64* %r14
  %1521 = add i64 %1520, 16
  %1522 = and i64 %1520, 15
  %1523 = icmp ugt i64 %1522, 15
  %1524 = icmp ult i64 %1521, %1520
  %1525 = xor i64 %1520, %1521
  %1526 = xor i64 16, %1521
  %1527 = and i64 %1525, %1526
  %1528 = icmp slt i64 %1527, 0
  store i1 %1523, i1* %az
  store i1 %1524, i1* %cf
  store i1 %1528, i1* %of
  %1529 = icmp eq i64 %1521, 0
  store i1 %1529, i1* %zf
  %1530 = icmp slt i64 %1521, 0
  store i1 %1530, i1* %sf
  %1531 = trunc i64 %1521 to i8
  %1532 = call i8 @llvm.ctpop.i8(i8 %1531)
  %1533 = and i8 %1532, 1
  %1534 = icmp eq i8 %1533, 0
  store i1 %1534, i1* %pf
  store i64 %1521, i64* %r14
  store volatile i64 32697, i64* @assembly_address
  %1535 = load i32* %stack_var_-1556
  %1536 = zext i32 %1535 to i64
  store i64 %1536, i64* %rax
  store volatile i64 32703, i64* @assembly_address
  %1537 = load i64* %rax
  %1538 = trunc i64 %1537 to i32
  %1539 = sext i32 %1538 to i64
  store i64 %1539, i64* %rax
  store volatile i64 32705, i64* @assembly_address
  %1540 = load i64* %r14
  %1541 = load i64* %rbp
  %1542 = load i64* %rax
  %1543 = mul i64 %1542, 8
  %1544 = add i64 %1541, -1344
  %1545 = add i64 %1544, %1543
  %1546 = inttoptr i64 %1545 to i64*
  store i64 %1540, i64* %1546
  store volatile i64 32713, i64* @assembly_address
  %1547 = load i32* %stack_var_-1556
  %1548 = and i32 %1547, 15
  %1549 = icmp ugt i32 %1548, 15
  %1550 = icmp ult i32 %1547, 0
  %1551 = xor i32 %1547, 0
  %1552 = and i32 %1551, 0
  %1553 = icmp slt i32 %1552, 0
  store i1 %1549, i1* %az
  store i1 %1550, i1* %cf
  store i1 %1553, i1* %of
  %1554 = icmp eq i32 %1547, 0
  store i1 %1554, i1* %zf
  %1555 = icmp slt i32 %1547, 0
  store i1 %1555, i1* %sf
  %1556 = trunc i32 %1547 to i8
  %1557 = call i8 @llvm.ctpop.i8(i8 %1556)
  %1558 = and i8 %1557, 1
  %1559 = icmp eq i8 %1558, 0
  store i1 %1559, i1* %pf
  store volatile i64 32720, i64* @assembly_address
  %1560 = load i1* %zf
  br i1 %1560, label %block_8042, label %block_7fd2

block_7fd2:                                       ; preds = %block_7f77
  store volatile i64 32722, i64* @assembly_address
  %1561 = load i32* %stack_var_-1556
  %1562 = zext i32 %1561 to i64
  store i64 %1562, i64* %rax
  store volatile i64 32728, i64* @assembly_address
  %1563 = load i64* %rax
  %1564 = trunc i64 %1563 to i32
  %1565 = sext i32 %1564 to i64
  store i64 %1565, i64* %rax
  store volatile i64 32730, i64* @assembly_address
  %1566 = load i64* %r12
  %1567 = trunc i64 %1566 to i32
  %1568 = load i64* %rbp
  %1569 = load i64* %rax
  %1570 = mul i64 %1569, 4
  %1571 = add i64 %1568, -1424
  %1572 = add i64 %1571, %1570
  %1573 = inttoptr i64 %1572 to i32*
  store i32 %1567, i32* %1573
  store volatile i64 32738, i64* @assembly_address
  %1574 = load i32* %stack_var_-1552
  %1575 = zext i32 %1574 to i64
  store i64 %1575, i64* %rax
  store volatile i64 32744, i64* @assembly_address
  %1576 = load i64* %rax
  %1577 = trunc i64 %1576 to i8
  store i8 %1577, i8* %stack_var_-1527
  store volatile i64 32750, i64* @assembly_address
  %1578 = load i64* %rbx
  %1579 = trunc i64 %1578 to i32
  %1580 = zext i32 %1579 to i64
  store i64 %1580, i64* %rax
  store volatile i64 32752, i64* @assembly_address
  %1581 = load i64* %rax
  %1582 = trunc i64 %1581 to i32
  %1583 = add i32 %1582, 16
  %1584 = and i32 %1582, 15
  %1585 = icmp ugt i32 %1584, 15
  %1586 = icmp ult i32 %1583, %1582
  %1587 = xor i32 %1582, %1583
  %1588 = xor i32 16, %1583
  %1589 = and i32 %1587, %1588
  %1590 = icmp slt i32 %1589, 0
  store i1 %1585, i1* %az
  store i1 %1586, i1* %cf
  store i1 %1590, i1* %of
  %1591 = icmp eq i32 %1583, 0
  store i1 %1591, i1* %zf
  %1592 = icmp slt i32 %1583, 0
  store i1 %1592, i1* %sf
  %1593 = trunc i32 %1583 to i8
  %1594 = call i8 @llvm.ctpop.i8(i8 %1593)
  %1595 = and i8 %1594, 1
  %1596 = icmp eq i8 %1595, 0
  store i1 %1596, i1* %pf
  %1597 = zext i32 %1583 to i64
  store i64 %1597, i64* %rax
  store volatile i64 32755, i64* @assembly_address
  %1598 = load i64* %rax
  %1599 = trunc i64 %1598 to i8
  store i8 %1599, i8* %stack_var_-1528
  store volatile i64 32761, i64* @assembly_address
  %1600 = load i64* %r14
  store i64 %1600, i64* %stack_var_-1520
  store volatile i64 32768, i64* @assembly_address
  %1601 = load i64* %r15
  %1602 = trunc i64 %1601 to i32
  %1603 = zext i32 %1602 to i64
  store i64 %1603, i64* %rax
  store volatile i64 32771, i64* @assembly_address
  %1604 = load i64* %rax
  %1605 = trunc i64 %1604 to i32
  %1606 = load i32* %stack_var_-1552
  %1607 = sub i32 %1605, %1606
  %1608 = and i32 %1605, 15
  %1609 = and i32 %1606, 15
  %1610 = sub i32 %1608, %1609
  %1611 = icmp ugt i32 %1610, 15
  %1612 = icmp ult i32 %1605, %1606
  %1613 = xor i32 %1605, %1606
  %1614 = xor i32 %1605, %1607
  %1615 = and i32 %1613, %1614
  %1616 = icmp slt i32 %1615, 0
  store i1 %1611, i1* %az
  store i1 %1612, i1* %cf
  store i1 %1616, i1* %of
  %1617 = icmp eq i32 %1607, 0
  store i1 %1617, i1* %zf
  %1618 = icmp slt i32 %1607, 0
  store i1 %1618, i1* %sf
  %1619 = trunc i32 %1607 to i8
  %1620 = call i8 @llvm.ctpop.i8(i8 %1619)
  %1621 = and i8 %1620, 1
  %1622 = icmp eq i8 %1621, 0
  store i1 %1622, i1* %pf
  %1623 = zext i32 %1607 to i64
  store i64 %1623, i64* %rax
  store volatile i64 32777, i64* @assembly_address
  %1624 = load i64* %r12
  %1625 = trunc i64 %1624 to i32
  %1626 = zext i32 %1625 to i64
  store i64 %1626, i64* %rbx
  store volatile i64 32780, i64* @assembly_address
  %1627 = load i64* %rax
  %1628 = trunc i64 %1627 to i32
  %1629 = zext i32 %1628 to i64
  store i64 %1629, i64* %rcx
  store volatile i64 32782, i64* @assembly_address
  %1630 = load i64* %rbx
  %1631 = trunc i64 %1630 to i32
  %1632 = load i64* %rcx
  %1633 = trunc i64 %1632 to i8
  %1634 = zext i8 %1633 to i32
  %1635 = and i32 %1634, 31
  %1636 = load i1* %of
  %1637 = icmp eq i32 %1635, 0
  br i1 %1637, label %1654, label %1638

; <label>:1638                                    ; preds = %block_7fd2
  %1639 = lshr i32 %1631, %1635
  %1640 = icmp eq i32 %1639, 0
  store i1 %1640, i1* %zf
  %1641 = icmp slt i32 %1639, 0
  store i1 %1641, i1* %sf
  %1642 = trunc i32 %1639 to i8
  %1643 = call i8 @llvm.ctpop.i8(i8 %1642)
  %1644 = and i8 %1643, 1
  %1645 = icmp eq i8 %1644, 0
  store i1 %1645, i1* %pf
  %1646 = zext i32 %1639 to i64
  store i64 %1646, i64* %rbx
  %1647 = sub i32 %1635, 1
  %1648 = shl i32 1, %1647
  %1649 = and i32 %1648, %1631
  %1650 = icmp ne i32 %1649, 0
  store i1 %1650, i1* %cf
  %1651 = icmp eq i32 %1635, 1
  %1652 = icmp slt i32 %1631, 0
  %1653 = select i1 %1651, i1 %1652, i1 %1636
  store i1 %1653, i1* %of
  br label %1654

; <label>:1654                                    ; preds = %block_7fd2, %1638
  store volatile i64 32784, i64* @assembly_address
  %1655 = load i32* %stack_var_-1556
  %1656 = zext i32 %1655 to i64
  store i64 %1656, i64* %rax
  store volatile i64 32790, i64* @assembly_address
  %1657 = load i64* %rax
  %1658 = trunc i64 %1657 to i32
  %1659 = sub i32 %1658, 1
  %1660 = and i32 %1658, 15
  %1661 = sub i32 %1660, 1
  %1662 = icmp ugt i32 %1661, 15
  %1663 = icmp ult i32 %1658, 1
  %1664 = xor i32 %1658, 1
  %1665 = xor i32 %1658, %1659
  %1666 = and i32 %1664, %1665
  %1667 = icmp slt i32 %1666, 0
  store i1 %1662, i1* %az
  store i1 %1663, i1* %cf
  store i1 %1667, i1* %of
  %1668 = icmp eq i32 %1659, 0
  store i1 %1668, i1* %zf
  %1669 = icmp slt i32 %1659, 0
  store i1 %1669, i1* %sf
  %1670 = trunc i32 %1659 to i8
  %1671 = call i8 @llvm.ctpop.i8(i8 %1670)
  %1672 = and i8 %1671, 1
  %1673 = icmp eq i8 %1672, 0
  store i1 %1673, i1* %pf
  %1674 = zext i32 %1659 to i64
  store i64 %1674, i64* %rax
  store volatile i64 32793, i64* @assembly_address
  %1675 = load i64* %rax
  %1676 = trunc i64 %1675 to i32
  %1677 = sext i32 %1676 to i64
  store i64 %1677, i64* %rax
  store volatile i64 32795, i64* @assembly_address
  %1678 = load i64* %rbp
  %1679 = load i64* %rax
  %1680 = mul i64 %1679, 8
  %1681 = add i64 %1678, -1344
  %1682 = add i64 %1681, %1680
  %1683 = inttoptr i64 %1682 to i64*
  %1684 = load i64* %1683
  store i64 %1684, i64* %rax
  store volatile i64 32803, i64* @assembly_address
  %1685 = load i64* %rbx
  %1686 = trunc i64 %1685 to i32
  %1687 = zext i32 %1686 to i64
  store i64 %1687, i64* %rdx
  store volatile i64 32805, i64* @assembly_address
  %1688 = load i64* %rdx
  %1689 = load i1* %of
  %1690 = shl i64 %1688, 4
  %1691 = icmp eq i64 %1690, 0
  store i1 %1691, i1* %zf
  %1692 = icmp slt i64 %1690, 0
  store i1 %1692, i1* %sf
  %1693 = trunc i64 %1690 to i8
  %1694 = call i8 @llvm.ctpop.i8(i8 %1693)
  %1695 = and i8 %1694, 1
  %1696 = icmp eq i8 %1695, 0
  store i1 %1696, i1* %pf
  store i64 %1690, i64* %rdx
  %1697 = shl i64 %1688, 3
  %1698 = lshr i64 %1697, 63
  %1699 = trunc i64 %1698 to i1
  store i1 %1699, i1* %cf
  %1700 = lshr i64 %1690, 63
  %1701 = icmp ne i64 %1700, %1698
  %1702 = select i1 false, i1 %1701, i1 %1689
  store i1 %1702, i1* %of
  store volatile i64 32809, i64* @assembly_address
  %1703 = load i64* %rax
  %1704 = load i64* %rdx
  %1705 = mul i64 %1704, 1
  %1706 = add i64 %1703, %1705
  store i64 %1706, i64* %rcx
  store volatile i64 32813, i64* @assembly_address
  %1707 = load i8* %stack_var_-1528
  %1708 = sext i8 %1707 to i64
  store i64 %1708, i64* %rax
  store volatile i64 32820, i64* @assembly_address
  %1709 = load i64* %stack_var_-1520
  store i64 %1709, i64* %rdx
  store volatile i64 32827, i64* @assembly_address
  %1710 = load i64* %rax
  %1711 = load i64* %rcx
  %1712 = inttoptr i64 %1711 to i64*
  store i64 %1710, i64* %1712
  store volatile i64 32830, i64* @assembly_address
  %1713 = load i64* %rdx
  %1714 = load i64* %rcx
  %1715 = add i64 %1714, 8
  %1716 = inttoptr i64 %1715 to i64*
  store i64 %1713, i64* %1716
  br label %block_8042

block_8042:                                       ; preds = %1654, %block_7f77, %block_81cd
  store volatile i64 32834, i64* @assembly_address
  %1717 = load i32* %stack_var_-1552
  %1718 = zext i32 %1717 to i64
  store i64 %1718, i64* %rax
  store volatile i64 32840, i64* @assembly_address
  %1719 = load i64* %rax
  %1720 = trunc i64 %1719 to i32
  %1721 = load i64* %r15
  %1722 = trunc i64 %1721 to i32
  %1723 = add i32 %1720, %1722
  %1724 = and i32 %1720, 15
  %1725 = and i32 %1722, 15
  %1726 = add i32 %1724, %1725
  %1727 = icmp ugt i32 %1726, 15
  %1728 = icmp ult i32 %1723, %1720
  %1729 = xor i32 %1720, %1723
  %1730 = xor i32 %1722, %1723
  %1731 = and i32 %1729, %1730
  %1732 = icmp slt i32 %1731, 0
  store i1 %1727, i1* %az
  store i1 %1728, i1* %cf
  store i1 %1732, i1* %of
  %1733 = icmp eq i32 %1723, 0
  store i1 %1733, i1* %zf
  %1734 = icmp slt i32 %1723, 0
  store i1 %1734, i1* %sf
  %1735 = trunc i32 %1723 to i8
  %1736 = call i8 @llvm.ctpop.i8(i8 %1735)
  %1737 = and i8 %1736, 1
  %1738 = icmp eq i8 %1737, 0
  store i1 %1738, i1* %pf
  %1739 = zext i32 %1723 to i64
  store i64 %1739, i64* %rax
  store volatile i64 32843, i64* @assembly_address
  %1740 = load i32* %stack_var_-1628
  %1741 = load i64* %rax
  %1742 = trunc i64 %1741 to i32
  store i32 %1740, i32* %17
  %1743 = trunc i64 %1741 to i32
  store i32 %1743, i32* %15
  %1744 = sub i32 %1740, %1742
  %1745 = and i32 %1740, 15
  %1746 = and i32 %1742, 15
  %1747 = sub i32 %1745, %1746
  %1748 = icmp ugt i32 %1747, 15
  %1749 = icmp ult i32 %1740, %1742
  %1750 = xor i32 %1740, %1742
  %1751 = xor i32 %1740, %1744
  %1752 = and i32 %1750, %1751
  %1753 = icmp slt i32 %1752, 0
  store i1 %1748, i1* %az
  store i1 %1749, i1* %cf
  store i1 %1753, i1* %of
  %1754 = icmp eq i32 %1744, 0
  store i1 %1754, i1* %zf
  %1755 = icmp slt i32 %1744, 0
  store i1 %1755, i1* %sf
  %1756 = trunc i32 %1744 to i8
  %1757 = call i8 @llvm.ctpop.i8(i8 %1756)
  %1758 = and i8 %1757, 1
  %1759 = icmp eq i8 %1758, 0
  store i1 %1759, i1* %pf
  store volatile i64 32849, i64* @assembly_address
  %1760 = load i32* %17
  %1761 = load i32* %15
  %1762 = sext i32 %1761 to i64
  %1763 = sext i32 %1760 to i64
  %1764 = icmp sgt i64 %1763, %1762
  br i1 %1764, label %block_7e3d, label %block_8057

block_8057:                                       ; preds = %block_8042
  store volatile i64 32855, i64* @assembly_address
  %1765 = load i32* %stack_var_-1628
  %1766 = trunc i32 %1765 to i8
  %1767 = zext i8 %1766 to i64
  store i64 %1767, i64* %rdx
  store volatile i64 32862, i64* @assembly_address
  %1768 = load i64* %r15
  %1769 = trunc i64 %1768 to i32
  %1770 = zext i32 %1769 to i64
  store i64 %1770, i64* %rax
  store volatile i64 32865, i64* @assembly_address
  %1771 = load i64* %rdx
  %1772 = trunc i64 %1771 to i32
  %1773 = load i64* %rax
  %1774 = trunc i64 %1773 to i32
  %1775 = sub i32 %1772, %1774
  %1776 = and i32 %1772, 15
  %1777 = and i32 %1774, 15
  %1778 = sub i32 %1776, %1777
  %1779 = icmp ugt i32 %1778, 15
  %1780 = icmp ult i32 %1772, %1774
  %1781 = xor i32 %1772, %1774
  %1782 = xor i32 %1772, %1775
  %1783 = and i32 %1781, %1782
  %1784 = icmp slt i32 %1783, 0
  store i1 %1779, i1* %az
  store i1 %1780, i1* %cf
  store i1 %1784, i1* %of
  %1785 = icmp eq i32 %1775, 0
  store i1 %1785, i1* %zf
  %1786 = icmp slt i32 %1775, 0
  store i1 %1786, i1* %sf
  %1787 = trunc i32 %1775 to i8
  %1788 = call i8 @llvm.ctpop.i8(i8 %1787)
  %1789 = and i8 %1788, 1
  %1790 = icmp eq i8 %1789, 0
  store i1 %1790, i1* %pf
  %1791 = zext i32 %1775 to i64
  store i64 %1791, i64* %rdx
  store volatile i64 32867, i64* @assembly_address
  %1792 = load i64* %rdx
  %1793 = trunc i64 %1792 to i32
  %1794 = zext i32 %1793 to i64
  store i64 %1794, i64* %rax
  store volatile i64 32869, i64* @assembly_address
  %1795 = load i64* %rax
  %1796 = trunc i64 %1795 to i8
  store i8 %1796, i8* %stack_var_-1527
  store volatile i64 32875, i64* @assembly_address
  %1797 = load i32* %stack_var_-1588
  %1798 = zext i32 %1797 to i64
  store i64 %1798, i64* %rax
  store volatile i64 32881, i64* @assembly_address
  %1799 = load i64* %rax
  %1800 = mul i64 %1799, 4
  store i64 %1800, i64* %rdx
  store volatile i64 32889, i64* @assembly_address
  %1801 = ptrtoint i64* %stack_var_-1224 to i64
  store i64 %1801, i64* %rax
  store volatile i64 32896, i64* @assembly_address
  %1802 = load i64* %rax
  %1803 = load i64* %rdx
  %1804 = add i64 %1802, %1803
  %1805 = and i64 %1802, 15
  %1806 = and i64 %1803, 15
  %1807 = add i64 %1805, %1806
  %1808 = icmp ugt i64 %1807, 15
  %1809 = icmp ult i64 %1804, %1802
  %1810 = xor i64 %1802, %1804
  %1811 = xor i64 %1803, %1804
  %1812 = and i64 %1810, %1811
  %1813 = icmp slt i64 %1812, 0
  store i1 %1808, i1* %az
  store i1 %1809, i1* %cf
  store i1 %1813, i1* %of
  %1814 = icmp eq i64 %1804, 0
  store i1 %1814, i1* %zf
  %1815 = icmp slt i64 %1804, 0
  store i1 %1815, i1* %sf
  %1816 = trunc i64 %1804 to i8
  %1817 = call i8 @llvm.ctpop.i8(i8 %1816)
  %1818 = and i8 %1817, 1
  %1819 = icmp eq i8 %1818, 0
  store i1 %1819, i1* %pf
  store i64 %1804, i64* %rax
  store volatile i64 32899, i64* @assembly_address
  %1820 = load i64* %r13
  %1821 = load i64* %rax
  %1822 = sub i64 %1820, %1821
  %1823 = and i64 %1820, 15
  %1824 = and i64 %1821, 15
  %1825 = sub i64 %1823, %1824
  %1826 = icmp ugt i64 %1825, 15
  %1827 = icmp ult i64 %1820, %1821
  %1828 = xor i64 %1820, %1821
  %1829 = xor i64 %1820, %1822
  %1830 = and i64 %1828, %1829
  %1831 = icmp slt i64 %1830, 0
  store i1 %1826, i1* %az
  store i1 %1827, i1* %cf
  store i1 %1831, i1* %of
  %1832 = icmp eq i64 %1822, 0
  store i1 %1832, i1* %zf
  %1833 = icmp slt i64 %1822, 0
  store i1 %1833, i1* %sf
  %1834 = trunc i64 %1822 to i8
  %1835 = call i8 @llvm.ctpop.i8(i8 %1834)
  %1836 = and i8 %1835, 1
  %1837 = icmp eq i8 %1836, 0
  store i1 %1837, i1* %pf
  store volatile i64 32902, i64* @assembly_address
  %1838 = load i1* %cf
  br i1 %1838, label %block_8094, label %block_8088

block_8088:                                       ; preds = %block_8057
  store volatile i64 32904, i64* @assembly_address
  store i8 99, i8* %stack_var_-1528
  store volatile i64 32911, i64* @assembly_address
  br label %block_811a

block_8094:                                       ; preds = %block_8057
  store volatile i64 32916, i64* @assembly_address
  %1839 = load i64* %r13
  %1840 = inttoptr i64 %1839 to i32*
  %1841 = load i32* %1840
  %1842 = zext i32 %1841 to i64
  store i64 %1842, i64* %rax
  store volatile i64 32920, i64* @assembly_address
  %1843 = load i32* %stack_var_-1592
  %1844 = load i64* %rax
  %1845 = trunc i64 %1844 to i32
  %1846 = sub i32 %1843, %1845
  %1847 = and i32 %1843, 15
  %1848 = and i32 %1845, 15
  %1849 = sub i32 %1847, %1848
  %1850 = icmp ugt i32 %1849, 15
  %1851 = icmp ult i32 %1843, %1845
  %1852 = xor i32 %1843, %1845
  %1853 = xor i32 %1843, %1846
  %1854 = and i32 %1852, %1853
  %1855 = icmp slt i32 %1854, 0
  store i1 %1850, i1* %az
  store i1 %1851, i1* %cf
  store i1 %1855, i1* %of
  %1856 = icmp eq i32 %1846, 0
  store i1 %1856, i1* %zf
  %1857 = icmp slt i32 %1846, 0
  store i1 %1857, i1* %sf
  %1858 = trunc i32 %1846 to i8
  %1859 = call i8 @llvm.ctpop.i8(i8 %1858)
  %1860 = and i8 %1859, 1
  %1861 = icmp eq i8 %1860, 0
  store i1 %1861, i1* %pf
  store volatile i64 32926, i64* @assembly_address
  %1862 = load i1* %cf
  %1863 = load i1* %zf
  %1864 = or i1 %1862, %1863
  br i1 %1864, label %block_80ce, label %block_80a0

block_80a0:                                       ; preds = %block_8094
  store volatile i64 32928, i64* @assembly_address
  %1865 = load i64* %r13
  %1866 = inttoptr i64 %1865 to i32*
  %1867 = load i32* %1866
  %1868 = zext i32 %1867 to i64
  store i64 %1868, i64* %rax
  store volatile i64 32932, i64* @assembly_address
  %1869 = load i64* %rax
  %1870 = trunc i64 %1869 to i32
  %1871 = sub i32 %1870, 255
  %1872 = and i32 %1870, 15
  %1873 = sub i32 %1872, 15
  %1874 = icmp ugt i32 %1873, 15
  %1875 = icmp ult i32 %1870, 255
  %1876 = xor i32 %1870, 255
  %1877 = xor i32 %1870, %1871
  %1878 = and i32 %1876, %1877
  %1879 = icmp slt i32 %1878, 0
  store i1 %1874, i1* %az
  store i1 %1875, i1* %cf
  store i1 %1879, i1* %of
  %1880 = icmp eq i32 %1871, 0
  store i1 %1880, i1* %zf
  %1881 = icmp slt i32 %1871, 0
  store i1 %1881, i1* %sf
  %1882 = trunc i32 %1871 to i8
  %1883 = call i8 @llvm.ctpop.i8(i8 %1882)
  %1884 = and i8 %1883, 1
  %1885 = icmp eq i8 %1884, 0
  store i1 %1885, i1* %pf
  store volatile i64 32937, i64* @assembly_address
  %1886 = load i1* %cf
  %1887 = load i1* %zf
  %1888 = or i1 %1886, %1887
  %1889 = icmp ne i1 %1888, true
  br i1 %1889, label %block_80b2, label %block_80ab

block_80ab:                                       ; preds = %block_80a0
  store volatile i64 32939, i64* @assembly_address
  store i64 16, i64* %rax
  store volatile i64 32944, i64* @assembly_address
  br label %block_80b7

block_80b2:                                       ; preds = %block_80a0
  store volatile i64 32946, i64* @assembly_address
  store i64 15, i64* %rax
  br label %block_80b7

block_80b7:                                       ; preds = %block_80b2, %block_80ab
  store volatile i64 32951, i64* @assembly_address
  %1890 = load i64* %rax
  %1891 = trunc i64 %1890 to i8
  store i8 %1891, i8* %stack_var_-1528
  store volatile i64 32957, i64* @assembly_address
  %1892 = load i64* %r13
  %1893 = inttoptr i64 %1892 to i32*
  %1894 = load i32* %1893
  %1895 = zext i32 %1894 to i64
  store i64 %1895, i64* %rax
  store volatile i64 32961, i64* @assembly_address
  %1896 = load i64* %rax
  %1897 = trunc i64 %1896 to i16
  %1898 = sext i16 %1897 to i64
  store i64 %1898, i64* %stack_var_-1520
  store volatile i64 32968, i64* @assembly_address
  %1899 = load i64* %r13
  %1900 = add i64 %1899, 4
  %1901 = and i64 %1899, 15
  %1902 = add i64 %1901, 4
  %1903 = icmp ugt i64 %1902, 15
  %1904 = icmp ult i64 %1900, %1899
  %1905 = xor i64 %1899, %1900
  %1906 = xor i64 4, %1900
  %1907 = and i64 %1905, %1906
  %1908 = icmp slt i64 %1907, 0
  store i1 %1903, i1* %az
  store i1 %1904, i1* %cf
  store i1 %1908, i1* %of
  %1909 = icmp eq i64 %1900, 0
  store i1 %1909, i1* %zf
  %1910 = icmp slt i64 %1900, 0
  store i1 %1910, i1* %sf
  %1911 = trunc i64 %1900 to i8
  %1912 = call i8 @llvm.ctpop.i8(i8 %1911)
  %1913 = and i8 %1912, 1
  %1914 = icmp eq i8 %1913, 0
  store i1 %1914, i1* %pf
  store i64 %1900, i64* %r13
  store volatile i64 32972, i64* @assembly_address
  br label %block_811a

block_80ce:                                       ; preds = %block_8094
  store volatile i64 32974, i64* @assembly_address
  %1915 = load i64* %r13
  %1916 = inttoptr i64 %1915 to i32*
  %1917 = load i32* %1916
  %1918 = zext i32 %1917 to i64
  store i64 %1918, i64* %rax
  store volatile i64 32978, i64* @assembly_address
  %1919 = load i64* %rax
  %1920 = trunc i64 %1919 to i32
  %1921 = load i32* %stack_var_-1592
  %1922 = sub i32 %1920, %1921
  %1923 = and i32 %1920, 15
  %1924 = and i32 %1921, 15
  %1925 = sub i32 %1923, %1924
  %1926 = icmp ugt i32 %1925, 15
  %1927 = icmp ult i32 %1920, %1921
  %1928 = xor i32 %1920, %1921
  %1929 = xor i32 %1920, %1922
  %1930 = and i32 %1928, %1929
  %1931 = icmp slt i32 %1930, 0
  store i1 %1926, i1* %az
  store i1 %1927, i1* %cf
  store i1 %1931, i1* %of
  %1932 = icmp eq i32 %1922, 0
  store i1 %1932, i1* %zf
  %1933 = icmp slt i32 %1922, 0
  store i1 %1933, i1* %sf
  %1934 = trunc i32 %1922 to i8
  %1935 = call i8 @llvm.ctpop.i8(i8 %1934)
  %1936 = and i8 %1935, 1
  %1937 = icmp eq i8 %1936, 0
  store i1 %1937, i1* %pf
  %1938 = zext i32 %1922 to i64
  store i64 %1938, i64* %rax
  store volatile i64 32984, i64* @assembly_address
  %1939 = load i64* %rax
  %1940 = trunc i64 %1939 to i32
  %1941 = zext i32 %1940 to i64
  store i64 %1941, i64* %rax
  store volatile i64 32986, i64* @assembly_address
  %1942 = load i64* %rax
  %1943 = load i64* %rax
  %1944 = mul i64 %1943, 1
  %1945 = add i64 %1942, %1944
  store i64 %1945, i64* %rdx
  store volatile i64 32990, i64* @assembly_address
  %1946 = load i16** %stack_var_-1608
  %1947 = ptrtoint i16* %1946 to i64
  store i64 %1947, i64* %rax
  store volatile i64 32997, i64* @assembly_address
  %1948 = load i64* %rax
  %1949 = load i64* %rdx
  %1950 = add i64 %1948, %1949
  %1951 = and i64 %1948, 15
  %1952 = and i64 %1949, 15
  %1953 = add i64 %1951, %1952
  %1954 = icmp ugt i64 %1953, 15
  %1955 = icmp ult i64 %1950, %1948
  %1956 = xor i64 %1948, %1950
  %1957 = xor i64 %1949, %1950
  %1958 = and i64 %1956, %1957
  %1959 = icmp slt i64 %1958, 0
  store i1 %1954, i1* %az
  store i1 %1955, i1* %cf
  store i1 %1959, i1* %of
  %1960 = icmp eq i64 %1950, 0
  store i1 %1960, i1* %zf
  %1961 = icmp slt i64 %1950, 0
  store i1 %1961, i1* %sf
  %1962 = trunc i64 %1950 to i8
  %1963 = call i8 @llvm.ctpop.i8(i8 %1962)
  %1964 = and i8 %1963, 1
  %1965 = icmp eq i8 %1964, 0
  store i1 %1965, i1* %pf
  store i64 %1950, i64* %rax
  store volatile i64 33000, i64* @assembly_address
  %1966 = load i64* %rax
  %1967 = inttoptr i64 %1966 to i16*
  %1968 = load i16* %1967
  %1969 = zext i16 %1968 to i64
  store i64 %1969, i64* %rax
  store volatile i64 33003, i64* @assembly_address
  %1970 = load i64* %rax
  %1971 = trunc i64 %1970 to i8
  store i8 %1971, i8* %stack_var_-1528
  store volatile i64 33009, i64* @assembly_address
  %1972 = load i64* %r13
  store i64 %1972, i64* %rax
  store volatile i64 33012, i64* @assembly_address
  %1973 = load i64* %rax
  %1974 = add i64 %1973, 4
  store i64 %1974, i64* %r13
  store volatile i64 33016, i64* @assembly_address
  %1975 = load i64* %rax
  %1976 = inttoptr i64 %1975 to i32*
  %1977 = load i32* %1976
  %1978 = zext i32 %1977 to i64
  store i64 %1978, i64* %rax
  store volatile i64 33018, i64* @assembly_address
  %1979 = load i64* %rax
  %1980 = trunc i64 %1979 to i32
  %1981 = load i32* %stack_var_-1592
  %1982 = sub i32 %1980, %1981
  %1983 = and i32 %1980, 15
  %1984 = and i32 %1981, 15
  %1985 = sub i32 %1983, %1984
  %1986 = icmp ugt i32 %1985, 15
  %1987 = icmp ult i32 %1980, %1981
  %1988 = xor i32 %1980, %1981
  %1989 = xor i32 %1980, %1982
  %1990 = and i32 %1988, %1989
  %1991 = icmp slt i32 %1990, 0
  store i1 %1986, i1* %az
  store i1 %1987, i1* %cf
  store i1 %1991, i1* %of
  %1992 = icmp eq i32 %1982, 0
  store i1 %1992, i1* %zf
  %1993 = icmp slt i32 %1982, 0
  store i1 %1993, i1* %sf
  %1994 = trunc i32 %1982 to i8
  %1995 = call i8 @llvm.ctpop.i8(i8 %1994)
  %1996 = and i8 %1995, 1
  %1997 = icmp eq i8 %1996, 0
  store i1 %1997, i1* %pf
  %1998 = zext i32 %1982 to i64
  store i64 %1998, i64* %rax
  store volatile i64 33024, i64* @assembly_address
  %1999 = load i64* %rax
  %2000 = trunc i64 %1999 to i32
  %2001 = zext i32 %2000 to i64
  store i64 %2001, i64* %rax
  store volatile i64 33026, i64* @assembly_address
  %2002 = load i64* %rax
  %2003 = load i64* %rax
  %2004 = mul i64 %2003, 1
  %2005 = add i64 %2002, %2004
  store i64 %2005, i64* %rdx
  store volatile i64 33030, i64* @assembly_address
  %2006 = load i16** %stack_var_-1600
  %2007 = ptrtoint i16* %2006 to i64
  store i64 %2007, i64* %rax
  store volatile i64 33037, i64* @assembly_address
  %2008 = load i64* %rax
  %2009 = load i64* %rdx
  %2010 = add i64 %2008, %2009
  %2011 = and i64 %2008, 15
  %2012 = and i64 %2009, 15
  %2013 = add i64 %2011, %2012
  %2014 = icmp ugt i64 %2013, 15
  %2015 = icmp ult i64 %2010, %2008
  %2016 = xor i64 %2008, %2010
  %2017 = xor i64 %2009, %2010
  %2018 = and i64 %2016, %2017
  %2019 = icmp slt i64 %2018, 0
  store i1 %2014, i1* %az
  store i1 %2015, i1* %cf
  store i1 %2019, i1* %of
  %2020 = icmp eq i64 %2010, 0
  store i1 %2020, i1* %zf
  %2021 = icmp slt i64 %2010, 0
  store i1 %2021, i1* %sf
  %2022 = trunc i64 %2010 to i8
  %2023 = call i8 @llvm.ctpop.i8(i8 %2022)
  %2024 = and i8 %2023, 1
  %2025 = icmp eq i8 %2024, 0
  store i1 %2025, i1* %pf
  store i64 %2010, i64* %rax
  store volatile i64 33040, i64* @assembly_address
  %2026 = load i64* %rax
  %2027 = inttoptr i64 %2026 to i16*
  %2028 = load i16* %2027
  %2029 = zext i16 %2028 to i64
  store i64 %2029, i64* %rax
  store volatile i64 33043, i64* @assembly_address
  %2030 = load i64* %rax
  %2031 = trunc i64 %2030 to i16
  %2032 = sext i16 %2031 to i64
  store i64 %2032, i64* %stack_var_-1520
  br label %block_811a

block_811a:                                       ; preds = %block_80ce, %block_80b7, %block_8088
  store volatile i64 33050, i64* @assembly_address
  %2033 = load i32* %stack_var_-1628
  %2034 = zext i32 %2033 to i64
  store i64 %2034, i64* %rax
  store volatile i64 33056, i64* @assembly_address
  %2035 = load i64* %rax
  %2036 = trunc i64 %2035 to i32
  %2037 = load i64* %r15
  %2038 = trunc i64 %2037 to i32
  %2039 = sub i32 %2036, %2038
  %2040 = and i32 %2036, 15
  %2041 = and i32 %2038, 15
  %2042 = sub i32 %2040, %2041
  %2043 = icmp ugt i32 %2042, 15
  %2044 = icmp ult i32 %2036, %2038
  %2045 = xor i32 %2036, %2038
  %2046 = xor i32 %2036, %2039
  %2047 = and i32 %2045, %2046
  %2048 = icmp slt i32 %2047, 0
  store i1 %2043, i1* %az
  store i1 %2044, i1* %cf
  store i1 %2048, i1* %of
  %2049 = icmp eq i32 %2039, 0
  store i1 %2049, i1* %zf
  %2050 = icmp slt i32 %2039, 0
  store i1 %2050, i1* %sf
  %2051 = trunc i32 %2039 to i8
  %2052 = call i8 @llvm.ctpop.i8(i8 %2051)
  %2053 = and i8 %2052, 1
  %2054 = icmp eq i8 %2053, 0
  store i1 %2054, i1* %pf
  %2055 = zext i32 %2039 to i64
  store i64 %2055, i64* %rax
  store volatile i64 33059, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 33064, i64* @assembly_address
  %2056 = load i64* %rax
  %2057 = trunc i64 %2056 to i32
  %2058 = zext i32 %2057 to i64
  store i64 %2058, i64* %rcx
  store volatile i64 33066, i64* @assembly_address
  %2059 = load i64* %rdx
  %2060 = trunc i64 %2059 to i32
  %2061 = load i64* %rcx
  %2062 = trunc i64 %2061 to i8
  %2063 = zext i8 %2062 to i32
  %2064 = and i32 %2063, 31
  %2065 = load i1* %of
  %2066 = icmp eq i32 %2064, 0
  br i1 %2066, label %2084, label %2067

; <label>:2067                                    ; preds = %block_811a
  %2068 = shl i32 %2060, %2064
  %2069 = icmp eq i32 %2068, 0
  store i1 %2069, i1* %zf
  %2070 = icmp slt i32 %2068, 0
  store i1 %2070, i1* %sf
  %2071 = trunc i32 %2068 to i8
  %2072 = call i8 @llvm.ctpop.i8(i8 %2071)
  %2073 = and i8 %2072, 1
  %2074 = icmp eq i8 %2073, 0
  store i1 %2074, i1* %pf
  %2075 = zext i32 %2068 to i64
  store i64 %2075, i64* %rdx
  %2076 = sub i32 %2064, 1
  %2077 = shl i32 %2060, %2076
  %2078 = lshr i32 %2077, 31
  %2079 = trunc i32 %2078 to i1
  store i1 %2079, i1* %cf
  %2080 = lshr i32 %2068, 31
  %2081 = icmp ne i32 %2080, %2078
  %2082 = icmp eq i32 %2064, 1
  %2083 = select i1 %2082, i1 %2081, i1 %2065
  store i1 %2083, i1* %of
  br label %2084

; <label>:2084                                    ; preds = %block_811a, %2067
  store volatile i64 33068, i64* @assembly_address
  %2085 = load i64* %rdx
  %2086 = trunc i64 %2085 to i32
  %2087 = zext i32 %2086 to i64
  store i64 %2087, i64* %rax
  store volatile i64 33070, i64* @assembly_address
  %2088 = load i64* %rax
  %2089 = trunc i64 %2088 to i32
  store i32 %2089, i32* %stack_var_-1560
  store volatile i64 33076, i64* @assembly_address
  %2090 = load i64* %r12
  %2091 = trunc i64 %2090 to i32
  %2092 = zext i32 %2091 to i64
  store i64 %2092, i64* %rbx
  store volatile i64 33079, i64* @assembly_address
  %2093 = load i64* %r15
  %2094 = trunc i64 %2093 to i32
  %2095 = zext i32 %2094 to i64
  store i64 %2095, i64* %rcx
  store volatile i64 33082, i64* @assembly_address
  %2096 = load i64* %rbx
  %2097 = trunc i64 %2096 to i32
  %2098 = load i64* %rcx
  %2099 = trunc i64 %2098 to i8
  %2100 = zext i8 %2099 to i32
  %2101 = and i32 %2100, 31
  %2102 = load i1* %of
  %2103 = icmp eq i32 %2101, 0
  br i1 %2103, label %2120, label %2104

; <label>:2104                                    ; preds = %2084
  %2105 = lshr i32 %2097, %2101
  %2106 = icmp eq i32 %2105, 0
  store i1 %2106, i1* %zf
  %2107 = icmp slt i32 %2105, 0
  store i1 %2107, i1* %sf
  %2108 = trunc i32 %2105 to i8
  %2109 = call i8 @llvm.ctpop.i8(i8 %2108)
  %2110 = and i8 %2109, 1
  %2111 = icmp eq i8 %2110, 0
  store i1 %2111, i1* %pf
  %2112 = zext i32 %2105 to i64
  store i64 %2112, i64* %rbx
  %2113 = sub i32 %2101, 1
  %2114 = shl i32 1, %2113
  %2115 = and i32 %2114, %2097
  %2116 = icmp ne i32 %2115, 0
  store i1 %2116, i1* %cf
  %2117 = icmp eq i32 %2101, 1
  %2118 = icmp slt i32 %2097, 0
  %2119 = select i1 %2117, i1 %2118, i1 %2102
  store i1 %2119, i1* %of
  br label %2120

; <label>:2120                                    ; preds = %2084, %2104
  store volatile i64 33084, i64* @assembly_address
  br label %block_8167

block_813e:                                       ; preds = %block_8167
  store volatile i64 33086, i64* @assembly_address
  %2121 = load i64* %rbx
  %2122 = trunc i64 %2121 to i32
  %2123 = zext i32 %2122 to i64
  store i64 %2123, i64* %rax
  store volatile i64 33088, i64* @assembly_address
  %2124 = load i64* %rax
  %2125 = load i1* %of
  %2126 = shl i64 %2124, 4
  %2127 = icmp eq i64 %2126, 0
  store i1 %2127, i1* %zf
  %2128 = icmp slt i64 %2126, 0
  store i1 %2128, i1* %sf
  %2129 = trunc i64 %2126 to i8
  %2130 = call i8 @llvm.ctpop.i8(i8 %2129)
  %2131 = and i8 %2130, 1
  %2132 = icmp eq i8 %2131, 0
  store i1 %2132, i1* %pf
  store i64 %2126, i64* %rax
  %2133 = shl i64 %2124, 3
  %2134 = lshr i64 %2133, 63
  %2135 = trunc i64 %2134 to i1
  store i1 %2135, i1* %cf
  %2136 = lshr i64 %2126, 63
  %2137 = icmp ne i64 %2136, %2134
  %2138 = select i1 false, i1 %2137, i1 %2125
  store i1 %2138, i1* %of
  store volatile i64 33092, i64* @assembly_address
  %2139 = load i64* %r14
  %2140 = load i64* %rax
  %2141 = mul i64 %2140, 1
  %2142 = add i64 %2139, %2141
  store i64 %2142, i64* %rcx
  store volatile i64 33096, i64* @assembly_address
  %2143 = load i8* %stack_var_-1528
  %2144 = sext i8 %2143 to i64
  store i64 %2144, i64* %rax
  store volatile i64 33103, i64* @assembly_address
  %2145 = load i64* %stack_var_-1520
  store i64 %2145, i64* %rdx
  store volatile i64 33110, i64* @assembly_address
  %2146 = load i64* %rax
  %2147 = load i64* %rcx
  %2148 = inttoptr i64 %2147 to i64*
  store i64 %2146, i64* %2148
  store volatile i64 33113, i64* @assembly_address
  %2149 = load i64* %rdx
  %2150 = load i64* %rcx
  %2151 = add i64 %2150, 8
  %2152 = inttoptr i64 %2151 to i64*
  store i64 %2149, i64* %2152
  store volatile i64 33117, i64* @assembly_address
  %2153 = load i32* %stack_var_-1560
  %2154 = zext i32 %2153 to i64
  store i64 %2154, i64* %rax
  store volatile i64 33123, i64* @assembly_address
  %2155 = load i64* %rax
  %2156 = trunc i64 %2155 to i32
  %2157 = load i64* %rbx
  %2158 = trunc i64 %2157 to i32
  %2159 = add i32 %2156, %2158
  %2160 = and i32 %2156, 15
  %2161 = and i32 %2158, 15
  %2162 = add i32 %2160, %2161
  %2163 = icmp ugt i32 %2162, 15
  %2164 = icmp ult i32 %2159, %2156
  %2165 = xor i32 %2156, %2159
  %2166 = xor i32 %2158, %2159
  %2167 = and i32 %2165, %2166
  %2168 = icmp slt i32 %2167, 0
  store i1 %2163, i1* %az
  store i1 %2164, i1* %cf
  store i1 %2168, i1* %of
  %2169 = icmp eq i32 %2159, 0
  store i1 %2169, i1* %zf
  %2170 = icmp slt i32 %2159, 0
  store i1 %2170, i1* %sf
  %2171 = trunc i32 %2159 to i8
  %2172 = call i8 @llvm.ctpop.i8(i8 %2171)
  %2173 = and i8 %2172, 1
  %2174 = icmp eq i8 %2173, 0
  store i1 %2174, i1* %pf
  %2175 = zext i32 %2159 to i64
  store i64 %2175, i64* %rax
  store volatile i64 33125, i64* @assembly_address
  %2176 = load i64* %rax
  %2177 = trunc i64 %2176 to i32
  %2178 = zext i32 %2177 to i64
  store i64 %2178, i64* %rbx
  br label %block_8167

block_8167:                                       ; preds = %block_813e, %2120
  store volatile i64 33127, i64* @assembly_address
  %2179 = load i64* %rbx
  %2180 = trunc i64 %2179 to i32
  %2181 = load i32* %stack_var_-1544
  %2182 = sub i32 %2180, %2181
  %2183 = and i32 %2180, 15
  %2184 = and i32 %2181, 15
  %2185 = sub i32 %2183, %2184
  %2186 = icmp ugt i32 %2185, 15
  %2187 = icmp ult i32 %2180, %2181
  %2188 = xor i32 %2180, %2181
  %2189 = xor i32 %2180, %2182
  %2190 = and i32 %2188, %2189
  %2191 = icmp slt i32 %2190, 0
  store i1 %2186, i1* %az
  store i1 %2187, i1* %cf
  store i1 %2191, i1* %of
  %2192 = icmp eq i32 %2182, 0
  store i1 %2192, i1* %zf
  %2193 = icmp slt i32 %2182, 0
  store i1 %2193, i1* %sf
  %2194 = trunc i32 %2182 to i8
  %2195 = call i8 @llvm.ctpop.i8(i8 %2194)
  %2196 = and i8 %2195, 1
  %2197 = icmp eq i8 %2196, 0
  store i1 %2197, i1* %pf
  store volatile i64 33133, i64* @assembly_address
  %2198 = load i1* %cf
  br i1 %2198, label %block_813e, label %block_816f

block_816f:                                       ; preds = %block_8167
  store volatile i64 33135, i64* @assembly_address
  %2199 = load i32* %stack_var_-1628
  %2200 = zext i32 %2199 to i64
  store i64 %2200, i64* %rax
  store volatile i64 33141, i64* @assembly_address
  %2201 = load i64* %rax
  %2202 = trunc i64 %2201 to i32
  %2203 = sub i32 %2202, 1
  %2204 = and i32 %2202, 15
  %2205 = sub i32 %2204, 1
  %2206 = icmp ugt i32 %2205, 15
  %2207 = icmp ult i32 %2202, 1
  %2208 = xor i32 %2202, 1
  %2209 = xor i32 %2202, %2203
  %2210 = and i32 %2208, %2209
  %2211 = icmp slt i32 %2210, 0
  store i1 %2206, i1* %az
  store i1 %2207, i1* %cf
  store i1 %2211, i1* %of
  %2212 = icmp eq i32 %2203, 0
  store i1 %2212, i1* %zf
  %2213 = icmp slt i32 %2203, 0
  store i1 %2213, i1* %sf
  %2214 = trunc i32 %2203 to i8
  %2215 = call i8 @llvm.ctpop.i8(i8 %2214)
  %2216 = and i8 %2215, 1
  %2217 = icmp eq i8 %2216, 0
  store i1 %2217, i1* %pf
  %2218 = zext i32 %2203 to i64
  store i64 %2218, i64* %rax
  store volatile i64 33144, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 33149, i64* @assembly_address
  %2219 = load i64* %rax
  %2220 = trunc i64 %2219 to i32
  %2221 = zext i32 %2220 to i64
  store i64 %2221, i64* %rcx
  store volatile i64 33151, i64* @assembly_address
  %2222 = load i64* %rdx
  %2223 = trunc i64 %2222 to i32
  %2224 = load i64* %rcx
  %2225 = trunc i64 %2224 to i8
  %2226 = zext i8 %2225 to i32
  %2227 = and i32 %2226, 31
  %2228 = load i1* %of
  %2229 = icmp eq i32 %2227, 0
  br i1 %2229, label %2247, label %2230

; <label>:2230                                    ; preds = %block_816f
  %2231 = shl i32 %2223, %2227
  %2232 = icmp eq i32 %2231, 0
  store i1 %2232, i1* %zf
  %2233 = icmp slt i32 %2231, 0
  store i1 %2233, i1* %sf
  %2234 = trunc i32 %2231 to i8
  %2235 = call i8 @llvm.ctpop.i8(i8 %2234)
  %2236 = and i8 %2235, 1
  %2237 = icmp eq i8 %2236, 0
  store i1 %2237, i1* %pf
  %2238 = zext i32 %2231 to i64
  store i64 %2238, i64* %rdx
  %2239 = sub i32 %2227, 1
  %2240 = shl i32 %2223, %2239
  %2241 = lshr i32 %2240, 31
  %2242 = trunc i32 %2241 to i1
  store i1 %2242, i1* %cf
  %2243 = lshr i32 %2231, 31
  %2244 = icmp ne i32 %2243, %2241
  %2245 = icmp eq i32 %2227, 1
  %2246 = select i1 %2245, i1 %2244, i1 %2228
  store i1 %2246, i1* %of
  br label %2247

; <label>:2247                                    ; preds = %block_816f, %2230
  store volatile i64 33153, i64* @assembly_address
  %2248 = load i64* %rdx
  %2249 = trunc i64 %2248 to i32
  %2250 = zext i32 %2249 to i64
  store i64 %2250, i64* %rax
  store volatile i64 33155, i64* @assembly_address
  %2251 = load i64* %rax
  %2252 = trunc i64 %2251 to i32
  %2253 = zext i32 %2252 to i64
  store i64 %2253, i64* %rbx
  store volatile i64 33157, i64* @assembly_address
  br label %block_818c

block_8187:                                       ; preds = %block_818c
  store volatile i64 33159, i64* @assembly_address
  %2254 = load i64* %r12
  %2255 = trunc i64 %2254 to i32
  %2256 = load i64* %rbx
  %2257 = trunc i64 %2256 to i32
  %2258 = xor i32 %2255, %2257
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2259 = icmp eq i32 %2258, 0
  store i1 %2259, i1* %zf
  %2260 = icmp slt i32 %2258, 0
  store i1 %2260, i1* %sf
  %2261 = trunc i32 %2258 to i8
  %2262 = call i8 @llvm.ctpop.i8(i8 %2261)
  %2263 = and i8 %2262, 1
  %2264 = icmp eq i8 %2263, 0
  store i1 %2264, i1* %pf
  %2265 = zext i32 %2258 to i64
  store i64 %2265, i64* %r12
  store volatile i64 33162, i64* @assembly_address
  %2266 = load i64* %rbx
  %2267 = trunc i64 %2266 to i32
  %2268 = load i1* %of
  %2269 = lshr i32 %2267, 1
  %2270 = icmp eq i32 %2269, 0
  store i1 %2270, i1* %zf
  %2271 = icmp slt i32 %2269, 0
  store i1 %2271, i1* %sf
  %2272 = trunc i32 %2269 to i8
  %2273 = call i8 @llvm.ctpop.i8(i8 %2272)
  %2274 = and i8 %2273, 1
  %2275 = icmp eq i8 %2274, 0
  store i1 %2275, i1* %pf
  %2276 = zext i32 %2269 to i64
  store i64 %2276, i64* %rbx
  %2277 = and i32 1, %2267
  %2278 = icmp ne i32 %2277, 0
  store i1 %2278, i1* %cf
  %2279 = icmp slt i32 %2267, 0
  %2280 = select i1 true, i1 %2279, i1 %2268
  store i1 %2280, i1* %of
  br label %block_818c

block_818c:                                       ; preds = %block_8187, %2247
  store volatile i64 33164, i64* @assembly_address
  %2281 = load i64* %r12
  %2282 = trunc i64 %2281 to i32
  %2283 = zext i32 %2282 to i64
  store i64 %2283, i64* %rax
  store volatile i64 33167, i64* @assembly_address
  %2284 = load i64* %rax
  %2285 = trunc i64 %2284 to i32
  %2286 = load i64* %rbx
  %2287 = trunc i64 %2286 to i32
  %2288 = and i32 %2285, %2287
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2289 = icmp eq i32 %2288, 0
  store i1 %2289, i1* %zf
  %2290 = icmp slt i32 %2288, 0
  store i1 %2290, i1* %sf
  %2291 = trunc i32 %2288 to i8
  %2292 = call i8 @llvm.ctpop.i8(i8 %2291)
  %2293 = and i8 %2292, 1
  %2294 = icmp eq i8 %2293, 0
  store i1 %2294, i1* %pf
  %2295 = zext i32 %2288 to i64
  store i64 %2295, i64* %rax
  store volatile i64 33169, i64* @assembly_address
  %2296 = load i64* %rax
  %2297 = trunc i64 %2296 to i32
  %2298 = load i64* %rax
  %2299 = trunc i64 %2298 to i32
  %2300 = and i32 %2297, %2299
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2301 = icmp eq i32 %2300, 0
  store i1 %2301, i1* %zf
  %2302 = icmp slt i32 %2300, 0
  store i1 %2302, i1* %sf
  %2303 = trunc i32 %2300 to i8
  %2304 = call i8 @llvm.ctpop.i8(i8 %2303)
  %2305 = and i8 %2304, 1
  %2306 = icmp eq i8 %2305, 0
  store i1 %2306, i1* %pf
  store volatile i64 33171, i64* @assembly_address
  %2307 = load i1* %zf
  %2308 = icmp eq i1 %2307, false
  br i1 %2308, label %block_8187, label %block_8195

block_8195:                                       ; preds = %block_818c
  store volatile i64 33173, i64* @assembly_address
  %2309 = load i64* %r12
  %2310 = trunc i64 %2309 to i32
  %2311 = load i64* %rbx
  %2312 = trunc i64 %2311 to i32
  %2313 = xor i32 %2310, %2312
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2314 = icmp eq i32 %2313, 0
  store i1 %2314, i1* %zf
  %2315 = icmp slt i32 %2313, 0
  store i1 %2315, i1* %sf
  %2316 = trunc i32 %2313 to i8
  %2317 = call i8 @llvm.ctpop.i8(i8 %2316)
  %2318 = and i8 %2317, 1
  %2319 = icmp eq i8 %2318, 0
  store i1 %2319, i1* %pf
  %2320 = zext i32 %2313 to i64
  store i64 %2320, i64* %r12
  store volatile i64 33176, i64* @assembly_address
  br label %block_81a8

block_819a:                                       ; preds = %2385
  store volatile i64 33178, i64* @assembly_address
  %2321 = load i32* %stack_var_-1556
  %2322 = sub i32 %2321, 1
  %2323 = and i32 %2321, 15
  %2324 = sub i32 %2323, 1
  %2325 = icmp ugt i32 %2324, 15
  %2326 = icmp ult i32 %2321, 1
  %2327 = xor i32 %2321, 1
  %2328 = xor i32 %2321, %2322
  %2329 = and i32 %2327, %2328
  %2330 = icmp slt i32 %2329, 0
  store i1 %2325, i1* %az
  store i1 %2326, i1* %cf
  store i1 %2330, i1* %of
  %2331 = icmp eq i32 %2322, 0
  store i1 %2331, i1* %zf
  %2332 = icmp slt i32 %2322, 0
  store i1 %2332, i1* %sf
  %2333 = trunc i32 %2322 to i8
  %2334 = call i8 @llvm.ctpop.i8(i8 %2333)
  %2335 = and i8 %2334, 1
  %2336 = icmp eq i8 %2335, 0
  store i1 %2336, i1* %pf
  store i32 %2322, i32* %stack_var_-1556
  store volatile i64 33185, i64* @assembly_address
  %2337 = load i64* %r15
  %2338 = trunc i64 %2337 to i32
  %2339 = load i32* %stack_var_-1552
  %2340 = sub i32 %2338, %2339
  %2341 = and i32 %2338, 15
  %2342 = and i32 %2339, 15
  %2343 = sub i32 %2341, %2342
  %2344 = icmp ugt i32 %2343, 15
  %2345 = icmp ult i32 %2338, %2339
  %2346 = xor i32 %2338, %2339
  %2347 = xor i32 %2338, %2340
  %2348 = and i32 %2346, %2347
  %2349 = icmp slt i32 %2348, 0
  store i1 %2344, i1* %az
  store i1 %2345, i1* %cf
  store i1 %2349, i1* %of
  %2350 = icmp eq i32 %2340, 0
  store i1 %2350, i1* %zf
  %2351 = icmp slt i32 %2340, 0
  store i1 %2351, i1* %sf
  %2352 = trunc i32 %2340 to i8
  %2353 = call i8 @llvm.ctpop.i8(i8 %2352)
  %2354 = and i8 %2353, 1
  %2355 = icmp eq i8 %2354, 0
  store i1 %2355, i1* %pf
  %2356 = zext i32 %2340 to i64
  store i64 %2356, i64* %r15
  br label %block_81a8

block_81a8:                                       ; preds = %block_819a, %block_8195
  store volatile i64 33192, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 33197, i64* @assembly_address
  %2357 = load i64* %r15
  %2358 = trunc i64 %2357 to i32
  %2359 = zext i32 %2358 to i64
  store i64 %2359, i64* %rcx
  store volatile i64 33200, i64* @assembly_address
  %2360 = load i64* %rax
  %2361 = trunc i64 %2360 to i32
  %2362 = load i64* %rcx
  %2363 = trunc i64 %2362 to i8
  %2364 = zext i8 %2363 to i32
  %2365 = and i32 %2364, 31
  %2366 = load i1* %of
  %2367 = icmp eq i32 %2365, 0
  br i1 %2367, label %2385, label %2368

; <label>:2368                                    ; preds = %block_81a8
  %2369 = shl i32 %2361, %2365
  %2370 = icmp eq i32 %2369, 0
  store i1 %2370, i1* %zf
  %2371 = icmp slt i32 %2369, 0
  store i1 %2371, i1* %sf
  %2372 = trunc i32 %2369 to i8
  %2373 = call i8 @llvm.ctpop.i8(i8 %2372)
  %2374 = and i8 %2373, 1
  %2375 = icmp eq i8 %2374, 0
  store i1 %2375, i1* %pf
  %2376 = zext i32 %2369 to i64
  store i64 %2376, i64* %rax
  %2377 = sub i32 %2365, 1
  %2378 = shl i32 %2361, %2377
  %2379 = lshr i32 %2378, 31
  %2380 = trunc i32 %2379 to i1
  store i1 %2380, i1* %cf
  %2381 = lshr i32 %2369, 31
  %2382 = icmp ne i32 %2381, %2379
  %2383 = icmp eq i32 %2365, 1
  %2384 = select i1 %2383, i1 %2382, i1 %2366
  store i1 %2384, i1* %of
  br label %2385

; <label>:2385                                    ; preds = %block_81a8, %2368
  store volatile i64 33202, i64* @assembly_address
  %2386 = load i64* %rax
  %2387 = trunc i64 %2386 to i32
  %2388 = sub i32 %2387, 1
  %2389 = and i32 %2387, 15
  %2390 = sub i32 %2389, 1
  %2391 = icmp ugt i32 %2390, 15
  %2392 = icmp ult i32 %2387, 1
  %2393 = xor i32 %2387, 1
  %2394 = xor i32 %2387, %2388
  %2395 = and i32 %2393, %2394
  %2396 = icmp slt i32 %2395, 0
  store i1 %2391, i1* %az
  store i1 %2392, i1* %cf
  store i1 %2396, i1* %of
  %2397 = icmp eq i32 %2388, 0
  store i1 %2397, i1* %zf
  %2398 = icmp slt i32 %2388, 0
  store i1 %2398, i1* %sf
  %2399 = trunc i32 %2388 to i8
  %2400 = call i8 @llvm.ctpop.i8(i8 %2399)
  %2401 = and i8 %2400, 1
  %2402 = icmp eq i8 %2401, 0
  store i1 %2402, i1* %pf
  %2403 = zext i32 %2388 to i64
  store i64 %2403, i64* %rax
  store volatile i64 33205, i64* @assembly_address
  %2404 = load i64* %rax
  %2405 = trunc i64 %2404 to i32
  %2406 = load i64* %r12
  %2407 = trunc i64 %2406 to i32
  %2408 = and i32 %2405, %2407
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2409 = icmp eq i32 %2408, 0
  store i1 %2409, i1* %zf
  %2410 = icmp slt i32 %2408, 0
  store i1 %2410, i1* %sf
  %2411 = trunc i32 %2408 to i8
  %2412 = call i8 @llvm.ctpop.i8(i8 %2411)
  %2413 = and i8 %2412, 1
  %2414 = icmp eq i8 %2413, 0
  store i1 %2414, i1* %pf
  %2415 = zext i32 %2408 to i64
  store i64 %2415, i64* %rax
  store volatile i64 33208, i64* @assembly_address
  %2416 = load i64* %rax
  %2417 = trunc i64 %2416 to i32
  %2418 = zext i32 %2417 to i64
  store i64 %2418, i64* %rdx
  store volatile i64 33210, i64* @assembly_address
  %2419 = load i32* %stack_var_-1556
  %2420 = zext i32 %2419 to i64
  store i64 %2420, i64* %rax
  store volatile i64 33216, i64* @assembly_address
  %2421 = load i64* %rax
  %2422 = trunc i64 %2421 to i32
  %2423 = sext i32 %2422 to i64
  store i64 %2423, i64* %rax
  store volatile i64 33218, i64* @assembly_address
  %2424 = load i64* %rbp
  %2425 = load i64* %rax
  %2426 = mul i64 %2425, 4
  %2427 = add i64 %2424, -1424
  %2428 = add i64 %2427, %2426
  %2429 = inttoptr i64 %2428 to i32*
  %2430 = load i32* %2429
  %2431 = zext i32 %2430 to i64
  store i64 %2431, i64* %rax
  store volatile i64 33225, i64* @assembly_address
  %2432 = load i64* %rdx
  %2433 = trunc i64 %2432 to i32
  %2434 = load i64* %rax
  %2435 = trunc i64 %2434 to i32
  %2436 = sub i32 %2433, %2435
  %2437 = and i32 %2433, 15
  %2438 = and i32 %2435, 15
  %2439 = sub i32 %2437, %2438
  %2440 = icmp ugt i32 %2439, 15
  %2441 = icmp ult i32 %2433, %2435
  %2442 = xor i32 %2433, %2435
  %2443 = xor i32 %2433, %2436
  %2444 = and i32 %2442, %2443
  %2445 = icmp slt i32 %2444, 0
  store i1 %2440, i1* %az
  store i1 %2441, i1* %cf
  store i1 %2445, i1* %of
  %2446 = icmp eq i32 %2436, 0
  store i1 %2446, i1* %zf
  %2447 = icmp slt i32 %2436, 0
  store i1 %2447, i1* %sf
  %2448 = trunc i32 %2436 to i8
  %2449 = call i8 @llvm.ctpop.i8(i8 %2448)
  %2450 = and i8 %2449, 1
  %2451 = icmp eq i8 %2450, 0
  store i1 %2451, i1* %pf
  store volatile i64 33227, i64* @assembly_address
  %2452 = load i1* %zf
  %2453 = icmp eq i1 %2452, false
  br i1 %2453, label %block_819a, label %block_81cd

block_81cd:                                       ; preds = %2385, %block_7e24
  store volatile i64 33229, i64* @assembly_address
  %2454 = load i32* %stack_var_-1564
  %2455 = zext i32 %2454 to i64
  store i64 %2455, i64* %rax
  store volatile i64 33235, i64* @assembly_address
  %2456 = load i64* %rax
  %2457 = add i64 %2456, -1
  %2458 = trunc i64 %2457 to i32
  %2459 = zext i32 %2458 to i64
  store i64 %2459, i64* %rdx
  store volatile i64 33238, i64* @assembly_address
  %2460 = load i64* %rdx
  %2461 = trunc i64 %2460 to i32
  store i32 %2461, i32* %stack_var_-1564
  store volatile i64 33244, i64* @assembly_address
  %2462 = load i64* %rax
  %2463 = trunc i64 %2462 to i32
  %2464 = load i64* %rax
  %2465 = trunc i64 %2464 to i32
  %2466 = and i32 %2463, %2465
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2467 = icmp eq i32 %2466, 0
  store i1 %2467, i1* %zf
  %2468 = icmp slt i32 %2466, 0
  store i1 %2468, i1* %sf
  %2469 = trunc i32 %2466 to i8
  %2470 = call i8 @llvm.ctpop.i8(i8 %2469)
  %2471 = and i8 %2470, 1
  %2472 = icmp eq i8 %2471, 0
  store i1 %2472, i1* %pf
  store volatile i64 33246, i64* @assembly_address
  %2473 = load i1* %zf
  %2474 = icmp eq i1 %2473, false
  br i1 %2474, label %block_8042, label %block_81e4

block_81e4:                                       ; preds = %block_81cd
  store volatile i64 33252, i64* @assembly_address
  %2475 = load i32* %stack_var_-1628
  %2476 = add i32 %2475, 1
  %2477 = and i32 %2475, 15
  %2478 = add i32 %2477, 1
  %2479 = icmp ugt i32 %2478, 15
  %2480 = icmp ult i32 %2476, %2475
  %2481 = xor i32 %2475, %2476
  %2482 = xor i32 1, %2476
  %2483 = and i32 %2481, %2482
  %2484 = icmp slt i32 %2483, 0
  store i1 %2479, i1* %az
  store i1 %2480, i1* %cf
  store i1 %2484, i1* %of
  %2485 = icmp eq i32 %2476, 0
  store i1 %2485, i1* %zf
  %2486 = icmp slt i32 %2476, 0
  store i1 %2486, i1* %sf
  %2487 = trunc i32 %2476 to i8
  %2488 = call i8 @llvm.ctpop.i8(i8 %2487)
  %2489 = and i8 %2488, 1
  %2490 = icmp eq i8 %2489, 0
  store i1 %2490, i1* %pf
  store i32 %2476, i32* %stack_var_-1628
  br label %block_81eb

block_81eb:                                       ; preds = %block_81e4, %block_7dc6
  store volatile i64 33259, i64* @assembly_address
  %2491 = load i32* %stack_var_-1628
  %2492 = zext i32 %2491 to i64
  store i64 %2492, i64* %rax
  store volatile i64 33265, i64* @assembly_address
  %2493 = load i64* %rax
  %2494 = trunc i64 %2493 to i32
  %2495 = load i32* %stack_var_-1540
  %2496 = trunc i64 %2493 to i32
  store i32 %2496, i32* %13
  store i32 %2495, i32* %12
  %2497 = sub i32 %2494, %2495
  %2498 = and i32 %2494, 15
  %2499 = and i32 %2495, 15
  %2500 = sub i32 %2498, %2499
  %2501 = icmp ugt i32 %2500, 15
  %2502 = icmp ult i32 %2494, %2495
  %2503 = xor i32 %2494, %2495
  %2504 = xor i32 %2494, %2497
  %2505 = and i32 %2503, %2504
  %2506 = icmp slt i32 %2505, 0
  store i1 %2501, i1* %az
  store i1 %2502, i1* %cf
  store i1 %2506, i1* %of
  %2507 = icmp eq i32 %2497, 0
  store i1 %2507, i1* %zf
  %2508 = icmp slt i32 %2497, 0
  store i1 %2508, i1* %sf
  %2509 = trunc i32 %2497 to i8
  %2510 = call i8 @llvm.ctpop.i8(i8 %2509)
  %2511 = and i8 %2510, 1
  %2512 = icmp eq i8 %2511, 0
  store i1 %2512, i1* %pf
  store volatile i64 33271, i64* @assembly_address
  %2513 = load i32* %13
  %2514 = sext i32 %2513 to i64
  %2515 = load i32* %12
  %2516 = trunc i64 %2514 to i32
  %2517 = icmp sle i32 %2516, %2515
  br i1 %2517, label %block_7e24, label %block_81fd

block_81fd:                                       ; preds = %block_81eb
  store volatile i64 33277, i64* @assembly_address
  %2518 = load i32* %stack_var_-1548
  %2519 = and i32 %2518, 15
  %2520 = icmp ugt i32 %2519, 15
  %2521 = icmp ult i32 %2518, 0
  %2522 = xor i32 %2518, 0
  %2523 = and i32 %2522, 0
  %2524 = icmp slt i32 %2523, 0
  store i1 %2520, i1* %az
  store i1 %2521, i1* %cf
  store i1 %2524, i1* %of
  %2525 = icmp eq i32 %2518, 0
  store i1 %2525, i1* %zf
  %2526 = icmp slt i32 %2518, 0
  store i1 %2526, i1* %sf
  %2527 = trunc i32 %2518 to i8
  %2528 = call i8 @llvm.ctpop.i8(i8 %2527)
  %2529 = and i8 %2528, 1
  %2530 = icmp eq i8 %2529, 0
  store i1 %2530, i1* %pf
  store volatile i64 33284, i64* @assembly_address
  %2531 = load i1* %zf
  br i1 %2531, label %block_8216, label %block_8206

block_8206:                                       ; preds = %block_81fd
  store volatile i64 33286, i64* @assembly_address
  %2532 = load i32* %stack_var_-1540
  %2533 = sub i32 %2532, 1
  %2534 = and i32 %2532, 15
  %2535 = sub i32 %2534, 1
  %2536 = icmp ugt i32 %2535, 15
  %2537 = icmp ult i32 %2532, 1
  %2538 = xor i32 %2532, 1
  %2539 = xor i32 %2532, %2533
  %2540 = and i32 %2538, %2539
  %2541 = icmp slt i32 %2540, 0
  store i1 %2536, i1* %az
  store i1 %2537, i1* %cf
  store i1 %2541, i1* %of
  %2542 = icmp eq i32 %2533, 0
  store i1 %2542, i1* %zf
  %2543 = icmp slt i32 %2533, 0
  store i1 %2543, i1* %sf
  %2544 = trunc i32 %2533 to i8
  %2545 = call i8 @llvm.ctpop.i8(i8 %2544)
  %2546 = and i8 %2545, 1
  %2547 = icmp eq i8 %2546, 0
  store i1 %2547, i1* %pf
  store volatile i64 33293, i64* @assembly_address
  %2548 = load i1* %zf
  br i1 %2548, label %block_8216, label %block_820f

block_820f:                                       ; preds = %block_8206
  store volatile i64 33295, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 33300, i64* @assembly_address
  br label %block_821b

block_8216:                                       ; preds = %block_8206, %block_81fd
  store volatile i64 33302, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_821b

block_821b:                                       ; preds = %block_8216, %block_7f6d, %block_820f, %block_7ccd, %block_7d08, %block_7bb2, %block_7ba8
  store volatile i64 33307, i64* @assembly_address
  %2549 = load i64* %stack_var_-64
  store i64 %2549, i64* %rsi
  store volatile i64 33311, i64* @assembly_address
  %2550 = load i64* %rsi
  %2551 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %2552 = xor i64 %2550, %2551
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2553 = icmp eq i64 %2552, 0
  store i1 %2553, i1* %zf
  %2554 = icmp slt i64 %2552, 0
  store i1 %2554, i1* %sf
  %2555 = trunc i64 %2552 to i8
  %2556 = call i8 @llvm.ctpop.i8(i8 %2555)
  %2557 = and i8 %2556, 1
  %2558 = icmp eq i8 %2557, 0
  store i1 %2558, i1* %pf
  store i64 %2552, i64* %rsi
  store volatile i64 33320, i64* @assembly_address
  %2559 = load i1* %zf
  br i1 %2559, label %block_822f, label %block_822a

block_822a:                                       ; preds = %block_821b
  store volatile i64 33322, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_822f:                                       ; preds = %block_821b
  store volatile i64 33327, i64* @assembly_address
  %2560 = load i64* %rsp
  %2561 = add i64 %2560, 1592
  %2562 = and i64 %2560, 15
  %2563 = add i64 %2562, 8
  %2564 = icmp ugt i64 %2563, 15
  %2565 = icmp ult i64 %2561, %2560
  %2566 = xor i64 %2560, %2561
  %2567 = xor i64 1592, %2561
  %2568 = and i64 %2566, %2567
  %2569 = icmp slt i64 %2568, 0
  store i1 %2564, i1* %az
  store i1 %2565, i1* %cf
  store i1 %2569, i1* %of
  %2570 = icmp eq i64 %2561, 0
  store i1 %2570, i1* %zf
  %2571 = icmp slt i64 %2561, 0
  store i1 %2571, i1* %sf
  %2572 = trunc i64 %2561 to i8
  %2573 = call i8 @llvm.ctpop.i8(i8 %2572)
  %2574 = and i8 %2573, 1
  %2575 = icmp eq i8 %2574, 0
  store i1 %2575, i1* %pf
  %2576 = ptrtoint i64* %stack_var_-48 to i64
  store i64 %2576, i64* %rsp
  store volatile i64 33334, i64* @assembly_address
  %2577 = load i64* %stack_var_-48
  store i64 %2577, i64* %rbx
  %2578 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %2578, i64* %rsp
  store volatile i64 33335, i64* @assembly_address
  %2579 = load i64* %stack_var_-40
  store i64 %2579, i64* %r12
  %2580 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %2580, i64* %rsp
  store volatile i64 33337, i64* @assembly_address
  %2581 = load i64* %stack_var_-32
  store i64 %2581, i64* %r13
  %2582 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %2582, i64* %rsp
  store volatile i64 33339, i64* @assembly_address
  %2583 = load i64* %stack_var_-24
  store i64 %2583, i64* %r14
  %2584 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %2584, i64* %rsp
  store volatile i64 33341, i64* @assembly_address
  %2585 = load i64* %stack_var_-16
  store i64 %2585, i64* %r15
  %2586 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2586, i64* %rsp
  store volatile i64 33343, i64* @assembly_address
  %2587 = load i64* %stack_var_-8
  store i64 %2587, i64* %rbp
  %2588 = ptrtoint i64* %stack_var_0 to i64
  store i64 %2588, i64* %rsp
  store volatile i64 33344, i64* @assembly_address
  %2589 = load i64* %rax
  ret i64 %2589
}

declare i64 @193(i64*, i32, i64, i64*, i64*, i64*, i32*)

declare i64 @194(i64*, i64, i32, i64*, i64*, i64*, i32*)

declare i64 @195(i64*, i64, i64, i16*, i64*, i64*, i32*)

declare i64 @196(i64*, i64, i64, i64*, i16*, i64*, i32*)

declare i64 @197(i64*, i64, i64, i64*, i64*, i64*, i32*)

define i64 @huft_free(i64* %arg1) {
block_8241:
  %r12 = alloca i64
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 33345, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 33346, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 33349, i64* @assembly_address
  %4 = load i64* %r12
  store i64 %4, i64* %stack_var_-16
  %5 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %5, i64* %rsp
  store volatile i64 33351, i64* @assembly_address
  %6 = load i64* %rbx
  store i64 %6, i64* %stack_var_-24
  %7 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %7, i64* %rsp
  store volatile i64 33352, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 16
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 16
  %13 = xor i64 %8, 16
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %23, i64* %rsp
  store volatile i64 33356, i64* @assembly_address
  %24 = load i64* %rdi
  store i64 %24, i64* %stack_var_-32
  store volatile i64 33360, i64* @assembly_address
  %25 = load i64* %stack_var_-32
  store i64 %25, i64* %rbx
  store volatile i64 33364, i64* @assembly_address
  br label %block_8269

block_8256:                                       ; preds = %block_8269
  store volatile i64 33366, i64* @assembly_address
  %26 = load i64* %rbx
  %27 = sub i64 %26, 16
  %28 = and i64 %26, 15
  %29 = icmp ugt i64 %28, 15
  %30 = icmp ult i64 %26, 16
  %31 = xor i64 %26, 16
  %32 = xor i64 %26, %27
  %33 = and i64 %31, %32
  %34 = icmp slt i64 %33, 0
  store i1 %29, i1* %az
  store i1 %30, i1* %cf
  store i1 %34, i1* %of
  %35 = icmp eq i64 %27, 0
  store i1 %35, i1* %zf
  %36 = icmp slt i64 %27, 0
  store i1 %36, i1* %sf
  %37 = trunc i64 %27 to i8
  %38 = call i8 @llvm.ctpop.i8(i8 %37)
  %39 = and i8 %38, 1
  %40 = icmp eq i8 %39, 0
  store i1 %40, i1* %pf
  store i64 %27, i64* %rbx
  store volatile i64 33370, i64* @assembly_address
  %41 = load i64* %rbx
  %42 = add i64 %41, 8
  %43 = inttoptr i64 %42 to i64*
  %44 = load i64* %43
  store i64 %44, i64* %r12
  store volatile i64 33374, i64* @assembly_address
  %45 = load i64* %rbx
  store i64 %45, i64* %rdi
  store volatile i64 33377, i64* @assembly_address
  %46 = load i64* %rdi
  %47 = inttoptr i64 %46 to i64*
  call void @free(i64* %47)
  store volatile i64 33382, i64* @assembly_address
  %48 = load i64* %r12
  store i64 %48, i64* %rbx
  br label %block_8269

block_8269:                                       ; preds = %block_8256, %block_8241
  store volatile i64 33385, i64* @assembly_address
  %49 = load i64* %rbx
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %50 = icmp eq i64 %49, 0
  store i1 %50, i1* %zf
  %51 = icmp slt i64 %49, 0
  store i1 %51, i1* %sf
  %52 = trunc i64 %49 to i8
  %53 = call i8 @llvm.ctpop.i8(i8 %52)
  %54 = and i8 %53, 1
  %55 = icmp eq i8 %54, 0
  store i1 %55, i1* %pf
  store volatile i64 33388, i64* @assembly_address
  %56 = load i1* %zf
  %57 = icmp eq i1 %56, false
  br i1 %57, label %block_8256, label %block_826e

block_826e:                                       ; preds = %block_8269
  store volatile i64 33390, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 33395, i64* @assembly_address
  %58 = load i64* %rsp
  %59 = add i64 %58, 16
  %60 = and i64 %58, 15
  %61 = icmp ugt i64 %60, 15
  %62 = icmp ult i64 %59, %58
  %63 = xor i64 %58, %59
  %64 = xor i64 16, %59
  %65 = and i64 %63, %64
  %66 = icmp slt i64 %65, 0
  store i1 %61, i1* %az
  store i1 %62, i1* %cf
  store i1 %66, i1* %of
  %67 = icmp eq i64 %59, 0
  store i1 %67, i1* %zf
  %68 = icmp slt i64 %59, 0
  store i1 %68, i1* %sf
  %69 = trunc i64 %59 to i8
  %70 = call i8 @llvm.ctpop.i8(i8 %69)
  %71 = and i8 %70, 1
  %72 = icmp eq i8 %71, 0
  store i1 %72, i1* %pf
  %73 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %73, i64* %rsp
  store volatile i64 33399, i64* @assembly_address
  %74 = load i64* %stack_var_-24
  store i64 %74, i64* %rbx
  %75 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %75, i64* %rsp
  store volatile i64 33400, i64* @assembly_address
  %76 = load i64* %stack_var_-16
  store i64 %76, i64* %r12
  %77 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %77, i64* %rsp
  store volatile i64 33402, i64* @assembly_address
  %78 = load i64* %stack_var_-8
  store i64 %78, i64* %rbp
  %79 = ptrtoint i64* %stack_var_0 to i64
  store i64 %79, i64* %rsp
  store volatile i64 33403, i64* @assembly_address
  %80 = load i64* %rax
  ret i64 %80
}

declare i64 @198(i64)

define i64 @inflate_codes(i8* %arg1, i64 %arg2, i32 %arg3, i32 %arg4) {
block_827c:
  %r13 = alloca i64
  %r12 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg4 to i64
  store i64 %0, i64* %rcx
  %1 = sext i32 %arg3 to i64
  store i64 %1, i64* %rdx
  store i64 %arg2, i64* %rsi
  %2 = ptrtoint i8* %arg1 to i64
  store i64 %2, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-64 = alloca i64*
  %3 = alloca i32
  %stack_var_-68 = alloca i64*
  %4 = alloca i32
  %stack_var_-48 = alloca i8*
  %5 = alloca i64
  %stack_var_-52 = alloca i32
  %stack_var_-56 = alloca i32
  %stack_var_-60 = alloca i64*
  %6 = alloca i32
  %stack_var_-96 = alloca i32
  %stack_var_-92 = alloca i32
  %stack_var_-88 = alloca i64
  %stack_var_-80 = alloca i8*
  %7 = alloca i64
  %stack_var_-104 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 33404, i64* @assembly_address
  %8 = load i64* %rbp
  store i64 %8, i64* %stack_var_-8
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rsp
  store volatile i64 33405, i64* @assembly_address
  %10 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %10, i64* %rbp
  store volatile i64 33408, i64* @assembly_address
  %11 = load i64* %r13
  store i64 %11, i64* %stack_var_-16
  %12 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %12, i64* %rsp
  store volatile i64 33410, i64* @assembly_address
  %13 = load i64* %r12
  store i64 %13, i64* %stack_var_-24
  %14 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %14, i64* %rsp
  store volatile i64 33412, i64* @assembly_address
  %15 = load i64* %rbx
  store i64 %15, i64* %stack_var_-32
  %16 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %16, i64* %rsp
  store volatile i64 33413, i64* @assembly_address
  %17 = load i64* %rsp
  %18 = sub i64 %17, 72
  %19 = and i64 %17, 15
  %20 = sub i64 %19, 8
  %21 = icmp ugt i64 %20, 15
  %22 = icmp ult i64 %17, 72
  %23 = xor i64 %17, 72
  %24 = xor i64 %17, %18
  %25 = and i64 %23, %24
  %26 = icmp slt i64 %25, 0
  store i1 %21, i1* %az
  store i1 %22, i1* %cf
  store i1 %26, i1* %of
  %27 = icmp eq i64 %18, 0
  store i1 %27, i1* %zf
  %28 = icmp slt i64 %18, 0
  store i1 %28, i1* %sf
  %29 = trunc i64 %18 to i8
  %30 = call i8 @llvm.ctpop.i8(i8 %29)
  %31 = and i8 %30, 1
  %32 = icmp eq i8 %31, 0
  store i1 %32, i1* %pf
  %33 = ptrtoint i64* %stack_var_-104 to i64
  store i64 %33, i64* %rsp
  store volatile i64 33417, i64* @assembly_address
  %34 = load i64* %rdi
  %35 = inttoptr i64 %34 to i8*
  store i8* %35, i8** %stack_var_-80
  store volatile i64 33421, i64* @assembly_address
  %36 = load i64* %rsi
  store i64 %36, i64* %stack_var_-88
  store volatile i64 33425, i64* @assembly_address
  %37 = load i64* %rdx
  %38 = trunc i64 %37 to i32
  store i32 %38, i32* %stack_var_-92
  store volatile i64 33428, i64* @assembly_address
  %39 = load i64* %rcx
  %40 = trunc i64 %39 to i32
  store i32 %40, i32* %stack_var_-96
  store volatile i64 33431, i64* @assembly_address
  %41 = load i64* @global_var_216f98
  store i64 %41, i64* %r13
  store volatile i64 33438, i64* @assembly_address
  %42 = load i32* bitcast (i64* @global_var_216fa0 to i32*)
  %43 = zext i32 %42 to i64
  store i64 %43, i64* %rbx
  store volatile i64 33444, i64* @assembly_address
  %44 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 33450, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = inttoptr i32 %47 to i64*
  store i64* %48, i64** %stack_var_-60
  store volatile i64 33453, i64* @assembly_address
  %49 = load i32* %stack_var_-92
  %50 = zext i32 %49 to i64
  store i64 %50, i64* %rax
  store volatile i64 33456, i64* @assembly_address
  %51 = load i64* %rax
  %52 = trunc i64 %51 to i32
  %53 = sext i32 %52 to i64
  store i64 %53, i64* %rax
  store volatile i64 33458, i64* @assembly_address
  %54 = load i64* %rax
  %55 = load i64* %rax
  %56 = mul i64 %55, 1
  %57 = add i64 %54, %56
  store i64 %57, i64* %rdx
  store volatile i64 33462, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162e0 to i64), i64* %rax
  store volatile i64 33469, i64* @assembly_address
  %58 = load i64* %rdx
  %59 = load i64* %rax
  %60 = mul i64 %59, 1
  %61 = add i64 %58, %60
  %62 = inttoptr i64 %61 to i16*
  %63 = load i16* %62
  %64 = zext i16 %63 to i64
  store i64 %64, i64* %rax
  store volatile i64 33473, i64* @assembly_address
  %65 = load i64* %rax
  %66 = trunc i64 %65 to i16
  %67 = zext i16 %66 to i64
  store i64 %67, i64* %rax
  store volatile i64 33476, i64* @assembly_address
  %68 = load i64* %rax
  %69 = trunc i64 %68 to i32
  store i32 %69, i32* %stack_var_-56
  store volatile i64 33479, i64* @assembly_address
  %70 = load i32* %stack_var_-96
  %71 = zext i32 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 33482, i64* @assembly_address
  %72 = load i64* %rax
  %73 = trunc i64 %72 to i32
  %74 = sext i32 %73 to i64
  store i64 %74, i64* %rax
  store volatile i64 33484, i64* @assembly_address
  %75 = load i64* %rax
  %76 = load i64* %rax
  %77 = mul i64 %76, 1
  %78 = add i64 %75, %77
  store i64 %78, i64* %rdx
  store volatile i64 33488, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162e0 to i64), i64* %rax
  store volatile i64 33495, i64* @assembly_address
  %79 = load i64* %rdx
  %80 = load i64* %rax
  %81 = mul i64 %80, 1
  %82 = add i64 %79, %81
  %83 = inttoptr i64 %82 to i16*
  %84 = load i16* %83
  %85 = zext i16 %84 to i64
  store i64 %85, i64* %rax
  store volatile i64 33499, i64* @assembly_address
  %86 = load i64* %rax
  %87 = trunc i64 %86 to i16
  %88 = zext i16 %87 to i64
  store i64 %88, i64* %rax
  store volatile i64 33502, i64* @assembly_address
  %89 = load i64* %rax
  %90 = trunc i64 %89 to i32
  store i32 %90, i32* %stack_var_-52
  store volatile i64 33505, i64* @assembly_address
  br label %block_8335

block_82e3:                                       ; preds = %block_8335
  store volatile i64 33507, i64* @assembly_address
  %91 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %92 = zext i32 %91 to i64
  store i64 %92, i64* %rdx
  store volatile i64 33513, i64* @assembly_address
  %93 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %94 = zext i32 %93 to i64
  store i64 %94, i64* %rax
  store volatile i64 33519, i64* @assembly_address
  %95 = load i64* %rdx
  %96 = trunc i64 %95 to i32
  %97 = load i64* %rax
  %98 = trunc i64 %97 to i32
  %99 = sub i32 %96, %98
  %100 = and i32 %96, 15
  %101 = and i32 %98, 15
  %102 = sub i32 %100, %101
  %103 = icmp ugt i32 %102, 15
  %104 = icmp ult i32 %96, %98
  %105 = xor i32 %96, %98
  %106 = xor i32 %96, %99
  %107 = and i32 %105, %106
  %108 = icmp slt i32 %107, 0
  store i1 %103, i1* %az
  store i1 %104, i1* %cf
  store i1 %108, i1* %of
  %109 = icmp eq i32 %99, 0
  store i1 %109, i1* %zf
  %110 = icmp slt i32 %99, 0
  store i1 %110, i1* %sf
  %111 = trunc i32 %99 to i8
  %112 = call i8 @llvm.ctpop.i8(i8 %111)
  %113 = and i8 %112, 1
  %114 = icmp eq i8 %113, 0
  store i1 %114, i1* %pf
  store volatile i64 33521, i64* @assembly_address
  %115 = load i1* %cf
  %116 = icmp eq i1 %115, false
  br i1 %116, label %block_8314, label %block_82f3

block_82f3:                                       ; preds = %block_82e3
  store volatile i64 33523, i64* @assembly_address
  %117 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %118 = zext i32 %117 to i64
  store i64 %118, i64* %rax
  store volatile i64 33529, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 33532, i64* @assembly_address
  %119 = load i64* %rdx
  %120 = trunc i64 %119 to i32
  store i32 %120, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 33538, i64* @assembly_address
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i32
  %123 = zext i32 %122 to i64
  store i64 %123, i64* %rdx
  store volatile i64 33540, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 33547, i64* @assembly_address
  %124 = load i64* %rdx
  %125 = load i64* %rax
  %126 = mul i64 %125, 1
  %127 = add i64 %124, %126
  %128 = inttoptr i64 %127 to i8*
  %129 = load i8* %128
  %130 = zext i8 %129 to i64
  store i64 %130, i64* %rax
  store volatile i64 33551, i64* @assembly_address
  %131 = load i64* %rax
  %132 = trunc i64 %131 to i8
  %133 = zext i8 %132 to i64
  store i64 %133, i64* %rax
  store volatile i64 33554, i64* @assembly_address
  br label %block_832a

block_8314:                                       ; preds = %block_82e3
  store volatile i64 33556, i64* @assembly_address
  %134 = load i64** %stack_var_-60
  %135 = ptrtoint i64* %134 to i32
  %136 = zext i32 %135 to i64
  store i64 %136, i64* %rax
  store volatile i64 33559, i64* @assembly_address
  %137 = load i64* %rax
  %138 = trunc i64 %137 to i32
  store i32 %138, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 33565, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 33570, i64* @assembly_address
  %139 = load i64* %rdi
  %140 = trunc i64 %139 to i32
  %141 = call i64 @fill_inbuf(i32 %140)
  store i64 %141, i64* %rax
  store i64 %141, i64* %rax
  store volatile i64 33575, i64* @assembly_address
  %142 = load i64* %rax
  %143 = trunc i64 %142 to i8
  %144 = zext i8 %143 to i64
  store i64 %144, i64* %rax
  br label %block_832a

block_832a:                                       ; preds = %block_8314, %block_82f3
  store volatile i64 33578, i64* @assembly_address
  %145 = load i64* %rbx
  %146 = trunc i64 %145 to i32
  %147 = zext i32 %146 to i64
  store i64 %147, i64* %rcx
  store volatile i64 33580, i64* @assembly_address
  %148 = load i64* %rax
  %149 = load i64* %rcx
  %150 = trunc i64 %149 to i8
  %151 = zext i8 %150 to i64
  %152 = and i64 %151, 63
  %153 = load i1* %of
  %154 = icmp eq i64 %152, 0
  br i1 %154, label %171, label %155

; <label>:155                                     ; preds = %block_832a
  %156 = shl i64 %148, %152
  %157 = icmp eq i64 %156, 0
  store i1 %157, i1* %zf
  %158 = icmp slt i64 %156, 0
  store i1 %158, i1* %sf
  %159 = trunc i64 %156 to i8
  %160 = call i8 @llvm.ctpop.i8(i8 %159)
  %161 = and i8 %160, 1
  %162 = icmp eq i8 %161, 0
  store i1 %162, i1* %pf
  store i64 %156, i64* %rax
  %163 = sub i64 %152, 1
  %164 = shl i64 %148, %163
  %165 = lshr i64 %164, 63
  %166 = trunc i64 %165 to i1
  store i1 %166, i1* %cf
  %167 = lshr i64 %156, 63
  %168 = icmp ne i64 %167, %165
  %169 = icmp eq i64 %152, 1
  %170 = select i1 %169, i1 %168, i1 %153
  store i1 %170, i1* %of
  br label %171

; <label>:171                                     ; preds = %block_832a, %155
  store volatile i64 33583, i64* @assembly_address
  %172 = load i64* %r13
  %173 = load i64* %rax
  %174 = or i64 %172, %173
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %175 = icmp eq i64 %174, 0
  store i1 %175, i1* %zf
  %176 = icmp slt i64 %174, 0
  store i1 %176, i1* %sf
  %177 = trunc i64 %174 to i8
  %178 = call i8 @llvm.ctpop.i8(i8 %177)
  %179 = and i8 %178, 1
  %180 = icmp eq i8 %179, 0
  store i1 %180, i1* %pf
  store i64 %174, i64* %r13
  store volatile i64 33586, i64* @assembly_address
  %181 = load i64* %rbx
  %182 = trunc i64 %181 to i32
  %183 = add i32 %182, 8
  %184 = and i32 %182, 15
  %185 = add i32 %184, 8
  %186 = icmp ugt i32 %185, 15
  %187 = icmp ult i32 %183, %182
  %188 = xor i32 %182, %183
  %189 = xor i32 8, %183
  %190 = and i32 %188, %189
  %191 = icmp slt i32 %190, 0
  store i1 %186, i1* %az
  store i1 %187, i1* %cf
  store i1 %191, i1* %of
  %192 = icmp eq i32 %183, 0
  store i1 %192, i1* %zf
  %193 = icmp slt i32 %183, 0
  store i1 %193, i1* %sf
  %194 = trunc i32 %183 to i8
  %195 = call i8 @llvm.ctpop.i8(i8 %194)
  %196 = and i8 %195, 1
  %197 = icmp eq i8 %196, 0
  store i1 %197, i1* %pf
  %198 = zext i32 %183 to i64
  store i64 %198, i64* %rbx
  br label %block_8335

block_8335:                                       ; preds = %block_8824, %block_8486, %block_845c, %171, %block_827c
  store volatile i64 33589, i64* @assembly_address
  %199 = load i32* %stack_var_-92
  %200 = zext i32 %199 to i64
  store i64 %200, i64* %rax
  store volatile i64 33592, i64* @assembly_address
  %201 = load i64* %rbx
  %202 = trunc i64 %201 to i32
  %203 = load i64* %rax
  %204 = trunc i64 %203 to i32
  %205 = sub i32 %202, %204
  %206 = and i32 %202, 15
  %207 = and i32 %204, 15
  %208 = sub i32 %206, %207
  %209 = icmp ugt i32 %208, 15
  %210 = icmp ult i32 %202, %204
  %211 = xor i32 %202, %204
  %212 = xor i32 %202, %205
  %213 = and i32 %211, %212
  %214 = icmp slt i32 %213, 0
  store i1 %209, i1* %az
  store i1 %210, i1* %cf
  store i1 %214, i1* %of
  %215 = icmp eq i32 %205, 0
  store i1 %215, i1* %zf
  %216 = icmp slt i32 %205, 0
  store i1 %216, i1* %sf
  %217 = trunc i32 %205 to i8
  %218 = call i8 @llvm.ctpop.i8(i8 %217)
  %219 = and i8 %218, 1
  %220 = icmp eq i8 %219, 0
  store i1 %220, i1* %pf
  store volatile i64 33594, i64* @assembly_address
  %221 = load i1* %cf
  br i1 %221, label %block_82e3, label %block_833c

block_833c:                                       ; preds = %block_8335
  store volatile i64 33596, i64* @assembly_address
  %222 = load i64* %r13
  %223 = trunc i64 %222 to i32
  %224 = zext i32 %223 to i64
  store i64 %224, i64* %rax
  store volatile i64 33599, i64* @assembly_address
  %225 = load i64* %rax
  %226 = trunc i64 %225 to i32
  %227 = load i32* %stack_var_-56
  %228 = and i32 %226, %227
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %229 = icmp eq i32 %228, 0
  store i1 %229, i1* %zf
  %230 = icmp slt i32 %228, 0
  store i1 %230, i1* %sf
  %231 = trunc i32 %228 to i8
  %232 = call i8 @llvm.ctpop.i8(i8 %231)
  %233 = and i8 %232, 1
  %234 = icmp eq i8 %233, 0
  store i1 %234, i1* %pf
  %235 = zext i32 %228 to i64
  store i64 %235, i64* %rax
  store volatile i64 33602, i64* @assembly_address
  %236 = load i64* %rax
  %237 = trunc i64 %236 to i32
  %238 = zext i32 %237 to i64
  store i64 %238, i64* %rax
  store volatile i64 33604, i64* @assembly_address
  %239 = load i64* %rax
  %240 = load i1* %of
  %241 = shl i64 %239, 4
  %242 = icmp eq i64 %241, 0
  store i1 %242, i1* %zf
  %243 = icmp slt i64 %241, 0
  store i1 %243, i1* %sf
  %244 = trunc i64 %241 to i8
  %245 = call i8 @llvm.ctpop.i8(i8 %244)
  %246 = and i8 %245, 1
  %247 = icmp eq i8 %246, 0
  store i1 %247, i1* %pf
  store i64 %241, i64* %rax
  %248 = shl i64 %239, 3
  %249 = lshr i64 %248, 63
  %250 = trunc i64 %249 to i1
  store i1 %250, i1* %cf
  %251 = lshr i64 %241, 63
  %252 = icmp ne i64 %251, %249
  %253 = select i1 false, i1 %252, i1 %240
  store i1 %253, i1* %of
  store volatile i64 33608, i64* @assembly_address
  %254 = load i64* %rax
  store i64 %254, i64* %rdx
  store volatile i64 33611, i64* @assembly_address
  %255 = load i8** %stack_var_-80
  %256 = ptrtoint i8* %255 to i64
  store i64 %256, i64* %rax
  store volatile i64 33615, i64* @assembly_address
  %257 = load i64* %rax
  %258 = load i64* %rdx
  %259 = add i64 %257, %258
  %260 = and i64 %257, 15
  %261 = and i64 %258, 15
  %262 = add i64 %260, %261
  %263 = icmp ugt i64 %262, 15
  %264 = icmp ult i64 %259, %257
  %265 = xor i64 %257, %259
  %266 = xor i64 %258, %259
  %267 = and i64 %265, %266
  %268 = icmp slt i64 %267, 0
  store i1 %263, i1* %az
  store i1 %264, i1* %cf
  store i1 %268, i1* %of
  %269 = icmp eq i64 %259, 0
  store i1 %269, i1* %zf
  %270 = icmp slt i64 %259, 0
  store i1 %270, i1* %sf
  %271 = trunc i64 %259 to i8
  %272 = call i8 @llvm.ctpop.i8(i8 %271)
  %273 = and i8 %272, 1
  %274 = icmp eq i8 %273, 0
  store i1 %274, i1* %pf
  store i64 %259, i64* %rax
  store volatile i64 33618, i64* @assembly_address
  %275 = load i64* %rax
  %276 = inttoptr i64 %275 to i8*
  store i8* %276, i8** %stack_var_-48
  store volatile i64 33622, i64* @assembly_address
  %277 = load i8** %stack_var_-48
  %278 = ptrtoint i8* %277 to i64
  store i64 %278, i64* %rax
  store volatile i64 33626, i64* @assembly_address
  %279 = load i64* %rax
  %280 = inttoptr i64 %279 to i8*
  %281 = load i8* %280
  %282 = zext i8 %281 to i64
  store i64 %282, i64* %rax
  store volatile i64 33629, i64* @assembly_address
  %283 = load i64* %rax
  %284 = trunc i64 %283 to i8
  %285 = zext i8 %284 to i64
  store i64 %285, i64* %r12
  store volatile i64 33633, i64* @assembly_address
  %286 = load i64* %r12
  %287 = trunc i64 %286 to i32
  %288 = sub i32 %287, 16
  %289 = and i32 %287, 15
  %290 = icmp ugt i32 %289, 15
  %291 = icmp ult i32 %287, 16
  %292 = xor i32 %287, 16
  %293 = xor i32 %287, %288
  %294 = and i32 %292, %293
  %295 = icmp slt i32 %294, 0
  store i1 %290, i1* %az
  store i1 %291, i1* %cf
  store i1 %295, i1* %of
  %296 = icmp eq i32 %288, 0
  store i1 %296, i1* %zf
  %297 = icmp slt i32 %288, 0
  store i1 %297, i1* %sf
  %298 = trunc i32 %288 to i8
  %299 = call i8 @llvm.ctpop.i8(i8 %298)
  %300 = and i8 %299, 1
  %301 = icmp eq i8 %300, 0
  store i1 %301, i1* %pf
  store volatile i64 33637, i64* @assembly_address
  %302 = load i1* %cf
  %303 = load i1* %zf
  %304 = or i1 %302, %303
  br i1 %304, label %block_8439, label %block_836b

block_836b:                                       ; preds = %block_83f5, %block_833c
  store volatile i64 33643, i64* @assembly_address
  %305 = load i64* %r12
  %306 = trunc i64 %305 to i32
  %307 = sub i32 %306, 99
  %308 = and i32 %306, 15
  %309 = sub i32 %308, 3
  %310 = icmp ugt i32 %309, 15
  %311 = icmp ult i32 %306, 99
  %312 = xor i32 %306, 99
  %313 = xor i32 %306, %307
  %314 = and i32 %312, %313
  %315 = icmp slt i32 %314, 0
  store i1 %310, i1* %az
  store i1 %311, i1* %cf
  store i1 %315, i1* %of
  %316 = icmp eq i32 %307, 0
  store i1 %316, i1* %zf
  %317 = icmp slt i32 %307, 0
  store i1 %317, i1* %sf
  %318 = trunc i32 %307 to i8
  %319 = call i8 @llvm.ctpop.i8(i8 %318)
  %320 = and i8 %319, 1
  %321 = icmp eq i8 %320, 0
  store i1 %321, i1* %pf
  store volatile i64 33647, i64* @assembly_address
  %322 = load i1* %zf
  %323 = icmp eq i1 %322, false
  br i1 %323, label %block_837b, label %block_8371

block_8371:                                       ; preds = %block_836b
  store volatile i64 33649, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 33654, i64* @assembly_address
  br label %block_8845

block_837b:                                       ; preds = %block_836b
  store volatile i64 33659, i64* @assembly_address
  %324 = load i8** %stack_var_-48
  %325 = ptrtoint i8* %324 to i64
  store i64 %325, i64* %rax
  store volatile i64 33663, i64* @assembly_address
  %326 = load i64* %rax
  %327 = add i64 %326, 1
  %328 = inttoptr i64 %327 to i8*
  %329 = load i8* %328
  %330 = zext i8 %329 to i64
  store i64 %330, i64* %rax
  store volatile i64 33667, i64* @assembly_address
  %331 = load i64* %rax
  %332 = trunc i64 %331 to i8
  %333 = zext i8 %332 to i64
  store i64 %333, i64* %rax
  store volatile i64 33670, i64* @assembly_address
  %334 = load i64* %rax
  %335 = trunc i64 %334 to i32
  %336 = zext i32 %335 to i64
  store i64 %336, i64* %rcx
  store volatile i64 33672, i64* @assembly_address
  %337 = load i64* %r13
  %338 = load i64* %rcx
  %339 = trunc i64 %338 to i8
  %340 = zext i8 %339 to i64
  %341 = and i64 %340, 63
  %342 = load i1* %of
  %343 = icmp eq i64 %341, 0
  br i1 %343, label %359, label %344

; <label>:344                                     ; preds = %block_837b
  %345 = lshr i64 %337, %341
  %346 = icmp eq i64 %345, 0
  store i1 %346, i1* %zf
  %347 = icmp slt i64 %345, 0
  store i1 %347, i1* %sf
  %348 = trunc i64 %345 to i8
  %349 = call i8 @llvm.ctpop.i8(i8 %348)
  %350 = and i8 %349, 1
  %351 = icmp eq i8 %350, 0
  store i1 %351, i1* %pf
  store i64 %345, i64* %r13
  %352 = sub i64 %341, 1
  %353 = shl i64 1, %352
  %354 = and i64 %353, %337
  %355 = icmp ne i64 %354, 0
  store i1 %355, i1* %cf
  %356 = icmp eq i64 %341, 1
  %357 = icmp slt i64 %337, 0
  %358 = select i1 %356, i1 %357, i1 %342
  store i1 %358, i1* %of
  br label %359

; <label>:359                                     ; preds = %block_837b, %344
  store volatile i64 33675, i64* @assembly_address
  %360 = load i8** %stack_var_-48
  %361 = ptrtoint i8* %360 to i64
  store i64 %361, i64* %rax
  store volatile i64 33679, i64* @assembly_address
  %362 = load i64* %rax
  %363 = add i64 %362, 1
  %364 = inttoptr i64 %363 to i8*
  %365 = load i8* %364
  %366 = zext i8 %365 to i64
  store i64 %366, i64* %rax
  store volatile i64 33683, i64* @assembly_address
  %367 = load i64* %rax
  %368 = trunc i64 %367 to i8
  %369 = zext i8 %368 to i64
  store i64 %369, i64* %rax
  store volatile i64 33686, i64* @assembly_address
  %370 = load i64* %rbx
  %371 = trunc i64 %370 to i32
  %372 = load i64* %rax
  %373 = trunc i64 %372 to i32
  %374 = sub i32 %371, %373
  %375 = and i32 %371, 15
  %376 = and i32 %373, 15
  %377 = sub i32 %375, %376
  %378 = icmp ugt i32 %377, 15
  %379 = icmp ult i32 %371, %373
  %380 = xor i32 %371, %373
  %381 = xor i32 %371, %374
  %382 = and i32 %380, %381
  %383 = icmp slt i32 %382, 0
  store i1 %378, i1* %az
  store i1 %379, i1* %cf
  store i1 %383, i1* %of
  %384 = icmp eq i32 %374, 0
  store i1 %384, i1* %zf
  %385 = icmp slt i32 %374, 0
  store i1 %385, i1* %sf
  %386 = trunc i32 %374 to i8
  %387 = call i8 @llvm.ctpop.i8(i8 %386)
  %388 = and i8 %387, 1
  %389 = icmp eq i8 %388, 0
  store i1 %389, i1* %pf
  %390 = zext i32 %374 to i64
  store i64 %390, i64* %rbx
  store volatile i64 33688, i64* @assembly_address
  %391 = load i64* %r12
  %392 = trunc i64 %391 to i32
  %393 = sub i32 %392, 16
  %394 = and i32 %392, 15
  %395 = icmp ugt i32 %394, 15
  %396 = icmp ult i32 %392, 16
  %397 = xor i32 %392, 16
  %398 = xor i32 %392, %393
  %399 = and i32 %397, %398
  %400 = icmp slt i32 %399, 0
  store i1 %395, i1* %az
  store i1 %396, i1* %cf
  store i1 %400, i1* %of
  %401 = icmp eq i32 %393, 0
  store i1 %401, i1* %zf
  %402 = icmp slt i32 %393, 0
  store i1 %402, i1* %sf
  %403 = trunc i32 %393 to i8
  %404 = call i8 @llvm.ctpop.i8(i8 %403)
  %405 = and i8 %404, 1
  %406 = icmp eq i8 %405, 0
  store i1 %406, i1* %pf
  %407 = zext i32 %393 to i64
  store i64 %407, i64* %r12
  store volatile i64 33692, i64* @assembly_address
  br label %block_83f0

block_839e:                                       ; preds = %block_83f0
  store volatile i64 33694, i64* @assembly_address
  %408 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %409 = zext i32 %408 to i64
  store i64 %409, i64* %rdx
  store volatile i64 33700, i64* @assembly_address
  %410 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %411 = zext i32 %410 to i64
  store i64 %411, i64* %rax
  store volatile i64 33706, i64* @assembly_address
  %412 = load i64* %rdx
  %413 = trunc i64 %412 to i32
  %414 = load i64* %rax
  %415 = trunc i64 %414 to i32
  %416 = sub i32 %413, %415
  %417 = and i32 %413, 15
  %418 = and i32 %415, 15
  %419 = sub i32 %417, %418
  %420 = icmp ugt i32 %419, 15
  %421 = icmp ult i32 %413, %415
  %422 = xor i32 %413, %415
  %423 = xor i32 %413, %416
  %424 = and i32 %422, %423
  %425 = icmp slt i32 %424, 0
  store i1 %420, i1* %az
  store i1 %421, i1* %cf
  store i1 %425, i1* %of
  %426 = icmp eq i32 %416, 0
  store i1 %426, i1* %zf
  %427 = icmp slt i32 %416, 0
  store i1 %427, i1* %sf
  %428 = trunc i32 %416 to i8
  %429 = call i8 @llvm.ctpop.i8(i8 %428)
  %430 = and i8 %429, 1
  %431 = icmp eq i8 %430, 0
  store i1 %431, i1* %pf
  store volatile i64 33708, i64* @assembly_address
  %432 = load i1* %cf
  %433 = icmp eq i1 %432, false
  br i1 %433, label %block_83cf, label %block_83ae

block_83ae:                                       ; preds = %block_839e
  store volatile i64 33710, i64* @assembly_address
  %434 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %435 = zext i32 %434 to i64
  store i64 %435, i64* %rax
  store volatile i64 33716, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 33719, i64* @assembly_address
  %436 = load i64* %rdx
  %437 = trunc i64 %436 to i32
  store i32 %437, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 33725, i64* @assembly_address
  %438 = load i64* %rax
  %439 = trunc i64 %438 to i32
  %440 = zext i32 %439 to i64
  store i64 %440, i64* %rdx
  store volatile i64 33727, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 33734, i64* @assembly_address
  %441 = load i64* %rdx
  %442 = load i64* %rax
  %443 = mul i64 %442, 1
  %444 = add i64 %441, %443
  %445 = inttoptr i64 %444 to i8*
  %446 = load i8* %445
  %447 = zext i8 %446 to i64
  store i64 %447, i64* %rax
  store volatile i64 33738, i64* @assembly_address
  %448 = load i64* %rax
  %449 = trunc i64 %448 to i8
  %450 = zext i8 %449 to i64
  store i64 %450, i64* %rax
  store volatile i64 33741, i64* @assembly_address
  br label %block_83e5

block_83cf:                                       ; preds = %block_839e
  store volatile i64 33743, i64* @assembly_address
  %451 = load i64** %stack_var_-60
  %452 = ptrtoint i64* %451 to i32
  %453 = zext i32 %452 to i64
  store i64 %453, i64* %rax
  store volatile i64 33746, i64* @assembly_address
  %454 = load i64* %rax
  %455 = trunc i64 %454 to i32
  store i32 %455, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 33752, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 33757, i64* @assembly_address
  %456 = load i64* %rdi
  %457 = trunc i64 %456 to i32
  %458 = call i64 @fill_inbuf(i32 %457)
  store i64 %458, i64* %rax
  store i64 %458, i64* %rax
  store volatile i64 33762, i64* @assembly_address
  %459 = load i64* %rax
  %460 = trunc i64 %459 to i8
  %461 = zext i8 %460 to i64
  store i64 %461, i64* %rax
  br label %block_83e5

block_83e5:                                       ; preds = %block_83cf, %block_83ae
  store volatile i64 33765, i64* @assembly_address
  %462 = load i64* %rbx
  %463 = trunc i64 %462 to i32
  %464 = zext i32 %463 to i64
  store i64 %464, i64* %rcx
  store volatile i64 33767, i64* @assembly_address
  %465 = load i64* %rax
  %466 = load i64* %rcx
  %467 = trunc i64 %466 to i8
  %468 = zext i8 %467 to i64
  %469 = and i64 %468, 63
  %470 = load i1* %of
  %471 = icmp eq i64 %469, 0
  br i1 %471, label %488, label %472

; <label>:472                                     ; preds = %block_83e5
  %473 = shl i64 %465, %469
  %474 = icmp eq i64 %473, 0
  store i1 %474, i1* %zf
  %475 = icmp slt i64 %473, 0
  store i1 %475, i1* %sf
  %476 = trunc i64 %473 to i8
  %477 = call i8 @llvm.ctpop.i8(i8 %476)
  %478 = and i8 %477, 1
  %479 = icmp eq i8 %478, 0
  store i1 %479, i1* %pf
  store i64 %473, i64* %rax
  %480 = sub i64 %469, 1
  %481 = shl i64 %465, %480
  %482 = lshr i64 %481, 63
  %483 = trunc i64 %482 to i1
  store i1 %483, i1* %cf
  %484 = lshr i64 %473, 63
  %485 = icmp ne i64 %484, %482
  %486 = icmp eq i64 %469, 1
  %487 = select i1 %486, i1 %485, i1 %470
  store i1 %487, i1* %of
  br label %488

; <label>:488                                     ; preds = %block_83e5, %472
  store volatile i64 33770, i64* @assembly_address
  %489 = load i64* %r13
  %490 = load i64* %rax
  %491 = or i64 %489, %490
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %492 = icmp eq i64 %491, 0
  store i1 %492, i1* %zf
  %493 = icmp slt i64 %491, 0
  store i1 %493, i1* %sf
  %494 = trunc i64 %491 to i8
  %495 = call i8 @llvm.ctpop.i8(i8 %494)
  %496 = and i8 %495, 1
  %497 = icmp eq i8 %496, 0
  store i1 %497, i1* %pf
  store i64 %491, i64* %r13
  store volatile i64 33773, i64* @assembly_address
  %498 = load i64* %rbx
  %499 = trunc i64 %498 to i32
  %500 = add i32 %499, 8
  %501 = and i32 %499, 15
  %502 = add i32 %501, 8
  %503 = icmp ugt i32 %502, 15
  %504 = icmp ult i32 %500, %499
  %505 = xor i32 %499, %500
  %506 = xor i32 8, %500
  %507 = and i32 %505, %506
  %508 = icmp slt i32 %507, 0
  store i1 %503, i1* %az
  store i1 %504, i1* %cf
  store i1 %508, i1* %of
  %509 = icmp eq i32 %500, 0
  store i1 %509, i1* %zf
  %510 = icmp slt i32 %500, 0
  store i1 %510, i1* %sf
  %511 = trunc i32 %500 to i8
  %512 = call i8 @llvm.ctpop.i8(i8 %511)
  %513 = and i8 %512, 1
  %514 = icmp eq i8 %513, 0
  store i1 %514, i1* %pf
  %515 = zext i32 %500 to i64
  store i64 %515, i64* %rbx
  br label %block_83f0

block_83f0:                                       ; preds = %488, %359
  store volatile i64 33776, i64* @assembly_address
  %516 = load i64* %rbx
  %517 = trunc i64 %516 to i32
  %518 = load i64* %r12
  %519 = trunc i64 %518 to i32
  %520 = sub i32 %517, %519
  %521 = and i32 %517, 15
  %522 = and i32 %519, 15
  %523 = sub i32 %521, %522
  %524 = icmp ugt i32 %523, 15
  %525 = icmp ult i32 %517, %519
  %526 = xor i32 %517, %519
  %527 = xor i32 %517, %520
  %528 = and i32 %526, %527
  %529 = icmp slt i32 %528, 0
  store i1 %524, i1* %az
  store i1 %525, i1* %cf
  store i1 %529, i1* %of
  %530 = icmp eq i32 %520, 0
  store i1 %530, i1* %zf
  %531 = icmp slt i32 %520, 0
  store i1 %531, i1* %sf
  %532 = trunc i32 %520 to i8
  %533 = call i8 @llvm.ctpop.i8(i8 %532)
  %534 = and i8 %533, 1
  %535 = icmp eq i8 %534, 0
  store i1 %535, i1* %pf
  store volatile i64 33779, i64* @assembly_address
  %536 = load i1* %cf
  br i1 %536, label %block_839e, label %block_83f5

block_83f5:                                       ; preds = %block_83f0
  store volatile i64 33781, i64* @assembly_address
  %537 = load i8** %stack_var_-48
  %538 = ptrtoint i8* %537 to i64
  store i64 %538, i64* %rax
  store volatile i64 33785, i64* @assembly_address
  %539 = load i64* %rax
  %540 = add i64 %539, 8
  %541 = inttoptr i64 %540 to i64*
  %542 = load i64* %541
  store i64 %542, i64* %rdx
  store volatile i64 33789, i64* @assembly_address
  %543 = load i64* %r13
  %544 = trunc i64 %543 to i32
  %545 = zext i32 %544 to i64
  store i64 %545, i64* %rsi
  store volatile i64 33792, i64* @assembly_address
  %546 = load i64* %r12
  %547 = trunc i64 %546 to i32
  %548 = zext i32 %547 to i64
  store i64 %548, i64* %rax
  store volatile i64 33795, i64* @assembly_address
  %549 = load i64* %rax
  %550 = load i64* %rax
  %551 = mul i64 %550, 1
  %552 = add i64 %549, %551
  store i64 %552, i64* %rcx
  store volatile i64 33799, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162e0 to i64), i64* %rax
  store volatile i64 33806, i64* @assembly_address
  %553 = load i64* %rcx
  %554 = load i64* %rax
  %555 = mul i64 %554, 1
  %556 = add i64 %553, %555
  %557 = inttoptr i64 %556 to i16*
  %558 = load i16* %557
  %559 = zext i16 %558 to i64
  store i64 %559, i64* %rax
  store volatile i64 33810, i64* @assembly_address
  %560 = load i64* %rax
  %561 = trunc i64 %560 to i16
  %562 = zext i16 %561 to i64
  store i64 %562, i64* %rax
  store volatile i64 33813, i64* @assembly_address
  %563 = load i64* %rax
  %564 = trunc i64 %563 to i32
  %565 = load i64* %rsi
  %566 = trunc i64 %565 to i32
  %567 = and i32 %564, %566
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %568 = icmp eq i32 %567, 0
  store i1 %568, i1* %zf
  %569 = icmp slt i32 %567, 0
  store i1 %569, i1* %sf
  %570 = trunc i32 %567 to i8
  %571 = call i8 @llvm.ctpop.i8(i8 %570)
  %572 = and i8 %571, 1
  %573 = icmp eq i8 %572, 0
  store i1 %573, i1* %pf
  %574 = zext i32 %567 to i64
  store i64 %574, i64* %rax
  store volatile i64 33815, i64* @assembly_address
  %575 = load i64* %rax
  %576 = trunc i64 %575 to i32
  %577 = zext i32 %576 to i64
  store i64 %577, i64* %rax
  store volatile i64 33817, i64* @assembly_address
  %578 = load i64* %rax
  %579 = load i1* %of
  %580 = shl i64 %578, 4
  %581 = icmp eq i64 %580, 0
  store i1 %581, i1* %zf
  %582 = icmp slt i64 %580, 0
  store i1 %582, i1* %sf
  %583 = trunc i64 %580 to i8
  %584 = call i8 @llvm.ctpop.i8(i8 %583)
  %585 = and i8 %584, 1
  %586 = icmp eq i8 %585, 0
  store i1 %586, i1* %pf
  store i64 %580, i64* %rax
  %587 = shl i64 %578, 3
  %588 = lshr i64 %587, 63
  %589 = trunc i64 %588 to i1
  store i1 %589, i1* %cf
  %590 = lshr i64 %580, 63
  %591 = icmp ne i64 %590, %588
  %592 = select i1 false, i1 %591, i1 %579
  store i1 %592, i1* %of
  store volatile i64 33821, i64* @assembly_address
  %593 = load i64* %rax
  %594 = load i64* %rdx
  %595 = add i64 %593, %594
  %596 = and i64 %593, 15
  %597 = and i64 %594, 15
  %598 = add i64 %596, %597
  %599 = icmp ugt i64 %598, 15
  %600 = icmp ult i64 %595, %593
  %601 = xor i64 %593, %595
  %602 = xor i64 %594, %595
  %603 = and i64 %601, %602
  %604 = icmp slt i64 %603, 0
  store i1 %599, i1* %az
  store i1 %600, i1* %cf
  store i1 %604, i1* %of
  %605 = icmp eq i64 %595, 0
  store i1 %605, i1* %zf
  %606 = icmp slt i64 %595, 0
  store i1 %606, i1* %sf
  %607 = trunc i64 %595 to i8
  %608 = call i8 @llvm.ctpop.i8(i8 %607)
  %609 = and i8 %608, 1
  %610 = icmp eq i8 %609, 0
  store i1 %610, i1* %pf
  store i64 %595, i64* %rax
  store volatile i64 33824, i64* @assembly_address
  %611 = load i64* %rax
  %612 = inttoptr i64 %611 to i8*
  store i8* %612, i8** %stack_var_-48
  store volatile i64 33828, i64* @assembly_address
  %613 = load i8** %stack_var_-48
  %614 = ptrtoint i8* %613 to i64
  store i64 %614, i64* %rax
  store volatile i64 33832, i64* @assembly_address
  %615 = load i64* %rax
  %616 = inttoptr i64 %615 to i8*
  %617 = load i8* %616
  %618 = zext i8 %617 to i64
  store i64 %618, i64* %rax
  store volatile i64 33835, i64* @assembly_address
  %619 = load i64* %rax
  %620 = trunc i64 %619 to i8
  %621 = zext i8 %620 to i64
  store i64 %621, i64* %r12
  store volatile i64 33839, i64* @assembly_address
  %622 = load i64* %r12
  %623 = trunc i64 %622 to i32
  %624 = sub i32 %623, 16
  %625 = and i32 %623, 15
  %626 = icmp ugt i32 %625, 15
  %627 = icmp ult i32 %623, 16
  %628 = xor i32 %623, 16
  %629 = xor i32 %623, %624
  %630 = and i32 %628, %629
  %631 = icmp slt i32 %630, 0
  store i1 %626, i1* %az
  store i1 %627, i1* %cf
  store i1 %631, i1* %of
  %632 = icmp eq i32 %624, 0
  store i1 %632, i1* %zf
  %633 = icmp slt i32 %624, 0
  store i1 %633, i1* %sf
  %634 = trunc i32 %624 to i8
  %635 = call i8 @llvm.ctpop.i8(i8 %634)
  %636 = and i8 %635, 1
  %637 = icmp eq i8 %636, 0
  store i1 %637, i1* %pf
  store volatile i64 33843, i64* @assembly_address
  %638 = load i1* %cf
  %639 = load i1* %zf
  %640 = or i1 %638, %639
  %641 = icmp ne i1 %640, true
  br i1 %641, label %block_836b, label %block_8439

block_8439:                                       ; preds = %block_83f5, %block_833c
  store volatile i64 33849, i64* @assembly_address
  %642 = load i8** %stack_var_-48
  %643 = ptrtoint i8* %642 to i64
  store i64 %643, i64* %rax
  store volatile i64 33853, i64* @assembly_address
  %644 = load i64* %rax
  %645 = add i64 %644, 1
  %646 = inttoptr i64 %645 to i8*
  %647 = load i8* %646
  %648 = zext i8 %647 to i64
  store i64 %648, i64* %rax
  store volatile i64 33857, i64* @assembly_address
  %649 = load i64* %rax
  %650 = trunc i64 %649 to i8
  %651 = zext i8 %650 to i64
  store i64 %651, i64* %rax
  store volatile i64 33860, i64* @assembly_address
  %652 = load i64* %rax
  %653 = trunc i64 %652 to i32
  %654 = zext i32 %653 to i64
  store i64 %654, i64* %rcx
  store volatile i64 33862, i64* @assembly_address
  %655 = load i64* %r13
  %656 = load i64* %rcx
  %657 = trunc i64 %656 to i8
  %658 = zext i8 %657 to i64
  %659 = and i64 %658, 63
  %660 = load i1* %of
  %661 = icmp eq i64 %659, 0
  br i1 %661, label %677, label %662

; <label>:662                                     ; preds = %block_8439
  %663 = lshr i64 %655, %659
  %664 = icmp eq i64 %663, 0
  store i1 %664, i1* %zf
  %665 = icmp slt i64 %663, 0
  store i1 %665, i1* %sf
  %666 = trunc i64 %663 to i8
  %667 = call i8 @llvm.ctpop.i8(i8 %666)
  %668 = and i8 %667, 1
  %669 = icmp eq i8 %668, 0
  store i1 %669, i1* %pf
  store i64 %663, i64* %r13
  %670 = sub i64 %659, 1
  %671 = shl i64 1, %670
  %672 = and i64 %671, %655
  %673 = icmp ne i64 %672, 0
  store i1 %673, i1* %cf
  %674 = icmp eq i64 %659, 1
  %675 = icmp slt i64 %655, 0
  %676 = select i1 %674, i1 %675, i1 %660
  store i1 %676, i1* %of
  br label %677

; <label>:677                                     ; preds = %block_8439, %662
  store volatile i64 33865, i64* @assembly_address
  %678 = load i8** %stack_var_-48
  %679 = ptrtoint i8* %678 to i64
  store i64 %679, i64* %rax
  store volatile i64 33869, i64* @assembly_address
  %680 = load i64* %rax
  %681 = add i64 %680, 1
  %682 = inttoptr i64 %681 to i8*
  %683 = load i8* %682
  %684 = zext i8 %683 to i64
  store i64 %684, i64* %rax
  store volatile i64 33873, i64* @assembly_address
  %685 = load i64* %rax
  %686 = trunc i64 %685 to i8
  %687 = zext i8 %686 to i64
  store i64 %687, i64* %rax
  store volatile i64 33876, i64* @assembly_address
  %688 = load i64* %rbx
  %689 = trunc i64 %688 to i32
  %690 = load i64* %rax
  %691 = trunc i64 %690 to i32
  %692 = sub i32 %689, %691
  %693 = and i32 %689, 15
  %694 = and i32 %691, 15
  %695 = sub i32 %693, %694
  %696 = icmp ugt i32 %695, 15
  %697 = icmp ult i32 %689, %691
  %698 = xor i32 %689, %691
  %699 = xor i32 %689, %692
  %700 = and i32 %698, %699
  %701 = icmp slt i32 %700, 0
  store i1 %696, i1* %az
  store i1 %697, i1* %cf
  store i1 %701, i1* %of
  %702 = icmp eq i32 %692, 0
  store i1 %702, i1* %zf
  %703 = icmp slt i32 %692, 0
  store i1 %703, i1* %sf
  %704 = trunc i32 %692 to i8
  %705 = call i8 @llvm.ctpop.i8(i8 %704)
  %706 = and i8 %705, 1
  %707 = icmp eq i8 %706, 0
  store i1 %707, i1* %pf
  %708 = zext i32 %692 to i64
  store i64 %708, i64* %rbx
  store volatile i64 33878, i64* @assembly_address
  %709 = load i64* %r12
  %710 = trunc i64 %709 to i32
  %711 = sub i32 %710, 16
  %712 = and i32 %710, 15
  %713 = icmp ugt i32 %712, 15
  %714 = icmp ult i32 %710, 16
  %715 = xor i32 %710, 16
  %716 = xor i32 %710, %711
  %717 = and i32 %715, %716
  %718 = icmp slt i32 %717, 0
  store i1 %713, i1* %az
  store i1 %714, i1* %cf
  store i1 %718, i1* %of
  %719 = icmp eq i32 %711, 0
  store i1 %719, i1* %zf
  %720 = icmp slt i32 %711, 0
  store i1 %720, i1* %sf
  %721 = trunc i32 %711 to i8
  %722 = call i8 @llvm.ctpop.i8(i8 %721)
  %723 = and i8 %722, 1
  %724 = icmp eq i8 %723, 0
  store i1 %724, i1* %pf
  store volatile i64 33882, i64* @assembly_address
  %725 = load i1* %zf
  %726 = icmp eq i1 %725, false
  br i1 %726, label %block_84a0, label %block_845c

block_845c:                                       ; preds = %677
  store volatile i64 33884, i64* @assembly_address
  %727 = load i8** %stack_var_-48
  %728 = ptrtoint i8* %727 to i64
  store i64 %728, i64* %rax
  store volatile i64 33888, i64* @assembly_address
  %729 = load i64* %rax
  %730 = add i64 %729, 8
  %731 = inttoptr i64 %730 to i16*
  %732 = load i16* %731
  %733 = zext i16 %732 to i64
  store i64 %733, i64* %rcx
  store volatile i64 33892, i64* @assembly_address
  %734 = load i64** %stack_var_-60
  %735 = ptrtoint i64* %734 to i32
  %736 = zext i32 %735 to i64
  store i64 %736, i64* %rax
  store volatile i64 33895, i64* @assembly_address
  %737 = load i64* %rax
  %738 = add i64 %737, 1
  %739 = trunc i64 %738 to i32
  %740 = zext i32 %739 to i64
  store i64 %740, i64* %rdx
  store volatile i64 33898, i64* @assembly_address
  %741 = load i64* %rdx
  %742 = trunc i64 %741 to i32
  %743 = inttoptr i32 %742 to i64*
  store i64* %743, i64** %stack_var_-60
  store volatile i64 33901, i64* @assembly_address
  %744 = load i64* %rax
  %745 = trunc i64 %744 to i32
  %746 = zext i32 %745 to i64
  store i64 %746, i64* %rdx
  store volatile i64 33903, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 33910, i64* @assembly_address
  %747 = load i64* %rcx
  %748 = trunc i64 %747 to i8
  %749 = load i64* %rdx
  %750 = load i64* %rax
  %751 = mul i64 %750, 1
  %752 = add i64 %749, %751
  %753 = inttoptr i64 %752 to i8*
  store i8 %748, i8* %753
  store volatile i64 33913, i64* @assembly_address
  %754 = load i64** %stack_var_-60
  %755 = ptrtoint i64* %754 to i32
  %756 = sub i32 %755, 32768
  %757 = and i32 %755, 15
  %758 = icmp ugt i32 %757, 15
  %759 = icmp ult i32 %755, 32768
  %760 = xor i32 %755, 32768
  %761 = xor i32 %755, %756
  %762 = and i32 %760, %761
  %763 = icmp slt i32 %762, 0
  store i1 %758, i1* %az
  store i1 %759, i1* %cf
  store i1 %763, i1* %of
  %764 = icmp eq i32 %756, 0
  store i1 %764, i1* %zf
  %765 = icmp slt i32 %756, 0
  store i1 %765, i1* %sf
  %766 = trunc i32 %756 to i8
  %767 = call i8 @llvm.ctpop.i8(i8 %766)
  %768 = and i8 %767, 1
  %769 = icmp eq i8 %768, 0
  store i1 %769, i1* %pf
  store volatile i64 33920, i64* @assembly_address
  %770 = load i1* %zf
  %771 = icmp eq i1 %770, false
  br i1 %771, label %block_8335, label %block_8486

block_8486:                                       ; preds = %block_845c
  store volatile i64 33926, i64* @assembly_address
  %772 = load i64** %stack_var_-60
  %773 = ptrtoint i64* %772 to i32
  %774 = zext i32 %773 to i64
  store i64 %774, i64* %rax
  store volatile i64 33929, i64* @assembly_address
  %775 = load i64* %rax
  %776 = trunc i64 %775 to i32
  store i32 %776, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 33935, i64* @assembly_address
  %777 = call i64 @flush_window()
  store i64 %777, i64* %rax
  store i64 %777, i64* %rax
  store i64 %777, i64* %rax
  store volatile i64 33940, i64* @assembly_address
  %778 = inttoptr i32 0 to i64*
  store i64* %778, i64** %stack_var_-60
  store volatile i64 33947, i64* @assembly_address
  br label %block_8335

block_84a0:                                       ; preds = %677
  store volatile i64 33952, i64* @assembly_address
  %779 = load i64* %r12
  %780 = trunc i64 %779 to i32
  %781 = sub i32 %780, 15
  %782 = and i32 %780, 15
  %783 = sub i32 %782, 15
  %784 = icmp ugt i32 %783, 15
  %785 = icmp ult i32 %780, 15
  %786 = xor i32 %780, 15
  %787 = xor i32 %780, %781
  %788 = and i32 %786, %787
  %789 = icmp slt i32 %788, 0
  store i1 %784, i1* %az
  store i1 %785, i1* %cf
  store i1 %789, i1* %of
  %790 = icmp eq i32 %781, 0
  store i1 %790, i1* %zf
  %791 = icmp slt i32 %781, 0
  store i1 %791, i1* %sf
  %792 = trunc i32 %781 to i8
  %793 = call i8 @llvm.ctpop.i8(i8 %792)
  %794 = and i8 %793, 1
  %795 = icmp eq i8 %794, 0
  store i1 %795, i1* %pf
  store volatile i64 33956, i64* @assembly_address
  %796 = load i1* %zf
  br i1 %796, label %block_8829, label %block_84aa

block_84aa:                                       ; preds = %block_84a0
  store volatile i64 33962, i64* @assembly_address
  br label %block_84fe

block_84ac:                                       ; preds = %block_84fe
  store volatile i64 33964, i64* @assembly_address
  %797 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %798 = zext i32 %797 to i64
  store i64 %798, i64* %rdx
  store volatile i64 33970, i64* @assembly_address
  %799 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %800 = zext i32 %799 to i64
  store i64 %800, i64* %rax
  store volatile i64 33976, i64* @assembly_address
  %801 = load i64* %rdx
  %802 = trunc i64 %801 to i32
  %803 = load i64* %rax
  %804 = trunc i64 %803 to i32
  %805 = sub i32 %802, %804
  %806 = and i32 %802, 15
  %807 = and i32 %804, 15
  %808 = sub i32 %806, %807
  %809 = icmp ugt i32 %808, 15
  %810 = icmp ult i32 %802, %804
  %811 = xor i32 %802, %804
  %812 = xor i32 %802, %805
  %813 = and i32 %811, %812
  %814 = icmp slt i32 %813, 0
  store i1 %809, i1* %az
  store i1 %810, i1* %cf
  store i1 %814, i1* %of
  %815 = icmp eq i32 %805, 0
  store i1 %815, i1* %zf
  %816 = icmp slt i32 %805, 0
  store i1 %816, i1* %sf
  %817 = trunc i32 %805 to i8
  %818 = call i8 @llvm.ctpop.i8(i8 %817)
  %819 = and i8 %818, 1
  %820 = icmp eq i8 %819, 0
  store i1 %820, i1* %pf
  store volatile i64 33978, i64* @assembly_address
  %821 = load i1* %cf
  %822 = icmp eq i1 %821, false
  br i1 %822, label %block_84dd, label %block_84bc

block_84bc:                                       ; preds = %block_84ac
  store volatile i64 33980, i64* @assembly_address
  %823 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %824 = zext i32 %823 to i64
  store i64 %824, i64* %rax
  store volatile i64 33986, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 33989, i64* @assembly_address
  %825 = load i64* %rdx
  %826 = trunc i64 %825 to i32
  store i32 %826, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 33995, i64* @assembly_address
  %827 = load i64* %rax
  %828 = trunc i64 %827 to i32
  %829 = zext i32 %828 to i64
  store i64 %829, i64* %rdx
  store volatile i64 33997, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 34004, i64* @assembly_address
  %830 = load i64* %rdx
  %831 = load i64* %rax
  %832 = mul i64 %831, 1
  %833 = add i64 %830, %832
  %834 = inttoptr i64 %833 to i8*
  %835 = load i8* %834
  %836 = zext i8 %835 to i64
  store i64 %836, i64* %rax
  store volatile i64 34008, i64* @assembly_address
  %837 = load i64* %rax
  %838 = trunc i64 %837 to i8
  %839 = zext i8 %838 to i64
  store i64 %839, i64* %rax
  store volatile i64 34011, i64* @assembly_address
  br label %block_84f3

block_84dd:                                       ; preds = %block_84ac
  store volatile i64 34013, i64* @assembly_address
  %840 = load i64** %stack_var_-60
  %841 = ptrtoint i64* %840 to i32
  %842 = zext i32 %841 to i64
  store i64 %842, i64* %rax
  store volatile i64 34016, i64* @assembly_address
  %843 = load i64* %rax
  %844 = trunc i64 %843 to i32
  store i32 %844, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 34022, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 34027, i64* @assembly_address
  %845 = load i64* %rdi
  %846 = trunc i64 %845 to i32
  %847 = call i64 @fill_inbuf(i32 %846)
  store i64 %847, i64* %rax
  store i64 %847, i64* %rax
  store volatile i64 34032, i64* @assembly_address
  %848 = load i64* %rax
  %849 = trunc i64 %848 to i8
  %850 = zext i8 %849 to i64
  store i64 %850, i64* %rax
  br label %block_84f3

block_84f3:                                       ; preds = %block_84dd, %block_84bc
  store volatile i64 34035, i64* @assembly_address
  %851 = load i64* %rbx
  %852 = trunc i64 %851 to i32
  %853 = zext i32 %852 to i64
  store i64 %853, i64* %rcx
  store volatile i64 34037, i64* @assembly_address
  %854 = load i64* %rax
  %855 = load i64* %rcx
  %856 = trunc i64 %855 to i8
  %857 = zext i8 %856 to i64
  %858 = and i64 %857, 63
  %859 = load i1* %of
  %860 = icmp eq i64 %858, 0
  br i1 %860, label %877, label %861

; <label>:861                                     ; preds = %block_84f3
  %862 = shl i64 %854, %858
  %863 = icmp eq i64 %862, 0
  store i1 %863, i1* %zf
  %864 = icmp slt i64 %862, 0
  store i1 %864, i1* %sf
  %865 = trunc i64 %862 to i8
  %866 = call i8 @llvm.ctpop.i8(i8 %865)
  %867 = and i8 %866, 1
  %868 = icmp eq i8 %867, 0
  store i1 %868, i1* %pf
  store i64 %862, i64* %rax
  %869 = sub i64 %858, 1
  %870 = shl i64 %854, %869
  %871 = lshr i64 %870, 63
  %872 = trunc i64 %871 to i1
  store i1 %872, i1* %cf
  %873 = lshr i64 %862, 63
  %874 = icmp ne i64 %873, %871
  %875 = icmp eq i64 %858, 1
  %876 = select i1 %875, i1 %874, i1 %859
  store i1 %876, i1* %of
  br label %877

; <label>:877                                     ; preds = %block_84f3, %861
  store volatile i64 34040, i64* @assembly_address
  %878 = load i64* %r13
  %879 = load i64* %rax
  %880 = or i64 %878, %879
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %881 = icmp eq i64 %880, 0
  store i1 %881, i1* %zf
  %882 = icmp slt i64 %880, 0
  store i1 %882, i1* %sf
  %883 = trunc i64 %880 to i8
  %884 = call i8 @llvm.ctpop.i8(i8 %883)
  %885 = and i8 %884, 1
  %886 = icmp eq i8 %885, 0
  store i1 %886, i1* %pf
  store i64 %880, i64* %r13
  store volatile i64 34043, i64* @assembly_address
  %887 = load i64* %rbx
  %888 = trunc i64 %887 to i32
  %889 = add i32 %888, 8
  %890 = and i32 %888, 15
  %891 = add i32 %890, 8
  %892 = icmp ugt i32 %891, 15
  %893 = icmp ult i32 %889, %888
  %894 = xor i32 %888, %889
  %895 = xor i32 8, %889
  %896 = and i32 %894, %895
  %897 = icmp slt i32 %896, 0
  store i1 %892, i1* %az
  store i1 %893, i1* %cf
  store i1 %897, i1* %of
  %898 = icmp eq i32 %889, 0
  store i1 %898, i1* %zf
  %899 = icmp slt i32 %889, 0
  store i1 %899, i1* %sf
  %900 = trunc i32 %889 to i8
  %901 = call i8 @llvm.ctpop.i8(i8 %900)
  %902 = and i8 %901, 1
  %903 = icmp eq i8 %902, 0
  store i1 %903, i1* %pf
  %904 = zext i32 %889 to i64
  store i64 %904, i64* %rbx
  br label %block_84fe

block_84fe:                                       ; preds = %877, %block_84aa
  store volatile i64 34046, i64* @assembly_address
  %905 = load i64* %rbx
  %906 = trunc i64 %905 to i32
  %907 = load i64* %r12
  %908 = trunc i64 %907 to i32
  %909 = sub i32 %906, %908
  %910 = and i32 %906, 15
  %911 = and i32 %908, 15
  %912 = sub i32 %910, %911
  %913 = icmp ugt i32 %912, 15
  %914 = icmp ult i32 %906, %908
  %915 = xor i32 %906, %908
  %916 = xor i32 %906, %909
  %917 = and i32 %915, %916
  %918 = icmp slt i32 %917, 0
  store i1 %913, i1* %az
  store i1 %914, i1* %cf
  store i1 %918, i1* %of
  %919 = icmp eq i32 %909, 0
  store i1 %919, i1* %zf
  %920 = icmp slt i32 %909, 0
  store i1 %920, i1* %sf
  %921 = trunc i32 %909 to i8
  %922 = call i8 @llvm.ctpop.i8(i8 %921)
  %923 = and i8 %922, 1
  %924 = icmp eq i8 %923, 0
  store i1 %924, i1* %pf
  store volatile i64 34049, i64* @assembly_address
  %925 = load i1* %cf
  br i1 %925, label %block_84ac, label %block_8503

block_8503:                                       ; preds = %block_84fe
  store volatile i64 34051, i64* @assembly_address
  %926 = load i8** %stack_var_-48
  %927 = ptrtoint i8* %926 to i64
  store i64 %927, i64* %rax
  store volatile i64 34055, i64* @assembly_address
  %928 = load i64* %rax
  %929 = add i64 %928, 8
  %930 = inttoptr i64 %929 to i16*
  %931 = load i16* %930
  %932 = zext i16 %931 to i64
  store i64 %932, i64* %rax
  store volatile i64 34059, i64* @assembly_address
  %933 = load i64* %rax
  %934 = trunc i64 %933 to i16
  %935 = zext i16 %934 to i64
  store i64 %935, i64* %rdx
  store volatile i64 34062, i64* @assembly_address
  %936 = load i64* %r13
  %937 = trunc i64 %936 to i32
  %938 = zext i32 %937 to i64
  store i64 %938, i64* %rsi
  store volatile i64 34065, i64* @assembly_address
  %939 = load i64* %r12
  %940 = trunc i64 %939 to i32
  %941 = zext i32 %940 to i64
  store i64 %941, i64* %rax
  store volatile i64 34068, i64* @assembly_address
  %942 = load i64* %rax
  %943 = load i64* %rax
  %944 = mul i64 %943, 1
  %945 = add i64 %942, %944
  store i64 %945, i64* %rcx
  store volatile i64 34072, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162e0 to i64), i64* %rax
  store volatile i64 34079, i64* @assembly_address
  %946 = load i64* %rcx
  %947 = load i64* %rax
  %948 = mul i64 %947, 1
  %949 = add i64 %946, %948
  %950 = inttoptr i64 %949 to i16*
  %951 = load i16* %950
  %952 = zext i16 %951 to i64
  store i64 %952, i64* %rax
  store volatile i64 34083, i64* @assembly_address
  %953 = load i64* %rax
  %954 = trunc i64 %953 to i16
  %955 = zext i16 %954 to i64
  store i64 %955, i64* %rax
  store volatile i64 34086, i64* @assembly_address
  %956 = load i64* %rax
  %957 = trunc i64 %956 to i32
  %958 = load i64* %rsi
  %959 = trunc i64 %958 to i32
  %960 = and i32 %957, %959
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %961 = icmp eq i32 %960, 0
  store i1 %961, i1* %zf
  %962 = icmp slt i32 %960, 0
  store i1 %962, i1* %sf
  %963 = trunc i32 %960 to i8
  %964 = call i8 @llvm.ctpop.i8(i8 %963)
  %965 = and i8 %964, 1
  %966 = icmp eq i8 %965, 0
  store i1 %966, i1* %pf
  %967 = zext i32 %960 to i64
  store i64 %967, i64* %rax
  store volatile i64 34088, i64* @assembly_address
  %968 = load i64* %rax
  %969 = trunc i64 %968 to i32
  %970 = load i64* %rdx
  %971 = trunc i64 %970 to i32
  %972 = add i32 %969, %971
  %973 = and i32 %969, 15
  %974 = and i32 %971, 15
  %975 = add i32 %973, %974
  %976 = icmp ugt i32 %975, 15
  %977 = icmp ult i32 %972, %969
  %978 = xor i32 %969, %972
  %979 = xor i32 %971, %972
  %980 = and i32 %978, %979
  %981 = icmp slt i32 %980, 0
  store i1 %976, i1* %az
  store i1 %977, i1* %cf
  store i1 %981, i1* %of
  %982 = icmp eq i32 %972, 0
  store i1 %982, i1* %zf
  %983 = icmp slt i32 %972, 0
  store i1 %983, i1* %sf
  %984 = trunc i32 %972 to i8
  %985 = call i8 @llvm.ctpop.i8(i8 %984)
  %986 = and i8 %985, 1
  %987 = icmp eq i8 %986, 0
  store i1 %987, i1* %pf
  %988 = zext i32 %972 to i64
  store i64 %988, i64* %rax
  store volatile i64 34090, i64* @assembly_address
  %989 = load i64* %rax
  %990 = trunc i64 %989 to i32
  %991 = inttoptr i32 %990 to i64*
  store i64* %991, i64** %stack_var_-68
  store volatile i64 34093, i64* @assembly_address
  %992 = load i64* %r12
  %993 = trunc i64 %992 to i32
  %994 = zext i32 %993 to i64
  store i64 %994, i64* %rcx
  store volatile i64 34096, i64* @assembly_address
  %995 = load i64* %r13
  %996 = load i64* %rcx
  %997 = trunc i64 %996 to i8
  %998 = zext i8 %997 to i64
  %999 = and i64 %998, 63
  %1000 = load i1* %of
  %1001 = icmp eq i64 %999, 0
  br i1 %1001, label %1017, label %1002

; <label>:1002                                    ; preds = %block_8503
  %1003 = lshr i64 %995, %999
  %1004 = icmp eq i64 %1003, 0
  store i1 %1004, i1* %zf
  %1005 = icmp slt i64 %1003, 0
  store i1 %1005, i1* %sf
  %1006 = trunc i64 %1003 to i8
  %1007 = call i8 @llvm.ctpop.i8(i8 %1006)
  %1008 = and i8 %1007, 1
  %1009 = icmp eq i8 %1008, 0
  store i1 %1009, i1* %pf
  store i64 %1003, i64* %r13
  %1010 = sub i64 %999, 1
  %1011 = shl i64 1, %1010
  %1012 = and i64 %1011, %995
  %1013 = icmp ne i64 %1012, 0
  store i1 %1013, i1* %cf
  %1014 = icmp eq i64 %999, 1
  %1015 = icmp slt i64 %995, 0
  %1016 = select i1 %1014, i1 %1015, i1 %1000
  store i1 %1016, i1* %of
  br label %1017

; <label>:1017                                    ; preds = %block_8503, %1002
  store volatile i64 34099, i64* @assembly_address
  %1018 = load i64* %rbx
  %1019 = trunc i64 %1018 to i32
  %1020 = load i64* %r12
  %1021 = trunc i64 %1020 to i32
  %1022 = sub i32 %1019, %1021
  %1023 = and i32 %1019, 15
  %1024 = and i32 %1021, 15
  %1025 = sub i32 %1023, %1024
  %1026 = icmp ugt i32 %1025, 15
  %1027 = icmp ult i32 %1019, %1021
  %1028 = xor i32 %1019, %1021
  %1029 = xor i32 %1019, %1022
  %1030 = and i32 %1028, %1029
  %1031 = icmp slt i32 %1030, 0
  store i1 %1026, i1* %az
  store i1 %1027, i1* %cf
  store i1 %1031, i1* %of
  %1032 = icmp eq i32 %1022, 0
  store i1 %1032, i1* %zf
  %1033 = icmp slt i32 %1022, 0
  store i1 %1033, i1* %sf
  %1034 = trunc i32 %1022 to i8
  %1035 = call i8 @llvm.ctpop.i8(i8 %1034)
  %1036 = and i8 %1035, 1
  %1037 = icmp eq i8 %1036, 0
  store i1 %1037, i1* %pf
  %1038 = zext i32 %1022 to i64
  store i64 %1038, i64* %rbx
  store volatile i64 34102, i64* @assembly_address
  br label %block_858a

block_8538:                                       ; preds = %block_858a
  store volatile i64 34104, i64* @assembly_address
  %1039 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1040 = zext i32 %1039 to i64
  store i64 %1040, i64* %rdx
  store volatile i64 34110, i64* @assembly_address
  %1041 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1042 = zext i32 %1041 to i64
  store i64 %1042, i64* %rax
  store volatile i64 34116, i64* @assembly_address
  %1043 = load i64* %rdx
  %1044 = trunc i64 %1043 to i32
  %1045 = load i64* %rax
  %1046 = trunc i64 %1045 to i32
  %1047 = sub i32 %1044, %1046
  %1048 = and i32 %1044, 15
  %1049 = and i32 %1046, 15
  %1050 = sub i32 %1048, %1049
  %1051 = icmp ugt i32 %1050, 15
  %1052 = icmp ult i32 %1044, %1046
  %1053 = xor i32 %1044, %1046
  %1054 = xor i32 %1044, %1047
  %1055 = and i32 %1053, %1054
  %1056 = icmp slt i32 %1055, 0
  store i1 %1051, i1* %az
  store i1 %1052, i1* %cf
  store i1 %1056, i1* %of
  %1057 = icmp eq i32 %1047, 0
  store i1 %1057, i1* %zf
  %1058 = icmp slt i32 %1047, 0
  store i1 %1058, i1* %sf
  %1059 = trunc i32 %1047 to i8
  %1060 = call i8 @llvm.ctpop.i8(i8 %1059)
  %1061 = and i8 %1060, 1
  %1062 = icmp eq i8 %1061, 0
  store i1 %1062, i1* %pf
  store volatile i64 34118, i64* @assembly_address
  %1063 = load i1* %cf
  %1064 = icmp eq i1 %1063, false
  br i1 %1064, label %block_8569, label %block_8548

block_8548:                                       ; preds = %block_8538
  store volatile i64 34120, i64* @assembly_address
  %1065 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1066 = zext i32 %1065 to i64
  store i64 %1066, i64* %rax
  store volatile i64 34126, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 34129, i64* @assembly_address
  %1067 = load i64* %rdx
  %1068 = trunc i64 %1067 to i32
  store i32 %1068, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 34135, i64* @assembly_address
  %1069 = load i64* %rax
  %1070 = trunc i64 %1069 to i32
  %1071 = zext i32 %1070 to i64
  store i64 %1071, i64* %rdx
  store volatile i64 34137, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 34144, i64* @assembly_address
  %1072 = load i64* %rdx
  %1073 = load i64* %rax
  %1074 = mul i64 %1073, 1
  %1075 = add i64 %1072, %1074
  %1076 = inttoptr i64 %1075 to i8*
  %1077 = load i8* %1076
  %1078 = zext i8 %1077 to i64
  store i64 %1078, i64* %rax
  store volatile i64 34148, i64* @assembly_address
  %1079 = load i64* %rax
  %1080 = trunc i64 %1079 to i8
  %1081 = zext i8 %1080 to i64
  store i64 %1081, i64* %rax
  store volatile i64 34151, i64* @assembly_address
  br label %block_857f

block_8569:                                       ; preds = %block_8538
  store volatile i64 34153, i64* @assembly_address
  %1082 = load i64** %stack_var_-60
  %1083 = ptrtoint i64* %1082 to i32
  %1084 = zext i32 %1083 to i64
  store i64 %1084, i64* %rax
  store volatile i64 34156, i64* @assembly_address
  %1085 = load i64* %rax
  %1086 = trunc i64 %1085 to i32
  store i32 %1086, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 34162, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 34167, i64* @assembly_address
  %1087 = load i64* %rdi
  %1088 = trunc i64 %1087 to i32
  %1089 = call i64 @fill_inbuf(i32 %1088)
  store i64 %1089, i64* %rax
  store i64 %1089, i64* %rax
  store volatile i64 34172, i64* @assembly_address
  %1090 = load i64* %rax
  %1091 = trunc i64 %1090 to i8
  %1092 = zext i8 %1091 to i64
  store i64 %1092, i64* %rax
  br label %block_857f

block_857f:                                       ; preds = %block_8569, %block_8548
  store volatile i64 34175, i64* @assembly_address
  %1093 = load i64* %rbx
  %1094 = trunc i64 %1093 to i32
  %1095 = zext i32 %1094 to i64
  store i64 %1095, i64* %rcx
  store volatile i64 34177, i64* @assembly_address
  %1096 = load i64* %rax
  %1097 = load i64* %rcx
  %1098 = trunc i64 %1097 to i8
  %1099 = zext i8 %1098 to i64
  %1100 = and i64 %1099, 63
  %1101 = load i1* %of
  %1102 = icmp eq i64 %1100, 0
  br i1 %1102, label %1119, label %1103

; <label>:1103                                    ; preds = %block_857f
  %1104 = shl i64 %1096, %1100
  %1105 = icmp eq i64 %1104, 0
  store i1 %1105, i1* %zf
  %1106 = icmp slt i64 %1104, 0
  store i1 %1106, i1* %sf
  %1107 = trunc i64 %1104 to i8
  %1108 = call i8 @llvm.ctpop.i8(i8 %1107)
  %1109 = and i8 %1108, 1
  %1110 = icmp eq i8 %1109, 0
  store i1 %1110, i1* %pf
  store i64 %1104, i64* %rax
  %1111 = sub i64 %1100, 1
  %1112 = shl i64 %1096, %1111
  %1113 = lshr i64 %1112, 63
  %1114 = trunc i64 %1113 to i1
  store i1 %1114, i1* %cf
  %1115 = lshr i64 %1104, 63
  %1116 = icmp ne i64 %1115, %1113
  %1117 = icmp eq i64 %1100, 1
  %1118 = select i1 %1117, i1 %1116, i1 %1101
  store i1 %1118, i1* %of
  br label %1119

; <label>:1119                                    ; preds = %block_857f, %1103
  store volatile i64 34180, i64* @assembly_address
  %1120 = load i64* %r13
  %1121 = load i64* %rax
  %1122 = or i64 %1120, %1121
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1123 = icmp eq i64 %1122, 0
  store i1 %1123, i1* %zf
  %1124 = icmp slt i64 %1122, 0
  store i1 %1124, i1* %sf
  %1125 = trunc i64 %1122 to i8
  %1126 = call i8 @llvm.ctpop.i8(i8 %1125)
  %1127 = and i8 %1126, 1
  %1128 = icmp eq i8 %1127, 0
  store i1 %1128, i1* %pf
  store i64 %1122, i64* %r13
  store volatile i64 34183, i64* @assembly_address
  %1129 = load i64* %rbx
  %1130 = trunc i64 %1129 to i32
  %1131 = add i32 %1130, 8
  %1132 = and i32 %1130, 15
  %1133 = add i32 %1132, 8
  %1134 = icmp ugt i32 %1133, 15
  %1135 = icmp ult i32 %1131, %1130
  %1136 = xor i32 %1130, %1131
  %1137 = xor i32 8, %1131
  %1138 = and i32 %1136, %1137
  %1139 = icmp slt i32 %1138, 0
  store i1 %1134, i1* %az
  store i1 %1135, i1* %cf
  store i1 %1139, i1* %of
  %1140 = icmp eq i32 %1131, 0
  store i1 %1140, i1* %zf
  %1141 = icmp slt i32 %1131, 0
  store i1 %1141, i1* %sf
  %1142 = trunc i32 %1131 to i8
  %1143 = call i8 @llvm.ctpop.i8(i8 %1142)
  %1144 = and i8 %1143, 1
  %1145 = icmp eq i8 %1144, 0
  store i1 %1145, i1* %pf
  %1146 = zext i32 %1131 to i64
  store i64 %1146, i64* %rbx
  br label %block_858a

block_858a:                                       ; preds = %1119, %1017
  store volatile i64 34186, i64* @assembly_address
  %1147 = load i32* %stack_var_-96
  %1148 = zext i32 %1147 to i64
  store i64 %1148, i64* %rax
  store volatile i64 34189, i64* @assembly_address
  %1149 = load i64* %rbx
  %1150 = trunc i64 %1149 to i32
  %1151 = load i64* %rax
  %1152 = trunc i64 %1151 to i32
  %1153 = sub i32 %1150, %1152
  %1154 = and i32 %1150, 15
  %1155 = and i32 %1152, 15
  %1156 = sub i32 %1154, %1155
  %1157 = icmp ugt i32 %1156, 15
  %1158 = icmp ult i32 %1150, %1152
  %1159 = xor i32 %1150, %1152
  %1160 = xor i32 %1150, %1153
  %1161 = and i32 %1159, %1160
  %1162 = icmp slt i32 %1161, 0
  store i1 %1157, i1* %az
  store i1 %1158, i1* %cf
  store i1 %1162, i1* %of
  %1163 = icmp eq i32 %1153, 0
  store i1 %1163, i1* %zf
  %1164 = icmp slt i32 %1153, 0
  store i1 %1164, i1* %sf
  %1165 = trunc i32 %1153 to i8
  %1166 = call i8 @llvm.ctpop.i8(i8 %1165)
  %1167 = and i8 %1166, 1
  %1168 = icmp eq i8 %1167, 0
  store i1 %1168, i1* %pf
  store volatile i64 34191, i64* @assembly_address
  %1169 = load i1* %cf
  br i1 %1169, label %block_8538, label %block_8591

block_8591:                                       ; preds = %block_858a
  store volatile i64 34193, i64* @assembly_address
  %1170 = load i64* %r13
  %1171 = trunc i64 %1170 to i32
  %1172 = zext i32 %1171 to i64
  store i64 %1172, i64* %rax
  store volatile i64 34196, i64* @assembly_address
  %1173 = load i64* %rax
  %1174 = trunc i64 %1173 to i32
  %1175 = load i32* %stack_var_-52
  %1176 = and i32 %1174, %1175
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1177 = icmp eq i32 %1176, 0
  store i1 %1177, i1* %zf
  %1178 = icmp slt i32 %1176, 0
  store i1 %1178, i1* %sf
  %1179 = trunc i32 %1176 to i8
  %1180 = call i8 @llvm.ctpop.i8(i8 %1179)
  %1181 = and i8 %1180, 1
  %1182 = icmp eq i8 %1181, 0
  store i1 %1182, i1* %pf
  %1183 = zext i32 %1176 to i64
  store i64 %1183, i64* %rax
  store volatile i64 34199, i64* @assembly_address
  %1184 = load i64* %rax
  %1185 = trunc i64 %1184 to i32
  %1186 = zext i32 %1185 to i64
  store i64 %1186, i64* %rax
  store volatile i64 34201, i64* @assembly_address
  %1187 = load i64* %rax
  %1188 = load i1* %of
  %1189 = shl i64 %1187, 4
  %1190 = icmp eq i64 %1189, 0
  store i1 %1190, i1* %zf
  %1191 = icmp slt i64 %1189, 0
  store i1 %1191, i1* %sf
  %1192 = trunc i64 %1189 to i8
  %1193 = call i8 @llvm.ctpop.i8(i8 %1192)
  %1194 = and i8 %1193, 1
  %1195 = icmp eq i8 %1194, 0
  store i1 %1195, i1* %pf
  store i64 %1189, i64* %rax
  %1196 = shl i64 %1187, 3
  %1197 = lshr i64 %1196, 63
  %1198 = trunc i64 %1197 to i1
  store i1 %1198, i1* %cf
  %1199 = lshr i64 %1189, 63
  %1200 = icmp ne i64 %1199, %1197
  %1201 = select i1 false, i1 %1200, i1 %1188
  store i1 %1201, i1* %of
  store volatile i64 34205, i64* @assembly_address
  %1202 = load i64* %rax
  store i64 %1202, i64* %rdx
  store volatile i64 34208, i64* @assembly_address
  %1203 = load i64* %stack_var_-88
  store i64 %1203, i64* %rax
  store volatile i64 34212, i64* @assembly_address
  %1204 = load i64* %rax
  %1205 = load i64* %rdx
  %1206 = add i64 %1204, %1205
  %1207 = and i64 %1204, 15
  %1208 = and i64 %1205, 15
  %1209 = add i64 %1207, %1208
  %1210 = icmp ugt i64 %1209, 15
  %1211 = icmp ult i64 %1206, %1204
  %1212 = xor i64 %1204, %1206
  %1213 = xor i64 %1205, %1206
  %1214 = and i64 %1212, %1213
  %1215 = icmp slt i64 %1214, 0
  store i1 %1210, i1* %az
  store i1 %1211, i1* %cf
  store i1 %1215, i1* %of
  %1216 = icmp eq i64 %1206, 0
  store i1 %1216, i1* %zf
  %1217 = icmp slt i64 %1206, 0
  store i1 %1217, i1* %sf
  %1218 = trunc i64 %1206 to i8
  %1219 = call i8 @llvm.ctpop.i8(i8 %1218)
  %1220 = and i8 %1219, 1
  %1221 = icmp eq i8 %1220, 0
  store i1 %1221, i1* %pf
  store i64 %1206, i64* %rax
  store volatile i64 34215, i64* @assembly_address
  %1222 = load i64* %rax
  %1223 = inttoptr i64 %1222 to i8*
  store i8* %1223, i8** %stack_var_-48
  store volatile i64 34219, i64* @assembly_address
  %1224 = load i8** %stack_var_-48
  %1225 = ptrtoint i8* %1224 to i64
  store i64 %1225, i64* %rax
  store volatile i64 34223, i64* @assembly_address
  %1226 = load i64* %rax
  %1227 = inttoptr i64 %1226 to i8*
  %1228 = load i8* %1227
  %1229 = zext i8 %1228 to i64
  store i64 %1229, i64* %rax
  store volatile i64 34226, i64* @assembly_address
  %1230 = load i64* %rax
  %1231 = trunc i64 %1230 to i8
  %1232 = zext i8 %1231 to i64
  store i64 %1232, i64* %r12
  store volatile i64 34230, i64* @assembly_address
  %1233 = load i64* %r12
  %1234 = trunc i64 %1233 to i32
  %1235 = sub i32 %1234, 16
  %1236 = and i32 %1234, 15
  %1237 = icmp ugt i32 %1236, 15
  %1238 = icmp ult i32 %1234, 16
  %1239 = xor i32 %1234, 16
  %1240 = xor i32 %1234, %1235
  %1241 = and i32 %1239, %1240
  %1242 = icmp slt i32 %1241, 0
  store i1 %1237, i1* %az
  store i1 %1238, i1* %cf
  store i1 %1242, i1* %of
  %1243 = icmp eq i32 %1235, 0
  store i1 %1243, i1* %zf
  %1244 = icmp slt i32 %1235, 0
  store i1 %1244, i1* %sf
  %1245 = trunc i32 %1235 to i8
  %1246 = call i8 @llvm.ctpop.i8(i8 %1245)
  %1247 = and i8 %1246, 1
  %1248 = icmp eq i8 %1247, 0
  store i1 %1248, i1* %pf
  store volatile i64 34234, i64* @assembly_address
  %1249 = load i1* %cf
  %1250 = load i1* %zf
  %1251 = or i1 %1249, %1250
  br i1 %1251, label %block_868e, label %block_85c0

block_85c0:                                       ; preds = %block_864a, %block_8591
  store volatile i64 34240, i64* @assembly_address
  %1252 = load i64* %r12
  %1253 = trunc i64 %1252 to i32
  %1254 = sub i32 %1253, 99
  %1255 = and i32 %1253, 15
  %1256 = sub i32 %1255, 3
  %1257 = icmp ugt i32 %1256, 15
  %1258 = icmp ult i32 %1253, 99
  %1259 = xor i32 %1253, 99
  %1260 = xor i32 %1253, %1254
  %1261 = and i32 %1259, %1260
  %1262 = icmp slt i32 %1261, 0
  store i1 %1257, i1* %az
  store i1 %1258, i1* %cf
  store i1 %1262, i1* %of
  %1263 = icmp eq i32 %1254, 0
  store i1 %1263, i1* %zf
  %1264 = icmp slt i32 %1254, 0
  store i1 %1264, i1* %sf
  %1265 = trunc i32 %1254 to i8
  %1266 = call i8 @llvm.ctpop.i8(i8 %1265)
  %1267 = and i8 %1266, 1
  %1268 = icmp eq i8 %1267, 0
  store i1 %1268, i1* %pf
  store volatile i64 34244, i64* @assembly_address
  %1269 = load i1* %zf
  %1270 = icmp eq i1 %1269, false
  br i1 %1270, label %block_85d0, label %block_85c6

block_85c6:                                       ; preds = %block_85c0
  store volatile i64 34246, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 34251, i64* @assembly_address
  br label %block_8845

block_85d0:                                       ; preds = %block_85c0
  store volatile i64 34256, i64* @assembly_address
  %1271 = load i8** %stack_var_-48
  %1272 = ptrtoint i8* %1271 to i64
  store i64 %1272, i64* %rax
  store volatile i64 34260, i64* @assembly_address
  %1273 = load i64* %rax
  %1274 = add i64 %1273, 1
  %1275 = inttoptr i64 %1274 to i8*
  %1276 = load i8* %1275
  %1277 = zext i8 %1276 to i64
  store i64 %1277, i64* %rax
  store volatile i64 34264, i64* @assembly_address
  %1278 = load i64* %rax
  %1279 = trunc i64 %1278 to i8
  %1280 = zext i8 %1279 to i64
  store i64 %1280, i64* %rax
  store volatile i64 34267, i64* @assembly_address
  %1281 = load i64* %rax
  %1282 = trunc i64 %1281 to i32
  %1283 = zext i32 %1282 to i64
  store i64 %1283, i64* %rcx
  store volatile i64 34269, i64* @assembly_address
  %1284 = load i64* %r13
  %1285 = load i64* %rcx
  %1286 = trunc i64 %1285 to i8
  %1287 = zext i8 %1286 to i64
  %1288 = and i64 %1287, 63
  %1289 = load i1* %of
  %1290 = icmp eq i64 %1288, 0
  br i1 %1290, label %1306, label %1291

; <label>:1291                                    ; preds = %block_85d0
  %1292 = lshr i64 %1284, %1288
  %1293 = icmp eq i64 %1292, 0
  store i1 %1293, i1* %zf
  %1294 = icmp slt i64 %1292, 0
  store i1 %1294, i1* %sf
  %1295 = trunc i64 %1292 to i8
  %1296 = call i8 @llvm.ctpop.i8(i8 %1295)
  %1297 = and i8 %1296, 1
  %1298 = icmp eq i8 %1297, 0
  store i1 %1298, i1* %pf
  store i64 %1292, i64* %r13
  %1299 = sub i64 %1288, 1
  %1300 = shl i64 1, %1299
  %1301 = and i64 %1300, %1284
  %1302 = icmp ne i64 %1301, 0
  store i1 %1302, i1* %cf
  %1303 = icmp eq i64 %1288, 1
  %1304 = icmp slt i64 %1284, 0
  %1305 = select i1 %1303, i1 %1304, i1 %1289
  store i1 %1305, i1* %of
  br label %1306

; <label>:1306                                    ; preds = %block_85d0, %1291
  store volatile i64 34272, i64* @assembly_address
  %1307 = load i8** %stack_var_-48
  %1308 = ptrtoint i8* %1307 to i64
  store i64 %1308, i64* %rax
  store volatile i64 34276, i64* @assembly_address
  %1309 = load i64* %rax
  %1310 = add i64 %1309, 1
  %1311 = inttoptr i64 %1310 to i8*
  %1312 = load i8* %1311
  %1313 = zext i8 %1312 to i64
  store i64 %1313, i64* %rax
  store volatile i64 34280, i64* @assembly_address
  %1314 = load i64* %rax
  %1315 = trunc i64 %1314 to i8
  %1316 = zext i8 %1315 to i64
  store i64 %1316, i64* %rax
  store volatile i64 34283, i64* @assembly_address
  %1317 = load i64* %rbx
  %1318 = trunc i64 %1317 to i32
  %1319 = load i64* %rax
  %1320 = trunc i64 %1319 to i32
  %1321 = sub i32 %1318, %1320
  %1322 = and i32 %1318, 15
  %1323 = and i32 %1320, 15
  %1324 = sub i32 %1322, %1323
  %1325 = icmp ugt i32 %1324, 15
  %1326 = icmp ult i32 %1318, %1320
  %1327 = xor i32 %1318, %1320
  %1328 = xor i32 %1318, %1321
  %1329 = and i32 %1327, %1328
  %1330 = icmp slt i32 %1329, 0
  store i1 %1325, i1* %az
  store i1 %1326, i1* %cf
  store i1 %1330, i1* %of
  %1331 = icmp eq i32 %1321, 0
  store i1 %1331, i1* %zf
  %1332 = icmp slt i32 %1321, 0
  store i1 %1332, i1* %sf
  %1333 = trunc i32 %1321 to i8
  %1334 = call i8 @llvm.ctpop.i8(i8 %1333)
  %1335 = and i8 %1334, 1
  %1336 = icmp eq i8 %1335, 0
  store i1 %1336, i1* %pf
  %1337 = zext i32 %1321 to i64
  store i64 %1337, i64* %rbx
  store volatile i64 34285, i64* @assembly_address
  %1338 = load i64* %r12
  %1339 = trunc i64 %1338 to i32
  %1340 = sub i32 %1339, 16
  %1341 = and i32 %1339, 15
  %1342 = icmp ugt i32 %1341, 15
  %1343 = icmp ult i32 %1339, 16
  %1344 = xor i32 %1339, 16
  %1345 = xor i32 %1339, %1340
  %1346 = and i32 %1344, %1345
  %1347 = icmp slt i32 %1346, 0
  store i1 %1342, i1* %az
  store i1 %1343, i1* %cf
  store i1 %1347, i1* %of
  %1348 = icmp eq i32 %1340, 0
  store i1 %1348, i1* %zf
  %1349 = icmp slt i32 %1340, 0
  store i1 %1349, i1* %sf
  %1350 = trunc i32 %1340 to i8
  %1351 = call i8 @llvm.ctpop.i8(i8 %1350)
  %1352 = and i8 %1351, 1
  %1353 = icmp eq i8 %1352, 0
  store i1 %1353, i1* %pf
  %1354 = zext i32 %1340 to i64
  store i64 %1354, i64* %r12
  store volatile i64 34289, i64* @assembly_address
  br label %block_8645

block_85f3:                                       ; preds = %block_8645
  store volatile i64 34291, i64* @assembly_address
  %1355 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1356 = zext i32 %1355 to i64
  store i64 %1356, i64* %rdx
  store volatile i64 34297, i64* @assembly_address
  %1357 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1358 = zext i32 %1357 to i64
  store i64 %1358, i64* %rax
  store volatile i64 34303, i64* @assembly_address
  %1359 = load i64* %rdx
  %1360 = trunc i64 %1359 to i32
  %1361 = load i64* %rax
  %1362 = trunc i64 %1361 to i32
  %1363 = sub i32 %1360, %1362
  %1364 = and i32 %1360, 15
  %1365 = and i32 %1362, 15
  %1366 = sub i32 %1364, %1365
  %1367 = icmp ugt i32 %1366, 15
  %1368 = icmp ult i32 %1360, %1362
  %1369 = xor i32 %1360, %1362
  %1370 = xor i32 %1360, %1363
  %1371 = and i32 %1369, %1370
  %1372 = icmp slt i32 %1371, 0
  store i1 %1367, i1* %az
  store i1 %1368, i1* %cf
  store i1 %1372, i1* %of
  %1373 = icmp eq i32 %1363, 0
  store i1 %1373, i1* %zf
  %1374 = icmp slt i32 %1363, 0
  store i1 %1374, i1* %sf
  %1375 = trunc i32 %1363 to i8
  %1376 = call i8 @llvm.ctpop.i8(i8 %1375)
  %1377 = and i8 %1376, 1
  %1378 = icmp eq i8 %1377, 0
  store i1 %1378, i1* %pf
  store volatile i64 34305, i64* @assembly_address
  %1379 = load i1* %cf
  %1380 = icmp eq i1 %1379, false
  br i1 %1380, label %block_8624, label %block_8603

block_8603:                                       ; preds = %block_85f3
  store volatile i64 34307, i64* @assembly_address
  %1381 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1382 = zext i32 %1381 to i64
  store i64 %1382, i64* %rax
  store volatile i64 34313, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 34316, i64* @assembly_address
  %1383 = load i64* %rdx
  %1384 = trunc i64 %1383 to i32
  store i32 %1384, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 34322, i64* @assembly_address
  %1385 = load i64* %rax
  %1386 = trunc i64 %1385 to i32
  %1387 = zext i32 %1386 to i64
  store i64 %1387, i64* %rdx
  store volatile i64 34324, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 34331, i64* @assembly_address
  %1388 = load i64* %rdx
  %1389 = load i64* %rax
  %1390 = mul i64 %1389, 1
  %1391 = add i64 %1388, %1390
  %1392 = inttoptr i64 %1391 to i8*
  %1393 = load i8* %1392
  %1394 = zext i8 %1393 to i64
  store i64 %1394, i64* %rax
  store volatile i64 34335, i64* @assembly_address
  %1395 = load i64* %rax
  %1396 = trunc i64 %1395 to i8
  %1397 = zext i8 %1396 to i64
  store i64 %1397, i64* %rax
  store volatile i64 34338, i64* @assembly_address
  br label %block_863a

block_8624:                                       ; preds = %block_85f3
  store volatile i64 34340, i64* @assembly_address
  %1398 = load i64** %stack_var_-60
  %1399 = ptrtoint i64* %1398 to i32
  %1400 = zext i32 %1399 to i64
  store i64 %1400, i64* %rax
  store volatile i64 34343, i64* @assembly_address
  %1401 = load i64* %rax
  %1402 = trunc i64 %1401 to i32
  store i32 %1402, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 34349, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 34354, i64* @assembly_address
  %1403 = load i64* %rdi
  %1404 = trunc i64 %1403 to i32
  %1405 = call i64 @fill_inbuf(i32 %1404)
  store i64 %1405, i64* %rax
  store i64 %1405, i64* %rax
  store volatile i64 34359, i64* @assembly_address
  %1406 = load i64* %rax
  %1407 = trunc i64 %1406 to i8
  %1408 = zext i8 %1407 to i64
  store i64 %1408, i64* %rax
  br label %block_863a

block_863a:                                       ; preds = %block_8624, %block_8603
  store volatile i64 34362, i64* @assembly_address
  %1409 = load i64* %rbx
  %1410 = trunc i64 %1409 to i32
  %1411 = zext i32 %1410 to i64
  store i64 %1411, i64* %rcx
  store volatile i64 34364, i64* @assembly_address
  %1412 = load i64* %rax
  %1413 = load i64* %rcx
  %1414 = trunc i64 %1413 to i8
  %1415 = zext i8 %1414 to i64
  %1416 = and i64 %1415, 63
  %1417 = load i1* %of
  %1418 = icmp eq i64 %1416, 0
  br i1 %1418, label %1435, label %1419

; <label>:1419                                    ; preds = %block_863a
  %1420 = shl i64 %1412, %1416
  %1421 = icmp eq i64 %1420, 0
  store i1 %1421, i1* %zf
  %1422 = icmp slt i64 %1420, 0
  store i1 %1422, i1* %sf
  %1423 = trunc i64 %1420 to i8
  %1424 = call i8 @llvm.ctpop.i8(i8 %1423)
  %1425 = and i8 %1424, 1
  %1426 = icmp eq i8 %1425, 0
  store i1 %1426, i1* %pf
  store i64 %1420, i64* %rax
  %1427 = sub i64 %1416, 1
  %1428 = shl i64 %1412, %1427
  %1429 = lshr i64 %1428, 63
  %1430 = trunc i64 %1429 to i1
  store i1 %1430, i1* %cf
  %1431 = lshr i64 %1420, 63
  %1432 = icmp ne i64 %1431, %1429
  %1433 = icmp eq i64 %1416, 1
  %1434 = select i1 %1433, i1 %1432, i1 %1417
  store i1 %1434, i1* %of
  br label %1435

; <label>:1435                                    ; preds = %block_863a, %1419
  store volatile i64 34367, i64* @assembly_address
  %1436 = load i64* %r13
  %1437 = load i64* %rax
  %1438 = or i64 %1436, %1437
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1439 = icmp eq i64 %1438, 0
  store i1 %1439, i1* %zf
  %1440 = icmp slt i64 %1438, 0
  store i1 %1440, i1* %sf
  %1441 = trunc i64 %1438 to i8
  %1442 = call i8 @llvm.ctpop.i8(i8 %1441)
  %1443 = and i8 %1442, 1
  %1444 = icmp eq i8 %1443, 0
  store i1 %1444, i1* %pf
  store i64 %1438, i64* %r13
  store volatile i64 34370, i64* @assembly_address
  %1445 = load i64* %rbx
  %1446 = trunc i64 %1445 to i32
  %1447 = add i32 %1446, 8
  %1448 = and i32 %1446, 15
  %1449 = add i32 %1448, 8
  %1450 = icmp ugt i32 %1449, 15
  %1451 = icmp ult i32 %1447, %1446
  %1452 = xor i32 %1446, %1447
  %1453 = xor i32 8, %1447
  %1454 = and i32 %1452, %1453
  %1455 = icmp slt i32 %1454, 0
  store i1 %1450, i1* %az
  store i1 %1451, i1* %cf
  store i1 %1455, i1* %of
  %1456 = icmp eq i32 %1447, 0
  store i1 %1456, i1* %zf
  %1457 = icmp slt i32 %1447, 0
  store i1 %1457, i1* %sf
  %1458 = trunc i32 %1447 to i8
  %1459 = call i8 @llvm.ctpop.i8(i8 %1458)
  %1460 = and i8 %1459, 1
  %1461 = icmp eq i8 %1460, 0
  store i1 %1461, i1* %pf
  %1462 = zext i32 %1447 to i64
  store i64 %1462, i64* %rbx
  br label %block_8645

block_8645:                                       ; preds = %1435, %1306
  store volatile i64 34373, i64* @assembly_address
  %1463 = load i64* %rbx
  %1464 = trunc i64 %1463 to i32
  %1465 = load i64* %r12
  %1466 = trunc i64 %1465 to i32
  %1467 = sub i32 %1464, %1466
  %1468 = and i32 %1464, 15
  %1469 = and i32 %1466, 15
  %1470 = sub i32 %1468, %1469
  %1471 = icmp ugt i32 %1470, 15
  %1472 = icmp ult i32 %1464, %1466
  %1473 = xor i32 %1464, %1466
  %1474 = xor i32 %1464, %1467
  %1475 = and i32 %1473, %1474
  %1476 = icmp slt i32 %1475, 0
  store i1 %1471, i1* %az
  store i1 %1472, i1* %cf
  store i1 %1476, i1* %of
  %1477 = icmp eq i32 %1467, 0
  store i1 %1477, i1* %zf
  %1478 = icmp slt i32 %1467, 0
  store i1 %1478, i1* %sf
  %1479 = trunc i32 %1467 to i8
  %1480 = call i8 @llvm.ctpop.i8(i8 %1479)
  %1481 = and i8 %1480, 1
  %1482 = icmp eq i8 %1481, 0
  store i1 %1482, i1* %pf
  store volatile i64 34376, i64* @assembly_address
  %1483 = load i1* %cf
  br i1 %1483, label %block_85f3, label %block_864a

block_864a:                                       ; preds = %block_8645
  store volatile i64 34378, i64* @assembly_address
  %1484 = load i8** %stack_var_-48
  %1485 = ptrtoint i8* %1484 to i64
  store i64 %1485, i64* %rax
  store volatile i64 34382, i64* @assembly_address
  %1486 = load i64* %rax
  %1487 = add i64 %1486, 8
  %1488 = inttoptr i64 %1487 to i64*
  %1489 = load i64* %1488
  store i64 %1489, i64* %rdx
  store volatile i64 34386, i64* @assembly_address
  %1490 = load i64* %r13
  %1491 = trunc i64 %1490 to i32
  %1492 = zext i32 %1491 to i64
  store i64 %1492, i64* %rsi
  store volatile i64 34389, i64* @assembly_address
  %1493 = load i64* %r12
  %1494 = trunc i64 %1493 to i32
  %1495 = zext i32 %1494 to i64
  store i64 %1495, i64* %rax
  store volatile i64 34392, i64* @assembly_address
  %1496 = load i64* %rax
  %1497 = load i64* %rax
  %1498 = mul i64 %1497, 1
  %1499 = add i64 %1496, %1498
  store i64 %1499, i64* %rcx
  store volatile i64 34396, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162e0 to i64), i64* %rax
  store volatile i64 34403, i64* @assembly_address
  %1500 = load i64* %rcx
  %1501 = load i64* %rax
  %1502 = mul i64 %1501, 1
  %1503 = add i64 %1500, %1502
  %1504 = inttoptr i64 %1503 to i16*
  %1505 = load i16* %1504
  %1506 = zext i16 %1505 to i64
  store i64 %1506, i64* %rax
  store volatile i64 34407, i64* @assembly_address
  %1507 = load i64* %rax
  %1508 = trunc i64 %1507 to i16
  %1509 = zext i16 %1508 to i64
  store i64 %1509, i64* %rax
  store volatile i64 34410, i64* @assembly_address
  %1510 = load i64* %rax
  %1511 = trunc i64 %1510 to i32
  %1512 = load i64* %rsi
  %1513 = trunc i64 %1512 to i32
  %1514 = and i32 %1511, %1513
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1515 = icmp eq i32 %1514, 0
  store i1 %1515, i1* %zf
  %1516 = icmp slt i32 %1514, 0
  store i1 %1516, i1* %sf
  %1517 = trunc i32 %1514 to i8
  %1518 = call i8 @llvm.ctpop.i8(i8 %1517)
  %1519 = and i8 %1518, 1
  %1520 = icmp eq i8 %1519, 0
  store i1 %1520, i1* %pf
  %1521 = zext i32 %1514 to i64
  store i64 %1521, i64* %rax
  store volatile i64 34412, i64* @assembly_address
  %1522 = load i64* %rax
  %1523 = trunc i64 %1522 to i32
  %1524 = zext i32 %1523 to i64
  store i64 %1524, i64* %rax
  store volatile i64 34414, i64* @assembly_address
  %1525 = load i64* %rax
  %1526 = load i1* %of
  %1527 = shl i64 %1525, 4
  %1528 = icmp eq i64 %1527, 0
  store i1 %1528, i1* %zf
  %1529 = icmp slt i64 %1527, 0
  store i1 %1529, i1* %sf
  %1530 = trunc i64 %1527 to i8
  %1531 = call i8 @llvm.ctpop.i8(i8 %1530)
  %1532 = and i8 %1531, 1
  %1533 = icmp eq i8 %1532, 0
  store i1 %1533, i1* %pf
  store i64 %1527, i64* %rax
  %1534 = shl i64 %1525, 3
  %1535 = lshr i64 %1534, 63
  %1536 = trunc i64 %1535 to i1
  store i1 %1536, i1* %cf
  %1537 = lshr i64 %1527, 63
  %1538 = icmp ne i64 %1537, %1535
  %1539 = select i1 false, i1 %1538, i1 %1526
  store i1 %1539, i1* %of
  store volatile i64 34418, i64* @assembly_address
  %1540 = load i64* %rax
  %1541 = load i64* %rdx
  %1542 = add i64 %1540, %1541
  %1543 = and i64 %1540, 15
  %1544 = and i64 %1541, 15
  %1545 = add i64 %1543, %1544
  %1546 = icmp ugt i64 %1545, 15
  %1547 = icmp ult i64 %1542, %1540
  %1548 = xor i64 %1540, %1542
  %1549 = xor i64 %1541, %1542
  %1550 = and i64 %1548, %1549
  %1551 = icmp slt i64 %1550, 0
  store i1 %1546, i1* %az
  store i1 %1547, i1* %cf
  store i1 %1551, i1* %of
  %1552 = icmp eq i64 %1542, 0
  store i1 %1552, i1* %zf
  %1553 = icmp slt i64 %1542, 0
  store i1 %1553, i1* %sf
  %1554 = trunc i64 %1542 to i8
  %1555 = call i8 @llvm.ctpop.i8(i8 %1554)
  %1556 = and i8 %1555, 1
  %1557 = icmp eq i8 %1556, 0
  store i1 %1557, i1* %pf
  store i64 %1542, i64* %rax
  store volatile i64 34421, i64* @assembly_address
  %1558 = load i64* %rax
  %1559 = inttoptr i64 %1558 to i8*
  store i8* %1559, i8** %stack_var_-48
  store volatile i64 34425, i64* @assembly_address
  %1560 = load i8** %stack_var_-48
  %1561 = ptrtoint i8* %1560 to i64
  store i64 %1561, i64* %rax
  store volatile i64 34429, i64* @assembly_address
  %1562 = load i64* %rax
  %1563 = inttoptr i64 %1562 to i8*
  %1564 = load i8* %1563
  %1565 = zext i8 %1564 to i64
  store i64 %1565, i64* %rax
  store volatile i64 34432, i64* @assembly_address
  %1566 = load i64* %rax
  %1567 = trunc i64 %1566 to i8
  %1568 = zext i8 %1567 to i64
  store i64 %1568, i64* %r12
  store volatile i64 34436, i64* @assembly_address
  %1569 = load i64* %r12
  %1570 = trunc i64 %1569 to i32
  %1571 = sub i32 %1570, 16
  %1572 = and i32 %1570, 15
  %1573 = icmp ugt i32 %1572, 15
  %1574 = icmp ult i32 %1570, 16
  %1575 = xor i32 %1570, 16
  %1576 = xor i32 %1570, %1571
  %1577 = and i32 %1575, %1576
  %1578 = icmp slt i32 %1577, 0
  store i1 %1573, i1* %az
  store i1 %1574, i1* %cf
  store i1 %1578, i1* %of
  %1579 = icmp eq i32 %1571, 0
  store i1 %1579, i1* %zf
  %1580 = icmp slt i32 %1571, 0
  store i1 %1580, i1* %sf
  %1581 = trunc i32 %1571 to i8
  %1582 = call i8 @llvm.ctpop.i8(i8 %1581)
  %1583 = and i8 %1582, 1
  %1584 = icmp eq i8 %1583, 0
  store i1 %1584, i1* %pf
  store volatile i64 34440, i64* @assembly_address
  %1585 = load i1* %cf
  %1586 = load i1* %zf
  %1587 = or i1 %1585, %1586
  %1588 = icmp ne i1 %1587, true
  br i1 %1588, label %block_85c0, label %block_868e

block_868e:                                       ; preds = %block_864a, %block_8591
  store volatile i64 34446, i64* @assembly_address
  %1589 = load i8** %stack_var_-48
  %1590 = ptrtoint i8* %1589 to i64
  store i64 %1590, i64* %rax
  store volatile i64 34450, i64* @assembly_address
  %1591 = load i64* %rax
  %1592 = add i64 %1591, 1
  %1593 = inttoptr i64 %1592 to i8*
  %1594 = load i8* %1593
  %1595 = zext i8 %1594 to i64
  store i64 %1595, i64* %rax
  store volatile i64 34454, i64* @assembly_address
  %1596 = load i64* %rax
  %1597 = trunc i64 %1596 to i8
  %1598 = zext i8 %1597 to i64
  store i64 %1598, i64* %rax
  store volatile i64 34457, i64* @assembly_address
  %1599 = load i64* %rax
  %1600 = trunc i64 %1599 to i32
  %1601 = zext i32 %1600 to i64
  store i64 %1601, i64* %rcx
  store volatile i64 34459, i64* @assembly_address
  %1602 = load i64* %r13
  %1603 = load i64* %rcx
  %1604 = trunc i64 %1603 to i8
  %1605 = zext i8 %1604 to i64
  %1606 = and i64 %1605, 63
  %1607 = load i1* %of
  %1608 = icmp eq i64 %1606, 0
  br i1 %1608, label %1624, label %1609

; <label>:1609                                    ; preds = %block_868e
  %1610 = lshr i64 %1602, %1606
  %1611 = icmp eq i64 %1610, 0
  store i1 %1611, i1* %zf
  %1612 = icmp slt i64 %1610, 0
  store i1 %1612, i1* %sf
  %1613 = trunc i64 %1610 to i8
  %1614 = call i8 @llvm.ctpop.i8(i8 %1613)
  %1615 = and i8 %1614, 1
  %1616 = icmp eq i8 %1615, 0
  store i1 %1616, i1* %pf
  store i64 %1610, i64* %r13
  %1617 = sub i64 %1606, 1
  %1618 = shl i64 1, %1617
  %1619 = and i64 %1618, %1602
  %1620 = icmp ne i64 %1619, 0
  store i1 %1620, i1* %cf
  %1621 = icmp eq i64 %1606, 1
  %1622 = icmp slt i64 %1602, 0
  %1623 = select i1 %1621, i1 %1622, i1 %1607
  store i1 %1623, i1* %of
  br label %1624

; <label>:1624                                    ; preds = %block_868e, %1609
  store volatile i64 34462, i64* @assembly_address
  %1625 = load i8** %stack_var_-48
  %1626 = ptrtoint i8* %1625 to i64
  store i64 %1626, i64* %rax
  store volatile i64 34466, i64* @assembly_address
  %1627 = load i64* %rax
  %1628 = add i64 %1627, 1
  %1629 = inttoptr i64 %1628 to i8*
  %1630 = load i8* %1629
  %1631 = zext i8 %1630 to i64
  store i64 %1631, i64* %rax
  store volatile i64 34470, i64* @assembly_address
  %1632 = load i64* %rax
  %1633 = trunc i64 %1632 to i8
  %1634 = zext i8 %1633 to i64
  store i64 %1634, i64* %rax
  store volatile i64 34473, i64* @assembly_address
  %1635 = load i64* %rbx
  %1636 = trunc i64 %1635 to i32
  %1637 = load i64* %rax
  %1638 = trunc i64 %1637 to i32
  %1639 = sub i32 %1636, %1638
  %1640 = and i32 %1636, 15
  %1641 = and i32 %1638, 15
  %1642 = sub i32 %1640, %1641
  %1643 = icmp ugt i32 %1642, 15
  %1644 = icmp ult i32 %1636, %1638
  %1645 = xor i32 %1636, %1638
  %1646 = xor i32 %1636, %1639
  %1647 = and i32 %1645, %1646
  %1648 = icmp slt i32 %1647, 0
  store i1 %1643, i1* %az
  store i1 %1644, i1* %cf
  store i1 %1648, i1* %of
  %1649 = icmp eq i32 %1639, 0
  store i1 %1649, i1* %zf
  %1650 = icmp slt i32 %1639, 0
  store i1 %1650, i1* %sf
  %1651 = trunc i32 %1639 to i8
  %1652 = call i8 @llvm.ctpop.i8(i8 %1651)
  %1653 = and i8 %1652, 1
  %1654 = icmp eq i8 %1653, 0
  store i1 %1654, i1* %pf
  %1655 = zext i32 %1639 to i64
  store i64 %1655, i64* %rbx
  store volatile i64 34475, i64* @assembly_address
  br label %block_86ff

block_86ad:                                       ; preds = %block_86ff
  store volatile i64 34477, i64* @assembly_address
  %1656 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1657 = zext i32 %1656 to i64
  store i64 %1657, i64* %rdx
  store volatile i64 34483, i64* @assembly_address
  %1658 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1659 = zext i32 %1658 to i64
  store i64 %1659, i64* %rax
  store volatile i64 34489, i64* @assembly_address
  %1660 = load i64* %rdx
  %1661 = trunc i64 %1660 to i32
  %1662 = load i64* %rax
  %1663 = trunc i64 %1662 to i32
  %1664 = sub i32 %1661, %1663
  %1665 = and i32 %1661, 15
  %1666 = and i32 %1663, 15
  %1667 = sub i32 %1665, %1666
  %1668 = icmp ugt i32 %1667, 15
  %1669 = icmp ult i32 %1661, %1663
  %1670 = xor i32 %1661, %1663
  %1671 = xor i32 %1661, %1664
  %1672 = and i32 %1670, %1671
  %1673 = icmp slt i32 %1672, 0
  store i1 %1668, i1* %az
  store i1 %1669, i1* %cf
  store i1 %1673, i1* %of
  %1674 = icmp eq i32 %1664, 0
  store i1 %1674, i1* %zf
  %1675 = icmp slt i32 %1664, 0
  store i1 %1675, i1* %sf
  %1676 = trunc i32 %1664 to i8
  %1677 = call i8 @llvm.ctpop.i8(i8 %1676)
  %1678 = and i8 %1677, 1
  %1679 = icmp eq i8 %1678, 0
  store i1 %1679, i1* %pf
  store volatile i64 34491, i64* @assembly_address
  %1680 = load i1* %cf
  %1681 = icmp eq i1 %1680, false
  br i1 %1681, label %block_86de, label %block_86bd

block_86bd:                                       ; preds = %block_86ad
  store volatile i64 34493, i64* @assembly_address
  %1682 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1683 = zext i32 %1682 to i64
  store i64 %1683, i64* %rax
  store volatile i64 34499, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 34502, i64* @assembly_address
  %1684 = load i64* %rdx
  %1685 = trunc i64 %1684 to i32
  store i32 %1685, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 34508, i64* @assembly_address
  %1686 = load i64* %rax
  %1687 = trunc i64 %1686 to i32
  %1688 = zext i32 %1687 to i64
  store i64 %1688, i64* %rdx
  store volatile i64 34510, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 34517, i64* @assembly_address
  %1689 = load i64* %rdx
  %1690 = load i64* %rax
  %1691 = mul i64 %1690, 1
  %1692 = add i64 %1689, %1691
  %1693 = inttoptr i64 %1692 to i8*
  %1694 = load i8* %1693
  %1695 = zext i8 %1694 to i64
  store i64 %1695, i64* %rax
  store volatile i64 34521, i64* @assembly_address
  %1696 = load i64* %rax
  %1697 = trunc i64 %1696 to i8
  %1698 = zext i8 %1697 to i64
  store i64 %1698, i64* %rax
  store volatile i64 34524, i64* @assembly_address
  br label %block_86f4

block_86de:                                       ; preds = %block_86ad
  store volatile i64 34526, i64* @assembly_address
  %1699 = load i64** %stack_var_-60
  %1700 = ptrtoint i64* %1699 to i32
  %1701 = zext i32 %1700 to i64
  store i64 %1701, i64* %rax
  store volatile i64 34529, i64* @assembly_address
  %1702 = load i64* %rax
  %1703 = trunc i64 %1702 to i32
  store i32 %1703, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 34535, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 34540, i64* @assembly_address
  %1704 = load i64* %rdi
  %1705 = trunc i64 %1704 to i32
  %1706 = call i64 @fill_inbuf(i32 %1705)
  store i64 %1706, i64* %rax
  store i64 %1706, i64* %rax
  store volatile i64 34545, i64* @assembly_address
  %1707 = load i64* %rax
  %1708 = trunc i64 %1707 to i8
  %1709 = zext i8 %1708 to i64
  store i64 %1709, i64* %rax
  br label %block_86f4

block_86f4:                                       ; preds = %block_86de, %block_86bd
  store volatile i64 34548, i64* @assembly_address
  %1710 = load i64* %rbx
  %1711 = trunc i64 %1710 to i32
  %1712 = zext i32 %1711 to i64
  store i64 %1712, i64* %rcx
  store volatile i64 34550, i64* @assembly_address
  %1713 = load i64* %rax
  %1714 = load i64* %rcx
  %1715 = trunc i64 %1714 to i8
  %1716 = zext i8 %1715 to i64
  %1717 = and i64 %1716, 63
  %1718 = load i1* %of
  %1719 = icmp eq i64 %1717, 0
  br i1 %1719, label %1736, label %1720

; <label>:1720                                    ; preds = %block_86f4
  %1721 = shl i64 %1713, %1717
  %1722 = icmp eq i64 %1721, 0
  store i1 %1722, i1* %zf
  %1723 = icmp slt i64 %1721, 0
  store i1 %1723, i1* %sf
  %1724 = trunc i64 %1721 to i8
  %1725 = call i8 @llvm.ctpop.i8(i8 %1724)
  %1726 = and i8 %1725, 1
  %1727 = icmp eq i8 %1726, 0
  store i1 %1727, i1* %pf
  store i64 %1721, i64* %rax
  %1728 = sub i64 %1717, 1
  %1729 = shl i64 %1713, %1728
  %1730 = lshr i64 %1729, 63
  %1731 = trunc i64 %1730 to i1
  store i1 %1731, i1* %cf
  %1732 = lshr i64 %1721, 63
  %1733 = icmp ne i64 %1732, %1730
  %1734 = icmp eq i64 %1717, 1
  %1735 = select i1 %1734, i1 %1733, i1 %1718
  store i1 %1735, i1* %of
  br label %1736

; <label>:1736                                    ; preds = %block_86f4, %1720
  store volatile i64 34553, i64* @assembly_address
  %1737 = load i64* %r13
  %1738 = load i64* %rax
  %1739 = or i64 %1737, %1738
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1740 = icmp eq i64 %1739, 0
  store i1 %1740, i1* %zf
  %1741 = icmp slt i64 %1739, 0
  store i1 %1741, i1* %sf
  %1742 = trunc i64 %1739 to i8
  %1743 = call i8 @llvm.ctpop.i8(i8 %1742)
  %1744 = and i8 %1743, 1
  %1745 = icmp eq i8 %1744, 0
  store i1 %1745, i1* %pf
  store i64 %1739, i64* %r13
  store volatile i64 34556, i64* @assembly_address
  %1746 = load i64* %rbx
  %1747 = trunc i64 %1746 to i32
  %1748 = add i32 %1747, 8
  %1749 = and i32 %1747, 15
  %1750 = add i32 %1749, 8
  %1751 = icmp ugt i32 %1750, 15
  %1752 = icmp ult i32 %1748, %1747
  %1753 = xor i32 %1747, %1748
  %1754 = xor i32 8, %1748
  %1755 = and i32 %1753, %1754
  %1756 = icmp slt i32 %1755, 0
  store i1 %1751, i1* %az
  store i1 %1752, i1* %cf
  store i1 %1756, i1* %of
  %1757 = icmp eq i32 %1748, 0
  store i1 %1757, i1* %zf
  %1758 = icmp slt i32 %1748, 0
  store i1 %1758, i1* %sf
  %1759 = trunc i32 %1748 to i8
  %1760 = call i8 @llvm.ctpop.i8(i8 %1759)
  %1761 = and i8 %1760, 1
  %1762 = icmp eq i8 %1761, 0
  store i1 %1762, i1* %pf
  %1763 = zext i32 %1748 to i64
  store i64 %1763, i64* %rbx
  br label %block_86ff

block_86ff:                                       ; preds = %1736, %1624
  store volatile i64 34559, i64* @assembly_address
  %1764 = load i64* %rbx
  %1765 = trunc i64 %1764 to i32
  %1766 = load i64* %r12
  %1767 = trunc i64 %1766 to i32
  %1768 = sub i32 %1765, %1767
  %1769 = and i32 %1765, 15
  %1770 = and i32 %1767, 15
  %1771 = sub i32 %1769, %1770
  %1772 = icmp ugt i32 %1771, 15
  %1773 = icmp ult i32 %1765, %1767
  %1774 = xor i32 %1765, %1767
  %1775 = xor i32 %1765, %1768
  %1776 = and i32 %1774, %1775
  %1777 = icmp slt i32 %1776, 0
  store i1 %1772, i1* %az
  store i1 %1773, i1* %cf
  store i1 %1777, i1* %of
  %1778 = icmp eq i32 %1768, 0
  store i1 %1778, i1* %zf
  %1779 = icmp slt i32 %1768, 0
  store i1 %1779, i1* %sf
  %1780 = trunc i32 %1768 to i8
  %1781 = call i8 @llvm.ctpop.i8(i8 %1780)
  %1782 = and i8 %1781, 1
  %1783 = icmp eq i8 %1782, 0
  store i1 %1783, i1* %pf
  store volatile i64 34562, i64* @assembly_address
  %1784 = load i1* %cf
  br i1 %1784, label %block_86ad, label %block_8704

block_8704:                                       ; preds = %block_86ff
  store volatile i64 34564, i64* @assembly_address
  %1785 = load i8** %stack_var_-48
  %1786 = ptrtoint i8* %1785 to i64
  store i64 %1786, i64* %rax
  store volatile i64 34568, i64* @assembly_address
  %1787 = load i64* %rax
  %1788 = add i64 %1787, 8
  %1789 = inttoptr i64 %1788 to i16*
  %1790 = load i16* %1789
  %1791 = zext i16 %1790 to i64
  store i64 %1791, i64* %rax
  store volatile i64 34572, i64* @assembly_address
  %1792 = load i64* %rax
  %1793 = trunc i64 %1792 to i16
  %1794 = zext i16 %1793 to i64
  store i64 %1794, i64* %rax
  store volatile i64 34575, i64* @assembly_address
  %1795 = load i64** %stack_var_-60
  %1796 = ptrtoint i64* %1795 to i32
  %1797 = zext i32 %1796 to i64
  store i64 %1797, i64* %rdx
  store volatile i64 34578, i64* @assembly_address
  %1798 = load i64* %rdx
  %1799 = trunc i64 %1798 to i32
  %1800 = zext i32 %1799 to i64
  store i64 %1800, i64* %rcx
  store volatile i64 34580, i64* @assembly_address
  %1801 = load i64* %rcx
  %1802 = trunc i64 %1801 to i32
  %1803 = load i64* %rax
  %1804 = trunc i64 %1803 to i32
  %1805 = sub i32 %1802, %1804
  %1806 = and i32 %1802, 15
  %1807 = and i32 %1804, 15
  %1808 = sub i32 %1806, %1807
  %1809 = icmp ugt i32 %1808, 15
  %1810 = icmp ult i32 %1802, %1804
  %1811 = xor i32 %1802, %1804
  %1812 = xor i32 %1802, %1805
  %1813 = and i32 %1811, %1812
  %1814 = icmp slt i32 %1813, 0
  store i1 %1809, i1* %az
  store i1 %1810, i1* %cf
  store i1 %1814, i1* %of
  %1815 = icmp eq i32 %1805, 0
  store i1 %1815, i1* %zf
  %1816 = icmp slt i32 %1805, 0
  store i1 %1816, i1* %sf
  %1817 = trunc i32 %1805 to i8
  %1818 = call i8 @llvm.ctpop.i8(i8 %1817)
  %1819 = and i8 %1818, 1
  %1820 = icmp eq i8 %1819, 0
  store i1 %1820, i1* %pf
  %1821 = zext i32 %1805 to i64
  store i64 %1821, i64* %rcx
  store volatile i64 34582, i64* @assembly_address
  %1822 = load i64* %r13
  %1823 = trunc i64 %1822 to i32
  %1824 = zext i32 %1823 to i64
  store i64 %1824, i64* %rsi
  store volatile i64 34585, i64* @assembly_address
  %1825 = load i64* %r12
  %1826 = trunc i64 %1825 to i32
  %1827 = zext i32 %1826 to i64
  store i64 %1827, i64* %rax
  store volatile i64 34588, i64* @assembly_address
  %1828 = load i64* %rax
  %1829 = load i64* %rax
  %1830 = mul i64 %1829, 1
  %1831 = add i64 %1828, %1830
  store i64 %1831, i64* %rdx
  store volatile i64 34592, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162e0 to i64), i64* %rax
  store volatile i64 34599, i64* @assembly_address
  %1832 = load i64* %rdx
  %1833 = load i64* %rax
  %1834 = mul i64 %1833, 1
  %1835 = add i64 %1832, %1834
  %1836 = inttoptr i64 %1835 to i16*
  %1837 = load i16* %1836
  %1838 = zext i16 %1837 to i64
  store i64 %1838, i64* %rax
  store volatile i64 34603, i64* @assembly_address
  %1839 = load i64* %rax
  %1840 = trunc i64 %1839 to i16
  %1841 = zext i16 %1840 to i64
  store i64 %1841, i64* %rax
  store volatile i64 34606, i64* @assembly_address
  %1842 = load i64* %rax
  %1843 = trunc i64 %1842 to i32
  %1844 = load i64* %rsi
  %1845 = trunc i64 %1844 to i32
  %1846 = and i32 %1843, %1845
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1847 = icmp eq i32 %1846, 0
  store i1 %1847, i1* %zf
  %1848 = icmp slt i32 %1846, 0
  store i1 %1848, i1* %sf
  %1849 = trunc i32 %1846 to i8
  %1850 = call i8 @llvm.ctpop.i8(i8 %1849)
  %1851 = and i8 %1850, 1
  %1852 = icmp eq i8 %1851, 0
  store i1 %1852, i1* %pf
  %1853 = zext i32 %1846 to i64
  store i64 %1853, i64* %rax
  store volatile i64 34608, i64* @assembly_address
  %1854 = load i64* %rcx
  %1855 = trunc i64 %1854 to i32
  %1856 = load i64* %rax
  %1857 = trunc i64 %1856 to i32
  %1858 = sub i32 %1855, %1857
  %1859 = and i32 %1855, 15
  %1860 = and i32 %1857, 15
  %1861 = sub i32 %1859, %1860
  %1862 = icmp ugt i32 %1861, 15
  %1863 = icmp ult i32 %1855, %1857
  %1864 = xor i32 %1855, %1857
  %1865 = xor i32 %1855, %1858
  %1866 = and i32 %1864, %1865
  %1867 = icmp slt i32 %1866, 0
  store i1 %1862, i1* %az
  store i1 %1863, i1* %cf
  store i1 %1867, i1* %of
  %1868 = icmp eq i32 %1858, 0
  store i1 %1868, i1* %zf
  %1869 = icmp slt i32 %1858, 0
  store i1 %1869, i1* %sf
  %1870 = trunc i32 %1858 to i8
  %1871 = call i8 @llvm.ctpop.i8(i8 %1870)
  %1872 = and i8 %1871, 1
  %1873 = icmp eq i8 %1872, 0
  store i1 %1873, i1* %pf
  %1874 = zext i32 %1858 to i64
  store i64 %1874, i64* %rcx
  store volatile i64 34610, i64* @assembly_address
  %1875 = load i64* %rcx
  %1876 = trunc i64 %1875 to i32
  %1877 = zext i32 %1876 to i64
  store i64 %1877, i64* %rax
  store volatile i64 34612, i64* @assembly_address
  %1878 = load i64* %rax
  %1879 = trunc i64 %1878 to i32
  %1880 = inttoptr i32 %1879 to i64*
  store i64* %1880, i64** %stack_var_-64
  store volatile i64 34615, i64* @assembly_address
  %1881 = load i64* %r12
  %1882 = trunc i64 %1881 to i32
  %1883 = zext i32 %1882 to i64
  store i64 %1883, i64* %rcx
  store volatile i64 34618, i64* @assembly_address
  %1884 = load i64* %r13
  %1885 = load i64* %rcx
  %1886 = trunc i64 %1885 to i8
  %1887 = zext i8 %1886 to i64
  %1888 = and i64 %1887, 63
  %1889 = load i1* %of
  %1890 = icmp eq i64 %1888, 0
  br i1 %1890, label %1906, label %1891

; <label>:1891                                    ; preds = %block_8704
  %1892 = lshr i64 %1884, %1888
  %1893 = icmp eq i64 %1892, 0
  store i1 %1893, i1* %zf
  %1894 = icmp slt i64 %1892, 0
  store i1 %1894, i1* %sf
  %1895 = trunc i64 %1892 to i8
  %1896 = call i8 @llvm.ctpop.i8(i8 %1895)
  %1897 = and i8 %1896, 1
  %1898 = icmp eq i8 %1897, 0
  store i1 %1898, i1* %pf
  store i64 %1892, i64* %r13
  %1899 = sub i64 %1888, 1
  %1900 = shl i64 1, %1899
  %1901 = and i64 %1900, %1884
  %1902 = icmp ne i64 %1901, 0
  store i1 %1902, i1* %cf
  %1903 = icmp eq i64 %1888, 1
  %1904 = icmp slt i64 %1884, 0
  %1905 = select i1 %1903, i1 %1904, i1 %1889
  store i1 %1905, i1* %of
  br label %1906

; <label>:1906                                    ; preds = %block_8704, %1891
  store volatile i64 34621, i64* @assembly_address
  %1907 = load i64* %rbx
  %1908 = trunc i64 %1907 to i32
  %1909 = load i64* %r12
  %1910 = trunc i64 %1909 to i32
  %1911 = sub i32 %1908, %1910
  %1912 = and i32 %1908, 15
  %1913 = and i32 %1910, 15
  %1914 = sub i32 %1912, %1913
  %1915 = icmp ugt i32 %1914, 15
  %1916 = icmp ult i32 %1908, %1910
  %1917 = xor i32 %1908, %1910
  %1918 = xor i32 %1908, %1911
  %1919 = and i32 %1917, %1918
  %1920 = icmp slt i32 %1919, 0
  store i1 %1915, i1* %az
  store i1 %1916, i1* %cf
  store i1 %1920, i1* %of
  %1921 = icmp eq i32 %1911, 0
  store i1 %1921, i1* %zf
  %1922 = icmp slt i32 %1911, 0
  store i1 %1922, i1* %sf
  %1923 = trunc i32 %1911 to i8
  %1924 = call i8 @llvm.ctpop.i8(i8 %1923)
  %1925 = and i8 %1924, 1
  %1926 = icmp eq i8 %1925, 0
  store i1 %1926, i1* %pf
  %1927 = zext i32 %1911 to i64
  store i64 %1927, i64* %rbx
  br label %block_8740

block_8740:                                       ; preds = %block_881a, %1906
  store volatile i64 34624, i64* @assembly_address
  %1928 = load i64** %stack_var_-64
  %1929 = ptrtoint i64* %1928 to i32
  %1930 = and i32 %1929, 32767
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1931 = icmp eq i32 %1930, 0
  store i1 %1931, i1* %zf
  %1932 = icmp slt i32 %1930, 0
  store i1 %1932, i1* %sf
  %1933 = trunc i32 %1930 to i8
  %1934 = call i8 @llvm.ctpop.i8(i8 %1933)
  %1935 = and i8 %1934, 1
  %1936 = icmp eq i8 %1935, 0
  store i1 %1936, i1* %pf
  %1937 = inttoptr i32 %1930 to i64*
  store i64* %1937, i64** %stack_var_-64
  store volatile i64 34631, i64* @assembly_address
  %1938 = load i64** %stack_var_-64
  %1939 = ptrtoint i64* %1938 to i32
  %1940 = zext i32 %1939 to i64
  store i64 %1940, i64* %rax
  store volatile i64 34634, i64* @assembly_address
  %1941 = load i64* %rax
  %1942 = trunc i64 %1941 to i32
  %1943 = load i64** %stack_var_-60
  %1944 = ptrtoint i64* %1943 to i32
  %1945 = sub i32 %1942, %1944
  %1946 = and i32 %1942, 15
  %1947 = and i32 %1944, 15
  %1948 = sub i32 %1946, %1947
  %1949 = icmp ugt i32 %1948, 15
  %1950 = icmp ult i32 %1942, %1944
  %1951 = xor i32 %1942, %1944
  %1952 = xor i32 %1942, %1945
  %1953 = and i32 %1951, %1952
  %1954 = icmp slt i32 %1953, 0
  store i1 %1949, i1* %az
  store i1 %1950, i1* %cf
  store i1 %1954, i1* %of
  %1955 = icmp eq i32 %1945, 0
  store i1 %1955, i1* %zf
  %1956 = icmp slt i32 %1945, 0
  store i1 %1956, i1* %sf
  %1957 = trunc i32 %1945 to i8
  %1958 = call i8 @llvm.ctpop.i8(i8 %1957)
  %1959 = and i8 %1958, 1
  %1960 = icmp eq i8 %1959, 0
  store i1 %1960, i1* %pf
  store volatile i64 34637, i64* @assembly_address
  %1961 = load i1* %cf
  %1962 = load i1* %zf
  %1963 = or i1 %1961, %1962
  br i1 %1963, label %block_8759, label %block_874f

block_874f:                                       ; preds = %block_8740
  store volatile i64 34639, i64* @assembly_address
  store i64 32768, i64* %rax
  store volatile i64 34644, i64* @assembly_address
  %1964 = load i64* %rax
  %1965 = trunc i64 %1964 to i32
  %1966 = load i64** %stack_var_-64
  %1967 = ptrtoint i64* %1966 to i32
  %1968 = sub i32 %1965, %1967
  %1969 = and i32 %1965, 15
  %1970 = and i32 %1967, 15
  %1971 = sub i32 %1969, %1970
  %1972 = icmp ugt i32 %1971, 15
  %1973 = icmp ult i32 %1965, %1967
  %1974 = xor i32 %1965, %1967
  %1975 = xor i32 %1965, %1968
  %1976 = and i32 %1974, %1975
  %1977 = icmp slt i32 %1976, 0
  store i1 %1972, i1* %az
  store i1 %1973, i1* %cf
  store i1 %1977, i1* %of
  %1978 = icmp eq i32 %1968, 0
  store i1 %1978, i1* %zf
  %1979 = icmp slt i32 %1968, 0
  store i1 %1979, i1* %sf
  %1980 = trunc i32 %1968 to i8
  %1981 = call i8 @llvm.ctpop.i8(i8 %1980)
  %1982 = and i8 %1981, 1
  %1983 = icmp eq i8 %1982, 0
  store i1 %1983, i1* %pf
  %1984 = zext i32 %1968 to i64
  store i64 %1984, i64* %rax
  store volatile i64 34647, i64* @assembly_address
  br label %block_8761

block_8759:                                       ; preds = %block_8740
  store volatile i64 34649, i64* @assembly_address
  store i64 32768, i64* %rax
  store volatile i64 34654, i64* @assembly_address
  %1985 = load i64* %rax
  %1986 = trunc i64 %1985 to i32
  %1987 = load i64** %stack_var_-60
  %1988 = ptrtoint i64* %1987 to i32
  %1989 = sub i32 %1986, %1988
  %1990 = and i32 %1986, 15
  %1991 = and i32 %1988, 15
  %1992 = sub i32 %1990, %1991
  %1993 = icmp ugt i32 %1992, 15
  %1994 = icmp ult i32 %1986, %1988
  %1995 = xor i32 %1986, %1988
  %1996 = xor i32 %1986, %1989
  %1997 = and i32 %1995, %1996
  %1998 = icmp slt i32 %1997, 0
  store i1 %1993, i1* %az
  store i1 %1994, i1* %cf
  store i1 %1998, i1* %of
  %1999 = icmp eq i32 %1989, 0
  store i1 %1999, i1* %zf
  %2000 = icmp slt i32 %1989, 0
  store i1 %2000, i1* %sf
  %2001 = trunc i32 %1989 to i8
  %2002 = call i8 @llvm.ctpop.i8(i8 %2001)
  %2003 = and i8 %2002, 1
  %2004 = icmp eq i8 %2003, 0
  store i1 %2004, i1* %pf
  %2005 = zext i32 %1989 to i64
  store i64 %2005, i64* %rax
  br label %block_8761

block_8761:                                       ; preds = %block_8759, %block_874f
  store volatile i64 34657, i64* @assembly_address
  %2006 = load i64* %rax
  %2007 = trunc i64 %2006 to i32
  %2008 = zext i32 %2007 to i64
  store i64 %2008, i64* %r12
  store volatile i64 34660, i64* @assembly_address
  %2009 = load i64* %r12
  %2010 = trunc i64 %2009 to i32
  %2011 = load i64** %stack_var_-68
  %2012 = ptrtoint i64* %2011 to i32
  %2013 = sub i32 %2010, %2012
  %2014 = and i32 %2010, 15
  %2015 = and i32 %2012, 15
  %2016 = sub i32 %2014, %2015
  %2017 = icmp ugt i32 %2016, 15
  %2018 = icmp ult i32 %2010, %2012
  %2019 = xor i32 %2010, %2012
  %2020 = xor i32 %2010, %2013
  %2021 = and i32 %2019, %2020
  %2022 = icmp slt i32 %2021, 0
  store i1 %2017, i1* %az
  store i1 %2018, i1* %cf
  store i1 %2022, i1* %of
  %2023 = icmp eq i32 %2013, 0
  store i1 %2023, i1* %zf
  %2024 = icmp slt i32 %2013, 0
  store i1 %2024, i1* %sf
  %2025 = trunc i32 %2013 to i8
  %2026 = call i8 @llvm.ctpop.i8(i8 %2025)
  %2027 = and i8 %2026, 1
  %2028 = icmp eq i8 %2027, 0
  store i1 %2028, i1* %pf
  store volatile i64 34664, i64* @assembly_address
  %2029 = load i1* %cf
  %2030 = load i1* %zf
  %2031 = or i1 %2029, %2030
  br i1 %2031, label %block_876f, label %block_876a

block_876a:                                       ; preds = %block_8761
  store volatile i64 34666, i64* @assembly_address
  %2032 = load i64** %stack_var_-68
  %2033 = ptrtoint i64* %2032 to i32
  %2034 = zext i32 %2033 to i64
  store i64 %2034, i64* %rax
  store volatile i64 34669, i64* @assembly_address
  br label %block_8772

block_876f:                                       ; preds = %block_8761
  store volatile i64 34671, i64* @assembly_address
  %2035 = load i64* %r12
  %2036 = trunc i64 %2035 to i32
  %2037 = zext i32 %2036 to i64
  store i64 %2037, i64* %rax
  br label %block_8772

block_8772:                                       ; preds = %block_876f, %block_876a
  store volatile i64 34674, i64* @assembly_address
  %2038 = load i64* %rax
  %2039 = trunc i64 %2038 to i32
  %2040 = zext i32 %2039 to i64
  store i64 %2040, i64* %r12
  store volatile i64 34677, i64* @assembly_address
  %2041 = load i64* %r12
  %2042 = trunc i64 %2041 to i32
  %2043 = zext i32 %2042 to i64
  store i64 %2043, i64* %rax
  store volatile i64 34680, i64* @assembly_address
  %2044 = load i64** %stack_var_-68
  %2045 = ptrtoint i64* %2044 to i32
  %2046 = load i64* %rax
  %2047 = trunc i64 %2046 to i32
  %2048 = sub i32 %2045, %2047
  %2049 = and i32 %2045, 15
  %2050 = and i32 %2047, 15
  %2051 = sub i32 %2049, %2050
  %2052 = icmp ugt i32 %2051, 15
  %2053 = icmp ult i32 %2045, %2047
  %2054 = xor i32 %2045, %2047
  %2055 = xor i32 %2045, %2048
  %2056 = and i32 %2054, %2055
  %2057 = icmp slt i32 %2056, 0
  store i1 %2052, i1* %az
  store i1 %2053, i1* %cf
  store i1 %2057, i1* %of
  %2058 = icmp eq i32 %2048, 0
  store i1 %2058, i1* %zf
  %2059 = icmp slt i32 %2048, 0
  store i1 %2059, i1* %sf
  %2060 = trunc i32 %2048 to i8
  %2061 = call i8 @llvm.ctpop.i8(i8 %2060)
  %2062 = and i8 %2061, 1
  %2063 = icmp eq i8 %2062, 0
  store i1 %2063, i1* %pf
  %2064 = inttoptr i32 %2048 to i64*
  store i64* %2064, i64** %stack_var_-68
  store volatile i64 34683, i64* @assembly_address
  %2065 = load i64** %stack_var_-64
  %2066 = ptrtoint i64* %2065 to i32
  %2067 = zext i32 %2066 to i64
  store i64 %2067, i64* %rax
  store volatile i64 34686, i64* @assembly_address
  %2068 = load i64* %rax
  %2069 = trunc i64 %2068 to i32
  %2070 = load i64** %stack_var_-60
  %2071 = ptrtoint i64* %2070 to i32
  %2072 = sub i32 %2069, %2071
  %2073 = and i32 %2069, 15
  %2074 = and i32 %2071, 15
  %2075 = sub i32 %2073, %2074
  %2076 = icmp ugt i32 %2075, 15
  %2077 = icmp ult i32 %2069, %2071
  %2078 = xor i32 %2069, %2071
  %2079 = xor i32 %2069, %2072
  %2080 = and i32 %2078, %2079
  %2081 = icmp slt i32 %2080, 0
  store i1 %2076, i1* %az
  store i1 %2077, i1* %cf
  store i1 %2081, i1* %of
  %2082 = icmp eq i32 %2072, 0
  store i1 %2082, i1* %zf
  %2083 = icmp slt i32 %2072, 0
  store i1 %2083, i1* %sf
  %2084 = trunc i32 %2072 to i8
  %2085 = call i8 @llvm.ctpop.i8(i8 %2084)
  %2086 = and i8 %2085, 1
  %2087 = icmp eq i8 %2086, 0
  store i1 %2087, i1* %pf
  store volatile i64 34689, i64* @assembly_address
  %2088 = load i1* %cf
  %2089 = icmp eq i1 %2088, false
  br i1 %2089, label %block_878b, label %block_8783

block_8783:                                       ; preds = %block_8772
  store volatile i64 34691, i64* @assembly_address
  %2090 = load i64** %stack_var_-60
  %2091 = ptrtoint i64* %2090 to i32
  %2092 = zext i32 %2091 to i64
  store i64 %2092, i64* %rax
  store volatile i64 34694, i64* @assembly_address
  %2093 = load i64* %rax
  %2094 = trunc i64 %2093 to i32
  %2095 = load i64** %stack_var_-64
  %2096 = ptrtoint i64* %2095 to i32
  %2097 = sub i32 %2094, %2096
  %2098 = and i32 %2094, 15
  %2099 = and i32 %2096, 15
  %2100 = sub i32 %2098, %2099
  %2101 = icmp ugt i32 %2100, 15
  %2102 = icmp ult i32 %2094, %2096
  %2103 = xor i32 %2094, %2096
  %2104 = xor i32 %2094, %2097
  %2105 = and i32 %2103, %2104
  %2106 = icmp slt i32 %2105, 0
  store i1 %2101, i1* %az
  store i1 %2102, i1* %cf
  store i1 %2106, i1* %of
  %2107 = icmp eq i32 %2097, 0
  store i1 %2107, i1* %zf
  %2108 = icmp slt i32 %2097, 0
  store i1 %2108, i1* %sf
  %2109 = trunc i32 %2097 to i8
  %2110 = call i8 @llvm.ctpop.i8(i8 %2109)
  %2111 = and i8 %2110, 1
  %2112 = icmp eq i8 %2111, 0
  store i1 %2112, i1* %pf
  %2113 = zext i32 %2097 to i64
  store i64 %2113, i64* %rax
  store volatile i64 34697, i64* @assembly_address
  br label %block_8791

block_878b:                                       ; preds = %block_8772
  store volatile i64 34699, i64* @assembly_address
  %2114 = load i64** %stack_var_-64
  %2115 = ptrtoint i64* %2114 to i32
  %2116 = zext i32 %2115 to i64
  store i64 %2116, i64* %rax
  store volatile i64 34702, i64* @assembly_address
  %2117 = load i64* %rax
  %2118 = trunc i64 %2117 to i32
  %2119 = load i64** %stack_var_-60
  %2120 = ptrtoint i64* %2119 to i32
  %2121 = sub i32 %2118, %2120
  %2122 = and i32 %2118, 15
  %2123 = and i32 %2120, 15
  %2124 = sub i32 %2122, %2123
  %2125 = icmp ugt i32 %2124, 15
  %2126 = icmp ult i32 %2118, %2120
  %2127 = xor i32 %2118, %2120
  %2128 = xor i32 %2118, %2121
  %2129 = and i32 %2127, %2128
  %2130 = icmp slt i32 %2129, 0
  store i1 %2125, i1* %az
  store i1 %2126, i1* %cf
  store i1 %2130, i1* %of
  %2131 = icmp eq i32 %2121, 0
  store i1 %2131, i1* %zf
  %2132 = icmp slt i32 %2121, 0
  store i1 %2132, i1* %sf
  %2133 = trunc i32 %2121 to i8
  %2134 = call i8 @llvm.ctpop.i8(i8 %2133)
  %2135 = and i8 %2134, 1
  %2136 = icmp eq i8 %2135, 0
  store i1 %2136, i1* %pf
  %2137 = zext i32 %2121 to i64
  store i64 %2137, i64* %rax
  br label %block_8791

block_8791:                                       ; preds = %block_878b, %block_8783
  store volatile i64 34705, i64* @assembly_address
  %2138 = load i64* %rax
  %2139 = trunc i64 %2138 to i32
  %2140 = load i64* %r12
  %2141 = trunc i64 %2140 to i32
  %2142 = sub i32 %2139, %2141
  %2143 = and i32 %2139, 15
  %2144 = and i32 %2141, 15
  %2145 = sub i32 %2143, %2144
  %2146 = icmp ugt i32 %2145, 15
  %2147 = icmp ult i32 %2139, %2141
  %2148 = xor i32 %2139, %2141
  %2149 = xor i32 %2139, %2142
  %2150 = and i32 %2148, %2149
  %2151 = icmp slt i32 %2150, 0
  store i1 %2146, i1* %az
  store i1 %2147, i1* %cf
  store i1 %2151, i1* %of
  %2152 = icmp eq i32 %2142, 0
  store i1 %2152, i1* %zf
  %2153 = icmp slt i32 %2142, 0
  store i1 %2153, i1* %sf
  %2154 = trunc i32 %2142 to i8
  %2155 = call i8 @llvm.ctpop.i8(i8 %2154)
  %2156 = and i8 %2155, 1
  %2157 = icmp eq i8 %2156, 0
  store i1 %2157, i1* %pf
  store volatile i64 34708, i64* @assembly_address
  %2158 = load i1* %cf
  br i1 %2158, label %block_87c8, label %block_8796

block_8796:                                       ; preds = %block_8791
  store volatile i64 34710, i64* @assembly_address
  %2159 = load i64* %r12
  %2160 = trunc i64 %2159 to i32
  %2161 = zext i32 %2160 to i64
  store i64 %2161, i64* %rdx
  store volatile i64 34713, i64* @assembly_address
  %2162 = load i64** %stack_var_-64
  %2163 = ptrtoint i64* %2162 to i32
  %2164 = zext i32 %2163 to i64
  store i64 %2164, i64* %rcx
  store volatile i64 34716, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 34723, i64* @assembly_address
  %2165 = load i64* %rcx
  %2166 = load i64* %rax
  %2167 = add i64 %2165, %2166
  %2168 = and i64 %2165, 15
  %2169 = and i64 %2166, 15
  %2170 = add i64 %2168, %2169
  %2171 = icmp ugt i64 %2170, 15
  %2172 = icmp ult i64 %2167, %2165
  %2173 = xor i64 %2165, %2167
  %2174 = xor i64 %2166, %2167
  %2175 = and i64 %2173, %2174
  %2176 = icmp slt i64 %2175, 0
  store i1 %2171, i1* %az
  store i1 %2172, i1* %cf
  store i1 %2176, i1* %of
  %2177 = icmp eq i64 %2167, 0
  store i1 %2177, i1* %zf
  %2178 = icmp slt i64 %2167, 0
  store i1 %2178, i1* %sf
  %2179 = trunc i64 %2167 to i8
  %2180 = call i8 @llvm.ctpop.i8(i8 %2179)
  %2181 = and i8 %2180, 1
  %2182 = icmp eq i8 %2181, 0
  store i1 %2182, i1* %pf
  store i64 %2167, i64* %rcx
  store volatile i64 34726, i64* @assembly_address
  %2183 = load i64** %stack_var_-60
  %2184 = ptrtoint i64* %2183 to i32
  %2185 = zext i32 %2184 to i64
  store i64 %2185, i64* %rsi
  store volatile i64 34729, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 34736, i64* @assembly_address
  %2186 = load i64* %rax
  %2187 = load i64* %rsi
  %2188 = add i64 %2186, %2187
  %2189 = and i64 %2186, 15
  %2190 = and i64 %2187, 15
  %2191 = add i64 %2189, %2190
  %2192 = icmp ugt i64 %2191, 15
  %2193 = icmp ult i64 %2188, %2186
  %2194 = xor i64 %2186, %2188
  %2195 = xor i64 %2187, %2188
  %2196 = and i64 %2194, %2195
  %2197 = icmp slt i64 %2196, 0
  store i1 %2192, i1* %az
  store i1 %2193, i1* %cf
  store i1 %2197, i1* %of
  %2198 = icmp eq i64 %2188, 0
  store i1 %2198, i1* %zf
  %2199 = icmp slt i64 %2188, 0
  store i1 %2199, i1* %sf
  %2200 = trunc i64 %2188 to i8
  %2201 = call i8 @llvm.ctpop.i8(i8 %2200)
  %2202 = and i8 %2201, 1
  %2203 = icmp eq i8 %2202, 0
  store i1 %2203, i1* %pf
  store i64 %2188, i64* %rax
  store volatile i64 34739, i64* @assembly_address
  %2204 = load i64* %rcx
  store i64 %2204, i64* %rsi
  store volatile i64 34742, i64* @assembly_address
  %2205 = load i64* %rax
  store i64 %2205, i64* %rdi
  store volatile i64 34745, i64* @assembly_address
  %2206 = load i64* %rdi
  %2207 = inttoptr i64 %2206 to i64*
  %2208 = load i64* %rsi
  %2209 = inttoptr i64 %2208 to i64*
  %2210 = load i64* %rdx
  %2211 = trunc i64 %2210 to i32
  %2212 = call i64* @memcpy(i64* %2207, i64* %2209, i32 %2211)
  %2213 = ptrtoint i64* %2212 to i64
  store i64 %2213, i64* %rax
  %2214 = ptrtoint i64* %2212 to i64
  store i64 %2214, i64* %rax
  store volatile i64 34750, i64* @assembly_address
  %2215 = load i64** %stack_var_-60
  %2216 = ptrtoint i64* %2215 to i32
  %2217 = load i64* %r12
  %2218 = trunc i64 %2217 to i32
  %2219 = add i32 %2216, %2218
  %2220 = and i32 %2216, 15
  %2221 = and i32 %2218, 15
  %2222 = add i32 %2220, %2221
  %2223 = icmp ugt i32 %2222, 15
  %2224 = icmp ult i32 %2219, %2216
  %2225 = xor i32 %2216, %2219
  %2226 = xor i32 %2218, %2219
  %2227 = and i32 %2225, %2226
  %2228 = icmp slt i32 %2227, 0
  store i1 %2223, i1* %az
  store i1 %2224, i1* %cf
  store i1 %2228, i1* %of
  %2229 = icmp eq i32 %2219, 0
  store i1 %2229, i1* %zf
  %2230 = icmp slt i32 %2219, 0
  store i1 %2230, i1* %sf
  %2231 = trunc i32 %2219 to i8
  %2232 = call i8 @llvm.ctpop.i8(i8 %2231)
  %2233 = and i8 %2232, 1
  %2234 = icmp eq i8 %2233, 0
  store i1 %2234, i1* %pf
  %2235 = inttoptr i32 %2219 to i64*
  store i64* %2235, i64** %stack_var_-60
  store volatile i64 34754, i64* @assembly_address
  %2236 = load i64** %stack_var_-64
  %2237 = ptrtoint i64* %2236 to i32
  %2238 = load i64* %r12
  %2239 = trunc i64 %2238 to i32
  %2240 = add i32 %2237, %2239
  %2241 = and i32 %2237, 15
  %2242 = and i32 %2239, 15
  %2243 = add i32 %2241, %2242
  %2244 = icmp ugt i32 %2243, 15
  %2245 = icmp ult i32 %2240, %2237
  %2246 = xor i32 %2237, %2240
  %2247 = xor i32 %2239, %2240
  %2248 = and i32 %2246, %2247
  %2249 = icmp slt i32 %2248, 0
  store i1 %2244, i1* %az
  store i1 %2245, i1* %cf
  store i1 %2249, i1* %of
  %2250 = icmp eq i32 %2240, 0
  store i1 %2250, i1* %zf
  %2251 = icmp slt i32 %2240, 0
  store i1 %2251, i1* %sf
  %2252 = trunc i32 %2240 to i8
  %2253 = call i8 @llvm.ctpop.i8(i8 %2252)
  %2254 = and i8 %2253, 1
  %2255 = icmp eq i8 %2254, 0
  store i1 %2255, i1* %pf
  %2256 = inttoptr i32 %2240 to i64*
  store i64* %2256, i64** %stack_var_-64
  store volatile i64 34758, i64* @assembly_address
  br label %block_87fc

block_87c8:                                       ; preds = %block_87c8, %block_8791
  store volatile i64 34760, i64* @assembly_address
  %2257 = load i64** %stack_var_-64
  %2258 = ptrtoint i64* %2257 to i32
  %2259 = zext i32 %2258 to i64
  store i64 %2259, i64* %rdx
  store volatile i64 34763, i64* @assembly_address
  %2260 = load i64* %rdx
  %2261 = add i64 %2260, 1
  %2262 = trunc i64 %2261 to i32
  %2263 = zext i32 %2262 to i64
  store i64 %2263, i64* %rax
  store volatile i64 34766, i64* @assembly_address
  %2264 = load i64* %rax
  %2265 = trunc i64 %2264 to i32
  %2266 = inttoptr i32 %2265 to i64*
  store i64* %2266, i64** %stack_var_-64
  store volatile i64 34769, i64* @assembly_address
  %2267 = load i64** %stack_var_-60
  %2268 = ptrtoint i64* %2267 to i32
  %2269 = zext i32 %2268 to i64
  store i64 %2269, i64* %rax
  store volatile i64 34772, i64* @assembly_address
  %2270 = load i64* %rax
  %2271 = add i64 %2270, 1
  %2272 = trunc i64 %2271 to i32
  %2273 = zext i32 %2272 to i64
  store i64 %2273, i64* %rcx
  store volatile i64 34775, i64* @assembly_address
  %2274 = load i64* %rcx
  %2275 = trunc i64 %2274 to i32
  %2276 = inttoptr i32 %2275 to i64*
  store i64* %2276, i64** %stack_var_-60
  store volatile i64 34778, i64* @assembly_address
  %2277 = load i64* %rdx
  %2278 = trunc i64 %2277 to i32
  %2279 = zext i32 %2278 to i64
  store i64 %2279, i64* %rcx
  store volatile i64 34780, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdx
  store volatile i64 34787, i64* @assembly_address
  %2280 = load i64* %rcx
  %2281 = load i64* %rdx
  %2282 = mul i64 %2281, 1
  %2283 = add i64 %2280, %2282
  %2284 = inttoptr i64 %2283 to i8*
  %2285 = load i8* %2284
  %2286 = zext i8 %2285 to i64
  store i64 %2286, i64* %rdx
  store volatile i64 34791, i64* @assembly_address
  %2287 = load i64* %rax
  %2288 = trunc i64 %2287 to i32
  %2289 = zext i32 %2288 to i64
  store i64 %2289, i64* %rcx
  store volatile i64 34793, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 34800, i64* @assembly_address
  %2290 = load i64* %rdx
  %2291 = trunc i64 %2290 to i8
  %2292 = load i64* %rcx
  %2293 = load i64* %rax
  %2294 = mul i64 %2293, 1
  %2295 = add i64 %2292, %2294
  %2296 = inttoptr i64 %2295 to i8*
  store i8 %2291, i8* %2296
  store volatile i64 34803, i64* @assembly_address
  %2297 = load i64* %r12
  %2298 = trunc i64 %2297 to i32
  %2299 = sub i32 %2298, 1
  %2300 = and i32 %2298, 15
  %2301 = sub i32 %2300, 1
  %2302 = icmp ugt i32 %2301, 15
  %2303 = icmp ult i32 %2298, 1
  %2304 = xor i32 %2298, 1
  %2305 = xor i32 %2298, %2299
  %2306 = and i32 %2304, %2305
  %2307 = icmp slt i32 %2306, 0
  store i1 %2302, i1* %az
  store i1 %2303, i1* %cf
  store i1 %2307, i1* %of
  %2308 = icmp eq i32 %2299, 0
  store i1 %2308, i1* %zf
  %2309 = icmp slt i32 %2299, 0
  store i1 %2309, i1* %sf
  %2310 = trunc i32 %2299 to i8
  %2311 = call i8 @llvm.ctpop.i8(i8 %2310)
  %2312 = and i8 %2311, 1
  %2313 = icmp eq i8 %2312, 0
  store i1 %2313, i1* %pf
  %2314 = zext i32 %2299 to i64
  store i64 %2314, i64* %r12
  store volatile i64 34807, i64* @assembly_address
  %2315 = load i64* %r12
  %2316 = trunc i64 %2315 to i32
  %2317 = load i64* %r12
  %2318 = trunc i64 %2317 to i32
  %2319 = and i32 %2316, %2318
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2320 = icmp eq i32 %2319, 0
  store i1 %2320, i1* %zf
  %2321 = icmp slt i32 %2319, 0
  store i1 %2321, i1* %sf
  %2322 = trunc i32 %2319 to i8
  %2323 = call i8 @llvm.ctpop.i8(i8 %2322)
  %2324 = and i8 %2323, 1
  %2325 = icmp eq i8 %2324, 0
  store i1 %2325, i1* %pf
  store volatile i64 34810, i64* @assembly_address
  %2326 = load i1* %zf
  %2327 = icmp eq i1 %2326, false
  br i1 %2327, label %block_87c8, label %block_87fc

block_87fc:                                       ; preds = %block_87c8, %block_8796
  store volatile i64 34812, i64* @assembly_address
  %2328 = load i64** %stack_var_-60
  %2329 = ptrtoint i64* %2328 to i32
  %2330 = sub i32 %2329, 32768
  %2331 = and i32 %2329, 15
  %2332 = icmp ugt i32 %2331, 15
  %2333 = icmp ult i32 %2329, 32768
  %2334 = xor i32 %2329, 32768
  %2335 = xor i32 %2329, %2330
  %2336 = and i32 %2334, %2335
  %2337 = icmp slt i32 %2336, 0
  store i1 %2332, i1* %az
  store i1 %2333, i1* %cf
  store i1 %2337, i1* %of
  %2338 = icmp eq i32 %2330, 0
  store i1 %2338, i1* %zf
  %2339 = icmp slt i32 %2330, 0
  store i1 %2339, i1* %sf
  %2340 = trunc i32 %2330 to i8
  %2341 = call i8 @llvm.ctpop.i8(i8 %2340)
  %2342 = and i8 %2341, 1
  %2343 = icmp eq i8 %2342, 0
  store i1 %2343, i1* %pf
  store volatile i64 34819, i64* @assembly_address
  %2344 = load i1* %zf
  %2345 = icmp eq i1 %2344, false
  br i1 %2345, label %block_881a, label %block_8805

block_8805:                                       ; preds = %block_87fc
  store volatile i64 34821, i64* @assembly_address
  %2346 = load i64** %stack_var_-60
  %2347 = ptrtoint i64* %2346 to i32
  %2348 = zext i32 %2347 to i64
  store i64 %2348, i64* %rax
  store volatile i64 34824, i64* @assembly_address
  %2349 = load i64* %rax
  %2350 = trunc i64 %2349 to i32
  store i32 %2350, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 34830, i64* @assembly_address
  %2351 = call i64 @flush_window()
  store i64 %2351, i64* %rax
  store i64 %2351, i64* %rax
  store i64 %2351, i64* %rax
  store volatile i64 34835, i64* @assembly_address
  %2352 = inttoptr i32 0 to i64*
  store i64* %2352, i64** %stack_var_-60
  br label %block_881a

block_881a:                                       ; preds = %block_8805, %block_87fc
  store volatile i64 34842, i64* @assembly_address
  %2353 = load i64** %stack_var_-68
  %2354 = ptrtoint i64* %2353 to i32
  %2355 = and i32 %2354, 15
  %2356 = icmp ugt i32 %2355, 15
  %2357 = icmp ult i32 %2354, 0
  %2358 = xor i32 %2354, 0
  %2359 = and i32 %2358, 0
  %2360 = icmp slt i32 %2359, 0
  store i1 %2356, i1* %az
  store i1 %2357, i1* %cf
  store i1 %2360, i1* %of
  %2361 = icmp eq i32 %2354, 0
  store i1 %2361, i1* %zf
  %2362 = icmp slt i32 %2354, 0
  store i1 %2362, i1* %sf
  %2363 = trunc i32 %2354 to i8
  %2364 = call i8 @llvm.ctpop.i8(i8 %2363)
  %2365 = and i8 %2364, 1
  %2366 = icmp eq i8 %2365, 0
  store i1 %2366, i1* %pf
  store volatile i64 34846, i64* @assembly_address
  %2367 = load i1* %zf
  %2368 = icmp eq i1 %2367, false
  br i1 %2368, label %block_8740, label %block_8824

block_8824:                                       ; preds = %block_881a
  store volatile i64 34852, i64* @assembly_address
  br label %block_8335

block_8829:                                       ; preds = %block_84a0
  store volatile i64 34857, i64* @assembly_address
  store volatile i64 34858, i64* @assembly_address
  %2369 = load i64** %stack_var_-60
  %2370 = ptrtoint i64* %2369 to i32
  %2371 = zext i32 %2370 to i64
  store i64 %2371, i64* %rax
  store volatile i64 34861, i64* @assembly_address
  %2372 = load i64* %rax
  %2373 = trunc i64 %2372 to i32
  store i32 %2373, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 34867, i64* @assembly_address
  %2374 = load i64* %r13
  store i64 %2374, i64* @global_var_216f98
  store volatile i64 34874, i64* @assembly_address
  %2375 = load i64* %rbx
  %2376 = trunc i64 %2375 to i32
  store i32 %2376, i32* bitcast (i64* @global_var_216fa0 to i32*)
  store volatile i64 34880, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_8845

block_8845:                                       ; preds = %block_8829, %block_85c6, %block_8371
  store volatile i64 34885, i64* @assembly_address
  %2377 = load i64* %rsp
  %2378 = add i64 %2377, 72
  %2379 = and i64 %2377, 15
  %2380 = add i64 %2379, 8
  %2381 = icmp ugt i64 %2380, 15
  %2382 = icmp ult i64 %2378, %2377
  %2383 = xor i64 %2377, %2378
  %2384 = xor i64 72, %2378
  %2385 = and i64 %2383, %2384
  %2386 = icmp slt i64 %2385, 0
  store i1 %2381, i1* %az
  store i1 %2382, i1* %cf
  store i1 %2386, i1* %of
  %2387 = icmp eq i64 %2378, 0
  store i1 %2387, i1* %zf
  %2388 = icmp slt i64 %2378, 0
  store i1 %2388, i1* %sf
  %2389 = trunc i64 %2378 to i8
  %2390 = call i8 @llvm.ctpop.i8(i8 %2389)
  %2391 = and i8 %2390, 1
  %2392 = icmp eq i8 %2391, 0
  store i1 %2392, i1* %pf
  %2393 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %2393, i64* %rsp
  store volatile i64 34889, i64* @assembly_address
  %2394 = load i64* %stack_var_-32
  store i64 %2394, i64* %rbx
  %2395 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %2395, i64* %rsp
  store volatile i64 34890, i64* @assembly_address
  %2396 = load i64* %stack_var_-24
  store i64 %2396, i64* %r12
  %2397 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %2397, i64* %rsp
  store volatile i64 34892, i64* @assembly_address
  %2398 = load i64* %stack_var_-16
  store i64 %2398, i64* %r13
  %2399 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2399, i64* %rsp
  store volatile i64 34894, i64* @assembly_address
  %2400 = load i64* %stack_var_-8
  store i64 %2400, i64* %rbp
  %2401 = ptrtoint i64* %stack_var_0 to i64
  store i64 %2401, i64* %rsp
  store volatile i64 34895, i64* @assembly_address
  %2402 = load i64* %rax
  ret i64 %2402
}

declare i64 @199(i64, i64, i32, i32)

define i64 @inflate_stored() {
block_8850:
  %r12 = alloca i64
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i64*
  %0 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 34896, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 34897, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 34900, i64* @assembly_address
  %4 = load i64* %r12
  store i64 %4, i64* %stack_var_-16
  %5 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %5, i64* %rsp
  store volatile i64 34902, i64* @assembly_address
  %6 = load i64* %rbx
  store i64 %6, i64* %stack_var_-24
  %7 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %7, i64* %rsp
  store volatile i64 34903, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 16
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 16
  %13 = xor i64 %8, 16
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %23, i64* %rsp
  store volatile i64 34907, i64* @assembly_address
  %24 = load i64* @global_var_216f98
  store i64 %24, i64* %r12
  store volatile i64 34914, i64* @assembly_address
  %25 = load i32* bitcast (i64* @global_var_216fa0 to i32*)
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rbx
  store volatile i64 34920, i64* @assembly_address
  %27 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %28 = zext i32 %27 to i64
  store i64 %28, i64* %rax
  store volatile i64 34926, i64* @assembly_address
  %29 = load i64* %rax
  %30 = trunc i64 %29 to i32
  %31 = inttoptr i32 %30 to i64*
  store i64* %31, i64** %stack_var_-28
  store volatile i64 34929, i64* @assembly_address
  %32 = load i64* %rbx
  %33 = trunc i64 %32 to i32
  %34 = zext i32 %33 to i64
  store i64 %34, i64* %rax
  store volatile i64 34931, i64* @assembly_address
  %35 = load i64* %rax
  %36 = trunc i64 %35 to i32
  %37 = and i32 %36, 7
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %38 = icmp eq i32 %37, 0
  store i1 %38, i1* %zf
  %39 = icmp slt i32 %37, 0
  store i1 %39, i1* %sf
  %40 = trunc i32 %37 to i8
  %41 = call i8 @llvm.ctpop.i8(i8 %40)
  %42 = and i8 %41, 1
  %43 = icmp eq i8 %42, 0
  store i1 %43, i1* %pf
  %44 = zext i32 %37 to i64
  store i64 %44, i64* %rax
  store volatile i64 34934, i64* @assembly_address
  %45 = load i64* %rax
  %46 = trunc i64 %45 to i32
  store i32 %46, i32* %stack_var_-32
  store volatile i64 34937, i64* @assembly_address
  %47 = load i32* %stack_var_-32
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rax
  store volatile i64 34940, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  %51 = zext i32 %50 to i64
  store i64 %51, i64* %rcx
  store volatile i64 34942, i64* @assembly_address
  %52 = load i64* %r12
  %53 = load i64* %rcx
  %54 = trunc i64 %53 to i8
  %55 = zext i8 %54 to i64
  %56 = and i64 %55, 63
  %57 = load i1* %of
  %58 = icmp eq i64 %56, 0
  br i1 %58, label %74, label %59

; <label>:59                                      ; preds = %block_8850
  %60 = lshr i64 %52, %56
  %61 = icmp eq i64 %60, 0
  store i1 %61, i1* %zf
  %62 = icmp slt i64 %60, 0
  store i1 %62, i1* %sf
  %63 = trunc i64 %60 to i8
  %64 = call i8 @llvm.ctpop.i8(i8 %63)
  %65 = and i8 %64, 1
  %66 = icmp eq i8 %65, 0
  store i1 %66, i1* %pf
  store i64 %60, i64* %r12
  %67 = sub i64 %56, 1
  %68 = shl i64 1, %67
  %69 = and i64 %68, %52
  %70 = icmp ne i64 %69, 0
  store i1 %70, i1* %cf
  %71 = icmp eq i64 %56, 1
  %72 = icmp slt i64 %52, 0
  %73 = select i1 %71, i1 %72, i1 %57
  store i1 %73, i1* %of
  br label %74

; <label>:74                                      ; preds = %block_8850, %59
  store volatile i64 34945, i64* @assembly_address
  %75 = load i64* %rbx
  %76 = trunc i64 %75 to i32
  %77 = load i32* %stack_var_-32
  %78 = sub i32 %76, %77
  %79 = and i32 %76, 15
  %80 = and i32 %77, 15
  %81 = sub i32 %79, %80
  %82 = icmp ugt i32 %81, 15
  %83 = icmp ult i32 %76, %77
  %84 = xor i32 %76, %77
  %85 = xor i32 %76, %78
  %86 = and i32 %84, %85
  %87 = icmp slt i32 %86, 0
  store i1 %82, i1* %az
  store i1 %83, i1* %cf
  store i1 %87, i1* %of
  %88 = icmp eq i32 %78, 0
  store i1 %88, i1* %zf
  %89 = icmp slt i32 %78, 0
  store i1 %89, i1* %sf
  %90 = trunc i32 %78 to i8
  %91 = call i8 @llvm.ctpop.i8(i8 %90)
  %92 = and i8 %91, 1
  %93 = icmp eq i8 %92, 0
  store i1 %93, i1* %pf
  %94 = zext i32 %78 to i64
  store i64 %94, i64* %rbx
  store volatile i64 34948, i64* @assembly_address
  br label %block_88d8

block_8886:                                       ; preds = %block_88d8
  store volatile i64 34950, i64* @assembly_address
  %95 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %96 = zext i32 %95 to i64
  store i64 %96, i64* %rdx
  store volatile i64 34956, i64* @assembly_address
  %97 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %98 = zext i32 %97 to i64
  store i64 %98, i64* %rax
  store volatile i64 34962, i64* @assembly_address
  %99 = load i64* %rdx
  %100 = trunc i64 %99 to i32
  %101 = load i64* %rax
  %102 = trunc i64 %101 to i32
  %103 = sub i32 %100, %102
  %104 = and i32 %100, 15
  %105 = and i32 %102, 15
  %106 = sub i32 %104, %105
  %107 = icmp ugt i32 %106, 15
  %108 = icmp ult i32 %100, %102
  %109 = xor i32 %100, %102
  %110 = xor i32 %100, %103
  %111 = and i32 %109, %110
  %112 = icmp slt i32 %111, 0
  store i1 %107, i1* %az
  store i1 %108, i1* %cf
  store i1 %112, i1* %of
  %113 = icmp eq i32 %103, 0
  store i1 %113, i1* %zf
  %114 = icmp slt i32 %103, 0
  store i1 %114, i1* %sf
  %115 = trunc i32 %103 to i8
  %116 = call i8 @llvm.ctpop.i8(i8 %115)
  %117 = and i8 %116, 1
  %118 = icmp eq i8 %117, 0
  store i1 %118, i1* %pf
  store volatile i64 34964, i64* @assembly_address
  %119 = load i1* %cf
  %120 = icmp eq i1 %119, false
  br i1 %120, label %block_88b7, label %block_8896

block_8896:                                       ; preds = %block_8886
  store volatile i64 34966, i64* @assembly_address
  %121 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %122 = zext i32 %121 to i64
  store i64 %122, i64* %rax
  store volatile i64 34972, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 34975, i64* @assembly_address
  %123 = load i64* %rdx
  %124 = trunc i64 %123 to i32
  store i32 %124, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 34981, i64* @assembly_address
  %125 = load i64* %rax
  %126 = trunc i64 %125 to i32
  %127 = zext i32 %126 to i64
  store i64 %127, i64* %rdx
  store volatile i64 34983, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 34990, i64* @assembly_address
  %128 = load i64* %rdx
  %129 = load i64* %rax
  %130 = mul i64 %129, 1
  %131 = add i64 %128, %130
  %132 = inttoptr i64 %131 to i8*
  %133 = load i8* %132
  %134 = zext i8 %133 to i64
  store i64 %134, i64* %rax
  store volatile i64 34994, i64* @assembly_address
  %135 = load i64* %rax
  %136 = trunc i64 %135 to i8
  %137 = zext i8 %136 to i64
  store i64 %137, i64* %rax
  store volatile i64 34997, i64* @assembly_address
  br label %block_88cd

block_88b7:                                       ; preds = %block_8886
  store volatile i64 34999, i64* @assembly_address
  %138 = load i64** %stack_var_-28
  %139 = ptrtoint i64* %138 to i32
  %140 = zext i32 %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 35002, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  store i32 %142, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 35008, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 35013, i64* @assembly_address
  %143 = load i64* %rdi
  %144 = trunc i64 %143 to i32
  %145 = call i64 @fill_inbuf(i32 %144)
  store i64 %145, i64* %rax
  store i64 %145, i64* %rax
  store volatile i64 35018, i64* @assembly_address
  %146 = load i64* %rax
  %147 = trunc i64 %146 to i8
  %148 = zext i8 %147 to i64
  store i64 %148, i64* %rax
  br label %block_88cd

block_88cd:                                       ; preds = %block_88b7, %block_8896
  store volatile i64 35021, i64* @assembly_address
  %149 = load i64* %rbx
  %150 = trunc i64 %149 to i32
  %151 = zext i32 %150 to i64
  store i64 %151, i64* %rcx
  store volatile i64 35023, i64* @assembly_address
  %152 = load i64* %rax
  %153 = load i64* %rcx
  %154 = trunc i64 %153 to i8
  %155 = zext i8 %154 to i64
  %156 = and i64 %155, 63
  %157 = load i1* %of
  %158 = icmp eq i64 %156, 0
  br i1 %158, label %175, label %159

; <label>:159                                     ; preds = %block_88cd
  %160 = shl i64 %152, %156
  %161 = icmp eq i64 %160, 0
  store i1 %161, i1* %zf
  %162 = icmp slt i64 %160, 0
  store i1 %162, i1* %sf
  %163 = trunc i64 %160 to i8
  %164 = call i8 @llvm.ctpop.i8(i8 %163)
  %165 = and i8 %164, 1
  %166 = icmp eq i8 %165, 0
  store i1 %166, i1* %pf
  store i64 %160, i64* %rax
  %167 = sub i64 %156, 1
  %168 = shl i64 %152, %167
  %169 = lshr i64 %168, 63
  %170 = trunc i64 %169 to i1
  store i1 %170, i1* %cf
  %171 = lshr i64 %160, 63
  %172 = icmp ne i64 %171, %169
  %173 = icmp eq i64 %156, 1
  %174 = select i1 %173, i1 %172, i1 %157
  store i1 %174, i1* %of
  br label %175

; <label>:175                                     ; preds = %block_88cd, %159
  store volatile i64 35026, i64* @assembly_address
  %176 = load i64* %r12
  %177 = load i64* %rax
  %178 = or i64 %176, %177
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %179 = icmp eq i64 %178, 0
  store i1 %179, i1* %zf
  %180 = icmp slt i64 %178, 0
  store i1 %180, i1* %sf
  %181 = trunc i64 %178 to i8
  %182 = call i8 @llvm.ctpop.i8(i8 %181)
  %183 = and i8 %182, 1
  %184 = icmp eq i8 %183, 0
  store i1 %184, i1* %pf
  store i64 %178, i64* %r12
  store volatile i64 35029, i64* @assembly_address
  %185 = load i64* %rbx
  %186 = trunc i64 %185 to i32
  %187 = add i32 %186, 8
  %188 = and i32 %186, 15
  %189 = add i32 %188, 8
  %190 = icmp ugt i32 %189, 15
  %191 = icmp ult i32 %187, %186
  %192 = xor i32 %186, %187
  %193 = xor i32 8, %187
  %194 = and i32 %192, %193
  %195 = icmp slt i32 %194, 0
  store i1 %190, i1* %az
  store i1 %191, i1* %cf
  store i1 %195, i1* %of
  %196 = icmp eq i32 %187, 0
  store i1 %196, i1* %zf
  %197 = icmp slt i32 %187, 0
  store i1 %197, i1* %sf
  %198 = trunc i32 %187 to i8
  %199 = call i8 @llvm.ctpop.i8(i8 %198)
  %200 = and i8 %199, 1
  %201 = icmp eq i8 %200, 0
  store i1 %201, i1* %pf
  %202 = zext i32 %187 to i64
  store i64 %202, i64* %rbx
  br label %block_88d8

block_88d8:                                       ; preds = %175, %74
  store volatile i64 35032, i64* @assembly_address
  %203 = load i64* %rbx
  %204 = trunc i64 %203 to i32
  %205 = sub i32 %204, 15
  %206 = and i32 %204, 15
  %207 = sub i32 %206, 15
  %208 = icmp ugt i32 %207, 15
  %209 = icmp ult i32 %204, 15
  %210 = xor i32 %204, 15
  %211 = xor i32 %204, %205
  %212 = and i32 %210, %211
  %213 = icmp slt i32 %212, 0
  store i1 %208, i1* %az
  store i1 %209, i1* %cf
  store i1 %213, i1* %of
  %214 = icmp eq i32 %205, 0
  store i1 %214, i1* %zf
  %215 = icmp slt i32 %205, 0
  store i1 %215, i1* %sf
  %216 = trunc i32 %205 to i8
  %217 = call i8 @llvm.ctpop.i8(i8 %216)
  %218 = and i8 %217, 1
  %219 = icmp eq i8 %218, 0
  store i1 %219, i1* %pf
  store volatile i64 35035, i64* @assembly_address
  %220 = load i1* %cf
  %221 = load i1* %zf
  %222 = or i1 %220, %221
  br i1 %222, label %block_8886, label %block_88dd

block_88dd:                                       ; preds = %block_88d8
  store volatile i64 35037, i64* @assembly_address
  %223 = load i64* %r12
  %224 = trunc i64 %223 to i32
  %225 = zext i32 %224 to i64
  store i64 %225, i64* %rax
  store volatile i64 35040, i64* @assembly_address
  %226 = load i64* %rax
  %227 = trunc i64 %226 to i32
  %228 = and i32 %227, 65535
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %229 = icmp eq i32 %228, 0
  store i1 %229, i1* %zf
  %230 = icmp slt i32 %228, 0
  store i1 %230, i1* %sf
  %231 = trunc i32 %228 to i8
  %232 = call i8 @llvm.ctpop.i8(i8 %231)
  %233 = and i8 %232, 1
  %234 = icmp eq i8 %233, 0
  store i1 %234, i1* %pf
  %235 = zext i32 %228 to i64
  store i64 %235, i64* %rax
  store volatile i64 35045, i64* @assembly_address
  %236 = load i64* %rax
  %237 = trunc i64 %236 to i32
  store i32 %237, i32* %stack_var_-32
  store volatile i64 35048, i64* @assembly_address
  %238 = load i64* %r12
  %239 = load i1* %of
  %240 = lshr i64 %238, 16
  %241 = icmp eq i64 %240, 0
  store i1 %241, i1* %zf
  %242 = icmp slt i64 %240, 0
  store i1 %242, i1* %sf
  %243 = trunc i64 %240 to i8
  %244 = call i8 @llvm.ctpop.i8(i8 %243)
  %245 = and i8 %244, 1
  %246 = icmp eq i8 %245, 0
  store i1 %246, i1* %pf
  store i64 %240, i64* %r12
  %247 = and i64 32768, %238
  %248 = icmp ne i64 %247, 0
  store i1 %248, i1* %cf
  %249 = icmp slt i64 %238, 0
  %250 = select i1 false, i1 %249, i1 %239
  store i1 %250, i1* %of
  store volatile i64 35052, i64* @assembly_address
  %251 = load i64* %rbx
  %252 = trunc i64 %251 to i32
  %253 = sub i32 %252, 16
  %254 = and i32 %252, 15
  %255 = icmp ugt i32 %254, 15
  %256 = icmp ult i32 %252, 16
  %257 = xor i32 %252, 16
  %258 = xor i32 %252, %253
  %259 = and i32 %257, %258
  %260 = icmp slt i32 %259, 0
  store i1 %255, i1* %az
  store i1 %256, i1* %cf
  store i1 %260, i1* %of
  %261 = icmp eq i32 %253, 0
  store i1 %261, i1* %zf
  %262 = icmp slt i32 %253, 0
  store i1 %262, i1* %sf
  %263 = trunc i32 %253 to i8
  %264 = call i8 @llvm.ctpop.i8(i8 %263)
  %265 = and i8 %264, 1
  %266 = icmp eq i8 %265, 0
  store i1 %266, i1* %pf
  %267 = zext i32 %253 to i64
  store i64 %267, i64* %rbx
  store volatile i64 35055, i64* @assembly_address
  br label %block_8943

block_88f1:                                       ; preds = %block_8943
  store volatile i64 35057, i64* @assembly_address
  %268 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %269 = zext i32 %268 to i64
  store i64 %269, i64* %rdx
  store volatile i64 35063, i64* @assembly_address
  %270 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %271 = zext i32 %270 to i64
  store i64 %271, i64* %rax
  store volatile i64 35069, i64* @assembly_address
  %272 = load i64* %rdx
  %273 = trunc i64 %272 to i32
  %274 = load i64* %rax
  %275 = trunc i64 %274 to i32
  %276 = sub i32 %273, %275
  %277 = and i32 %273, 15
  %278 = and i32 %275, 15
  %279 = sub i32 %277, %278
  %280 = icmp ugt i32 %279, 15
  %281 = icmp ult i32 %273, %275
  %282 = xor i32 %273, %275
  %283 = xor i32 %273, %276
  %284 = and i32 %282, %283
  %285 = icmp slt i32 %284, 0
  store i1 %280, i1* %az
  store i1 %281, i1* %cf
  store i1 %285, i1* %of
  %286 = icmp eq i32 %276, 0
  store i1 %286, i1* %zf
  %287 = icmp slt i32 %276, 0
  store i1 %287, i1* %sf
  %288 = trunc i32 %276 to i8
  %289 = call i8 @llvm.ctpop.i8(i8 %288)
  %290 = and i8 %289, 1
  %291 = icmp eq i8 %290, 0
  store i1 %291, i1* %pf
  store volatile i64 35071, i64* @assembly_address
  %292 = load i1* %cf
  %293 = icmp eq i1 %292, false
  br i1 %293, label %block_8922, label %block_8901

block_8901:                                       ; preds = %block_88f1
  store volatile i64 35073, i64* @assembly_address
  %294 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %295 = zext i32 %294 to i64
  store i64 %295, i64* %rax
  store volatile i64 35079, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 35082, i64* @assembly_address
  %296 = load i64* %rdx
  %297 = trunc i64 %296 to i32
  store i32 %297, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 35088, i64* @assembly_address
  %298 = load i64* %rax
  %299 = trunc i64 %298 to i32
  %300 = zext i32 %299 to i64
  store i64 %300, i64* %rdx
  store volatile i64 35090, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 35097, i64* @assembly_address
  %301 = load i64* %rdx
  %302 = load i64* %rax
  %303 = mul i64 %302, 1
  %304 = add i64 %301, %303
  %305 = inttoptr i64 %304 to i8*
  %306 = load i8* %305
  %307 = zext i8 %306 to i64
  store i64 %307, i64* %rax
  store volatile i64 35101, i64* @assembly_address
  %308 = load i64* %rax
  %309 = trunc i64 %308 to i8
  %310 = zext i8 %309 to i64
  store i64 %310, i64* %rax
  store volatile i64 35104, i64* @assembly_address
  br label %block_8938

block_8922:                                       ; preds = %block_88f1
  store volatile i64 35106, i64* @assembly_address
  %311 = load i64** %stack_var_-28
  %312 = ptrtoint i64* %311 to i32
  %313 = zext i32 %312 to i64
  store i64 %313, i64* %rax
  store volatile i64 35109, i64* @assembly_address
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i32
  store i32 %315, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 35115, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 35120, i64* @assembly_address
  %316 = load i64* %rdi
  %317 = trunc i64 %316 to i32
  %318 = call i64 @fill_inbuf(i32 %317)
  store i64 %318, i64* %rax
  store i64 %318, i64* %rax
  store volatile i64 35125, i64* @assembly_address
  %319 = load i64* %rax
  %320 = trunc i64 %319 to i8
  %321 = zext i8 %320 to i64
  store i64 %321, i64* %rax
  br label %block_8938

block_8938:                                       ; preds = %block_8922, %block_8901
  store volatile i64 35128, i64* @assembly_address
  %322 = load i64* %rbx
  %323 = trunc i64 %322 to i32
  %324 = zext i32 %323 to i64
  store i64 %324, i64* %rcx
  store volatile i64 35130, i64* @assembly_address
  %325 = load i64* %rax
  %326 = load i64* %rcx
  %327 = trunc i64 %326 to i8
  %328 = zext i8 %327 to i64
  %329 = and i64 %328, 63
  %330 = load i1* %of
  %331 = icmp eq i64 %329, 0
  br i1 %331, label %348, label %332

; <label>:332                                     ; preds = %block_8938
  %333 = shl i64 %325, %329
  %334 = icmp eq i64 %333, 0
  store i1 %334, i1* %zf
  %335 = icmp slt i64 %333, 0
  store i1 %335, i1* %sf
  %336 = trunc i64 %333 to i8
  %337 = call i8 @llvm.ctpop.i8(i8 %336)
  %338 = and i8 %337, 1
  %339 = icmp eq i8 %338, 0
  store i1 %339, i1* %pf
  store i64 %333, i64* %rax
  %340 = sub i64 %329, 1
  %341 = shl i64 %325, %340
  %342 = lshr i64 %341, 63
  %343 = trunc i64 %342 to i1
  store i1 %343, i1* %cf
  %344 = lshr i64 %333, 63
  %345 = icmp ne i64 %344, %342
  %346 = icmp eq i64 %329, 1
  %347 = select i1 %346, i1 %345, i1 %330
  store i1 %347, i1* %of
  br label %348

; <label>:348                                     ; preds = %block_8938, %332
  store volatile i64 35133, i64* @assembly_address
  %349 = load i64* %r12
  %350 = load i64* %rax
  %351 = or i64 %349, %350
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %352 = icmp eq i64 %351, 0
  store i1 %352, i1* %zf
  %353 = icmp slt i64 %351, 0
  store i1 %353, i1* %sf
  %354 = trunc i64 %351 to i8
  %355 = call i8 @llvm.ctpop.i8(i8 %354)
  %356 = and i8 %355, 1
  %357 = icmp eq i8 %356, 0
  store i1 %357, i1* %pf
  store i64 %351, i64* %r12
  store volatile i64 35136, i64* @assembly_address
  %358 = load i64* %rbx
  %359 = trunc i64 %358 to i32
  %360 = add i32 %359, 8
  %361 = and i32 %359, 15
  %362 = add i32 %361, 8
  %363 = icmp ugt i32 %362, 15
  %364 = icmp ult i32 %360, %359
  %365 = xor i32 %359, %360
  %366 = xor i32 8, %360
  %367 = and i32 %365, %366
  %368 = icmp slt i32 %367, 0
  store i1 %363, i1* %az
  store i1 %364, i1* %cf
  store i1 %368, i1* %of
  %369 = icmp eq i32 %360, 0
  store i1 %369, i1* %zf
  %370 = icmp slt i32 %360, 0
  store i1 %370, i1* %sf
  %371 = trunc i32 %360 to i8
  %372 = call i8 @llvm.ctpop.i8(i8 %371)
  %373 = and i8 %372, 1
  %374 = icmp eq i8 %373, 0
  store i1 %374, i1* %pf
  %375 = zext i32 %360 to i64
  store i64 %375, i64* %rbx
  br label %block_8943

block_8943:                                       ; preds = %348, %block_88dd
  store volatile i64 35139, i64* @assembly_address
  %376 = load i64* %rbx
  %377 = trunc i64 %376 to i32
  %378 = sub i32 %377, 15
  %379 = and i32 %377, 15
  %380 = sub i32 %379, 15
  %381 = icmp ugt i32 %380, 15
  %382 = icmp ult i32 %377, 15
  %383 = xor i32 %377, 15
  %384 = xor i32 %377, %378
  %385 = and i32 %383, %384
  %386 = icmp slt i32 %385, 0
  store i1 %381, i1* %az
  store i1 %382, i1* %cf
  store i1 %386, i1* %of
  %387 = icmp eq i32 %378, 0
  store i1 %387, i1* %zf
  %388 = icmp slt i32 %378, 0
  store i1 %388, i1* %sf
  %389 = trunc i32 %378 to i8
  %390 = call i8 @llvm.ctpop.i8(i8 %389)
  %391 = and i8 %390, 1
  %392 = icmp eq i8 %391, 0
  store i1 %392, i1* %pf
  store volatile i64 35142, i64* @assembly_address
  %393 = load i1* %cf
  %394 = load i1* %zf
  %395 = or i1 %393, %394
  br i1 %395, label %block_88f1, label %block_8948

block_8948:                                       ; preds = %block_8943
  store volatile i64 35144, i64* @assembly_address
  %396 = load i64* %r12
  %397 = trunc i64 %396 to i32
  %398 = zext i32 %397 to i64
  store i64 %398, i64* %rax
  store volatile i64 35147, i64* @assembly_address
  %399 = load i64* %rax
  %400 = trunc i64 %399 to i32
  %401 = xor i32 %400, -1
  %402 = zext i32 %401 to i64
  store i64 %402, i64* %rax
  store volatile i64 35149, i64* @assembly_address
  %403 = load i64* %rax
  %404 = trunc i64 %403 to i16
  %405 = zext i16 %404 to i64
  store i64 %405, i64* %rax
  store volatile i64 35152, i64* @assembly_address
  %406 = load i32* %stack_var_-32
  %407 = load i64* %rax
  %408 = trunc i64 %407 to i32
  %409 = sub i32 %406, %408
  %410 = and i32 %406, 15
  %411 = and i32 %408, 15
  %412 = sub i32 %410, %411
  %413 = icmp ugt i32 %412, 15
  %414 = icmp ult i32 %406, %408
  %415 = xor i32 %406, %408
  %416 = xor i32 %406, %409
  %417 = and i32 %415, %416
  %418 = icmp slt i32 %417, 0
  store i1 %413, i1* %az
  store i1 %414, i1* %cf
  store i1 %418, i1* %of
  %419 = icmp eq i32 %409, 0
  store i1 %419, i1* %zf
  %420 = icmp slt i32 %409, 0
  store i1 %420, i1* %sf
  %421 = trunc i32 %409 to i8
  %422 = call i8 @llvm.ctpop.i8(i8 %421)
  %423 = and i8 %422, 1
  %424 = icmp eq i8 %423, 0
  store i1 %424, i1* %pf
  store volatile i64 35155, i64* @assembly_address
  %425 = load i1* %zf
  br i1 %425, label %block_895f, label %block_8955

block_8955:                                       ; preds = %block_8948
  store volatile i64 35157, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 35162, i64* @assembly_address
  br label %block_8a27

block_895f:                                       ; preds = %block_8948
  store volatile i64 35167, i64* @assembly_address
  %426 = load i64* %r12
  %427 = load i1* %of
  %428 = lshr i64 %426, 16
  %429 = icmp eq i64 %428, 0
  store i1 %429, i1* %zf
  %430 = icmp slt i64 %428, 0
  store i1 %430, i1* %sf
  %431 = trunc i64 %428 to i8
  %432 = call i8 @llvm.ctpop.i8(i8 %431)
  %433 = and i8 %432, 1
  %434 = icmp eq i8 %433, 0
  store i1 %434, i1* %pf
  store i64 %428, i64* %r12
  %435 = and i64 32768, %426
  %436 = icmp ne i64 %435, 0
  store i1 %436, i1* %cf
  %437 = icmp slt i64 %426, 0
  %438 = select i1 false, i1 %437, i1 %427
  store i1 %438, i1* %of
  store volatile i64 35171, i64* @assembly_address
  %439 = load i64* %rbx
  %440 = trunc i64 %439 to i32
  %441 = sub i32 %440, 16
  %442 = and i32 %440, 15
  %443 = icmp ugt i32 %442, 15
  %444 = icmp ult i32 %440, 16
  %445 = xor i32 %440, 16
  %446 = xor i32 %440, %441
  %447 = and i32 %445, %446
  %448 = icmp slt i32 %447, 0
  store i1 %443, i1* %az
  store i1 %444, i1* %cf
  store i1 %448, i1* %of
  %449 = icmp eq i32 %441, 0
  store i1 %449, i1* %zf
  %450 = icmp slt i32 %441, 0
  store i1 %450, i1* %sf
  %451 = trunc i32 %441 to i8
  %452 = call i8 @llvm.ctpop.i8(i8 %451)
  %453 = and i8 %452, 1
  %454 = icmp eq i8 %453, 0
  store i1 %454, i1* %pf
  %455 = zext i32 %441 to i64
  store i64 %455, i64* %rbx
  store volatile i64 35174, i64* @assembly_address
  br label %block_89ff

block_896b:                                       ; preds = %block_89bd
  store volatile i64 35179, i64* @assembly_address
  %456 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %457 = zext i32 %456 to i64
  store i64 %457, i64* %rdx
  store volatile i64 35185, i64* @assembly_address
  %458 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %459 = zext i32 %458 to i64
  store i64 %459, i64* %rax
  store volatile i64 35191, i64* @assembly_address
  %460 = load i64* %rdx
  %461 = trunc i64 %460 to i32
  %462 = load i64* %rax
  %463 = trunc i64 %462 to i32
  %464 = sub i32 %461, %463
  %465 = and i32 %461, 15
  %466 = and i32 %463, 15
  %467 = sub i32 %465, %466
  %468 = icmp ugt i32 %467, 15
  %469 = icmp ult i32 %461, %463
  %470 = xor i32 %461, %463
  %471 = xor i32 %461, %464
  %472 = and i32 %470, %471
  %473 = icmp slt i32 %472, 0
  store i1 %468, i1* %az
  store i1 %469, i1* %cf
  store i1 %473, i1* %of
  %474 = icmp eq i32 %464, 0
  store i1 %474, i1* %zf
  %475 = icmp slt i32 %464, 0
  store i1 %475, i1* %sf
  %476 = trunc i32 %464 to i8
  %477 = call i8 @llvm.ctpop.i8(i8 %476)
  %478 = and i8 %477, 1
  %479 = icmp eq i8 %478, 0
  store i1 %479, i1* %pf
  store volatile i64 35193, i64* @assembly_address
  %480 = load i1* %cf
  %481 = icmp eq i1 %480, false
  br i1 %481, label %block_899c, label %block_897b

block_897b:                                       ; preds = %block_896b
  store volatile i64 35195, i64* @assembly_address
  %482 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %483 = zext i32 %482 to i64
  store i64 %483, i64* %rax
  store volatile i64 35201, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 35204, i64* @assembly_address
  %484 = load i64* %rdx
  %485 = trunc i64 %484 to i32
  store i32 %485, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 35210, i64* @assembly_address
  %486 = load i64* %rax
  %487 = trunc i64 %486 to i32
  %488 = zext i32 %487 to i64
  store i64 %488, i64* %rdx
  store volatile i64 35212, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 35219, i64* @assembly_address
  %489 = load i64* %rdx
  %490 = load i64* %rax
  %491 = mul i64 %490, 1
  %492 = add i64 %489, %491
  %493 = inttoptr i64 %492 to i8*
  %494 = load i8* %493
  %495 = zext i8 %494 to i64
  store i64 %495, i64* %rax
  store volatile i64 35223, i64* @assembly_address
  %496 = load i64* %rax
  %497 = trunc i64 %496 to i8
  %498 = zext i8 %497 to i64
  store i64 %498, i64* %rax
  store volatile i64 35226, i64* @assembly_address
  br label %block_89b2

block_899c:                                       ; preds = %block_896b
  store volatile i64 35228, i64* @assembly_address
  %499 = load i64** %stack_var_-28
  %500 = ptrtoint i64* %499 to i32
  %501 = zext i32 %500 to i64
  store i64 %501, i64* %rax
  store volatile i64 35231, i64* @assembly_address
  %502 = load i64* %rax
  %503 = trunc i64 %502 to i32
  store i32 %503, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 35237, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 35242, i64* @assembly_address
  %504 = load i64* %rdi
  %505 = trunc i64 %504 to i32
  %506 = call i64 @fill_inbuf(i32 %505)
  store i64 %506, i64* %rax
  store i64 %506, i64* %rax
  store volatile i64 35247, i64* @assembly_address
  %507 = load i64* %rax
  %508 = trunc i64 %507 to i8
  %509 = zext i8 %508 to i64
  store i64 %509, i64* %rax
  br label %block_89b2

block_89b2:                                       ; preds = %block_899c, %block_897b
  store volatile i64 35250, i64* @assembly_address
  %510 = load i64* %rbx
  %511 = trunc i64 %510 to i32
  %512 = zext i32 %511 to i64
  store i64 %512, i64* %rcx
  store volatile i64 35252, i64* @assembly_address
  %513 = load i64* %rax
  %514 = load i64* %rcx
  %515 = trunc i64 %514 to i8
  %516 = zext i8 %515 to i64
  %517 = and i64 %516, 63
  %518 = load i1* %of
  %519 = icmp eq i64 %517, 0
  br i1 %519, label %536, label %520

; <label>:520                                     ; preds = %block_89b2
  %521 = shl i64 %513, %517
  %522 = icmp eq i64 %521, 0
  store i1 %522, i1* %zf
  %523 = icmp slt i64 %521, 0
  store i1 %523, i1* %sf
  %524 = trunc i64 %521 to i8
  %525 = call i8 @llvm.ctpop.i8(i8 %524)
  %526 = and i8 %525, 1
  %527 = icmp eq i8 %526, 0
  store i1 %527, i1* %pf
  store i64 %521, i64* %rax
  %528 = sub i64 %517, 1
  %529 = shl i64 %513, %528
  %530 = lshr i64 %529, 63
  %531 = trunc i64 %530 to i1
  store i1 %531, i1* %cf
  %532 = lshr i64 %521, 63
  %533 = icmp ne i64 %532, %530
  %534 = icmp eq i64 %517, 1
  %535 = select i1 %534, i1 %533, i1 %518
  store i1 %535, i1* %of
  br label %536

; <label>:536                                     ; preds = %block_89b2, %520
  store volatile i64 35255, i64* @assembly_address
  %537 = load i64* %r12
  %538 = load i64* %rax
  %539 = or i64 %537, %538
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %540 = icmp eq i64 %539, 0
  store i1 %540, i1* %zf
  %541 = icmp slt i64 %539, 0
  store i1 %541, i1* %sf
  %542 = trunc i64 %539 to i8
  %543 = call i8 @llvm.ctpop.i8(i8 %542)
  %544 = and i8 %543, 1
  %545 = icmp eq i8 %544, 0
  store i1 %545, i1* %pf
  store i64 %539, i64* %r12
  store volatile i64 35258, i64* @assembly_address
  %546 = load i64* %rbx
  %547 = trunc i64 %546 to i32
  %548 = add i32 %547, 8
  %549 = and i32 %547, 15
  %550 = add i32 %549, 8
  %551 = icmp ugt i32 %550, 15
  %552 = icmp ult i32 %548, %547
  %553 = xor i32 %547, %548
  %554 = xor i32 8, %548
  %555 = and i32 %553, %554
  %556 = icmp slt i32 %555, 0
  store i1 %551, i1* %az
  store i1 %552, i1* %cf
  store i1 %556, i1* %of
  %557 = icmp eq i32 %548, 0
  store i1 %557, i1* %zf
  %558 = icmp slt i32 %548, 0
  store i1 %558, i1* %sf
  %559 = trunc i32 %548 to i8
  %560 = call i8 @llvm.ctpop.i8(i8 %559)
  %561 = and i8 %560, 1
  %562 = icmp eq i8 %561, 0
  store i1 %562, i1* %pf
  %563 = zext i32 %548 to i64
  store i64 %563, i64* %rbx
  br label %block_89bd

block_89bd:                                       ; preds = %536, %block_89ff
  store volatile i64 35261, i64* @assembly_address
  %564 = load i64* %rbx
  %565 = trunc i64 %564 to i32
  %566 = sub i32 %565, 7
  %567 = and i32 %565, 15
  %568 = sub i32 %567, 7
  %569 = icmp ugt i32 %568, 15
  %570 = icmp ult i32 %565, 7
  %571 = xor i32 %565, 7
  %572 = xor i32 %565, %566
  %573 = and i32 %571, %572
  %574 = icmp slt i32 %573, 0
  store i1 %569, i1* %az
  store i1 %570, i1* %cf
  store i1 %574, i1* %of
  %575 = icmp eq i32 %566, 0
  store i1 %575, i1* %zf
  %576 = icmp slt i32 %566, 0
  store i1 %576, i1* %sf
  %577 = trunc i32 %566 to i8
  %578 = call i8 @llvm.ctpop.i8(i8 %577)
  %579 = and i8 %578, 1
  %580 = icmp eq i8 %579, 0
  store i1 %580, i1* %pf
  store volatile i64 35264, i64* @assembly_address
  %581 = load i1* %cf
  %582 = load i1* %zf
  %583 = or i1 %581, %582
  br i1 %583, label %block_896b, label %block_89c2

block_89c2:                                       ; preds = %block_89bd
  store volatile i64 35266, i64* @assembly_address
  %584 = load i64** %stack_var_-28
  %585 = ptrtoint i64* %584 to i32
  %586 = zext i32 %585 to i64
  store i64 %586, i64* %rax
  store volatile i64 35269, i64* @assembly_address
  %587 = load i64* %rax
  %588 = add i64 %587, 1
  %589 = trunc i64 %588 to i32
  %590 = zext i32 %589 to i64
  store i64 %590, i64* %rdx
  store volatile i64 35272, i64* @assembly_address
  %591 = load i64* %rdx
  %592 = trunc i64 %591 to i32
  %593 = inttoptr i32 %592 to i64*
  store i64* %593, i64** %stack_var_-28
  store volatile i64 35275, i64* @assembly_address
  %594 = load i64* %r12
  %595 = trunc i64 %594 to i32
  %596 = zext i32 %595 to i64
  store i64 %596, i64* %rcx
  store volatile i64 35278, i64* @assembly_address
  %597 = load i64* %rax
  %598 = trunc i64 %597 to i32
  %599 = zext i32 %598 to i64
  store i64 %599, i64* %rdx
  store volatile i64 35280, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 35287, i64* @assembly_address
  %600 = load i64* %rcx
  %601 = trunc i64 %600 to i8
  %602 = load i64* %rdx
  %603 = load i64* %rax
  %604 = mul i64 %603, 1
  %605 = add i64 %602, %604
  %606 = inttoptr i64 %605 to i8*
  store i8 %601, i8* %606
  store volatile i64 35290, i64* @assembly_address
  %607 = load i64** %stack_var_-28
  %608 = ptrtoint i64* %607 to i32
  %609 = sub i32 %608, 32768
  %610 = and i32 %608, 15
  %611 = icmp ugt i32 %610, 15
  %612 = icmp ult i32 %608, 32768
  %613 = xor i32 %608, 32768
  %614 = xor i32 %608, %609
  %615 = and i32 %613, %614
  %616 = icmp slt i32 %615, 0
  store i1 %611, i1* %az
  store i1 %612, i1* %cf
  store i1 %616, i1* %of
  %617 = icmp eq i32 %609, 0
  store i1 %617, i1* %zf
  %618 = icmp slt i32 %609, 0
  store i1 %618, i1* %sf
  %619 = trunc i32 %609 to i8
  %620 = call i8 @llvm.ctpop.i8(i8 %619)
  %621 = and i8 %620, 1
  %622 = icmp eq i8 %621, 0
  store i1 %622, i1* %pf
  store volatile i64 35297, i64* @assembly_address
  %623 = load i1* %zf
  %624 = icmp eq i1 %623, false
  br i1 %624, label %block_89f8, label %block_89e3

block_89e3:                                       ; preds = %block_89c2
  store volatile i64 35299, i64* @assembly_address
  %625 = load i64** %stack_var_-28
  %626 = ptrtoint i64* %625 to i32
  %627 = zext i32 %626 to i64
  store i64 %627, i64* %rax
  store volatile i64 35302, i64* @assembly_address
  %628 = load i64* %rax
  %629 = trunc i64 %628 to i32
  store i32 %629, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 35308, i64* @assembly_address
  %630 = call i64 @flush_window()
  store i64 %630, i64* %rax
  store i64 %630, i64* %rax
  store i64 %630, i64* %rax
  store volatile i64 35313, i64* @assembly_address
  %631 = inttoptr i32 0 to i64*
  store i64* %631, i64** %stack_var_-28
  br label %block_89f8

block_89f8:                                       ; preds = %block_89e3, %block_89c2
  store volatile i64 35320, i64* @assembly_address
  %632 = load i64* %r12
  %633 = load i1* %of
  %634 = lshr i64 %632, 8
  %635 = icmp eq i64 %634, 0
  store i1 %635, i1* %zf
  %636 = icmp slt i64 %634, 0
  store i1 %636, i1* %sf
  %637 = trunc i64 %634 to i8
  %638 = call i8 @llvm.ctpop.i8(i8 %637)
  %639 = and i8 %638, 1
  %640 = icmp eq i8 %639, 0
  store i1 %640, i1* %pf
  store i64 %634, i64* %r12
  %641 = and i64 128, %632
  %642 = icmp ne i64 %641, 0
  store i1 %642, i1* %cf
  %643 = icmp slt i64 %632, 0
  %644 = select i1 false, i1 %643, i1 %633
  store i1 %644, i1* %of
  store volatile i64 35324, i64* @assembly_address
  %645 = load i64* %rbx
  %646 = trunc i64 %645 to i32
  %647 = sub i32 %646, 8
  %648 = and i32 %646, 15
  %649 = sub i32 %648, 8
  %650 = icmp ugt i32 %649, 15
  %651 = icmp ult i32 %646, 8
  %652 = xor i32 %646, 8
  %653 = xor i32 %646, %647
  %654 = and i32 %652, %653
  %655 = icmp slt i32 %654, 0
  store i1 %650, i1* %az
  store i1 %651, i1* %cf
  store i1 %655, i1* %of
  %656 = icmp eq i32 %647, 0
  store i1 %656, i1* %zf
  %657 = icmp slt i32 %647, 0
  store i1 %657, i1* %sf
  %658 = trunc i32 %647 to i8
  %659 = call i8 @llvm.ctpop.i8(i8 %658)
  %660 = and i8 %659, 1
  %661 = icmp eq i8 %660, 0
  store i1 %661, i1* %pf
  %662 = zext i32 %647 to i64
  store i64 %662, i64* %rbx
  br label %block_89ff

block_89ff:                                       ; preds = %block_89f8, %block_895f
  store volatile i64 35327, i64* @assembly_address
  %663 = load i32* %stack_var_-32
  %664 = zext i32 %663 to i64
  store i64 %664, i64* %rax
  store volatile i64 35330, i64* @assembly_address
  %665 = load i64* %rax
  %666 = add i64 %665, -1
  %667 = trunc i64 %666 to i32
  %668 = zext i32 %667 to i64
  store i64 %668, i64* %rdx
  store volatile i64 35333, i64* @assembly_address
  %669 = load i64* %rdx
  %670 = trunc i64 %669 to i32
  store i32 %670, i32* %stack_var_-32
  store volatile i64 35336, i64* @assembly_address
  %671 = load i64* %rax
  %672 = trunc i64 %671 to i32
  %673 = load i64* %rax
  %674 = trunc i64 %673 to i32
  %675 = and i32 %672, %674
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %676 = icmp eq i32 %675, 0
  store i1 %676, i1* %zf
  %677 = icmp slt i32 %675, 0
  store i1 %677, i1* %sf
  %678 = trunc i32 %675 to i8
  %679 = call i8 @llvm.ctpop.i8(i8 %678)
  %680 = and i8 %679, 1
  %681 = icmp eq i8 %680, 0
  store i1 %681, i1* %pf
  store volatile i64 35338, i64* @assembly_address
  %682 = load i1* %zf
  %683 = icmp eq i1 %682, false
  br i1 %683, label %block_89bd, label %block_8a0c

block_8a0c:                                       ; preds = %block_89ff
  store volatile i64 35340, i64* @assembly_address
  %684 = load i64** %stack_var_-28
  %685 = ptrtoint i64* %684 to i32
  %686 = zext i32 %685 to i64
  store i64 %686, i64* %rax
  store volatile i64 35343, i64* @assembly_address
  %687 = load i64* %rax
  %688 = trunc i64 %687 to i32
  store i32 %688, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 35349, i64* @assembly_address
  %689 = load i64* %r12
  store i64 %689, i64* @global_var_216f98
  store volatile i64 35356, i64* @assembly_address
  %690 = load i64* %rbx
  %691 = trunc i64 %690 to i32
  store i32 %691, i32* bitcast (i64* @global_var_216fa0 to i32*)
  store volatile i64 35362, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_8a27

block_8a27:                                       ; preds = %block_8a0c, %block_8955
  store volatile i64 35367, i64* @assembly_address
  %692 = load i64* %rsp
  %693 = add i64 %692, 16
  %694 = and i64 %692, 15
  %695 = icmp ugt i64 %694, 15
  %696 = icmp ult i64 %693, %692
  %697 = xor i64 %692, %693
  %698 = xor i64 16, %693
  %699 = and i64 %697, %698
  %700 = icmp slt i64 %699, 0
  store i1 %695, i1* %az
  store i1 %696, i1* %cf
  store i1 %700, i1* %of
  %701 = icmp eq i64 %693, 0
  store i1 %701, i1* %zf
  %702 = icmp slt i64 %693, 0
  store i1 %702, i1* %sf
  %703 = trunc i64 %693 to i8
  %704 = call i8 @llvm.ctpop.i8(i8 %703)
  %705 = and i8 %704, 1
  %706 = icmp eq i8 %705, 0
  store i1 %706, i1* %pf
  %707 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %707, i64* %rsp
  store volatile i64 35371, i64* @assembly_address
  %708 = load i64* %stack_var_-24
  store i64 %708, i64* %rbx
  %709 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %709, i64* %rsp
  store volatile i64 35372, i64* @assembly_address
  %710 = load i64* %stack_var_-16
  store i64 %710, i64* %r12
  %711 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %711, i64* %rsp
  store volatile i64 35374, i64* @assembly_address
  %712 = load i64* %stack_var_-8
  store i64 %712, i64* %rbp
  %713 = ptrtoint i64* %stack_var_0 to i64
  store i64 %713, i64* %rsp
  store volatile i64 35375, i64* @assembly_address
  %714 = load i64* %rax
  %715 = load i64* %rax
  ret i64 %715
}

define i64 @inflate_fixed() {
block_8a30:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-1184 = alloca i64
  %stack_var_-1200 = alloca i32
  %stack_var_-1224 = alloca i32*
  %0 = alloca i64
  %stack_var_-1216 = alloca i64
  %stack_var_-1176 = alloca i64
  %stack_var_-1192 = alloca i64
  %stack_var_-1204 = alloca i32
  %stack_var_-1196 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-1208 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i32
  %9 = alloca i32
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i32
  store volatile i64 35376, i64* @assembly_address
  %13 = load i64* %rbp
  store i64 %13, i64* %stack_var_-8
  %14 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %14, i64* %rsp
  store volatile i64 35377, i64* @assembly_address
  %15 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %15, i64* %rbp
  store volatile i64 35380, i64* @assembly_address
  %16 = load i64* %rsp
  %17 = sub i64 %16, 1200
  %18 = and i64 %16, 15
  %19 = icmp ugt i64 %18, 15
  %20 = icmp ult i64 %16, 1200
  %21 = xor i64 %16, 1200
  %22 = xor i64 %16, %17
  %23 = and i64 %21, %22
  %24 = icmp slt i64 %23, 0
  store i1 %19, i1* %az
  store i1 %20, i1* %cf
  store i1 %24, i1* %of
  %25 = icmp eq i64 %17, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i64 %17, 0
  store i1 %26, i1* %sf
  %27 = trunc i64 %17 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  %31 = ptrtoint i64* %stack_var_-1208 to i64
  store i64 %31, i64* %rsp
  store volatile i64 35387, i64* @assembly_address
  %32 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %32, i64* %rax
  store volatile i64 35396, i64* @assembly_address
  %33 = load i64* %rax
  store i64 %33, i64* %stack_var_-16
  store volatile i64 35400, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %34 = icmp eq i32 0, 0
  store i1 %34, i1* %zf
  %35 = icmp slt i32 0, 0
  store i1 %35, i1* %sf
  %36 = trunc i32 0 to i8
  %37 = call i8 @llvm.ctpop.i8(i8 %36)
  %38 = and i8 %37, 1
  %39 = icmp eq i8 %38, 0
  store i1 %39, i1* %pf
  %40 = zext i32 0 to i64
  store i64 %40, i64* %rax
  store volatile i64 35402, i64* @assembly_address
  store i32 0, i32* %stack_var_-1196
  store volatile i64 35412, i64* @assembly_address
  br label %block_8a70

block_8a56:                                       ; preds = %block_8a70
  store volatile i64 35414, i64* @assembly_address
  %41 = load i32* %stack_var_-1196
  %42 = zext i32 %41 to i64
  store i64 %42, i64* %rax
  store volatile i64 35420, i64* @assembly_address
  %43 = load i64* %rax
  %44 = trunc i64 %43 to i32
  %45 = sext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 35422, i64* @assembly_address
  %46 = load i64* %rbp
  %47 = load i64* %rax
  %48 = mul i64 %47, 4
  %49 = add i64 %46, -1168
  %50 = add i64 %49, %48
  %51 = inttoptr i64 %50 to i32*
  store i32 8, i32* %51
  store volatile i64 35433, i64* @assembly_address
  %52 = load i32* %stack_var_-1196
  %53 = add i32 %52, 1
  %54 = and i32 %52, 15
  %55 = add i32 %54, 1
  %56 = icmp ugt i32 %55, 15
  %57 = icmp ult i32 %53, %52
  %58 = xor i32 %52, %53
  %59 = xor i32 1, %53
  %60 = and i32 %58, %59
  %61 = icmp slt i32 %60, 0
  store i1 %56, i1* %az
  store i1 %57, i1* %cf
  store i1 %61, i1* %of
  %62 = icmp eq i32 %53, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i32 %53, 0
  store i1 %63, i1* %sf
  %64 = trunc i32 %53 to i8
  %65 = call i8 @llvm.ctpop.i8(i8 %64)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store i32 %53, i32* %stack_var_-1196
  br label %block_8a70

block_8a70:                                       ; preds = %block_8a56, %block_8a30
  store volatile i64 35440, i64* @assembly_address
  %68 = load i32* %stack_var_-1196
  store i32 %68, i32* %12
  store i32 143, i32* %11
  %69 = sub i32 %68, 143
  %70 = and i32 %68, 15
  %71 = sub i32 %70, 15
  %72 = icmp ugt i32 %71, 15
  %73 = icmp ult i32 %68, 143
  %74 = xor i32 %68, 143
  %75 = xor i32 %68, %69
  %76 = and i32 %74, %75
  %77 = icmp slt i32 %76, 0
  store i1 %72, i1* %az
  store i1 %73, i1* %cf
  store i1 %77, i1* %of
  %78 = icmp eq i32 %69, 0
  store i1 %78, i1* %zf
  %79 = icmp slt i32 %69, 0
  store i1 %79, i1* %sf
  %80 = trunc i32 %69 to i8
  %81 = call i8 @llvm.ctpop.i8(i8 %80)
  %82 = and i8 %81, 1
  %83 = icmp eq i8 %82, 0
  store i1 %83, i1* %pf
  store volatile i64 35450, i64* @assembly_address
  %84 = load i32* %12
  %85 = load i32* %11
  %86 = icmp sle i32 %84, %85
  br i1 %86, label %block_8a56, label %block_8a7c

block_8a7c:                                       ; preds = %block_8a70
  store volatile i64 35452, i64* @assembly_address
  br label %block_8a98

block_8a7e:                                       ; preds = %block_8a98
  store volatile i64 35454, i64* @assembly_address
  %87 = load i32* %stack_var_-1196
  %88 = zext i32 %87 to i64
  store i64 %88, i64* %rax
  store volatile i64 35460, i64* @assembly_address
  %89 = load i64* %rax
  %90 = trunc i64 %89 to i32
  %91 = sext i32 %90 to i64
  store i64 %91, i64* %rax
  store volatile i64 35462, i64* @assembly_address
  %92 = load i64* %rbp
  %93 = load i64* %rax
  %94 = mul i64 %93, 4
  %95 = add i64 %92, -1168
  %96 = add i64 %95, %94
  %97 = inttoptr i64 %96 to i32*
  store i32 9, i32* %97
  store volatile i64 35473, i64* @assembly_address
  %98 = load i32* %stack_var_-1196
  %99 = add i32 %98, 1
  %100 = and i32 %98, 15
  %101 = add i32 %100, 1
  %102 = icmp ugt i32 %101, 15
  %103 = icmp ult i32 %99, %98
  %104 = xor i32 %98, %99
  %105 = xor i32 1, %99
  %106 = and i32 %104, %105
  %107 = icmp slt i32 %106, 0
  store i1 %102, i1* %az
  store i1 %103, i1* %cf
  store i1 %107, i1* %of
  %108 = icmp eq i32 %99, 0
  store i1 %108, i1* %zf
  %109 = icmp slt i32 %99, 0
  store i1 %109, i1* %sf
  %110 = trunc i32 %99 to i8
  %111 = call i8 @llvm.ctpop.i8(i8 %110)
  %112 = and i8 %111, 1
  %113 = icmp eq i8 %112, 0
  store i1 %113, i1* %pf
  store i32 %99, i32* %stack_var_-1196
  br label %block_8a98

block_8a98:                                       ; preds = %block_8a7e, %block_8a7c
  store volatile i64 35480, i64* @assembly_address
  %114 = load i32* %stack_var_-1196
  store i32 %114, i32* %10
  store i32 255, i32* %9
  %115 = sub i32 %114, 255
  %116 = and i32 %114, 15
  %117 = sub i32 %116, 15
  %118 = icmp ugt i32 %117, 15
  %119 = icmp ult i32 %114, 255
  %120 = xor i32 %114, 255
  %121 = xor i32 %114, %115
  %122 = and i32 %120, %121
  %123 = icmp slt i32 %122, 0
  store i1 %118, i1* %az
  store i1 %119, i1* %cf
  store i1 %123, i1* %of
  %124 = icmp eq i32 %115, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i32 %115, 0
  store i1 %125, i1* %sf
  %126 = trunc i32 %115 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  store volatile i64 35490, i64* @assembly_address
  %130 = load i32* %10
  %131 = load i32* %9
  %132 = icmp sle i32 %130, %131
  br i1 %132, label %block_8a7e, label %block_8aa4

block_8aa4:                                       ; preds = %block_8a98
  store volatile i64 35492, i64* @assembly_address
  br label %block_8ac0

block_8aa6:                                       ; preds = %block_8ac0
  store volatile i64 35494, i64* @assembly_address
  %133 = load i32* %stack_var_-1196
  %134 = zext i32 %133 to i64
  store i64 %134, i64* %rax
  store volatile i64 35500, i64* @assembly_address
  %135 = load i64* %rax
  %136 = trunc i64 %135 to i32
  %137 = sext i32 %136 to i64
  store i64 %137, i64* %rax
  store volatile i64 35502, i64* @assembly_address
  %138 = load i64* %rbp
  %139 = load i64* %rax
  %140 = mul i64 %139, 4
  %141 = add i64 %138, -1168
  %142 = add i64 %141, %140
  %143 = inttoptr i64 %142 to i32*
  store i32 7, i32* %143
  store volatile i64 35513, i64* @assembly_address
  %144 = load i32* %stack_var_-1196
  %145 = add i32 %144, 1
  %146 = and i32 %144, 15
  %147 = add i32 %146, 1
  %148 = icmp ugt i32 %147, 15
  %149 = icmp ult i32 %145, %144
  %150 = xor i32 %144, %145
  %151 = xor i32 1, %145
  %152 = and i32 %150, %151
  %153 = icmp slt i32 %152, 0
  store i1 %148, i1* %az
  store i1 %149, i1* %cf
  store i1 %153, i1* %of
  %154 = icmp eq i32 %145, 0
  store i1 %154, i1* %zf
  %155 = icmp slt i32 %145, 0
  store i1 %155, i1* %sf
  %156 = trunc i32 %145 to i8
  %157 = call i8 @llvm.ctpop.i8(i8 %156)
  %158 = and i8 %157, 1
  %159 = icmp eq i8 %158, 0
  store i1 %159, i1* %pf
  store i32 %145, i32* %stack_var_-1196
  br label %block_8ac0

block_8ac0:                                       ; preds = %block_8aa6, %block_8aa4
  store volatile i64 35520, i64* @assembly_address
  %160 = load i32* %stack_var_-1196
  store i32 %160, i32* %8
  store i32 279, i32* %7
  %161 = sub i32 %160, 279
  %162 = and i32 %160, 15
  %163 = sub i32 %162, 7
  %164 = icmp ugt i32 %163, 15
  %165 = icmp ult i32 %160, 279
  %166 = xor i32 %160, 279
  %167 = xor i32 %160, %161
  %168 = and i32 %166, %167
  %169 = icmp slt i32 %168, 0
  store i1 %164, i1* %az
  store i1 %165, i1* %cf
  store i1 %169, i1* %of
  %170 = icmp eq i32 %161, 0
  store i1 %170, i1* %zf
  %171 = icmp slt i32 %161, 0
  store i1 %171, i1* %sf
  %172 = trunc i32 %161 to i8
  %173 = call i8 @llvm.ctpop.i8(i8 %172)
  %174 = and i8 %173, 1
  %175 = icmp eq i8 %174, 0
  store i1 %175, i1* %pf
  store volatile i64 35530, i64* @assembly_address
  %176 = load i32* %8
  %177 = load i32* %7
  %178 = icmp sle i32 %176, %177
  br i1 %178, label %block_8aa6, label %block_8acc

block_8acc:                                       ; preds = %block_8ac0
  store volatile i64 35532, i64* @assembly_address
  br label %block_8ae8

block_8ace:                                       ; preds = %block_8ae8
  store volatile i64 35534, i64* @assembly_address
  %179 = load i32* %stack_var_-1196
  %180 = zext i32 %179 to i64
  store i64 %180, i64* %rax
  store volatile i64 35540, i64* @assembly_address
  %181 = load i64* %rax
  %182 = trunc i64 %181 to i32
  %183 = sext i32 %182 to i64
  store i64 %183, i64* %rax
  store volatile i64 35542, i64* @assembly_address
  %184 = load i64* %rbp
  %185 = load i64* %rax
  %186 = mul i64 %185, 4
  %187 = add i64 %184, -1168
  %188 = add i64 %187, %186
  %189 = inttoptr i64 %188 to i32*
  store i32 8, i32* %189
  store volatile i64 35553, i64* @assembly_address
  %190 = load i32* %stack_var_-1196
  %191 = add i32 %190, 1
  %192 = and i32 %190, 15
  %193 = add i32 %192, 1
  %194 = icmp ugt i32 %193, 15
  %195 = icmp ult i32 %191, %190
  %196 = xor i32 %190, %191
  %197 = xor i32 1, %191
  %198 = and i32 %196, %197
  %199 = icmp slt i32 %198, 0
  store i1 %194, i1* %az
  store i1 %195, i1* %cf
  store i1 %199, i1* %of
  %200 = icmp eq i32 %191, 0
  store i1 %200, i1* %zf
  %201 = icmp slt i32 %191, 0
  store i1 %201, i1* %sf
  %202 = trunc i32 %191 to i8
  %203 = call i8 @llvm.ctpop.i8(i8 %202)
  %204 = and i8 %203, 1
  %205 = icmp eq i8 %204, 0
  store i1 %205, i1* %pf
  store i32 %191, i32* %stack_var_-1196
  br label %block_8ae8

block_8ae8:                                       ; preds = %block_8ace, %block_8acc
  store volatile i64 35560, i64* @assembly_address
  %206 = load i32* %stack_var_-1196
  store i32 %206, i32* %6
  store i32 287, i32* %5
  %207 = sub i32 %206, 287
  %208 = and i32 %206, 15
  %209 = sub i32 %208, 15
  %210 = icmp ugt i32 %209, 15
  %211 = icmp ult i32 %206, 287
  %212 = xor i32 %206, 287
  %213 = xor i32 %206, %207
  %214 = and i32 %212, %213
  %215 = icmp slt i32 %214, 0
  store i1 %210, i1* %az
  store i1 %211, i1* %cf
  store i1 %215, i1* %of
  %216 = icmp eq i32 %207, 0
  store i1 %216, i1* %zf
  %217 = icmp slt i32 %207, 0
  store i1 %217, i1* %sf
  %218 = trunc i32 %207 to i8
  %219 = call i8 @llvm.ctpop.i8(i8 %218)
  %220 = and i8 %219, 1
  %221 = icmp eq i8 %220, 0
  store i1 %221, i1* %pf
  store volatile i64 35570, i64* @assembly_address
  %222 = load i32* %6
  %223 = load i32* %5
  %224 = icmp sle i32 %222, %223
  br i1 %224, label %block_8ace, label %block_8af4

block_8af4:                                       ; preds = %block_8ae8
  store volatile i64 35572, i64* @assembly_address
  store i32 7, i32* %stack_var_-1204
  store volatile i64 35582, i64* @assembly_address
  %225 = ptrtoint i64* %stack_var_-1192 to i64
  store i64 %225, i64* %rcx
  store volatile i64 35589, i64* @assembly_address
  %226 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %226, i64* %rax
  store volatile i64 35596, i64* @assembly_address
  %227 = load i64* %rsp
  %228 = sub i64 %227, 8
  %229 = and i64 %227, 15
  %230 = sub i64 %229, 8
  %231 = icmp ugt i64 %230, 15
  %232 = icmp ult i64 %227, 8
  %233 = xor i64 %227, 8
  %234 = xor i64 %227, %228
  %235 = and i64 %233, %234
  %236 = icmp slt i64 %235, 0
  store i1 %231, i1* %az
  store i1 %232, i1* %cf
  store i1 %236, i1* %of
  %237 = icmp eq i64 %228, 0
  store i1 %237, i1* %zf
  %238 = icmp slt i64 %228, 0
  store i1 %238, i1* %sf
  %239 = trunc i64 %228 to i8
  %240 = call i8 @llvm.ctpop.i8(i8 %239)
  %241 = and i8 %240, 1
  %242 = icmp eq i8 %241, 0
  store i1 %242, i1* %pf
  %243 = ptrtoint i64* %stack_var_-1216 to i64
  store i64 %243, i64* %rsp
  store volatile i64 35600, i64* @assembly_address
  %244 = ptrtoint i32* %stack_var_-1204 to i64
  store i64 %244, i64* %rdx
  store volatile i64 35607, i64* @assembly_address
  store i32* %stack_var_-1204, i32** %stack_var_-1224
  %245 = ptrtoint i32** %stack_var_-1224 to i64
  store i64 %245, i64* %rsp
  store volatile i64 35608, i64* @assembly_address
  %246 = ptrtoint i64* %stack_var_-1192 to i64
  store i64 %246, i64* %r9
  store volatile i64 35611, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216220 to i64), i64* %r8
  store volatile i64 35618, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2161e0 to i64), i64* %rcx
  store volatile i64 35625, i64* @assembly_address
  store i64 257, i64* %rdx
  store volatile i64 35630, i64* @assembly_address
  store i64 288, i64* %rsi
  store volatile i64 35635, i64* @assembly_address
  %247 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %247, i64* %rdi
  store volatile i64 35638, i64* @assembly_address
  %248 = load i64* %rdi
  %249 = inttoptr i64 %248 to i64*
  %250 = load i64* %rsi
  %251 = load i64* %rdx
  %252 = load i64* %rcx
  %253 = inttoptr i64 %252 to i64*
  %254 = load i64* %r8
  %255 = inttoptr i64 %254 to i64*
  %256 = load i64* %r9
  %257 = inttoptr i64 %256 to i64*
  %258 = load i32** %stack_var_-1224
  %259 = bitcast i64* %249 to i32*
  %260 = call i64 @huft_build(i32* %259, i64 %250, i64 %251, i64* %253, i64* %255, i64* %257, i32* %258)
  store i64 %260, i64* %rax
  store i64 %260, i64* %rax
  store volatile i64 35643, i64* @assembly_address
  %261 = load i64* %rsp
  %262 = add i64 %261, 16
  %263 = and i64 %261, 15
  %264 = icmp ugt i64 %263, 15
  %265 = icmp ult i64 %262, %261
  %266 = xor i64 %261, %262
  %267 = xor i64 16, %262
  %268 = and i64 %266, %267
  %269 = icmp slt i64 %268, 0
  store i1 %264, i1* %az
  store i1 %265, i1* %cf
  store i1 %269, i1* %of
  %270 = icmp eq i64 %262, 0
  store i1 %270, i1* %zf
  %271 = icmp slt i64 %262, 0
  store i1 %271, i1* %sf
  %272 = trunc i64 %262 to i8
  %273 = call i8 @llvm.ctpop.i8(i8 %272)
  %274 = and i8 %273, 1
  %275 = icmp eq i8 %274, 0
  store i1 %275, i1* %pf
  %276 = ptrtoint i64* %stack_var_-1208 to i64
  store i64 %276, i64* %rsp
  store volatile i64 35647, i64* @assembly_address
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i32
  store i32 %278, i32* %stack_var_-1196
  store volatile i64 35653, i64* @assembly_address
  %279 = load i32* %stack_var_-1196
  %280 = and i32 %279, 15
  %281 = icmp ugt i32 %280, 15
  %282 = icmp ult i32 %279, 0
  %283 = xor i32 %279, 0
  %284 = and i32 %283, 0
  %285 = icmp slt i32 %284, 0
  store i1 %281, i1* %az
  store i1 %282, i1* %cf
  store i1 %285, i1* %of
  %286 = icmp eq i32 %279, 0
  store i1 %286, i1* %zf
  %287 = icmp slt i32 %279, 0
  store i1 %287, i1* %sf
  %288 = trunc i32 %279 to i8
  %289 = call i8 @llvm.ctpop.i8(i8 %288)
  %290 = and i8 %289, 1
  %291 = icmp eq i8 %290, 0
  store i1 %291, i1* %pf
  store volatile i64 35660, i64* @assembly_address
  %292 = load i1* %zf
  br i1 %292, label %block_8b59, label %block_8b4e

block_8b4e:                                       ; preds = %block_8af4
  store volatile i64 35662, i64* @assembly_address
  %293 = load i32* %stack_var_-1196
  %294 = zext i32 %293 to i64
  store i64 %294, i64* %rax
  store volatile i64 35668, i64* @assembly_address
  br label %block_8c49

block_8b59:                                       ; preds = %block_8af4
  store volatile i64 35673, i64* @assembly_address
  store i32 0, i32* %stack_var_-1196
  store volatile i64 35683, i64* @assembly_address
  br label %block_8b7f

block_8b65:                                       ; preds = %block_8b7f
  store volatile i64 35685, i64* @assembly_address
  %295 = load i32* %stack_var_-1196
  %296 = zext i32 %295 to i64
  store i64 %296, i64* %rax
  store volatile i64 35691, i64* @assembly_address
  %297 = load i64* %rax
  %298 = trunc i64 %297 to i32
  %299 = sext i32 %298 to i64
  store i64 %299, i64* %rax
  store volatile i64 35693, i64* @assembly_address
  %300 = load i64* %rbp
  %301 = load i64* %rax
  %302 = mul i64 %301, 4
  %303 = add i64 %300, -1168
  %304 = add i64 %303, %302
  %305 = inttoptr i64 %304 to i32*
  store i32 5, i32* %305
  store volatile i64 35704, i64* @assembly_address
  %306 = load i32* %stack_var_-1196
  %307 = add i32 %306, 1
  %308 = and i32 %306, 15
  %309 = add i32 %308, 1
  %310 = icmp ugt i32 %309, 15
  %311 = icmp ult i32 %307, %306
  %312 = xor i32 %306, %307
  %313 = xor i32 1, %307
  %314 = and i32 %312, %313
  %315 = icmp slt i32 %314, 0
  store i1 %310, i1* %az
  store i1 %311, i1* %cf
  store i1 %315, i1* %of
  %316 = icmp eq i32 %307, 0
  store i1 %316, i1* %zf
  %317 = icmp slt i32 %307, 0
  store i1 %317, i1* %sf
  %318 = trunc i32 %307 to i8
  %319 = call i8 @llvm.ctpop.i8(i8 %318)
  %320 = and i8 %319, 1
  %321 = icmp eq i8 %320, 0
  store i1 %321, i1* %pf
  store i32 %307, i32* %stack_var_-1196
  br label %block_8b7f

block_8b7f:                                       ; preds = %block_8b65, %block_8b59
  store volatile i64 35711, i64* @assembly_address
  %322 = load i32* %stack_var_-1196
  store i32 %322, i32* %4
  store i32 29, i32* %3
  %323 = sub i32 %322, 29
  %324 = and i32 %322, 15
  %325 = sub i32 %324, 13
  %326 = icmp ugt i32 %325, 15
  %327 = icmp ult i32 %322, 29
  %328 = xor i32 %322, 29
  %329 = xor i32 %322, %323
  %330 = and i32 %328, %329
  %331 = icmp slt i32 %330, 0
  store i1 %326, i1* %az
  store i1 %327, i1* %cf
  store i1 %331, i1* %of
  %332 = icmp eq i32 %323, 0
  store i1 %332, i1* %zf
  %333 = icmp slt i32 %323, 0
  store i1 %333, i1* %sf
  %334 = trunc i32 %323 to i8
  %335 = call i8 @llvm.ctpop.i8(i8 %334)
  %336 = and i8 %335, 1
  %337 = icmp eq i8 %336, 0
  store i1 %337, i1* %pf
  store volatile i64 35718, i64* @assembly_address
  %338 = load i32* %4
  %339 = load i32* %3
  %340 = icmp sle i32 %338, %339
  br i1 %340, label %block_8b65, label %block_8b88

block_8b88:                                       ; preds = %block_8b7f
  store volatile i64 35720, i64* @assembly_address
  store i32 5, i32* %stack_var_-1200
  store volatile i64 35730, i64* @assembly_address
  %341 = ptrtoint i64* %stack_var_-1184 to i64
  store i64 %341, i64* %rcx
  store volatile i64 35737, i64* @assembly_address
  %342 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %342, i64* %rax
  store volatile i64 35744, i64* @assembly_address
  %343 = load i64* %rsp
  %344 = sub i64 %343, 8
  %345 = and i64 %343, 15
  %346 = sub i64 %345, 8
  %347 = icmp ugt i64 %346, 15
  %348 = icmp ult i64 %343, 8
  %349 = xor i64 %343, 8
  %350 = xor i64 %343, %344
  %351 = and i64 %349, %350
  %352 = icmp slt i64 %351, 0
  store i1 %347, i1* %az
  store i1 %348, i1* %cf
  store i1 %352, i1* %of
  %353 = icmp eq i64 %344, 0
  store i1 %353, i1* %zf
  %354 = icmp slt i64 %344, 0
  store i1 %354, i1* %sf
  %355 = trunc i64 %344 to i8
  %356 = call i8 @llvm.ctpop.i8(i8 %355)
  %357 = and i8 %356, 1
  %358 = icmp eq i8 %357, 0
  store i1 %358, i1* %pf
  %359 = ptrtoint i64* %stack_var_-1216 to i64
  store i64 %359, i64* %rsp
  store volatile i64 35748, i64* @assembly_address
  %360 = ptrtoint i32* %stack_var_-1200 to i64
  store i64 %360, i64* %rdx
  store volatile i64 35755, i64* @assembly_address
  store i32* %stack_var_-1200, i32** %stack_var_-1224
  %361 = ptrtoint i32** %stack_var_-1224 to i64
  store i64 %361, i64* %rsp
  store volatile i64 35756, i64* @assembly_address
  %362 = ptrtoint i64* %stack_var_-1184 to i64
  store i64 %362, i64* %r9
  store volatile i64 35759, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162a0 to i64), i64* %r8
  store volatile i64 35766, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216260 to i64), i64* %rcx
  store volatile i64 35773, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 35778, i64* @assembly_address
  store i64 30, i64* %rsi
  store volatile i64 35783, i64* @assembly_address
  %363 = ptrtoint i64* %stack_var_-1176 to i64
  store i64 %363, i64* %rdi
  store volatile i64 35786, i64* @assembly_address
  %364 = load i64* %rdi
  %365 = inttoptr i64 %364 to i64*
  %366 = load i64* %rsi
  %367 = load i64* %rdx
  %368 = load i64* %rcx
  %369 = inttoptr i64 %368 to i64*
  %370 = load i64* %r8
  %371 = inttoptr i64 %370 to i64*
  %372 = load i64* %r9
  %373 = inttoptr i64 %372 to i64*
  %374 = load i32** %stack_var_-1224
  %375 = bitcast i64* %365 to i32*
  %376 = call i64 @huft_build(i32* %375, i64 %366, i64 %367, i64* %369, i64* %371, i64* %373, i32* %374)
  store i64 %376, i64* %rax
  store i64 %376, i64* %rax
  store volatile i64 35791, i64* @assembly_address
  %377 = load i64* %rsp
  %378 = add i64 %377, 16
  %379 = and i64 %377, 15
  %380 = icmp ugt i64 %379, 15
  %381 = icmp ult i64 %378, %377
  %382 = xor i64 %377, %378
  %383 = xor i64 16, %378
  %384 = and i64 %382, %383
  %385 = icmp slt i64 %384, 0
  store i1 %380, i1* %az
  store i1 %381, i1* %cf
  store i1 %385, i1* %of
  %386 = icmp eq i64 %378, 0
  store i1 %386, i1* %zf
  %387 = icmp slt i64 %378, 0
  store i1 %387, i1* %sf
  %388 = trunc i64 %378 to i8
  %389 = call i8 @llvm.ctpop.i8(i8 %388)
  %390 = and i8 %389, 1
  %391 = icmp eq i8 %390, 0
  store i1 %391, i1* %pf
  %392 = ptrtoint i64* %stack_var_-1208 to i64
  store i64 %392, i64* %rsp
  store volatile i64 35795, i64* @assembly_address
  %393 = load i64* %rax
  %394 = trunc i64 %393 to i32
  store i32 %394, i32* %stack_var_-1196
  store volatile i64 35801, i64* @assembly_address
  %395 = load i32* %stack_var_-1196
  store i32 %395, i32* %2
  store i32 1, i32* %1
  %396 = sub i32 %395, 1
  %397 = and i32 %395, 15
  %398 = sub i32 %397, 1
  %399 = icmp ugt i32 %398, 15
  %400 = icmp ult i32 %395, 1
  %401 = xor i32 %395, 1
  %402 = xor i32 %395, %396
  %403 = and i32 %401, %402
  %404 = icmp slt i32 %403, 0
  store i1 %399, i1* %az
  store i1 %400, i1* %cf
  store i1 %404, i1* %of
  %405 = icmp eq i32 %396, 0
  store i1 %405, i1* %zf
  %406 = icmp slt i32 %396, 0
  store i1 %406, i1* %sf
  %407 = trunc i32 %396 to i8
  %408 = call i8 @llvm.ctpop.i8(i8 %407)
  %409 = and i8 %408, 1
  %410 = icmp eq i8 %409, 0
  store i1 %410, i1* %pf
  store volatile i64 35808, i64* @assembly_address
  %411 = load i32* %2
  %412 = load i32* %1
  %413 = icmp sle i32 %411, %412
  br i1 %413, label %block_8bf9, label %block_8be2

block_8be2:                                       ; preds = %block_8b88
  store volatile i64 35810, i64* @assembly_address
  %414 = load i64* %stack_var_-1192
  store i64 %414, i64* %rax
  store volatile i64 35817, i64* @assembly_address
  %415 = load i64* %rax
  store i64 %415, i64* %rdi
  store volatile i64 35820, i64* @assembly_address
  %416 = load i64* %rdi
  %417 = inttoptr i64 %416 to i64*
  %418 = call i64 @huft_free(i64* %417)
  store i64 %418, i64* %rax
  store i64 %418, i64* %rax
  store volatile i64 35825, i64* @assembly_address
  %419 = load i32* %stack_var_-1196
  %420 = zext i32 %419 to i64
  store i64 %420, i64* %rax
  store volatile i64 35831, i64* @assembly_address
  br label %block_8c49

block_8bf9:                                       ; preds = %block_8b88
  store volatile i64 35833, i64* @assembly_address
  %421 = load i32* %stack_var_-1200
  %422 = zext i32 %421 to i64
  store i64 %422, i64* %rcx
  store volatile i64 35839, i64* @assembly_address
  %423 = load i32* %stack_var_-1204
  %424 = zext i32 %423 to i64
  store i64 %424, i64* %rdx
  store volatile i64 35845, i64* @assembly_address
  %425 = load i64* %stack_var_-1184
  store i64 %425, i64* %rsi
  store volatile i64 35852, i64* @assembly_address
  %426 = load i64* %stack_var_-1192
  store i64 %426, i64* %rax
  store volatile i64 35859, i64* @assembly_address
  %427 = load i64* %rax
  store i64 %427, i64* %rdi
  store volatile i64 35862, i64* @assembly_address
  %428 = load i64* %rdi
  %429 = load i64* %rsi
  %430 = load i64* %rdx
  %431 = trunc i64 %430 to i32
  %432 = load i64* %rcx
  %433 = trunc i64 %432 to i32
  %434 = inttoptr i64 %428 to i8*
  %435 = call i64 @inflate_codes(i8* %434, i64 %429, i32 %431, i32 %433)
  store i64 %435, i64* %rax
  store i64 %435, i64* %rax
  store volatile i64 35867, i64* @assembly_address
  %436 = load i64* %rax
  %437 = trunc i64 %436 to i32
  %438 = load i64* %rax
  %439 = trunc i64 %438 to i32
  %440 = and i32 %437, %439
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %441 = icmp eq i32 %440, 0
  store i1 %441, i1* %zf
  %442 = icmp slt i32 %440, 0
  store i1 %442, i1* %sf
  %443 = trunc i32 %440 to i8
  %444 = call i8 @llvm.ctpop.i8(i8 %443)
  %445 = and i8 %444, 1
  %446 = icmp eq i8 %445, 0
  store i1 %446, i1* %pf
  store volatile i64 35869, i64* @assembly_address
  %447 = load i1* %zf
  br i1 %447, label %block_8c26, label %block_8c1f

block_8c1f:                                       ; preds = %block_8bf9
  store volatile i64 35871, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 35876, i64* @assembly_address
  br label %block_8c49

block_8c26:                                       ; preds = %block_8bf9
  store volatile i64 35878, i64* @assembly_address
  %448 = load i64* %stack_var_-1192
  store i64 %448, i64* %rax
  store volatile i64 35885, i64* @assembly_address
  %449 = load i64* %rax
  store i64 %449, i64* %rdi
  store volatile i64 35888, i64* @assembly_address
  %450 = load i64* %rdi
  %451 = inttoptr i64 %450 to i64*
  %452 = call i64 @huft_free(i64* %451)
  store i64 %452, i64* %rax
  store i64 %452, i64* %rax
  store volatile i64 35893, i64* @assembly_address
  %453 = load i64* %stack_var_-1184
  store i64 %453, i64* %rax
  store volatile i64 35900, i64* @assembly_address
  %454 = load i64* %rax
  store i64 %454, i64* %rdi
  store volatile i64 35903, i64* @assembly_address
  %455 = load i64* %rdi
  %456 = inttoptr i64 %455 to i64*
  %457 = call i64 @huft_free(i64* %456)
  store i64 %457, i64* %rax
  store i64 %457, i64* %rax
  store volatile i64 35908, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_8c49

block_8c49:                                       ; preds = %block_8c26, %block_8c1f, %block_8be2, %block_8b4e
  store volatile i64 35913, i64* @assembly_address
  %458 = load i64* %stack_var_-16
  store i64 %458, i64* %rsi
  store volatile i64 35917, i64* @assembly_address
  %459 = load i64* %rsi
  %460 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %461 = xor i64 %459, %460
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %462 = icmp eq i64 %461, 0
  store i1 %462, i1* %zf
  %463 = icmp slt i64 %461, 0
  store i1 %463, i1* %sf
  %464 = trunc i64 %461 to i8
  %465 = call i8 @llvm.ctpop.i8(i8 %464)
  %466 = and i8 %465, 1
  %467 = icmp eq i8 %466, 0
  store i1 %467, i1* %pf
  store i64 %461, i64* %rsi
  store volatile i64 35926, i64* @assembly_address
  %468 = load i1* %zf
  br i1 %468, label %block_8c5d, label %block_8c58

block_8c58:                                       ; preds = %block_8c49
  store volatile i64 35928, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_8c5d:                                       ; preds = %block_8c49
  store volatile i64 35933, i64* @assembly_address
  %469 = load i64* %stack_var_-8
  store i64 %469, i64* %rbp
  %470 = ptrtoint i64* %stack_var_0 to i64
  store i64 %470, i64* %rsp
  store volatile i64 35934, i64* @assembly_address
  %471 = load i64* %rax
  %472 = load i64* %rax
  ret i64 %472
}

define i64 @inflate_dynamic() {
block_8c5f:
  %r12 = alloca i64
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-1324 = alloca i32
  %stack_var_-1364 = alloca i32
  %stack_var_-1312 = alloca i8*
  %0 = alloca i64
  %stack_var_-1352 = alloca i32
  %stack_var_-1328 = alloca i32
  %stack_var_-1332 = alloca i32
  %stack_var_-1360 = alloca i32
  %stack_var_-1384 = alloca i32*
  %1 = alloca i64
  %stack_var_-1376 = alloca i64
  %stack_var_-1304 = alloca i64
  %stack_var_-1320 = alloca i64
  %stack_var_-1356 = alloca i32
  %stack_var_-1336 = alloca i32
  %stack_var_-1340 = alloca i32
  %stack_var_-1344 = alloca i32
  %stack_var_-1348 = alloca i64*
  %2 = alloca i32
  %stack_var_-32 = alloca i64
  %stack_var_-1368 = alloca i8*
  %3 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 35935, i64* @assembly_address
  %4 = load i64* %rbp
  store i64 %4, i64* %stack_var_-8
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rsp
  store volatile i64 35936, i64* @assembly_address
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rbp
  store volatile i64 35939, i64* @assembly_address
  %7 = load i64* %r12
  store i64 %7, i64* %stack_var_-16
  %8 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %8, i64* %rsp
  store volatile i64 35941, i64* @assembly_address
  %9 = load i64* %rbx
  store i64 %9, i64* %stack_var_-24
  %10 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %10, i64* %rsp
  store volatile i64 35942, i64* @assembly_address
  %11 = load i64* %rsp
  %12 = sub i64 %11, 1344
  %13 = and i64 %11, 15
  %14 = icmp ugt i64 %13, 15
  %15 = icmp ult i64 %11, 1344
  %16 = xor i64 %11, 1344
  %17 = xor i64 %11, %12
  %18 = and i64 %16, %17
  %19 = icmp slt i64 %18, 0
  store i1 %14, i1* %az
  store i1 %15, i1* %cf
  store i1 %19, i1* %of
  %20 = icmp eq i64 %12, 0
  store i1 %20, i1* %zf
  %21 = icmp slt i64 %12, 0
  store i1 %21, i1* %sf
  %22 = trunc i64 %12 to i8
  %23 = call i8 @llvm.ctpop.i8(i8 %22)
  %24 = and i8 %23, 1
  %25 = icmp eq i8 %24, 0
  store i1 %25, i1* %pf
  %26 = ptrtoint i8** %stack_var_-1368 to i64
  store i64 %26, i64* %rsp
  store volatile i64 35949, i64* @assembly_address
  %27 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %27, i64* %rax
  store volatile i64 35958, i64* @assembly_address
  %28 = load i64* %rax
  store i64 %28, i64* %stack_var_-32
  store volatile i64 35962, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %29 = icmp eq i32 0, 0
  store i1 %29, i1* %zf
  %30 = icmp slt i32 0, 0
  store i1 %30, i1* %sf
  %31 = trunc i32 0 to i8
  %32 = call i8 @llvm.ctpop.i8(i8 %31)
  %33 = and i8 %32, 1
  %34 = icmp eq i8 %33, 0
  store i1 %34, i1* %pf
  %35 = zext i32 0 to i64
  store i64 %35, i64* %rax
  store volatile i64 35964, i64* @assembly_address
  %36 = load i64* @global_var_216f98
  store i64 %36, i64* %r12
  store volatile i64 35971, i64* @assembly_address
  %37 = load i32* bitcast (i64* @global_var_216fa0 to i32*)
  %38 = zext i32 %37 to i64
  store i64 %38, i64* %rbx
  store volatile i64 35977, i64* @assembly_address
  %39 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %40 = zext i32 %39 to i64
  store i64 %40, i64* %rax
  store volatile i64 35983, i64* @assembly_address
  %41 = load i64* %rax
  %42 = trunc i64 %41 to i32
  %43 = inttoptr i32 %42 to i64*
  store i64* %43, i64** %stack_var_-1348
  store volatile i64 35989, i64* @assembly_address
  br label %block_8cec

block_8c97:                                       ; preds = %block_8cec
  store volatile i64 35991, i64* @assembly_address
  %44 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rdx
  store volatile i64 35997, i64* @assembly_address
  %46 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %47 = zext i32 %46 to i64
  store i64 %47, i64* %rax
  store volatile i64 36003, i64* @assembly_address
  %48 = load i64* %rdx
  %49 = trunc i64 %48 to i32
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  %52 = sub i32 %49, %51
  %53 = and i32 %49, 15
  %54 = and i32 %51, 15
  %55 = sub i32 %53, %54
  %56 = icmp ugt i32 %55, 15
  %57 = icmp ult i32 %49, %51
  %58 = xor i32 %49, %51
  %59 = xor i32 %49, %52
  %60 = and i32 %58, %59
  %61 = icmp slt i32 %60, 0
  store i1 %56, i1* %az
  store i1 %57, i1* %cf
  store i1 %61, i1* %of
  %62 = icmp eq i32 %52, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i32 %52, 0
  store i1 %63, i1* %sf
  %64 = trunc i32 %52 to i8
  %65 = call i8 @llvm.ctpop.i8(i8 %64)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store volatile i64 36005, i64* @assembly_address
  %68 = load i1* %cf
  %69 = icmp eq i1 %68, false
  br i1 %69, label %block_8cc8, label %block_8ca7

block_8ca7:                                       ; preds = %block_8c97
  store volatile i64 36007, i64* @assembly_address
  %70 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %71 = zext i32 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 36013, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 36016, i64* @assembly_address
  %72 = load i64* %rdx
  %73 = trunc i64 %72 to i32
  store i32 %73, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 36022, i64* @assembly_address
  %74 = load i64* %rax
  %75 = trunc i64 %74 to i32
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rdx
  store volatile i64 36024, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 36031, i64* @assembly_address
  %77 = load i64* %rdx
  %78 = load i64* %rax
  %79 = mul i64 %78, 1
  %80 = add i64 %77, %79
  %81 = inttoptr i64 %80 to i8*
  %82 = load i8* %81
  %83 = zext i8 %82 to i64
  store i64 %83, i64* %rax
  store volatile i64 36035, i64* @assembly_address
  %84 = load i64* %rax
  %85 = trunc i64 %84 to i8
  %86 = zext i8 %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 36038, i64* @assembly_address
  br label %block_8ce1

block_8cc8:                                       ; preds = %block_8c97
  store volatile i64 36040, i64* @assembly_address
  %87 = load i64** %stack_var_-1348
  %88 = ptrtoint i64* %87 to i32
  %89 = zext i32 %88 to i64
  store i64 %89, i64* %rax
  store volatile i64 36046, i64* @assembly_address
  %90 = load i64* %rax
  %91 = trunc i64 %90 to i32
  store i32 %91, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 36052, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 36057, i64* @assembly_address
  %92 = load i64* %rdi
  %93 = trunc i64 %92 to i32
  %94 = call i64 @fill_inbuf(i32 %93)
  store i64 %94, i64* %rax
  store i64 %94, i64* %rax
  store volatile i64 36062, i64* @assembly_address
  %95 = load i64* %rax
  %96 = trunc i64 %95 to i8
  %97 = zext i8 %96 to i64
  store i64 %97, i64* %rax
  br label %block_8ce1

block_8ce1:                                       ; preds = %block_8cc8, %block_8ca7
  store volatile i64 36065, i64* @assembly_address
  %98 = load i64* %rbx
  %99 = trunc i64 %98 to i32
  %100 = zext i32 %99 to i64
  store i64 %100, i64* %rcx
  store volatile i64 36067, i64* @assembly_address
  %101 = load i64* %rax
  %102 = load i64* %rcx
  %103 = trunc i64 %102 to i8
  %104 = zext i8 %103 to i64
  %105 = and i64 %104, 63
  %106 = load i1* %of
  %107 = icmp eq i64 %105, 0
  br i1 %107, label %124, label %108

; <label>:108                                     ; preds = %block_8ce1
  %109 = shl i64 %101, %105
  %110 = icmp eq i64 %109, 0
  store i1 %110, i1* %zf
  %111 = icmp slt i64 %109, 0
  store i1 %111, i1* %sf
  %112 = trunc i64 %109 to i8
  %113 = call i8 @llvm.ctpop.i8(i8 %112)
  %114 = and i8 %113, 1
  %115 = icmp eq i8 %114, 0
  store i1 %115, i1* %pf
  store i64 %109, i64* %rax
  %116 = sub i64 %105, 1
  %117 = shl i64 %101, %116
  %118 = lshr i64 %117, 63
  %119 = trunc i64 %118 to i1
  store i1 %119, i1* %cf
  %120 = lshr i64 %109, 63
  %121 = icmp ne i64 %120, %118
  %122 = icmp eq i64 %105, 1
  %123 = select i1 %122, i1 %121, i1 %106
  store i1 %123, i1* %of
  br label %124

; <label>:124                                     ; preds = %block_8ce1, %108
  store volatile i64 36070, i64* @assembly_address
  %125 = load i64* %r12
  %126 = load i64* %rax
  %127 = or i64 %125, %126
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %128 = icmp eq i64 %127, 0
  store i1 %128, i1* %zf
  %129 = icmp slt i64 %127, 0
  store i1 %129, i1* %sf
  %130 = trunc i64 %127 to i8
  %131 = call i8 @llvm.ctpop.i8(i8 %130)
  %132 = and i8 %131, 1
  %133 = icmp eq i8 %132, 0
  store i1 %133, i1* %pf
  store i64 %127, i64* %r12
  store volatile i64 36073, i64* @assembly_address
  %134 = load i64* %rbx
  %135 = trunc i64 %134 to i32
  %136 = add i32 %135, 8
  %137 = and i32 %135, 15
  %138 = add i32 %137, 8
  %139 = icmp ugt i32 %138, 15
  %140 = icmp ult i32 %136, %135
  %141 = xor i32 %135, %136
  %142 = xor i32 8, %136
  %143 = and i32 %141, %142
  %144 = icmp slt i32 %143, 0
  store i1 %139, i1* %az
  store i1 %140, i1* %cf
  store i1 %144, i1* %of
  %145 = icmp eq i32 %136, 0
  store i1 %145, i1* %zf
  %146 = icmp slt i32 %136, 0
  store i1 %146, i1* %sf
  %147 = trunc i32 %136 to i8
  %148 = call i8 @llvm.ctpop.i8(i8 %147)
  %149 = and i8 %148, 1
  %150 = icmp eq i8 %149, 0
  store i1 %150, i1* %pf
  %151 = zext i32 %136 to i64
  store i64 %151, i64* %rbx
  br label %block_8cec

block_8cec:                                       ; preds = %124, %block_8c5f
  store volatile i64 36076, i64* @assembly_address
  %152 = load i64* %rbx
  %153 = trunc i64 %152 to i32
  %154 = sub i32 %153, 4
  %155 = and i32 %153, 15
  %156 = sub i32 %155, 4
  %157 = icmp ugt i32 %156, 15
  %158 = icmp ult i32 %153, 4
  %159 = xor i32 %153, 4
  %160 = xor i32 %153, %154
  %161 = and i32 %159, %160
  %162 = icmp slt i32 %161, 0
  store i1 %157, i1* %az
  store i1 %158, i1* %cf
  store i1 %162, i1* %of
  %163 = icmp eq i32 %154, 0
  store i1 %163, i1* %zf
  %164 = icmp slt i32 %154, 0
  store i1 %164, i1* %sf
  %165 = trunc i32 %154 to i8
  %166 = call i8 @llvm.ctpop.i8(i8 %165)
  %167 = and i8 %166, 1
  %168 = icmp eq i8 %167, 0
  store i1 %168, i1* %pf
  store volatile i64 36079, i64* @assembly_address
  %169 = load i1* %cf
  %170 = load i1* %zf
  %171 = or i1 %169, %170
  br i1 %171, label %block_8c97, label %block_8cf1

block_8cf1:                                       ; preds = %block_8cec
  store volatile i64 36081, i64* @assembly_address
  %172 = load i64* %r12
  %173 = trunc i64 %172 to i32
  %174 = zext i32 %173 to i64
  store i64 %174, i64* %rax
  store volatile i64 36084, i64* @assembly_address
  %175 = load i64* %rax
  %176 = trunc i64 %175 to i32
  %177 = and i32 %176, 31
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %178 = icmp eq i32 %177, 0
  store i1 %178, i1* %zf
  %179 = icmp slt i32 %177, 0
  store i1 %179, i1* %sf
  %180 = trunc i32 %177 to i8
  %181 = call i8 @llvm.ctpop.i8(i8 %180)
  %182 = and i8 %181, 1
  %183 = icmp eq i8 %182, 0
  store i1 %183, i1* %pf
  %184 = zext i32 %177 to i64
  store i64 %184, i64* %rax
  store volatile i64 36087, i64* @assembly_address
  %185 = load i64* %rax
  %186 = trunc i64 %185 to i32
  %187 = add i32 %186, 257
  %188 = and i32 %186, 15
  %189 = add i32 %188, 1
  %190 = icmp ugt i32 %189, 15
  %191 = icmp ult i32 %187, %186
  %192 = xor i32 %186, %187
  %193 = xor i32 257, %187
  %194 = and i32 %192, %193
  %195 = icmp slt i32 %194, 0
  store i1 %190, i1* %az
  store i1 %191, i1* %cf
  store i1 %195, i1* %of
  %196 = icmp eq i32 %187, 0
  store i1 %196, i1* %zf
  %197 = icmp slt i32 %187, 0
  store i1 %197, i1* %sf
  %198 = trunc i32 %187 to i8
  %199 = call i8 @llvm.ctpop.i8(i8 %198)
  %200 = and i8 %199, 1
  %201 = icmp eq i8 %200, 0
  store i1 %201, i1* %pf
  %202 = zext i32 %187 to i64
  store i64 %202, i64* %rax
  store volatile i64 36092, i64* @assembly_address
  %203 = load i64* %rax
  %204 = trunc i64 %203 to i32
  store i32 %204, i32* %stack_var_-1344
  store volatile i64 36098, i64* @assembly_address
  %205 = load i64* %r12
  %206 = load i1* %of
  %207 = lshr i64 %205, 5
  %208 = icmp eq i64 %207, 0
  store i1 %208, i1* %zf
  %209 = icmp slt i64 %207, 0
  store i1 %209, i1* %sf
  %210 = trunc i64 %207 to i8
  %211 = call i8 @llvm.ctpop.i8(i8 %210)
  %212 = and i8 %211, 1
  %213 = icmp eq i8 %212, 0
  store i1 %213, i1* %pf
  store i64 %207, i64* %r12
  %214 = and i64 16, %205
  %215 = icmp ne i64 %214, 0
  store i1 %215, i1* %cf
  %216 = icmp slt i64 %205, 0
  %217 = select i1 false, i1 %216, i1 %206
  store i1 %217, i1* %of
  store volatile i64 36102, i64* @assembly_address
  %218 = load i64* %rbx
  %219 = trunc i64 %218 to i32
  %220 = sub i32 %219, 5
  %221 = and i32 %219, 15
  %222 = sub i32 %221, 5
  %223 = icmp ugt i32 %222, 15
  %224 = icmp ult i32 %219, 5
  %225 = xor i32 %219, 5
  %226 = xor i32 %219, %220
  %227 = and i32 %225, %226
  %228 = icmp slt i32 %227, 0
  store i1 %223, i1* %az
  store i1 %224, i1* %cf
  store i1 %228, i1* %of
  %229 = icmp eq i32 %220, 0
  store i1 %229, i1* %zf
  %230 = icmp slt i32 %220, 0
  store i1 %230, i1* %sf
  %231 = trunc i32 %220 to i8
  %232 = call i8 @llvm.ctpop.i8(i8 %231)
  %233 = and i8 %232, 1
  %234 = icmp eq i8 %233, 0
  store i1 %234, i1* %pf
  %235 = zext i32 %220 to i64
  store i64 %235, i64* %rbx
  store volatile i64 36105, i64* @assembly_address
  br label %block_8d60

block_8d0b:                                       ; preds = %block_8d60
  store volatile i64 36107, i64* @assembly_address
  %236 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %237 = zext i32 %236 to i64
  store i64 %237, i64* %rdx
  store volatile i64 36113, i64* @assembly_address
  %238 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %239 = zext i32 %238 to i64
  store i64 %239, i64* %rax
  store volatile i64 36119, i64* @assembly_address
  %240 = load i64* %rdx
  %241 = trunc i64 %240 to i32
  %242 = load i64* %rax
  %243 = trunc i64 %242 to i32
  %244 = sub i32 %241, %243
  %245 = and i32 %241, 15
  %246 = and i32 %243, 15
  %247 = sub i32 %245, %246
  %248 = icmp ugt i32 %247, 15
  %249 = icmp ult i32 %241, %243
  %250 = xor i32 %241, %243
  %251 = xor i32 %241, %244
  %252 = and i32 %250, %251
  %253 = icmp slt i32 %252, 0
  store i1 %248, i1* %az
  store i1 %249, i1* %cf
  store i1 %253, i1* %of
  %254 = icmp eq i32 %244, 0
  store i1 %254, i1* %zf
  %255 = icmp slt i32 %244, 0
  store i1 %255, i1* %sf
  %256 = trunc i32 %244 to i8
  %257 = call i8 @llvm.ctpop.i8(i8 %256)
  %258 = and i8 %257, 1
  %259 = icmp eq i8 %258, 0
  store i1 %259, i1* %pf
  store volatile i64 36121, i64* @assembly_address
  %260 = load i1* %cf
  %261 = icmp eq i1 %260, false
  br i1 %261, label %block_8d3c, label %block_8d1b

block_8d1b:                                       ; preds = %block_8d0b
  store volatile i64 36123, i64* @assembly_address
  %262 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %263 = zext i32 %262 to i64
  store i64 %263, i64* %rax
  store volatile i64 36129, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 36132, i64* @assembly_address
  %264 = load i64* %rdx
  %265 = trunc i64 %264 to i32
  store i32 %265, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 36138, i64* @assembly_address
  %266 = load i64* %rax
  %267 = trunc i64 %266 to i32
  %268 = zext i32 %267 to i64
  store i64 %268, i64* %rdx
  store volatile i64 36140, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 36147, i64* @assembly_address
  %269 = load i64* %rdx
  %270 = load i64* %rax
  %271 = mul i64 %270, 1
  %272 = add i64 %269, %271
  %273 = inttoptr i64 %272 to i8*
  %274 = load i8* %273
  %275 = zext i8 %274 to i64
  store i64 %275, i64* %rax
  store volatile i64 36151, i64* @assembly_address
  %276 = load i64* %rax
  %277 = trunc i64 %276 to i8
  %278 = zext i8 %277 to i64
  store i64 %278, i64* %rax
  store volatile i64 36154, i64* @assembly_address
  br label %block_8d55

block_8d3c:                                       ; preds = %block_8d0b
  store volatile i64 36156, i64* @assembly_address
  %279 = load i64** %stack_var_-1348
  %280 = ptrtoint i64* %279 to i32
  %281 = zext i32 %280 to i64
  store i64 %281, i64* %rax
  store volatile i64 36162, i64* @assembly_address
  %282 = load i64* %rax
  %283 = trunc i64 %282 to i32
  store i32 %283, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 36168, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 36173, i64* @assembly_address
  %284 = load i64* %rdi
  %285 = trunc i64 %284 to i32
  %286 = call i64 @fill_inbuf(i32 %285)
  store i64 %286, i64* %rax
  store i64 %286, i64* %rax
  store volatile i64 36178, i64* @assembly_address
  %287 = load i64* %rax
  %288 = trunc i64 %287 to i8
  %289 = zext i8 %288 to i64
  store i64 %289, i64* %rax
  br label %block_8d55

block_8d55:                                       ; preds = %block_8d3c, %block_8d1b
  store volatile i64 36181, i64* @assembly_address
  %290 = load i64* %rbx
  %291 = trunc i64 %290 to i32
  %292 = zext i32 %291 to i64
  store i64 %292, i64* %rcx
  store volatile i64 36183, i64* @assembly_address
  %293 = load i64* %rax
  %294 = load i64* %rcx
  %295 = trunc i64 %294 to i8
  %296 = zext i8 %295 to i64
  %297 = and i64 %296, 63
  %298 = load i1* %of
  %299 = icmp eq i64 %297, 0
  br i1 %299, label %316, label %300

; <label>:300                                     ; preds = %block_8d55
  %301 = shl i64 %293, %297
  %302 = icmp eq i64 %301, 0
  store i1 %302, i1* %zf
  %303 = icmp slt i64 %301, 0
  store i1 %303, i1* %sf
  %304 = trunc i64 %301 to i8
  %305 = call i8 @llvm.ctpop.i8(i8 %304)
  %306 = and i8 %305, 1
  %307 = icmp eq i8 %306, 0
  store i1 %307, i1* %pf
  store i64 %301, i64* %rax
  %308 = sub i64 %297, 1
  %309 = shl i64 %293, %308
  %310 = lshr i64 %309, 63
  %311 = trunc i64 %310 to i1
  store i1 %311, i1* %cf
  %312 = lshr i64 %301, 63
  %313 = icmp ne i64 %312, %310
  %314 = icmp eq i64 %297, 1
  %315 = select i1 %314, i1 %313, i1 %298
  store i1 %315, i1* %of
  br label %316

; <label>:316                                     ; preds = %block_8d55, %300
  store volatile i64 36186, i64* @assembly_address
  %317 = load i64* %r12
  %318 = load i64* %rax
  %319 = or i64 %317, %318
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %320 = icmp eq i64 %319, 0
  store i1 %320, i1* %zf
  %321 = icmp slt i64 %319, 0
  store i1 %321, i1* %sf
  %322 = trunc i64 %319 to i8
  %323 = call i8 @llvm.ctpop.i8(i8 %322)
  %324 = and i8 %323, 1
  %325 = icmp eq i8 %324, 0
  store i1 %325, i1* %pf
  store i64 %319, i64* %r12
  store volatile i64 36189, i64* @assembly_address
  %326 = load i64* %rbx
  %327 = trunc i64 %326 to i32
  %328 = add i32 %327, 8
  %329 = and i32 %327, 15
  %330 = add i32 %329, 8
  %331 = icmp ugt i32 %330, 15
  %332 = icmp ult i32 %328, %327
  %333 = xor i32 %327, %328
  %334 = xor i32 8, %328
  %335 = and i32 %333, %334
  %336 = icmp slt i32 %335, 0
  store i1 %331, i1* %az
  store i1 %332, i1* %cf
  store i1 %336, i1* %of
  %337 = icmp eq i32 %328, 0
  store i1 %337, i1* %zf
  %338 = icmp slt i32 %328, 0
  store i1 %338, i1* %sf
  %339 = trunc i32 %328 to i8
  %340 = call i8 @llvm.ctpop.i8(i8 %339)
  %341 = and i8 %340, 1
  %342 = icmp eq i8 %341, 0
  store i1 %342, i1* %pf
  %343 = zext i32 %328 to i64
  store i64 %343, i64* %rbx
  br label %block_8d60

block_8d60:                                       ; preds = %316, %block_8cf1
  store volatile i64 36192, i64* @assembly_address
  %344 = load i64* %rbx
  %345 = trunc i64 %344 to i32
  %346 = sub i32 %345, 4
  %347 = and i32 %345, 15
  %348 = sub i32 %347, 4
  %349 = icmp ugt i32 %348, 15
  %350 = icmp ult i32 %345, 4
  %351 = xor i32 %345, 4
  %352 = xor i32 %345, %346
  %353 = and i32 %351, %352
  %354 = icmp slt i32 %353, 0
  store i1 %349, i1* %az
  store i1 %350, i1* %cf
  store i1 %354, i1* %of
  %355 = icmp eq i32 %346, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i32 %346, 0
  store i1 %356, i1* %sf
  %357 = trunc i32 %346 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  store volatile i64 36195, i64* @assembly_address
  %361 = load i1* %cf
  %362 = load i1* %zf
  %363 = or i1 %361, %362
  br i1 %363, label %block_8d0b, label %block_8d65

block_8d65:                                       ; preds = %block_8d60
  store volatile i64 36197, i64* @assembly_address
  %364 = load i64* %r12
  %365 = trunc i64 %364 to i32
  %366 = zext i32 %365 to i64
  store i64 %366, i64* %rax
  store volatile i64 36200, i64* @assembly_address
  %367 = load i64* %rax
  %368 = trunc i64 %367 to i32
  %369 = and i32 %368, 31
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %370 = icmp eq i32 %369, 0
  store i1 %370, i1* %zf
  %371 = icmp slt i32 %369, 0
  store i1 %371, i1* %sf
  %372 = trunc i32 %369 to i8
  %373 = call i8 @llvm.ctpop.i8(i8 %372)
  %374 = and i8 %373, 1
  %375 = icmp eq i8 %374, 0
  store i1 %375, i1* %pf
  %376 = zext i32 %369 to i64
  store i64 %376, i64* %rax
  store volatile i64 36203, i64* @assembly_address
  %377 = load i64* %rax
  %378 = trunc i64 %377 to i32
  %379 = add i32 %378, 1
  %380 = and i32 %378, 15
  %381 = add i32 %380, 1
  %382 = icmp ugt i32 %381, 15
  %383 = icmp ult i32 %379, %378
  %384 = xor i32 %378, %379
  %385 = xor i32 1, %379
  %386 = and i32 %384, %385
  %387 = icmp slt i32 %386, 0
  store i1 %382, i1* %az
  store i1 %383, i1* %cf
  store i1 %387, i1* %of
  %388 = icmp eq i32 %379, 0
  store i1 %388, i1* %zf
  %389 = icmp slt i32 %379, 0
  store i1 %389, i1* %sf
  %390 = trunc i32 %379 to i8
  %391 = call i8 @llvm.ctpop.i8(i8 %390)
  %392 = and i8 %391, 1
  %393 = icmp eq i8 %392, 0
  store i1 %393, i1* %pf
  %394 = zext i32 %379 to i64
  store i64 %394, i64* %rax
  store volatile i64 36206, i64* @assembly_address
  %395 = load i64* %rax
  %396 = trunc i64 %395 to i32
  store i32 %396, i32* %stack_var_-1340
  store volatile i64 36212, i64* @assembly_address
  %397 = load i64* %r12
  %398 = load i1* %of
  %399 = lshr i64 %397, 5
  %400 = icmp eq i64 %399, 0
  store i1 %400, i1* %zf
  %401 = icmp slt i64 %399, 0
  store i1 %401, i1* %sf
  %402 = trunc i64 %399 to i8
  %403 = call i8 @llvm.ctpop.i8(i8 %402)
  %404 = and i8 %403, 1
  %405 = icmp eq i8 %404, 0
  store i1 %405, i1* %pf
  store i64 %399, i64* %r12
  %406 = and i64 16, %397
  %407 = icmp ne i64 %406, 0
  store i1 %407, i1* %cf
  %408 = icmp slt i64 %397, 0
  %409 = select i1 false, i1 %408, i1 %398
  store i1 %409, i1* %of
  store volatile i64 36216, i64* @assembly_address
  %410 = load i64* %rbx
  %411 = trunc i64 %410 to i32
  %412 = sub i32 %411, 5
  %413 = and i32 %411, 15
  %414 = sub i32 %413, 5
  %415 = icmp ugt i32 %414, 15
  %416 = icmp ult i32 %411, 5
  %417 = xor i32 %411, 5
  %418 = xor i32 %411, %412
  %419 = and i32 %417, %418
  %420 = icmp slt i32 %419, 0
  store i1 %415, i1* %az
  store i1 %416, i1* %cf
  store i1 %420, i1* %of
  %421 = icmp eq i32 %412, 0
  store i1 %421, i1* %zf
  %422 = icmp slt i32 %412, 0
  store i1 %422, i1* %sf
  %423 = trunc i32 %412 to i8
  %424 = call i8 @llvm.ctpop.i8(i8 %423)
  %425 = and i8 %424, 1
  %426 = icmp eq i8 %425, 0
  store i1 %426, i1* %pf
  %427 = zext i32 %412 to i64
  store i64 %427, i64* %rbx
  store volatile i64 36219, i64* @assembly_address
  br label %block_8dd2

block_8d7d:                                       ; preds = %block_8dd2
  store volatile i64 36221, i64* @assembly_address
  %428 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %429 = zext i32 %428 to i64
  store i64 %429, i64* %rdx
  store volatile i64 36227, i64* @assembly_address
  %430 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %431 = zext i32 %430 to i64
  store i64 %431, i64* %rax
  store volatile i64 36233, i64* @assembly_address
  %432 = load i64* %rdx
  %433 = trunc i64 %432 to i32
  %434 = load i64* %rax
  %435 = trunc i64 %434 to i32
  %436 = sub i32 %433, %435
  %437 = and i32 %433, 15
  %438 = and i32 %435, 15
  %439 = sub i32 %437, %438
  %440 = icmp ugt i32 %439, 15
  %441 = icmp ult i32 %433, %435
  %442 = xor i32 %433, %435
  %443 = xor i32 %433, %436
  %444 = and i32 %442, %443
  %445 = icmp slt i32 %444, 0
  store i1 %440, i1* %az
  store i1 %441, i1* %cf
  store i1 %445, i1* %of
  %446 = icmp eq i32 %436, 0
  store i1 %446, i1* %zf
  %447 = icmp slt i32 %436, 0
  store i1 %447, i1* %sf
  %448 = trunc i32 %436 to i8
  %449 = call i8 @llvm.ctpop.i8(i8 %448)
  %450 = and i8 %449, 1
  %451 = icmp eq i8 %450, 0
  store i1 %451, i1* %pf
  store volatile i64 36235, i64* @assembly_address
  %452 = load i1* %cf
  %453 = icmp eq i1 %452, false
  br i1 %453, label %block_8dae, label %block_8d8d

block_8d8d:                                       ; preds = %block_8d7d
  store volatile i64 36237, i64* @assembly_address
  %454 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %455 = zext i32 %454 to i64
  store i64 %455, i64* %rax
  store volatile i64 36243, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 36246, i64* @assembly_address
  %456 = load i64* %rdx
  %457 = trunc i64 %456 to i32
  store i32 %457, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 36252, i64* @assembly_address
  %458 = load i64* %rax
  %459 = trunc i64 %458 to i32
  %460 = zext i32 %459 to i64
  store i64 %460, i64* %rdx
  store volatile i64 36254, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 36261, i64* @assembly_address
  %461 = load i64* %rdx
  %462 = load i64* %rax
  %463 = mul i64 %462, 1
  %464 = add i64 %461, %463
  %465 = inttoptr i64 %464 to i8*
  %466 = load i8* %465
  %467 = zext i8 %466 to i64
  store i64 %467, i64* %rax
  store volatile i64 36265, i64* @assembly_address
  %468 = load i64* %rax
  %469 = trunc i64 %468 to i8
  %470 = zext i8 %469 to i64
  store i64 %470, i64* %rax
  store volatile i64 36268, i64* @assembly_address
  br label %block_8dc7

block_8dae:                                       ; preds = %block_8d7d
  store volatile i64 36270, i64* @assembly_address
  %471 = load i64** %stack_var_-1348
  %472 = ptrtoint i64* %471 to i32
  %473 = zext i32 %472 to i64
  store i64 %473, i64* %rax
  store volatile i64 36276, i64* @assembly_address
  %474 = load i64* %rax
  %475 = trunc i64 %474 to i32
  store i32 %475, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 36282, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 36287, i64* @assembly_address
  %476 = load i64* %rdi
  %477 = trunc i64 %476 to i32
  %478 = call i64 @fill_inbuf(i32 %477)
  store i64 %478, i64* %rax
  store i64 %478, i64* %rax
  store volatile i64 36292, i64* @assembly_address
  %479 = load i64* %rax
  %480 = trunc i64 %479 to i8
  %481 = zext i8 %480 to i64
  store i64 %481, i64* %rax
  br label %block_8dc7

block_8dc7:                                       ; preds = %block_8dae, %block_8d8d
  store volatile i64 36295, i64* @assembly_address
  %482 = load i64* %rbx
  %483 = trunc i64 %482 to i32
  %484 = zext i32 %483 to i64
  store i64 %484, i64* %rcx
  store volatile i64 36297, i64* @assembly_address
  %485 = load i64* %rax
  %486 = load i64* %rcx
  %487 = trunc i64 %486 to i8
  %488 = zext i8 %487 to i64
  %489 = and i64 %488, 63
  %490 = load i1* %of
  %491 = icmp eq i64 %489, 0
  br i1 %491, label %508, label %492

; <label>:492                                     ; preds = %block_8dc7
  %493 = shl i64 %485, %489
  %494 = icmp eq i64 %493, 0
  store i1 %494, i1* %zf
  %495 = icmp slt i64 %493, 0
  store i1 %495, i1* %sf
  %496 = trunc i64 %493 to i8
  %497 = call i8 @llvm.ctpop.i8(i8 %496)
  %498 = and i8 %497, 1
  %499 = icmp eq i8 %498, 0
  store i1 %499, i1* %pf
  store i64 %493, i64* %rax
  %500 = sub i64 %489, 1
  %501 = shl i64 %485, %500
  %502 = lshr i64 %501, 63
  %503 = trunc i64 %502 to i1
  store i1 %503, i1* %cf
  %504 = lshr i64 %493, 63
  %505 = icmp ne i64 %504, %502
  %506 = icmp eq i64 %489, 1
  %507 = select i1 %506, i1 %505, i1 %490
  store i1 %507, i1* %of
  br label %508

; <label>:508                                     ; preds = %block_8dc7, %492
  store volatile i64 36300, i64* @assembly_address
  %509 = load i64* %r12
  %510 = load i64* %rax
  %511 = or i64 %509, %510
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %512 = icmp eq i64 %511, 0
  store i1 %512, i1* %zf
  %513 = icmp slt i64 %511, 0
  store i1 %513, i1* %sf
  %514 = trunc i64 %511 to i8
  %515 = call i8 @llvm.ctpop.i8(i8 %514)
  %516 = and i8 %515, 1
  %517 = icmp eq i8 %516, 0
  store i1 %517, i1* %pf
  store i64 %511, i64* %r12
  store volatile i64 36303, i64* @assembly_address
  %518 = load i64* %rbx
  %519 = trunc i64 %518 to i32
  %520 = add i32 %519, 8
  %521 = and i32 %519, 15
  %522 = add i32 %521, 8
  %523 = icmp ugt i32 %522, 15
  %524 = icmp ult i32 %520, %519
  %525 = xor i32 %519, %520
  %526 = xor i32 8, %520
  %527 = and i32 %525, %526
  %528 = icmp slt i32 %527, 0
  store i1 %523, i1* %az
  store i1 %524, i1* %cf
  store i1 %528, i1* %of
  %529 = icmp eq i32 %520, 0
  store i1 %529, i1* %zf
  %530 = icmp slt i32 %520, 0
  store i1 %530, i1* %sf
  %531 = trunc i32 %520 to i8
  %532 = call i8 @llvm.ctpop.i8(i8 %531)
  %533 = and i8 %532, 1
  %534 = icmp eq i8 %533, 0
  store i1 %534, i1* %pf
  %535 = zext i32 %520 to i64
  store i64 %535, i64* %rbx
  br label %block_8dd2

block_8dd2:                                       ; preds = %508, %block_8d65
  store volatile i64 36306, i64* @assembly_address
  %536 = load i64* %rbx
  %537 = trunc i64 %536 to i32
  %538 = sub i32 %537, 3
  %539 = and i32 %537, 15
  %540 = sub i32 %539, 3
  %541 = icmp ugt i32 %540, 15
  %542 = icmp ult i32 %537, 3
  %543 = xor i32 %537, 3
  %544 = xor i32 %537, %538
  %545 = and i32 %543, %544
  %546 = icmp slt i32 %545, 0
  store i1 %541, i1* %az
  store i1 %542, i1* %cf
  store i1 %546, i1* %of
  %547 = icmp eq i32 %538, 0
  store i1 %547, i1* %zf
  %548 = icmp slt i32 %538, 0
  store i1 %548, i1* %sf
  %549 = trunc i32 %538 to i8
  %550 = call i8 @llvm.ctpop.i8(i8 %549)
  %551 = and i8 %550, 1
  %552 = icmp eq i8 %551, 0
  store i1 %552, i1* %pf
  store volatile i64 36309, i64* @assembly_address
  %553 = load i1* %cf
  %554 = load i1* %zf
  %555 = or i1 %553, %554
  br i1 %555, label %block_8d7d, label %block_8dd7

block_8dd7:                                       ; preds = %block_8dd2
  store volatile i64 36311, i64* @assembly_address
  %556 = load i64* %r12
  %557 = trunc i64 %556 to i32
  %558 = zext i32 %557 to i64
  store i64 %558, i64* %rax
  store volatile i64 36314, i64* @assembly_address
  %559 = load i64* %rax
  %560 = trunc i64 %559 to i32
  %561 = and i32 %560, 15
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %562 = icmp eq i32 %561, 0
  store i1 %562, i1* %zf
  %563 = icmp slt i32 %561, 0
  store i1 %563, i1* %sf
  %564 = trunc i32 %561 to i8
  %565 = call i8 @llvm.ctpop.i8(i8 %564)
  %566 = and i8 %565, 1
  %567 = icmp eq i8 %566, 0
  store i1 %567, i1* %pf
  %568 = zext i32 %561 to i64
  store i64 %568, i64* %rax
  store volatile i64 36317, i64* @assembly_address
  %569 = load i64* %rax
  %570 = trunc i64 %569 to i32
  %571 = add i32 %570, 4
  %572 = and i32 %570, 15
  %573 = add i32 %572, 4
  %574 = icmp ugt i32 %573, 15
  %575 = icmp ult i32 %571, %570
  %576 = xor i32 %570, %571
  %577 = xor i32 4, %571
  %578 = and i32 %576, %577
  %579 = icmp slt i32 %578, 0
  store i1 %574, i1* %az
  store i1 %575, i1* %cf
  store i1 %579, i1* %of
  %580 = icmp eq i32 %571, 0
  store i1 %580, i1* %zf
  %581 = icmp slt i32 %571, 0
  store i1 %581, i1* %sf
  %582 = trunc i32 %571 to i8
  %583 = call i8 @llvm.ctpop.i8(i8 %582)
  %584 = and i8 %583, 1
  %585 = icmp eq i8 %584, 0
  store i1 %585, i1* %pf
  %586 = zext i32 %571 to i64
  store i64 %586, i64* %rax
  store volatile i64 36320, i64* @assembly_address
  %587 = load i64* %rax
  %588 = trunc i64 %587 to i32
  store i32 %588, i32* %stack_var_-1336
  store volatile i64 36326, i64* @assembly_address
  %589 = load i64* %r12
  %590 = load i1* %of
  %591 = lshr i64 %589, 4
  %592 = icmp eq i64 %591, 0
  store i1 %592, i1* %zf
  %593 = icmp slt i64 %591, 0
  store i1 %593, i1* %sf
  %594 = trunc i64 %591 to i8
  %595 = call i8 @llvm.ctpop.i8(i8 %594)
  %596 = and i8 %595, 1
  %597 = icmp eq i8 %596, 0
  store i1 %597, i1* %pf
  store i64 %591, i64* %r12
  %598 = and i64 8, %589
  %599 = icmp ne i64 %598, 0
  store i1 %599, i1* %cf
  %600 = icmp slt i64 %589, 0
  %601 = select i1 false, i1 %600, i1 %590
  store i1 %601, i1* %of
  store volatile i64 36330, i64* @assembly_address
  %602 = load i64* %rbx
  %603 = trunc i64 %602 to i32
  %604 = sub i32 %603, 4
  %605 = and i32 %603, 15
  %606 = sub i32 %605, 4
  %607 = icmp ugt i32 %606, 15
  %608 = icmp ult i32 %603, 4
  %609 = xor i32 %603, 4
  %610 = xor i32 %603, %604
  %611 = and i32 %609, %610
  %612 = icmp slt i32 %611, 0
  store i1 %607, i1* %az
  store i1 %608, i1* %cf
  store i1 %612, i1* %of
  %613 = icmp eq i32 %604, 0
  store i1 %613, i1* %zf
  %614 = icmp slt i32 %604, 0
  store i1 %614, i1* %sf
  %615 = trunc i32 %604 to i8
  %616 = call i8 @llvm.ctpop.i8(i8 %615)
  %617 = and i8 %616, 1
  %618 = icmp eq i8 %617, 0
  store i1 %618, i1* %pf
  %619 = zext i32 %604 to i64
  store i64 %619, i64* %rbx
  store volatile i64 36333, i64* @assembly_address
  %620 = load i32* %stack_var_-1344
  %621 = sub i32 %620, 286
  %622 = and i32 %620, 15
  %623 = sub i32 %622, 14
  %624 = icmp ugt i32 %623, 15
  %625 = icmp ult i32 %620, 286
  %626 = xor i32 %620, 286
  %627 = xor i32 %620, %621
  %628 = and i32 %626, %627
  %629 = icmp slt i32 %628, 0
  store i1 %624, i1* %az
  store i1 %625, i1* %cf
  store i1 %629, i1* %of
  %630 = icmp eq i32 %621, 0
  store i1 %630, i1* %zf
  %631 = icmp slt i32 %621, 0
  store i1 %631, i1* %sf
  %632 = trunc i32 %621 to i8
  %633 = call i8 @llvm.ctpop.i8(i8 %632)
  %634 = and i8 %633, 1
  %635 = icmp eq i8 %634, 0
  store i1 %635, i1* %pf
  store volatile i64 36343, i64* @assembly_address
  %636 = load i1* %cf
  %637 = load i1* %zf
  %638 = or i1 %636, %637
  %639 = icmp ne i1 %638, true
  br i1 %639, label %block_8e02, label %block_8df9

block_8df9:                                       ; preds = %block_8dd7
  store volatile i64 36345, i64* @assembly_address
  %640 = load i32* %stack_var_-1340
  %641 = sub i32 %640, 30
  %642 = and i32 %640, 15
  %643 = sub i32 %642, 14
  %644 = icmp ugt i32 %643, 15
  %645 = icmp ult i32 %640, 30
  %646 = xor i32 %640, 30
  %647 = xor i32 %640, %641
  %648 = and i32 %646, %647
  %649 = icmp slt i32 %648, 0
  store i1 %644, i1* %az
  store i1 %645, i1* %cf
  store i1 %649, i1* %of
  %650 = icmp eq i32 %641, 0
  store i1 %650, i1* %zf
  %651 = icmp slt i32 %641, 0
  store i1 %651, i1* %sf
  %652 = trunc i32 %641 to i8
  %653 = call i8 @llvm.ctpop.i8(i8 %652)
  %654 = and i8 %653, 1
  %655 = icmp eq i8 %654, 0
  store i1 %655, i1* %pf
  store volatile i64 36352, i64* @assembly_address
  %656 = load i1* %cf
  %657 = load i1* %zf
  %658 = or i1 %656, %657
  br i1 %658, label %block_8e0c, label %block_8e02

block_8e02:                                       ; preds = %block_8df9, %block_8dd7
  store volatile i64 36354, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 36359, i64* @assembly_address
  br label %block_94ff

block_8e0c:                                       ; preds = %block_8df9
  store volatile i64 36364, i64* @assembly_address
  store i32 0, i32* %stack_var_-1356
  store volatile i64 36374, i64* @assembly_address
  br label %block_8eac

block_8e1b:                                       ; preds = %block_8e70
  store volatile i64 36379, i64* @assembly_address
  %659 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %660 = zext i32 %659 to i64
  store i64 %660, i64* %rdx
  store volatile i64 36385, i64* @assembly_address
  %661 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %662 = zext i32 %661 to i64
  store i64 %662, i64* %rax
  store volatile i64 36391, i64* @assembly_address
  %663 = load i64* %rdx
  %664 = trunc i64 %663 to i32
  %665 = load i64* %rax
  %666 = trunc i64 %665 to i32
  %667 = sub i32 %664, %666
  %668 = and i32 %664, 15
  %669 = and i32 %666, 15
  %670 = sub i32 %668, %669
  %671 = icmp ugt i32 %670, 15
  %672 = icmp ult i32 %664, %666
  %673 = xor i32 %664, %666
  %674 = xor i32 %664, %667
  %675 = and i32 %673, %674
  %676 = icmp slt i32 %675, 0
  store i1 %671, i1* %az
  store i1 %672, i1* %cf
  store i1 %676, i1* %of
  %677 = icmp eq i32 %667, 0
  store i1 %677, i1* %zf
  %678 = icmp slt i32 %667, 0
  store i1 %678, i1* %sf
  %679 = trunc i32 %667 to i8
  %680 = call i8 @llvm.ctpop.i8(i8 %679)
  %681 = and i8 %680, 1
  %682 = icmp eq i8 %681, 0
  store i1 %682, i1* %pf
  store volatile i64 36393, i64* @assembly_address
  %683 = load i1* %cf
  %684 = icmp eq i1 %683, false
  br i1 %684, label %block_8e4c, label %block_8e2b

block_8e2b:                                       ; preds = %block_8e1b
  store volatile i64 36395, i64* @assembly_address
  %685 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %686 = zext i32 %685 to i64
  store i64 %686, i64* %rax
  store volatile i64 36401, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 36404, i64* @assembly_address
  %687 = load i64* %rdx
  %688 = trunc i64 %687 to i32
  store i32 %688, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 36410, i64* @assembly_address
  %689 = load i64* %rax
  %690 = trunc i64 %689 to i32
  %691 = zext i32 %690 to i64
  store i64 %691, i64* %rdx
  store volatile i64 36412, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 36419, i64* @assembly_address
  %692 = load i64* %rdx
  %693 = load i64* %rax
  %694 = mul i64 %693, 1
  %695 = add i64 %692, %694
  %696 = inttoptr i64 %695 to i8*
  %697 = load i8* %696
  %698 = zext i8 %697 to i64
  store i64 %698, i64* %rax
  store volatile i64 36423, i64* @assembly_address
  %699 = load i64* %rax
  %700 = trunc i64 %699 to i8
  %701 = zext i8 %700 to i64
  store i64 %701, i64* %rax
  store volatile i64 36426, i64* @assembly_address
  br label %block_8e65

block_8e4c:                                       ; preds = %block_8e1b
  store volatile i64 36428, i64* @assembly_address
  %702 = load i64** %stack_var_-1348
  %703 = ptrtoint i64* %702 to i32
  %704 = zext i32 %703 to i64
  store i64 %704, i64* %rax
  store volatile i64 36434, i64* @assembly_address
  %705 = load i64* %rax
  %706 = trunc i64 %705 to i32
  store i32 %706, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 36440, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 36445, i64* @assembly_address
  %707 = load i64* %rdi
  %708 = trunc i64 %707 to i32
  %709 = call i64 @fill_inbuf(i32 %708)
  store i64 %709, i64* %rax
  store i64 %709, i64* %rax
  store volatile i64 36450, i64* @assembly_address
  %710 = load i64* %rax
  %711 = trunc i64 %710 to i8
  %712 = zext i8 %711 to i64
  store i64 %712, i64* %rax
  br label %block_8e65

block_8e65:                                       ; preds = %block_8e4c, %block_8e2b
  store volatile i64 36453, i64* @assembly_address
  %713 = load i64* %rbx
  %714 = trunc i64 %713 to i32
  %715 = zext i32 %714 to i64
  store i64 %715, i64* %rcx
  store volatile i64 36455, i64* @assembly_address
  %716 = load i64* %rax
  %717 = load i64* %rcx
  %718 = trunc i64 %717 to i8
  %719 = zext i8 %718 to i64
  %720 = and i64 %719, 63
  %721 = load i1* %of
  %722 = icmp eq i64 %720, 0
  br i1 %722, label %739, label %723

; <label>:723                                     ; preds = %block_8e65
  %724 = shl i64 %716, %720
  %725 = icmp eq i64 %724, 0
  store i1 %725, i1* %zf
  %726 = icmp slt i64 %724, 0
  store i1 %726, i1* %sf
  %727 = trunc i64 %724 to i8
  %728 = call i8 @llvm.ctpop.i8(i8 %727)
  %729 = and i8 %728, 1
  %730 = icmp eq i8 %729, 0
  store i1 %730, i1* %pf
  store i64 %724, i64* %rax
  %731 = sub i64 %720, 1
  %732 = shl i64 %716, %731
  %733 = lshr i64 %732, 63
  %734 = trunc i64 %733 to i1
  store i1 %734, i1* %cf
  %735 = lshr i64 %724, 63
  %736 = icmp ne i64 %735, %733
  %737 = icmp eq i64 %720, 1
  %738 = select i1 %737, i1 %736, i1 %721
  store i1 %738, i1* %of
  br label %739

; <label>:739                                     ; preds = %block_8e65, %723
  store volatile i64 36458, i64* @assembly_address
  %740 = load i64* %r12
  %741 = load i64* %rax
  %742 = or i64 %740, %741
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %743 = icmp eq i64 %742, 0
  store i1 %743, i1* %zf
  %744 = icmp slt i64 %742, 0
  store i1 %744, i1* %sf
  %745 = trunc i64 %742 to i8
  %746 = call i8 @llvm.ctpop.i8(i8 %745)
  %747 = and i8 %746, 1
  %748 = icmp eq i8 %747, 0
  store i1 %748, i1* %pf
  store i64 %742, i64* %r12
  store volatile i64 36461, i64* @assembly_address
  %749 = load i64* %rbx
  %750 = trunc i64 %749 to i32
  %751 = add i32 %750, 8
  %752 = and i32 %750, 15
  %753 = add i32 %752, 8
  %754 = icmp ugt i32 %753, 15
  %755 = icmp ult i32 %751, %750
  %756 = xor i32 %750, %751
  %757 = xor i32 8, %751
  %758 = and i32 %756, %757
  %759 = icmp slt i32 %758, 0
  store i1 %754, i1* %az
  store i1 %755, i1* %cf
  store i1 %759, i1* %of
  %760 = icmp eq i32 %751, 0
  store i1 %760, i1* %zf
  %761 = icmp slt i32 %751, 0
  store i1 %761, i1* %sf
  %762 = trunc i32 %751 to i8
  %763 = call i8 @llvm.ctpop.i8(i8 %762)
  %764 = and i8 %763, 1
  %765 = icmp eq i8 %764, 0
  store i1 %765, i1* %pf
  %766 = zext i32 %751 to i64
  store i64 %766, i64* %rbx
  br label %block_8e70

block_8e70:                                       ; preds = %739, %block_8eac
  store volatile i64 36464, i64* @assembly_address
  %767 = load i64* %rbx
  %768 = trunc i64 %767 to i32
  %769 = sub i32 %768, 2
  %770 = and i32 %768, 15
  %771 = sub i32 %770, 2
  %772 = icmp ugt i32 %771, 15
  %773 = icmp ult i32 %768, 2
  %774 = xor i32 %768, 2
  %775 = xor i32 %768, %769
  %776 = and i32 %774, %775
  %777 = icmp slt i32 %776, 0
  store i1 %772, i1* %az
  store i1 %773, i1* %cf
  store i1 %777, i1* %of
  %778 = icmp eq i32 %769, 0
  store i1 %778, i1* %zf
  %779 = icmp slt i32 %769, 0
  store i1 %779, i1* %sf
  %780 = trunc i32 %769 to i8
  %781 = call i8 @llvm.ctpop.i8(i8 %780)
  %782 = and i8 %781, 1
  %783 = icmp eq i8 %782, 0
  store i1 %783, i1* %pf
  store volatile i64 36467, i64* @assembly_address
  %784 = load i1* %cf
  %785 = load i1* %zf
  %786 = or i1 %784, %785
  br i1 %786, label %block_8e1b, label %block_8e75

block_8e75:                                       ; preds = %block_8e70
  store volatile i64 36469, i64* @assembly_address
  %787 = load i64* %r12
  %788 = trunc i64 %787 to i32
  %789 = zext i32 %788 to i64
  store i64 %789, i64* %rcx
  store volatile i64 36472, i64* @assembly_address
  %790 = load i32* %stack_var_-1356
  %791 = zext i32 %790 to i64
  store i64 %791, i64* %rax
  store volatile i64 36478, i64* @assembly_address
  %792 = load i64* %rax
  %793 = mul i64 %792, 4
  store i64 %793, i64* %rdx
  store volatile i64 36486, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216180 to i64), i64* %rax
  store volatile i64 36493, i64* @assembly_address
  %794 = load i64* %rdx
  %795 = load i64* %rax
  %796 = mul i64 %795, 1
  %797 = add i64 %794, %796
  %798 = inttoptr i64 %797 to i32*
  %799 = load i32* %798
  %800 = zext i32 %799 to i64
  store i64 %800, i64* %rdx
  store volatile i64 36496, i64* @assembly_address
  %801 = load i64* %rcx
  %802 = trunc i64 %801 to i32
  %803 = zext i32 %802 to i64
  store i64 %803, i64* %rax
  store volatile i64 36498, i64* @assembly_address
  %804 = load i64* %rax
  %805 = trunc i64 %804 to i32
  %806 = and i32 %805, 7
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %807 = icmp eq i32 %806, 0
  store i1 %807, i1* %zf
  %808 = icmp slt i32 %806, 0
  store i1 %808, i1* %sf
  %809 = trunc i32 %806 to i8
  %810 = call i8 @llvm.ctpop.i8(i8 %809)
  %811 = and i8 %810, 1
  %812 = icmp eq i8 %811, 0
  store i1 %812, i1* %pf
  %813 = zext i32 %806 to i64
  store i64 %813, i64* %rax
  store volatile i64 36501, i64* @assembly_address
  %814 = load i64* %rdx
  %815 = trunc i64 %814 to i32
  %816 = zext i32 %815 to i64
  store i64 %816, i64* %rdx
  store volatile i64 36503, i64* @assembly_address
  %817 = load i64* %rax
  %818 = trunc i64 %817 to i32
  %819 = load i64* %rbp
  %820 = load i64* %rdx
  %821 = mul i64 %820, 4
  %822 = add i64 %819, -1296
  %823 = add i64 %822, %821
  %824 = inttoptr i64 %823 to i32*
  store i32 %818, i32* %824
  store volatile i64 36510, i64* @assembly_address
  %825 = load i64* %r12
  %826 = load i1* %of
  %827 = lshr i64 %825, 3
  %828 = icmp eq i64 %827, 0
  store i1 %828, i1* %zf
  %829 = icmp slt i64 %827, 0
  store i1 %829, i1* %sf
  %830 = trunc i64 %827 to i8
  %831 = call i8 @llvm.ctpop.i8(i8 %830)
  %832 = and i8 %831, 1
  %833 = icmp eq i8 %832, 0
  store i1 %833, i1* %pf
  store i64 %827, i64* %r12
  %834 = and i64 4, %825
  %835 = icmp ne i64 %834, 0
  store i1 %835, i1* %cf
  %836 = icmp slt i64 %825, 0
  %837 = select i1 false, i1 %836, i1 %826
  store i1 %837, i1* %of
  store volatile i64 36514, i64* @assembly_address
  %838 = load i64* %rbx
  %839 = trunc i64 %838 to i32
  %840 = sub i32 %839, 3
  %841 = and i32 %839, 15
  %842 = sub i32 %841, 3
  %843 = icmp ugt i32 %842, 15
  %844 = icmp ult i32 %839, 3
  %845 = xor i32 %839, 3
  %846 = xor i32 %839, %840
  %847 = and i32 %845, %846
  %848 = icmp slt i32 %847, 0
  store i1 %843, i1* %az
  store i1 %844, i1* %cf
  store i1 %848, i1* %of
  %849 = icmp eq i32 %840, 0
  store i1 %849, i1* %zf
  %850 = icmp slt i32 %840, 0
  store i1 %850, i1* %sf
  %851 = trunc i32 %840 to i8
  %852 = call i8 @llvm.ctpop.i8(i8 %851)
  %853 = and i8 %852, 1
  %854 = icmp eq i8 %853, 0
  store i1 %854, i1* %pf
  %855 = zext i32 %840 to i64
  store i64 %855, i64* %rbx
  store volatile i64 36517, i64* @assembly_address
  %856 = load i32* %stack_var_-1356
  %857 = add i32 %856, 1
  %858 = and i32 %856, 15
  %859 = add i32 %858, 1
  %860 = icmp ugt i32 %859, 15
  %861 = icmp ult i32 %857, %856
  %862 = xor i32 %856, %857
  %863 = xor i32 1, %857
  %864 = and i32 %862, %863
  %865 = icmp slt i32 %864, 0
  store i1 %860, i1* %az
  store i1 %861, i1* %cf
  store i1 %865, i1* %of
  %866 = icmp eq i32 %857, 0
  store i1 %866, i1* %zf
  %867 = icmp slt i32 %857, 0
  store i1 %867, i1* %sf
  %868 = trunc i32 %857 to i8
  %869 = call i8 @llvm.ctpop.i8(i8 %868)
  %870 = and i8 %869, 1
  %871 = icmp eq i8 %870, 0
  store i1 %871, i1* %pf
  store i32 %857, i32* %stack_var_-1356
  br label %block_8eac

block_8eac:                                       ; preds = %block_8e75, %block_8e0c
  store volatile i64 36524, i64* @assembly_address
  %872 = load i32* %stack_var_-1356
  %873 = zext i32 %872 to i64
  store i64 %873, i64* %rax
  store volatile i64 36530, i64* @assembly_address
  %874 = load i64* %rax
  %875 = trunc i64 %874 to i32
  %876 = load i32* %stack_var_-1336
  %877 = sub i32 %875, %876
  %878 = and i32 %875, 15
  %879 = and i32 %876, 15
  %880 = sub i32 %878, %879
  %881 = icmp ugt i32 %880, 15
  %882 = icmp ult i32 %875, %876
  %883 = xor i32 %875, %876
  %884 = xor i32 %875, %877
  %885 = and i32 %883, %884
  %886 = icmp slt i32 %885, 0
  store i1 %881, i1* %az
  store i1 %882, i1* %cf
  store i1 %886, i1* %of
  %887 = icmp eq i32 %877, 0
  store i1 %887, i1* %zf
  %888 = icmp slt i32 %877, 0
  store i1 %888, i1* %sf
  %889 = trunc i32 %877 to i8
  %890 = call i8 @llvm.ctpop.i8(i8 %889)
  %891 = and i8 %890, 1
  %892 = icmp eq i8 %891, 0
  store i1 %892, i1* %pf
  store volatile i64 36536, i64* @assembly_address
  %893 = load i1* %cf
  br i1 %893, label %block_8e70, label %block_8eba

block_8eba:                                       ; preds = %block_8eac
  store volatile i64 36538, i64* @assembly_address
  br label %block_8ee8

block_8ebc:                                       ; preds = %block_8ee8
  store volatile i64 36540, i64* @assembly_address
  %894 = load i32* %stack_var_-1356
  %895 = zext i32 %894 to i64
  store i64 %895, i64* %rax
  store volatile i64 36546, i64* @assembly_address
  %896 = load i64* %rax
  %897 = mul i64 %896, 4
  store i64 %897, i64* %rdx
  store volatile i64 36554, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216180 to i64), i64* %rax
  store volatile i64 36561, i64* @assembly_address
  %898 = load i64* %rdx
  %899 = load i64* %rax
  %900 = mul i64 %899, 1
  %901 = add i64 %898, %900
  %902 = inttoptr i64 %901 to i32*
  %903 = load i32* %902
  %904 = zext i32 %903 to i64
  store i64 %904, i64* %rax
  store volatile i64 36564, i64* @assembly_address
  %905 = load i64* %rax
  %906 = trunc i64 %905 to i32
  %907 = zext i32 %906 to i64
  store i64 %907, i64* %rax
  store volatile i64 36566, i64* @assembly_address
  %908 = load i64* %rbp
  %909 = load i64* %rax
  %910 = mul i64 %909, 4
  %911 = add i64 %908, -1296
  %912 = add i64 %911, %910
  %913 = inttoptr i64 %912 to i32*
  store i32 0, i32* %913
  store volatile i64 36577, i64* @assembly_address
  %914 = load i32* %stack_var_-1356
  %915 = add i32 %914, 1
  %916 = and i32 %914, 15
  %917 = add i32 %916, 1
  %918 = icmp ugt i32 %917, 15
  %919 = icmp ult i32 %915, %914
  %920 = xor i32 %914, %915
  %921 = xor i32 1, %915
  %922 = and i32 %920, %921
  %923 = icmp slt i32 %922, 0
  store i1 %918, i1* %az
  store i1 %919, i1* %cf
  store i1 %923, i1* %of
  %924 = icmp eq i32 %915, 0
  store i1 %924, i1* %zf
  %925 = icmp slt i32 %915, 0
  store i1 %925, i1* %sf
  %926 = trunc i32 %915 to i8
  %927 = call i8 @llvm.ctpop.i8(i8 %926)
  %928 = and i8 %927, 1
  %929 = icmp eq i8 %928, 0
  store i1 %929, i1* %pf
  store i32 %915, i32* %stack_var_-1356
  br label %block_8ee8

block_8ee8:                                       ; preds = %block_8ebc, %block_8eba
  store volatile i64 36584, i64* @assembly_address
  %930 = load i32* %stack_var_-1356
  %931 = sub i32 %930, 18
  %932 = and i32 %930, 15
  %933 = sub i32 %932, 2
  %934 = icmp ugt i32 %933, 15
  %935 = icmp ult i32 %930, 18
  %936 = xor i32 %930, 18
  %937 = xor i32 %930, %931
  %938 = and i32 %936, %937
  %939 = icmp slt i32 %938, 0
  store i1 %934, i1* %az
  store i1 %935, i1* %cf
  store i1 %939, i1* %of
  %940 = icmp eq i32 %931, 0
  store i1 %940, i1* %zf
  %941 = icmp slt i32 %931, 0
  store i1 %941, i1* %sf
  %942 = trunc i32 %931 to i8
  %943 = call i8 @llvm.ctpop.i8(i8 %942)
  %944 = and i8 %943, 1
  %945 = icmp eq i8 %944, 0
  store i1 %945, i1* %pf
  store volatile i64 36591, i64* @assembly_address
  %946 = load i1* %cf
  %947 = load i1* %zf
  %948 = or i1 %946, %947
  br i1 %948, label %block_8ebc, label %block_8ef1

block_8ef1:                                       ; preds = %block_8ee8
  store volatile i64 36593, i64* @assembly_address
  %949 = sext i32 7 to i64
  %950 = inttoptr i64 %949 to i8*
  store i8* %950, i8** %stack_var_-1368
  store volatile i64 36603, i64* @assembly_address
  %951 = ptrtoint i64* %stack_var_-1320 to i64
  store i64 %951, i64* %rcx
  store volatile i64 36610, i64* @assembly_address
  %952 = ptrtoint i64* %stack_var_-1304 to i64
  store i64 %952, i64* %rax
  store volatile i64 36617, i64* @assembly_address
  %953 = load i64* %rsp
  %954 = sub i64 %953, 8
  %955 = and i64 %953, 15
  %956 = sub i64 %955, 8
  %957 = icmp ugt i64 %956, 15
  %958 = icmp ult i64 %953, 8
  %959 = xor i64 %953, 8
  %960 = xor i64 %953, %954
  %961 = and i64 %959, %960
  %962 = icmp slt i64 %961, 0
  store i1 %957, i1* %az
  store i1 %958, i1* %cf
  store i1 %962, i1* %of
  %963 = icmp eq i64 %954, 0
  store i1 %963, i1* %zf
  %964 = icmp slt i64 %954, 0
  store i1 %964, i1* %sf
  %965 = trunc i64 %954 to i8
  %966 = call i8 @llvm.ctpop.i8(i8 %965)
  %967 = and i8 %966, 1
  %968 = icmp eq i8 %967, 0
  store i1 %968, i1* %pf
  %969 = ptrtoint i64* %stack_var_-1376 to i64
  store i64 %969, i64* %rsp
  store volatile i64 36621, i64* @assembly_address
  %970 = ptrtoint i8** %stack_var_-1368 to i64
  store i64 %970, i64* %rdx
  store volatile i64 36628, i64* @assembly_address
  %971 = bitcast i8** %stack_var_-1368 to i32*
  store i32* %971, i32** %stack_var_-1384
  %972 = ptrtoint i32** %stack_var_-1384 to i64
  store i64 %972, i64* %rsp
  store volatile i64 36629, i64* @assembly_address
  %973 = ptrtoint i64* %stack_var_-1320 to i64
  store i64 %973, i64* %r9
  store volatile i64 36632, i64* @assembly_address
  store i64 0, i64* %r8
  store volatile i64 36638, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 36643, i64* @assembly_address
  store i64 19, i64* %rdx
  store volatile i64 36648, i64* @assembly_address
  store i64 19, i64* %rsi
  store volatile i64 36653, i64* @assembly_address
  %974 = ptrtoint i64* %stack_var_-1304 to i64
  store i64 %974, i64* %rdi
  store volatile i64 36656, i64* @assembly_address
  %975 = load i64* %rdi
  %976 = inttoptr i64 %975 to i64*
  %977 = load i64* %rsi
  %978 = load i64* %rdx
  %979 = load i64* %rcx
  %980 = inttoptr i64 %979 to i64*
  %981 = load i64* %r8
  %982 = inttoptr i64 %981 to i64*
  %983 = load i64* %r9
  %984 = inttoptr i64 %983 to i64*
  %985 = load i32** %stack_var_-1384
  %986 = bitcast i64* %976 to i32*
  %987 = call i64 @huft_build(i32* %986, i64 %977, i64 %978, i64* %980, i64* %982, i64* %984, i32* %985)
  store i64 %987, i64* %rax
  store i64 %987, i64* %rax
  store volatile i64 36661, i64* @assembly_address
  %988 = load i64* %rsp
  %989 = add i64 %988, 16
  %990 = and i64 %988, 15
  %991 = icmp ugt i64 %990, 15
  %992 = icmp ult i64 %989, %988
  %993 = xor i64 %988, %989
  %994 = xor i64 16, %989
  %995 = and i64 %993, %994
  %996 = icmp slt i64 %995, 0
  store i1 %991, i1* %az
  store i1 %992, i1* %cf
  store i1 %996, i1* %of
  %997 = icmp eq i64 %989, 0
  store i1 %997, i1* %zf
  %998 = icmp slt i64 %989, 0
  store i1 %998, i1* %sf
  %999 = trunc i64 %989 to i8
  %1000 = call i8 @llvm.ctpop.i8(i8 %999)
  %1001 = and i8 %1000, 1
  %1002 = icmp eq i8 %1001, 0
  store i1 %1002, i1* %pf
  %1003 = ptrtoint i8** %stack_var_-1368 to i64
  store i64 %1003, i64* %rsp
  store volatile i64 36665, i64* @assembly_address
  %1004 = load i64* %rax
  %1005 = trunc i64 %1004 to i32
  store i32 %1005, i32* %stack_var_-1360
  store volatile i64 36671, i64* @assembly_address
  %1006 = load i32* %stack_var_-1360
  %1007 = and i32 %1006, 15
  %1008 = icmp ugt i32 %1007, 15
  %1009 = icmp ult i32 %1006, 0
  %1010 = xor i32 %1006, 0
  %1011 = and i32 %1010, 0
  %1012 = icmp slt i32 %1011, 0
  store i1 %1008, i1* %az
  store i1 %1009, i1* %cf
  store i1 %1012, i1* %of
  %1013 = icmp eq i32 %1006, 0
  store i1 %1013, i1* %zf
  %1014 = icmp slt i32 %1006, 0
  store i1 %1014, i1* %sf
  %1015 = trunc i32 %1006 to i8
  %1016 = call i8 @llvm.ctpop.i8(i8 %1015)
  %1017 = and i8 %1016, 1
  %1018 = icmp eq i8 %1017, 0
  store i1 %1018, i1* %pf
  store volatile i64 36678, i64* @assembly_address
  %1019 = load i1* %zf
  br i1 %1019, label %block_8f6b, label %block_8f48

block_8f48:                                       ; preds = %block_8ef1
  store volatile i64 36680, i64* @assembly_address
  %1020 = load i32* %stack_var_-1360
  %1021 = sub i32 %1020, 1
  %1022 = and i32 %1020, 15
  %1023 = sub i32 %1022, 1
  %1024 = icmp ugt i32 %1023, 15
  %1025 = icmp ult i32 %1020, 1
  %1026 = xor i32 %1020, 1
  %1027 = xor i32 %1020, %1021
  %1028 = and i32 %1026, %1027
  %1029 = icmp slt i32 %1028, 0
  store i1 %1024, i1* %az
  store i1 %1025, i1* %cf
  store i1 %1029, i1* %of
  %1030 = icmp eq i32 %1021, 0
  store i1 %1030, i1* %zf
  %1031 = icmp slt i32 %1021, 0
  store i1 %1031, i1* %sf
  %1032 = trunc i32 %1021 to i8
  %1033 = call i8 @llvm.ctpop.i8(i8 %1032)
  %1034 = and i8 %1033, 1
  %1035 = icmp eq i8 %1034, 0
  store i1 %1035, i1* %pf
  store volatile i64 36687, i64* @assembly_address
  %1036 = load i1* %zf
  %1037 = icmp eq i1 %1036, false
  br i1 %1037, label %block_8f60, label %block_8f51

block_8f51:                                       ; preds = %block_8f48
  store volatile i64 36689, i64* @assembly_address
  %1038 = load i64* %stack_var_-1320
  store i64 %1038, i64* %rax
  store volatile i64 36696, i64* @assembly_address
  %1039 = load i64* %rax
  store i64 %1039, i64* %rdi
  store volatile i64 36699, i64* @assembly_address
  %1040 = load i64* %rdi
  %1041 = inttoptr i64 %1040 to i64*
  %1042 = call i64 @huft_free(i64* %1041)
  store i64 %1042, i64* %rax
  store i64 %1042, i64* %rax
  br label %block_8f60

block_8f60:                                       ; preds = %block_8f51, %block_8f48
  store volatile i64 36704, i64* @assembly_address
  %1043 = load i32* %stack_var_-1360
  %1044 = zext i32 %1043 to i64
  store i64 %1044, i64* %rax
  store volatile i64 36710, i64* @assembly_address
  br label %block_94ff

block_8f6b:                                       ; preds = %block_8ef1
  store volatile i64 36715, i64* @assembly_address
  %1045 = load i64* %stack_var_-1320
  store i64 %1045, i64* %rax
  store volatile i64 36722, i64* @assembly_address
  %1046 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1047 = icmp eq i64 %1046, 0
  store i1 %1047, i1* %zf
  %1048 = icmp slt i64 %1046, 0
  store i1 %1048, i1* %sf
  %1049 = trunc i64 %1046 to i8
  %1050 = call i8 @llvm.ctpop.i8(i8 %1049)
  %1051 = and i8 %1050, 1
  %1052 = icmp eq i8 %1051, 0
  store i1 %1052, i1* %pf
  store volatile i64 36725, i64* @assembly_address
  %1053 = load i1* %zf
  %1054 = icmp eq i1 %1053, false
  br i1 %1054, label %block_8f81, label %block_8f77

block_8f77:                                       ; preds = %block_8f6b
  store volatile i64 36727, i64* @assembly_address
  store i64 2, i64* %rax
  store volatile i64 36732, i64* @assembly_address
  br label %block_94ff

block_8f81:                                       ; preds = %block_8f6b
  store volatile i64 36737, i64* @assembly_address
  %1055 = load i32* %stack_var_-1344
  %1056 = zext i32 %1055 to i64
  store i64 %1056, i64* %rdx
  store volatile i64 36743, i64* @assembly_address
  %1057 = load i32* %stack_var_-1340
  %1058 = zext i32 %1057 to i64
  store i64 %1058, i64* %rax
  store volatile i64 36749, i64* @assembly_address
  %1059 = load i64* %rax
  %1060 = trunc i64 %1059 to i32
  %1061 = load i64* %rdx
  %1062 = trunc i64 %1061 to i32
  %1063 = add i32 %1060, %1062
  %1064 = and i32 %1060, 15
  %1065 = and i32 %1062, 15
  %1066 = add i32 %1064, %1065
  %1067 = icmp ugt i32 %1066, 15
  %1068 = icmp ult i32 %1063, %1060
  %1069 = xor i32 %1060, %1063
  %1070 = xor i32 %1062, %1063
  %1071 = and i32 %1069, %1070
  %1072 = icmp slt i32 %1071, 0
  store i1 %1067, i1* %az
  store i1 %1068, i1* %cf
  store i1 %1072, i1* %of
  %1073 = icmp eq i32 %1063, 0
  store i1 %1073, i1* %zf
  %1074 = icmp slt i32 %1063, 0
  store i1 %1074, i1* %sf
  %1075 = trunc i32 %1063 to i8
  %1076 = call i8 @llvm.ctpop.i8(i8 %1075)
  %1077 = and i8 %1076, 1
  %1078 = icmp eq i8 %1077, 0
  store i1 %1078, i1* %pf
  %1079 = zext i32 %1063 to i64
  store i64 %1079, i64* %rax
  store volatile i64 36751, i64* @assembly_address
  %1080 = load i64* %rax
  %1081 = trunc i64 %1080 to i32
  store i32 %1081, i32* %stack_var_-1332
  store volatile i64 36757, i64* @assembly_address
  %1082 = load i8** %stack_var_-1368
  %1083 = ptrtoint i8* %1082 to i64
  %1084 = trunc i64 %1083 to i32
  %1085 = zext i32 %1084 to i64
  store i64 %1085, i64* %rax
  store volatile i64 36763, i64* @assembly_address
  %1086 = load i64* %rax
  %1087 = trunc i64 %1086 to i32
  %1088 = sext i32 %1087 to i64
  store i64 %1088, i64* %rax
  store volatile i64 36765, i64* @assembly_address
  %1089 = load i64* %rax
  %1090 = load i64* %rax
  %1091 = mul i64 %1090, 1
  %1092 = add i64 %1089, %1091
  store i64 %1092, i64* %rdx
  store volatile i64 36769, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162e0 to i64), i64* %rax
  store volatile i64 36776, i64* @assembly_address
  %1093 = load i64* %rdx
  %1094 = load i64* %rax
  %1095 = mul i64 %1094, 1
  %1096 = add i64 %1093, %1095
  %1097 = inttoptr i64 %1096 to i16*
  %1098 = load i16* %1097
  %1099 = zext i16 %1098 to i64
  store i64 %1099, i64* %rax
  store volatile i64 36780, i64* @assembly_address
  %1100 = load i64* %rax
  %1101 = trunc i64 %1100 to i16
  %1102 = zext i16 %1101 to i64
  store i64 %1102, i64* %rax
  store volatile i64 36783, i64* @assembly_address
  %1103 = load i64* %rax
  %1104 = trunc i64 %1103 to i32
  store i32 %1104, i32* %stack_var_-1328
  store volatile i64 36789, i64* @assembly_address
  store i32 0, i32* %stack_var_-1352
  store volatile i64 36799, i64* @assembly_address
  store i32 0, i32* %stack_var_-1360
  store volatile i64 36809, i64* @assembly_address
  br label %block_9360

block_8fce:                                       ; preds = %block_9023
  store volatile i64 36814, i64* @assembly_address
  %1105 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1106 = zext i32 %1105 to i64
  store i64 %1106, i64* %rdx
  store volatile i64 36820, i64* @assembly_address
  %1107 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1108 = zext i32 %1107 to i64
  store i64 %1108, i64* %rax
  store volatile i64 36826, i64* @assembly_address
  %1109 = load i64* %rdx
  %1110 = trunc i64 %1109 to i32
  %1111 = load i64* %rax
  %1112 = trunc i64 %1111 to i32
  %1113 = sub i32 %1110, %1112
  %1114 = and i32 %1110, 15
  %1115 = and i32 %1112, 15
  %1116 = sub i32 %1114, %1115
  %1117 = icmp ugt i32 %1116, 15
  %1118 = icmp ult i32 %1110, %1112
  %1119 = xor i32 %1110, %1112
  %1120 = xor i32 %1110, %1113
  %1121 = and i32 %1119, %1120
  %1122 = icmp slt i32 %1121, 0
  store i1 %1117, i1* %az
  store i1 %1118, i1* %cf
  store i1 %1122, i1* %of
  %1123 = icmp eq i32 %1113, 0
  store i1 %1123, i1* %zf
  %1124 = icmp slt i32 %1113, 0
  store i1 %1124, i1* %sf
  %1125 = trunc i32 %1113 to i8
  %1126 = call i8 @llvm.ctpop.i8(i8 %1125)
  %1127 = and i8 %1126, 1
  %1128 = icmp eq i8 %1127, 0
  store i1 %1128, i1* %pf
  store volatile i64 36828, i64* @assembly_address
  %1129 = load i1* %cf
  %1130 = icmp eq i1 %1129, false
  br i1 %1130, label %block_8fff, label %block_8fde

block_8fde:                                       ; preds = %block_8fce
  store volatile i64 36830, i64* @assembly_address
  %1131 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1132 = zext i32 %1131 to i64
  store i64 %1132, i64* %rax
  store volatile i64 36836, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 36839, i64* @assembly_address
  %1133 = load i64* %rdx
  %1134 = trunc i64 %1133 to i32
  store i32 %1134, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 36845, i64* @assembly_address
  %1135 = load i64* %rax
  %1136 = trunc i64 %1135 to i32
  %1137 = zext i32 %1136 to i64
  store i64 %1137, i64* %rdx
  store volatile i64 36847, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 36854, i64* @assembly_address
  %1138 = load i64* %rdx
  %1139 = load i64* %rax
  %1140 = mul i64 %1139, 1
  %1141 = add i64 %1138, %1140
  %1142 = inttoptr i64 %1141 to i8*
  %1143 = load i8* %1142
  %1144 = zext i8 %1143 to i64
  store i64 %1144, i64* %rax
  store volatile i64 36858, i64* @assembly_address
  %1145 = load i64* %rax
  %1146 = trunc i64 %1145 to i8
  %1147 = zext i8 %1146 to i64
  store i64 %1147, i64* %rax
  store volatile i64 36861, i64* @assembly_address
  br label %block_9018

block_8fff:                                       ; preds = %block_8fce
  store volatile i64 36863, i64* @assembly_address
  %1148 = load i64** %stack_var_-1348
  %1149 = ptrtoint i64* %1148 to i32
  %1150 = zext i32 %1149 to i64
  store i64 %1150, i64* %rax
  store volatile i64 36869, i64* @assembly_address
  %1151 = load i64* %rax
  %1152 = trunc i64 %1151 to i32
  store i32 %1152, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 36875, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 36880, i64* @assembly_address
  %1153 = load i64* %rdi
  %1154 = trunc i64 %1153 to i32
  %1155 = call i64 @fill_inbuf(i32 %1154)
  store i64 %1155, i64* %rax
  store i64 %1155, i64* %rax
  store volatile i64 36885, i64* @assembly_address
  %1156 = load i64* %rax
  %1157 = trunc i64 %1156 to i8
  %1158 = zext i8 %1157 to i64
  store i64 %1158, i64* %rax
  br label %block_9018

block_9018:                                       ; preds = %block_8fff, %block_8fde
  store volatile i64 36888, i64* @assembly_address
  %1159 = load i64* %rbx
  %1160 = trunc i64 %1159 to i32
  %1161 = zext i32 %1160 to i64
  store i64 %1161, i64* %rcx
  store volatile i64 36890, i64* @assembly_address
  %1162 = load i64* %rax
  %1163 = load i64* %rcx
  %1164 = trunc i64 %1163 to i8
  %1165 = zext i8 %1164 to i64
  %1166 = and i64 %1165, 63
  %1167 = load i1* %of
  %1168 = icmp eq i64 %1166, 0
  br i1 %1168, label %1185, label %1169

; <label>:1169                                    ; preds = %block_9018
  %1170 = shl i64 %1162, %1166
  %1171 = icmp eq i64 %1170, 0
  store i1 %1171, i1* %zf
  %1172 = icmp slt i64 %1170, 0
  store i1 %1172, i1* %sf
  %1173 = trunc i64 %1170 to i8
  %1174 = call i8 @llvm.ctpop.i8(i8 %1173)
  %1175 = and i8 %1174, 1
  %1176 = icmp eq i8 %1175, 0
  store i1 %1176, i1* %pf
  store i64 %1170, i64* %rax
  %1177 = sub i64 %1166, 1
  %1178 = shl i64 %1162, %1177
  %1179 = lshr i64 %1178, 63
  %1180 = trunc i64 %1179 to i1
  store i1 %1180, i1* %cf
  %1181 = lshr i64 %1170, 63
  %1182 = icmp ne i64 %1181, %1179
  %1183 = icmp eq i64 %1166, 1
  %1184 = select i1 %1183, i1 %1182, i1 %1167
  store i1 %1184, i1* %of
  br label %1185

; <label>:1185                                    ; preds = %block_9018, %1169
  store volatile i64 36893, i64* @assembly_address
  %1186 = load i64* %r12
  %1187 = load i64* %rax
  %1188 = or i64 %1186, %1187
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1189 = icmp eq i64 %1188, 0
  store i1 %1189, i1* %zf
  %1190 = icmp slt i64 %1188, 0
  store i1 %1190, i1* %sf
  %1191 = trunc i64 %1188 to i8
  %1192 = call i8 @llvm.ctpop.i8(i8 %1191)
  %1193 = and i8 %1192, 1
  %1194 = icmp eq i8 %1193, 0
  store i1 %1194, i1* %pf
  store i64 %1188, i64* %r12
  store volatile i64 36896, i64* @assembly_address
  %1195 = load i64* %rbx
  %1196 = trunc i64 %1195 to i32
  %1197 = add i32 %1196, 8
  %1198 = and i32 %1196, 15
  %1199 = add i32 %1198, 8
  %1200 = icmp ugt i32 %1199, 15
  %1201 = icmp ult i32 %1197, %1196
  %1202 = xor i32 %1196, %1197
  %1203 = xor i32 8, %1197
  %1204 = and i32 %1202, %1203
  %1205 = icmp slt i32 %1204, 0
  store i1 %1200, i1* %az
  store i1 %1201, i1* %cf
  store i1 %1205, i1* %of
  %1206 = icmp eq i32 %1197, 0
  store i1 %1206, i1* %zf
  %1207 = icmp slt i32 %1197, 0
  store i1 %1207, i1* %sf
  %1208 = trunc i32 %1197 to i8
  %1209 = call i8 @llvm.ctpop.i8(i8 %1208)
  %1210 = and i8 %1209, 1
  %1211 = icmp eq i8 %1210, 0
  store i1 %1211, i1* %pf
  %1212 = zext i32 %1197 to i64
  store i64 %1212, i64* %rbx
  br label %block_9023

block_9023:                                       ; preds = %1185, %block_9360
  store volatile i64 36899, i64* @assembly_address
  %1213 = load i8** %stack_var_-1368
  %1214 = ptrtoint i8* %1213 to i64
  %1215 = trunc i64 %1214 to i32
  %1216 = zext i32 %1215 to i64
  store i64 %1216, i64* %rax
  store volatile i64 36905, i64* @assembly_address
  %1217 = load i64* %rbx
  %1218 = trunc i64 %1217 to i32
  %1219 = load i64* %rax
  %1220 = trunc i64 %1219 to i32
  %1221 = sub i32 %1218, %1220
  %1222 = and i32 %1218, 15
  %1223 = and i32 %1220, 15
  %1224 = sub i32 %1222, %1223
  %1225 = icmp ugt i32 %1224, 15
  %1226 = icmp ult i32 %1218, %1220
  %1227 = xor i32 %1218, %1220
  %1228 = xor i32 %1218, %1221
  %1229 = and i32 %1227, %1228
  %1230 = icmp slt i32 %1229, 0
  store i1 %1225, i1* %az
  store i1 %1226, i1* %cf
  store i1 %1230, i1* %of
  %1231 = icmp eq i32 %1221, 0
  store i1 %1231, i1* %zf
  %1232 = icmp slt i32 %1221, 0
  store i1 %1232, i1* %sf
  %1233 = trunc i32 %1221 to i8
  %1234 = call i8 @llvm.ctpop.i8(i8 %1233)
  %1235 = and i8 %1234, 1
  %1236 = icmp eq i8 %1235, 0
  store i1 %1236, i1* %pf
  store volatile i64 36907, i64* @assembly_address
  %1237 = load i1* %cf
  br i1 %1237, label %block_8fce, label %block_902d

block_902d:                                       ; preds = %block_9023
  store volatile i64 36909, i64* @assembly_address
  %1238 = load i64* %stack_var_-1320
  store i64 %1238, i64* %rax
  store volatile i64 36916, i64* @assembly_address
  %1239 = load i64* %r12
  %1240 = trunc i64 %1239 to i32
  %1241 = zext i32 %1240 to i64
  store i64 %1241, i64* %rdx
  store volatile i64 36919, i64* @assembly_address
  %1242 = load i64* %rdx
  %1243 = trunc i64 %1242 to i32
  %1244 = load i32* %stack_var_-1328
  %1245 = and i32 %1243, %1244
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1246 = icmp eq i32 %1245, 0
  store i1 %1246, i1* %zf
  %1247 = icmp slt i32 %1245, 0
  store i1 %1247, i1* %sf
  %1248 = trunc i32 %1245 to i8
  %1249 = call i8 @llvm.ctpop.i8(i8 %1248)
  %1250 = and i8 %1249, 1
  %1251 = icmp eq i8 %1250, 0
  store i1 %1251, i1* %pf
  %1252 = zext i32 %1245 to i64
  store i64 %1252, i64* %rdx
  store volatile i64 36925, i64* @assembly_address
  %1253 = load i64* %rdx
  %1254 = trunc i64 %1253 to i32
  %1255 = zext i32 %1254 to i64
  store i64 %1255, i64* %rdx
  store volatile i64 36927, i64* @assembly_address
  %1256 = load i64* %rdx
  %1257 = load i1* %of
  %1258 = shl i64 %1256, 4
  %1259 = icmp eq i64 %1258, 0
  store i1 %1259, i1* %zf
  %1260 = icmp slt i64 %1258, 0
  store i1 %1260, i1* %sf
  %1261 = trunc i64 %1258 to i8
  %1262 = call i8 @llvm.ctpop.i8(i8 %1261)
  %1263 = and i8 %1262, 1
  %1264 = icmp eq i8 %1263, 0
  store i1 %1264, i1* %pf
  store i64 %1258, i64* %rdx
  %1265 = shl i64 %1256, 3
  %1266 = lshr i64 %1265, 63
  %1267 = trunc i64 %1266 to i1
  store i1 %1267, i1* %cf
  %1268 = lshr i64 %1258, 63
  %1269 = icmp ne i64 %1268, %1266
  %1270 = select i1 false, i1 %1269, i1 %1257
  store i1 %1270, i1* %of
  store volatile i64 36931, i64* @assembly_address
  %1271 = load i64* %rax
  %1272 = load i64* %rdx
  %1273 = add i64 %1271, %1272
  %1274 = and i64 %1271, 15
  %1275 = and i64 %1272, 15
  %1276 = add i64 %1274, %1275
  %1277 = icmp ugt i64 %1276, 15
  %1278 = icmp ult i64 %1273, %1271
  %1279 = xor i64 %1271, %1273
  %1280 = xor i64 %1272, %1273
  %1281 = and i64 %1279, %1280
  %1282 = icmp slt i64 %1281, 0
  store i1 %1277, i1* %az
  store i1 %1278, i1* %cf
  store i1 %1282, i1* %of
  %1283 = icmp eq i64 %1273, 0
  store i1 %1283, i1* %zf
  %1284 = icmp slt i64 %1273, 0
  store i1 %1284, i1* %sf
  %1285 = trunc i64 %1273 to i8
  %1286 = call i8 @llvm.ctpop.i8(i8 %1285)
  %1287 = and i8 %1286, 1
  %1288 = icmp eq i8 %1287, 0
  store i1 %1288, i1* %pf
  store i64 %1273, i64* %rax
  store volatile i64 36934, i64* @assembly_address
  %1289 = load i64* %rax
  %1290 = inttoptr i64 %1289 to i8*
  store i8* %1290, i8** %stack_var_-1312
  store volatile i64 36941, i64* @assembly_address
  %1291 = load i8** %stack_var_-1312
  %1292 = ptrtoint i8* %1291 to i64
  store i64 %1292, i64* %rax
  store volatile i64 36948, i64* @assembly_address
  %1293 = load i64* %rax
  %1294 = add i64 %1293, 1
  %1295 = inttoptr i64 %1294 to i8*
  %1296 = load i8* %1295
  %1297 = zext i8 %1296 to i64
  store i64 %1297, i64* %rax
  store volatile i64 36952, i64* @assembly_address
  %1298 = load i64* %rax
  %1299 = trunc i64 %1298 to i8
  %1300 = zext i8 %1299 to i64
  store i64 %1300, i64* %rax
  store volatile i64 36955, i64* @assembly_address
  %1301 = load i64* %rax
  %1302 = trunc i64 %1301 to i32
  store i32 %1302, i32* %stack_var_-1356
  store volatile i64 36961, i64* @assembly_address
  %1303 = load i32* %stack_var_-1356
  %1304 = zext i32 %1303 to i64
  store i64 %1304, i64* %rax
  store volatile i64 36967, i64* @assembly_address
  %1305 = load i64* %rax
  %1306 = trunc i64 %1305 to i32
  %1307 = zext i32 %1306 to i64
  store i64 %1307, i64* %rcx
  store volatile i64 36969, i64* @assembly_address
  %1308 = load i64* %r12
  %1309 = load i64* %rcx
  %1310 = trunc i64 %1309 to i8
  %1311 = zext i8 %1310 to i64
  %1312 = and i64 %1311, 63
  %1313 = load i1* %of
  %1314 = icmp eq i64 %1312, 0
  br i1 %1314, label %1330, label %1315

; <label>:1315                                    ; preds = %block_902d
  %1316 = lshr i64 %1308, %1312
  %1317 = icmp eq i64 %1316, 0
  store i1 %1317, i1* %zf
  %1318 = icmp slt i64 %1316, 0
  store i1 %1318, i1* %sf
  %1319 = trunc i64 %1316 to i8
  %1320 = call i8 @llvm.ctpop.i8(i8 %1319)
  %1321 = and i8 %1320, 1
  %1322 = icmp eq i8 %1321, 0
  store i1 %1322, i1* %pf
  store i64 %1316, i64* %r12
  %1323 = sub i64 %1312, 1
  %1324 = shl i64 1, %1323
  %1325 = and i64 %1324, %1308
  %1326 = icmp ne i64 %1325, 0
  store i1 %1326, i1* %cf
  %1327 = icmp eq i64 %1312, 1
  %1328 = icmp slt i64 %1308, 0
  %1329 = select i1 %1327, i1 %1328, i1 %1313
  store i1 %1329, i1* %of
  br label %1330

; <label>:1330                                    ; preds = %block_902d, %1315
  store volatile i64 36972, i64* @assembly_address
  %1331 = load i64* %rbx
  %1332 = trunc i64 %1331 to i32
  %1333 = load i32* %stack_var_-1356
  %1334 = sub i32 %1332, %1333
  %1335 = and i32 %1332, 15
  %1336 = and i32 %1333, 15
  %1337 = sub i32 %1335, %1336
  %1338 = icmp ugt i32 %1337, 15
  %1339 = icmp ult i32 %1332, %1333
  %1340 = xor i32 %1332, %1333
  %1341 = xor i32 %1332, %1334
  %1342 = and i32 %1340, %1341
  %1343 = icmp slt i32 %1342, 0
  store i1 %1338, i1* %az
  store i1 %1339, i1* %cf
  store i1 %1343, i1* %of
  %1344 = icmp eq i32 %1334, 0
  store i1 %1344, i1* %zf
  %1345 = icmp slt i32 %1334, 0
  store i1 %1345, i1* %sf
  %1346 = trunc i32 %1334 to i8
  %1347 = call i8 @llvm.ctpop.i8(i8 %1346)
  %1348 = and i8 %1347, 1
  %1349 = icmp eq i8 %1348, 0
  store i1 %1349, i1* %pf
  %1350 = zext i32 %1334 to i64
  store i64 %1350, i64* %rbx
  store volatile i64 36978, i64* @assembly_address
  %1351 = load i8** %stack_var_-1312
  %1352 = ptrtoint i8* %1351 to i64
  store i64 %1352, i64* %rax
  store volatile i64 36985, i64* @assembly_address
  %1353 = load i64* %rax
  %1354 = inttoptr i64 %1353 to i8*
  %1355 = load i8* %1354
  %1356 = zext i8 %1355 to i64
  store i64 %1356, i64* %rax
  store volatile i64 36988, i64* @assembly_address
  %1357 = load i64* %rax
  %1358 = trunc i64 %1357 to i8
  %1359 = sub i8 %1358, 99
  %1360 = and i8 %1358, 15
  %1361 = sub i8 %1360, 3
  %1362 = icmp ugt i8 %1361, 15
  %1363 = icmp ult i8 %1358, 99
  %1364 = xor i8 %1358, 99
  %1365 = xor i8 %1358, %1359
  %1366 = and i8 %1364, %1365
  %1367 = icmp slt i8 %1366, 0
  store i1 %1362, i1* %az
  store i1 %1363, i1* %cf
  store i1 %1367, i1* %of
  %1368 = icmp eq i8 %1359, 0
  store i1 %1368, i1* %zf
  %1369 = icmp slt i8 %1359, 0
  store i1 %1369, i1* %sf
  %1370 = call i8 @llvm.ctpop.i8(i8 %1359)
  %1371 = and i8 %1370, 1
  %1372 = icmp eq i8 %1371, 0
  store i1 %1372, i1* %pf
  store volatile i64 36990, i64* @assembly_address
  %1373 = load i1* %zf
  %1374 = icmp eq i1 %1373, false
  br i1 %1374, label %block_9099, label %block_9080

block_9080:                                       ; preds = %1330
  store volatile i64 36992, i64* @assembly_address
  %1375 = load i64* %stack_var_-1320
  store i64 %1375, i64* %rax
  store volatile i64 36999, i64* @assembly_address
  %1376 = load i64* %rax
  store i64 %1376, i64* %rdi
  store volatile i64 37002, i64* @assembly_address
  %1377 = load i64* %rdi
  %1378 = inttoptr i64 %1377 to i64*
  %1379 = call i64 @huft_free(i64* %1378)
  store i64 %1379, i64* %rax
  store i64 %1379, i64* %rax
  store volatile i64 37007, i64* @assembly_address
  store i64 2, i64* %rax
  store volatile i64 37012, i64* @assembly_address
  br label %block_94ff

block_9099:                                       ; preds = %1330
  store volatile i64 37017, i64* @assembly_address
  %1380 = load i8** %stack_var_-1312
  %1381 = ptrtoint i8* %1380 to i64
  store i64 %1381, i64* %rax
  store volatile i64 37024, i64* @assembly_address
  %1382 = load i64* %rax
  %1383 = add i64 %1382, 8
  %1384 = inttoptr i64 %1383 to i16*
  %1385 = load i16* %1384
  %1386 = zext i16 %1385 to i64
  store i64 %1386, i64* %rax
  store volatile i64 37028, i64* @assembly_address
  %1387 = load i64* %rax
  %1388 = trunc i64 %1387 to i16
  %1389 = zext i16 %1388 to i64
  store i64 %1389, i64* %rax
  store volatile i64 37031, i64* @assembly_address
  %1390 = load i64* %rax
  %1391 = trunc i64 %1390 to i32
  store i32 %1391, i32* %stack_var_-1356
  store volatile i64 37037, i64* @assembly_address
  %1392 = load i32* %stack_var_-1356
  %1393 = sub i32 %1392, 15
  %1394 = and i32 %1392, 15
  %1395 = sub i32 %1394, 15
  %1396 = icmp ugt i32 %1395, 15
  %1397 = icmp ult i32 %1392, 15
  %1398 = xor i32 %1392, 15
  %1399 = xor i32 %1392, %1393
  %1400 = and i32 %1398, %1399
  %1401 = icmp slt i32 %1400, 0
  store i1 %1396, i1* %az
  store i1 %1397, i1* %cf
  store i1 %1401, i1* %of
  %1402 = icmp eq i32 %1393, 0
  store i1 %1402, i1* %zf
  %1403 = icmp slt i32 %1393, 0
  store i1 %1403, i1* %sf
  %1404 = trunc i32 %1393 to i8
  %1405 = call i8 @llvm.ctpop.i8(i8 %1404)
  %1406 = and i8 %1405, 1
  %1407 = icmp eq i8 %1406, 0
  store i1 %1407, i1* %pf
  store volatile i64 37044, i64* @assembly_address
  %1408 = load i1* %cf
  %1409 = load i1* %zf
  %1410 = or i1 %1408, %1409
  %1411 = icmp ne i1 %1410, true
  br i1 %1411, label %block_90e5, label %block_90b6

block_90b6:                                       ; preds = %block_9099
  store volatile i64 37046, i64* @assembly_address
  %1412 = load i32* %stack_var_-1356
  %1413 = zext i32 %1412 to i64
  store i64 %1413, i64* %rax
  store volatile i64 37052, i64* @assembly_address
  %1414 = load i64* %rax
  %1415 = trunc i64 %1414 to i32
  store i32 %1415, i32* %stack_var_-1352
  store volatile i64 37058, i64* @assembly_address
  %1416 = load i32* %stack_var_-1360
  %1417 = zext i32 %1416 to i64
  store i64 %1417, i64* %rax
  store volatile i64 37064, i64* @assembly_address
  %1418 = load i64* %rax
  %1419 = add i64 %1418, 1
  %1420 = trunc i64 %1419 to i32
  %1421 = zext i32 %1420 to i64
  store i64 %1421, i64* %rdx
  store volatile i64 37067, i64* @assembly_address
  %1422 = load i64* %rdx
  %1423 = trunc i64 %1422 to i32
  store i32 %1423, i32* %stack_var_-1360
  store volatile i64 37073, i64* @assembly_address
  %1424 = load i64* %rax
  %1425 = trunc i64 %1424 to i32
  %1426 = sext i32 %1425 to i64
  store i64 %1426, i64* %rax
  store volatile i64 37075, i64* @assembly_address
  %1427 = load i32* %stack_var_-1352
  %1428 = zext i32 %1427 to i64
  store i64 %1428, i64* %rdx
  store volatile i64 37081, i64* @assembly_address
  %1429 = load i64* %rdx
  %1430 = trunc i64 %1429 to i32
  %1431 = load i64* %rbp
  %1432 = load i64* %rax
  %1433 = mul i64 %1432, 4
  %1434 = add i64 %1431, -1296
  %1435 = add i64 %1434, %1433
  %1436 = inttoptr i64 %1435 to i32*
  store i32 %1430, i32* %1436
  store volatile i64 37088, i64* @assembly_address
  br label %block_9360

block_90e5:                                       ; preds = %block_9099
  store volatile i64 37093, i64* @assembly_address
  %1437 = load i32* %stack_var_-1356
  %1438 = sub i32 %1437, 16
  %1439 = and i32 %1437, 15
  %1440 = icmp ugt i32 %1439, 15
  %1441 = icmp ult i32 %1437, 16
  %1442 = xor i32 %1437, 16
  %1443 = xor i32 %1437, %1438
  %1444 = and i32 %1442, %1443
  %1445 = icmp slt i32 %1444, 0
  store i1 %1440, i1* %az
  store i1 %1441, i1* %cf
  store i1 %1445, i1* %of
  %1446 = icmp eq i32 %1438, 0
  store i1 %1446, i1* %zf
  %1447 = icmp slt i32 %1438, 0
  store i1 %1447, i1* %sf
  %1448 = trunc i32 %1438 to i8
  %1449 = call i8 @llvm.ctpop.i8(i8 %1448)
  %1450 = and i8 %1449, 1
  %1451 = icmp eq i8 %1450, 0
  store i1 %1451, i1* %pf
  store volatile i64 37100, i64* @assembly_address
  %1452 = load i1* %zf
  %1453 = icmp eq i1 %1452, false
  br i1 %1453, label %block_91ba, label %block_90f2

block_90f2:                                       ; preds = %block_90e5
  store volatile i64 37106, i64* @assembly_address
  br label %block_9149

block_90f4:                                       ; preds = %block_9149
  store volatile i64 37108, i64* @assembly_address
  %1454 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1455 = zext i32 %1454 to i64
  store i64 %1455, i64* %rdx
  store volatile i64 37114, i64* @assembly_address
  %1456 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1457 = zext i32 %1456 to i64
  store i64 %1457, i64* %rax
  store volatile i64 37120, i64* @assembly_address
  %1458 = load i64* %rdx
  %1459 = trunc i64 %1458 to i32
  %1460 = load i64* %rax
  %1461 = trunc i64 %1460 to i32
  %1462 = sub i32 %1459, %1461
  %1463 = and i32 %1459, 15
  %1464 = and i32 %1461, 15
  %1465 = sub i32 %1463, %1464
  %1466 = icmp ugt i32 %1465, 15
  %1467 = icmp ult i32 %1459, %1461
  %1468 = xor i32 %1459, %1461
  %1469 = xor i32 %1459, %1462
  %1470 = and i32 %1468, %1469
  %1471 = icmp slt i32 %1470, 0
  store i1 %1466, i1* %az
  store i1 %1467, i1* %cf
  store i1 %1471, i1* %of
  %1472 = icmp eq i32 %1462, 0
  store i1 %1472, i1* %zf
  %1473 = icmp slt i32 %1462, 0
  store i1 %1473, i1* %sf
  %1474 = trunc i32 %1462 to i8
  %1475 = call i8 @llvm.ctpop.i8(i8 %1474)
  %1476 = and i8 %1475, 1
  %1477 = icmp eq i8 %1476, 0
  store i1 %1477, i1* %pf
  store volatile i64 37122, i64* @assembly_address
  %1478 = load i1* %cf
  %1479 = icmp eq i1 %1478, false
  br i1 %1479, label %block_9125, label %block_9104

block_9104:                                       ; preds = %block_90f4
  store volatile i64 37124, i64* @assembly_address
  %1480 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1481 = zext i32 %1480 to i64
  store i64 %1481, i64* %rax
  store volatile i64 37130, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 37133, i64* @assembly_address
  %1482 = load i64* %rdx
  %1483 = trunc i64 %1482 to i32
  store i32 %1483, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 37139, i64* @assembly_address
  %1484 = load i64* %rax
  %1485 = trunc i64 %1484 to i32
  %1486 = zext i32 %1485 to i64
  store i64 %1486, i64* %rdx
  store volatile i64 37141, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 37148, i64* @assembly_address
  %1487 = load i64* %rdx
  %1488 = load i64* %rax
  %1489 = mul i64 %1488, 1
  %1490 = add i64 %1487, %1489
  %1491 = inttoptr i64 %1490 to i8*
  %1492 = load i8* %1491
  %1493 = zext i8 %1492 to i64
  store i64 %1493, i64* %rax
  store volatile i64 37152, i64* @assembly_address
  %1494 = load i64* %rax
  %1495 = trunc i64 %1494 to i8
  %1496 = zext i8 %1495 to i64
  store i64 %1496, i64* %rax
  store volatile i64 37155, i64* @assembly_address
  br label %block_913e

block_9125:                                       ; preds = %block_90f4
  store volatile i64 37157, i64* @assembly_address
  %1497 = load i64** %stack_var_-1348
  %1498 = ptrtoint i64* %1497 to i32
  %1499 = zext i32 %1498 to i64
  store i64 %1499, i64* %rax
  store volatile i64 37163, i64* @assembly_address
  %1500 = load i64* %rax
  %1501 = trunc i64 %1500 to i32
  store i32 %1501, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 37169, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 37174, i64* @assembly_address
  %1502 = load i64* %rdi
  %1503 = trunc i64 %1502 to i32
  %1504 = call i64 @fill_inbuf(i32 %1503)
  store i64 %1504, i64* %rax
  store i64 %1504, i64* %rax
  store volatile i64 37179, i64* @assembly_address
  %1505 = load i64* %rax
  %1506 = trunc i64 %1505 to i8
  %1507 = zext i8 %1506 to i64
  store i64 %1507, i64* %rax
  br label %block_913e

block_913e:                                       ; preds = %block_9125, %block_9104
  store volatile i64 37182, i64* @assembly_address
  %1508 = load i64* %rbx
  %1509 = trunc i64 %1508 to i32
  %1510 = zext i32 %1509 to i64
  store i64 %1510, i64* %rcx
  store volatile i64 37184, i64* @assembly_address
  %1511 = load i64* %rax
  %1512 = load i64* %rcx
  %1513 = trunc i64 %1512 to i8
  %1514 = zext i8 %1513 to i64
  %1515 = and i64 %1514, 63
  %1516 = load i1* %of
  %1517 = icmp eq i64 %1515, 0
  br i1 %1517, label %1534, label %1518

; <label>:1518                                    ; preds = %block_913e
  %1519 = shl i64 %1511, %1515
  %1520 = icmp eq i64 %1519, 0
  store i1 %1520, i1* %zf
  %1521 = icmp slt i64 %1519, 0
  store i1 %1521, i1* %sf
  %1522 = trunc i64 %1519 to i8
  %1523 = call i8 @llvm.ctpop.i8(i8 %1522)
  %1524 = and i8 %1523, 1
  %1525 = icmp eq i8 %1524, 0
  store i1 %1525, i1* %pf
  store i64 %1519, i64* %rax
  %1526 = sub i64 %1515, 1
  %1527 = shl i64 %1511, %1526
  %1528 = lshr i64 %1527, 63
  %1529 = trunc i64 %1528 to i1
  store i1 %1529, i1* %cf
  %1530 = lshr i64 %1519, 63
  %1531 = icmp ne i64 %1530, %1528
  %1532 = icmp eq i64 %1515, 1
  %1533 = select i1 %1532, i1 %1531, i1 %1516
  store i1 %1533, i1* %of
  br label %1534

; <label>:1534                                    ; preds = %block_913e, %1518
  store volatile i64 37187, i64* @assembly_address
  %1535 = load i64* %r12
  %1536 = load i64* %rax
  %1537 = or i64 %1535, %1536
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1538 = icmp eq i64 %1537, 0
  store i1 %1538, i1* %zf
  %1539 = icmp slt i64 %1537, 0
  store i1 %1539, i1* %sf
  %1540 = trunc i64 %1537 to i8
  %1541 = call i8 @llvm.ctpop.i8(i8 %1540)
  %1542 = and i8 %1541, 1
  %1543 = icmp eq i8 %1542, 0
  store i1 %1543, i1* %pf
  store i64 %1537, i64* %r12
  store volatile i64 37190, i64* @assembly_address
  %1544 = load i64* %rbx
  %1545 = trunc i64 %1544 to i32
  %1546 = add i32 %1545, 8
  %1547 = and i32 %1545, 15
  %1548 = add i32 %1547, 8
  %1549 = icmp ugt i32 %1548, 15
  %1550 = icmp ult i32 %1546, %1545
  %1551 = xor i32 %1545, %1546
  %1552 = xor i32 8, %1546
  %1553 = and i32 %1551, %1552
  %1554 = icmp slt i32 %1553, 0
  store i1 %1549, i1* %az
  store i1 %1550, i1* %cf
  store i1 %1554, i1* %of
  %1555 = icmp eq i32 %1546, 0
  store i1 %1555, i1* %zf
  %1556 = icmp slt i32 %1546, 0
  store i1 %1556, i1* %sf
  %1557 = trunc i32 %1546 to i8
  %1558 = call i8 @llvm.ctpop.i8(i8 %1557)
  %1559 = and i8 %1558, 1
  %1560 = icmp eq i8 %1559, 0
  store i1 %1560, i1* %pf
  %1561 = zext i32 %1546 to i64
  store i64 %1561, i64* %rbx
  br label %block_9149

block_9149:                                       ; preds = %1534, %block_90f2
  store volatile i64 37193, i64* @assembly_address
  %1562 = load i64* %rbx
  %1563 = trunc i64 %1562 to i32
  %1564 = sub i32 %1563, 1
  %1565 = and i32 %1563, 15
  %1566 = sub i32 %1565, 1
  %1567 = icmp ugt i32 %1566, 15
  %1568 = icmp ult i32 %1563, 1
  %1569 = xor i32 %1563, 1
  %1570 = xor i32 %1563, %1564
  %1571 = and i32 %1569, %1570
  %1572 = icmp slt i32 %1571, 0
  store i1 %1567, i1* %az
  store i1 %1568, i1* %cf
  store i1 %1572, i1* %of
  %1573 = icmp eq i32 %1564, 0
  store i1 %1573, i1* %zf
  %1574 = icmp slt i32 %1564, 0
  store i1 %1574, i1* %sf
  %1575 = trunc i32 %1564 to i8
  %1576 = call i8 @llvm.ctpop.i8(i8 %1575)
  %1577 = and i8 %1576, 1
  %1578 = icmp eq i8 %1577, 0
  store i1 %1578, i1* %pf
  store volatile i64 37196, i64* @assembly_address
  %1579 = load i1* %cf
  %1580 = load i1* %zf
  %1581 = or i1 %1579, %1580
  br i1 %1581, label %block_90f4, label %block_914e

block_914e:                                       ; preds = %block_9149
  store volatile i64 37198, i64* @assembly_address
  %1582 = load i64* %r12
  %1583 = trunc i64 %1582 to i32
  %1584 = zext i32 %1583 to i64
  store i64 %1584, i64* %rax
  store volatile i64 37201, i64* @assembly_address
  %1585 = load i64* %rax
  %1586 = trunc i64 %1585 to i32
  %1587 = and i32 %1586, 3
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1588 = icmp eq i32 %1587, 0
  store i1 %1588, i1* %zf
  %1589 = icmp slt i32 %1587, 0
  store i1 %1589, i1* %sf
  %1590 = trunc i32 %1587 to i8
  %1591 = call i8 @llvm.ctpop.i8(i8 %1590)
  %1592 = and i8 %1591, 1
  %1593 = icmp eq i8 %1592, 0
  store i1 %1593, i1* %pf
  %1594 = zext i32 %1587 to i64
  store i64 %1594, i64* %rax
  store volatile i64 37204, i64* @assembly_address
  %1595 = load i64* %rax
  %1596 = trunc i64 %1595 to i32
  %1597 = add i32 %1596, 3
  %1598 = and i32 %1596, 15
  %1599 = add i32 %1598, 3
  %1600 = icmp ugt i32 %1599, 15
  %1601 = icmp ult i32 %1597, %1596
  %1602 = xor i32 %1596, %1597
  %1603 = xor i32 3, %1597
  %1604 = and i32 %1602, %1603
  %1605 = icmp slt i32 %1604, 0
  store i1 %1600, i1* %az
  store i1 %1601, i1* %cf
  store i1 %1605, i1* %of
  %1606 = icmp eq i32 %1597, 0
  store i1 %1606, i1* %zf
  %1607 = icmp slt i32 %1597, 0
  store i1 %1607, i1* %sf
  %1608 = trunc i32 %1597 to i8
  %1609 = call i8 @llvm.ctpop.i8(i8 %1608)
  %1610 = and i8 %1609, 1
  %1611 = icmp eq i8 %1610, 0
  store i1 %1611, i1* %pf
  %1612 = zext i32 %1597 to i64
  store i64 %1612, i64* %rax
  store volatile i64 37207, i64* @assembly_address
  %1613 = load i64* %rax
  %1614 = trunc i64 %1613 to i32
  store i32 %1614, i32* %stack_var_-1356
  store volatile i64 37213, i64* @assembly_address
  %1615 = load i64* %r12
  %1616 = load i1* %of
  %1617 = lshr i64 %1615, 2
  %1618 = icmp eq i64 %1617, 0
  store i1 %1618, i1* %zf
  %1619 = icmp slt i64 %1617, 0
  store i1 %1619, i1* %sf
  %1620 = trunc i64 %1617 to i8
  %1621 = call i8 @llvm.ctpop.i8(i8 %1620)
  %1622 = and i8 %1621, 1
  %1623 = icmp eq i8 %1622, 0
  store i1 %1623, i1* %pf
  store i64 %1617, i64* %r12
  %1624 = and i64 2, %1615
  %1625 = icmp ne i64 %1624, 0
  store i1 %1625, i1* %cf
  %1626 = icmp slt i64 %1615, 0
  %1627 = select i1 false, i1 %1626, i1 %1616
  store i1 %1627, i1* %of
  store volatile i64 37217, i64* @assembly_address
  %1628 = load i64* %rbx
  %1629 = trunc i64 %1628 to i32
  %1630 = sub i32 %1629, 2
  %1631 = and i32 %1629, 15
  %1632 = sub i32 %1631, 2
  %1633 = icmp ugt i32 %1632, 15
  %1634 = icmp ult i32 %1629, 2
  %1635 = xor i32 %1629, 2
  %1636 = xor i32 %1629, %1630
  %1637 = and i32 %1635, %1636
  %1638 = icmp slt i32 %1637, 0
  store i1 %1633, i1* %az
  store i1 %1634, i1* %cf
  store i1 %1638, i1* %of
  %1639 = icmp eq i32 %1630, 0
  store i1 %1639, i1* %zf
  %1640 = icmp slt i32 %1630, 0
  store i1 %1640, i1* %sf
  %1641 = trunc i32 %1630 to i8
  %1642 = call i8 @llvm.ctpop.i8(i8 %1641)
  %1643 = and i8 %1642, 1
  %1644 = icmp eq i8 %1643, 0
  store i1 %1644, i1* %pf
  %1645 = zext i32 %1630 to i64
  store i64 %1645, i64* %rbx
  store volatile i64 37220, i64* @assembly_address
  %1646 = load i32* %stack_var_-1360
  %1647 = zext i32 %1646 to i64
  store i64 %1647, i64* %rdx
  store volatile i64 37226, i64* @assembly_address
  %1648 = load i32* %stack_var_-1356
  %1649 = zext i32 %1648 to i64
  store i64 %1649, i64* %rax
  store volatile i64 37232, i64* @assembly_address
  %1650 = load i64* %rax
  %1651 = trunc i64 %1650 to i32
  %1652 = load i64* %rdx
  %1653 = trunc i64 %1652 to i32
  %1654 = add i32 %1651, %1653
  %1655 = and i32 %1651, 15
  %1656 = and i32 %1653, 15
  %1657 = add i32 %1655, %1656
  %1658 = icmp ugt i32 %1657, 15
  %1659 = icmp ult i32 %1654, %1651
  %1660 = xor i32 %1651, %1654
  %1661 = xor i32 %1653, %1654
  %1662 = and i32 %1660, %1661
  %1663 = icmp slt i32 %1662, 0
  store i1 %1658, i1* %az
  store i1 %1659, i1* %cf
  store i1 %1663, i1* %of
  %1664 = icmp eq i32 %1654, 0
  store i1 %1664, i1* %zf
  %1665 = icmp slt i32 %1654, 0
  store i1 %1665, i1* %sf
  %1666 = trunc i32 %1654 to i8
  %1667 = call i8 @llvm.ctpop.i8(i8 %1666)
  %1668 = and i8 %1667, 1
  %1669 = icmp eq i8 %1668, 0
  store i1 %1669, i1* %pf
  %1670 = zext i32 %1654 to i64
  store i64 %1670, i64* %rax
  store volatile i64 37234, i64* @assembly_address
  %1671 = load i32* %stack_var_-1332
  %1672 = load i64* %rax
  %1673 = trunc i64 %1672 to i32
  %1674 = sub i32 %1671, %1673
  %1675 = and i32 %1671, 15
  %1676 = and i32 %1673, 15
  %1677 = sub i32 %1675, %1676
  %1678 = icmp ugt i32 %1677, 15
  %1679 = icmp ult i32 %1671, %1673
  %1680 = xor i32 %1671, %1673
  %1681 = xor i32 %1671, %1674
  %1682 = and i32 %1680, %1681
  %1683 = icmp slt i32 %1682, 0
  store i1 %1678, i1* %az
  store i1 %1679, i1* %cf
  store i1 %1683, i1* %of
  %1684 = icmp eq i32 %1674, 0
  store i1 %1684, i1* %zf
  %1685 = icmp slt i32 %1674, 0
  store i1 %1685, i1* %sf
  %1686 = trunc i32 %1674 to i8
  %1687 = call i8 @llvm.ctpop.i8(i8 %1686)
  %1688 = and i8 %1687, 1
  %1689 = icmp eq i8 %1688, 0
  store i1 %1689, i1* %pf
  store volatile i64 37240, i64* @assembly_address
  %1690 = load i1* %cf
  %1691 = icmp eq i1 %1690, false
  br i1 %1691, label %block_91a2, label %block_917a

block_917a:                                       ; preds = %block_914e
  store volatile i64 37242, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 37247, i64* @assembly_address
  br label %block_94ff

block_9184:                                       ; preds = %block_91a2
  store volatile i64 37252, i64* @assembly_address
  %1692 = load i32* %stack_var_-1360
  %1693 = zext i32 %1692 to i64
  store i64 %1693, i64* %rax
  store volatile i64 37258, i64* @assembly_address
  %1694 = load i64* %rax
  %1695 = add i64 %1694, 1
  %1696 = trunc i64 %1695 to i32
  %1697 = zext i32 %1696 to i64
  store i64 %1697, i64* %rdx
  store volatile i64 37261, i64* @assembly_address
  %1698 = load i64* %rdx
  %1699 = trunc i64 %1698 to i32
  store i32 %1699, i32* %stack_var_-1360
  store volatile i64 37267, i64* @assembly_address
  %1700 = load i64* %rax
  %1701 = trunc i64 %1700 to i32
  %1702 = sext i32 %1701 to i64
  store i64 %1702, i64* %rax
  store volatile i64 37269, i64* @assembly_address
  %1703 = load i32* %stack_var_-1352
  %1704 = zext i32 %1703 to i64
  store i64 %1704, i64* %rdx
  store volatile i64 37275, i64* @assembly_address
  %1705 = load i64* %rdx
  %1706 = trunc i64 %1705 to i32
  %1707 = load i64* %rbp
  %1708 = load i64* %rax
  %1709 = mul i64 %1708, 4
  %1710 = add i64 %1707, -1296
  %1711 = add i64 %1710, %1709
  %1712 = inttoptr i64 %1711 to i32*
  store i32 %1706, i32* %1712
  br label %block_91a2

block_91a2:                                       ; preds = %block_9184, %block_914e
  store volatile i64 37282, i64* @assembly_address
  %1713 = load i32* %stack_var_-1356
  %1714 = zext i32 %1713 to i64
  store i64 %1714, i64* %rax
  store volatile i64 37288, i64* @assembly_address
  %1715 = load i64* %rax
  %1716 = add i64 %1715, -1
  %1717 = trunc i64 %1716 to i32
  %1718 = zext i32 %1717 to i64
  store i64 %1718, i64* %rdx
  store volatile i64 37291, i64* @assembly_address
  %1719 = load i64* %rdx
  %1720 = trunc i64 %1719 to i32
  store i32 %1720, i32* %stack_var_-1356
  store volatile i64 37297, i64* @assembly_address
  %1721 = load i64* %rax
  %1722 = trunc i64 %1721 to i32
  %1723 = load i64* %rax
  %1724 = trunc i64 %1723 to i32
  %1725 = and i32 %1722, %1724
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1726 = icmp eq i32 %1725, 0
  store i1 %1726, i1* %zf
  %1727 = icmp slt i32 %1725, 0
  store i1 %1727, i1* %sf
  %1728 = trunc i32 %1725 to i8
  %1729 = call i8 @llvm.ctpop.i8(i8 %1728)
  %1730 = and i8 %1729, 1
  %1731 = icmp eq i8 %1730, 0
  store i1 %1731, i1* %pf
  store volatile i64 37299, i64* @assembly_address
  %1732 = load i1* %zf
  %1733 = icmp eq i1 %1732, false
  br i1 %1733, label %block_9184, label %block_91b5

block_91b5:                                       ; preds = %block_91a2
  store volatile i64 37301, i64* @assembly_address
  br label %block_9360

block_91ba:                                       ; preds = %block_90e5
  store volatile i64 37306, i64* @assembly_address
  %1734 = load i32* %stack_var_-1356
  %1735 = sub i32 %1734, 17
  %1736 = and i32 %1734, 15
  %1737 = sub i32 %1736, 1
  %1738 = icmp ugt i32 %1737, 15
  %1739 = icmp ult i32 %1734, 17
  %1740 = xor i32 %1734, 17
  %1741 = xor i32 %1734, %1735
  %1742 = and i32 %1740, %1741
  %1743 = icmp slt i32 %1742, 0
  store i1 %1738, i1* %az
  store i1 %1739, i1* %cf
  store i1 %1743, i1* %of
  %1744 = icmp eq i32 %1735, 0
  store i1 %1744, i1* %zf
  %1745 = icmp slt i32 %1735, 0
  store i1 %1745, i1* %sf
  %1746 = trunc i32 %1735 to i8
  %1747 = call i8 @llvm.ctpop.i8(i8 %1746)
  %1748 = and i8 %1747, 1
  %1749 = icmp eq i8 %1748, 0
  store i1 %1749, i1* %pf
  store volatile i64 37313, i64* @assembly_address
  %1750 = load i1* %zf
  %1751 = icmp eq i1 %1750, false
  br i1 %1751, label %block_92ec, label %block_91c7

block_91c7:                                       ; preds = %block_91ba
  store volatile i64 37319, i64* @assembly_address
  br label %block_921e

block_91c9:                                       ; preds = %block_921e
  store volatile i64 37321, i64* @assembly_address
  %1752 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1753 = zext i32 %1752 to i64
  store i64 %1753, i64* %rdx
  store volatile i64 37327, i64* @assembly_address
  %1754 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1755 = zext i32 %1754 to i64
  store i64 %1755, i64* %rax
  store volatile i64 37333, i64* @assembly_address
  %1756 = load i64* %rdx
  %1757 = trunc i64 %1756 to i32
  %1758 = load i64* %rax
  %1759 = trunc i64 %1758 to i32
  %1760 = sub i32 %1757, %1759
  %1761 = and i32 %1757, 15
  %1762 = and i32 %1759, 15
  %1763 = sub i32 %1761, %1762
  %1764 = icmp ugt i32 %1763, 15
  %1765 = icmp ult i32 %1757, %1759
  %1766 = xor i32 %1757, %1759
  %1767 = xor i32 %1757, %1760
  %1768 = and i32 %1766, %1767
  %1769 = icmp slt i32 %1768, 0
  store i1 %1764, i1* %az
  store i1 %1765, i1* %cf
  store i1 %1769, i1* %of
  %1770 = icmp eq i32 %1760, 0
  store i1 %1770, i1* %zf
  %1771 = icmp slt i32 %1760, 0
  store i1 %1771, i1* %sf
  %1772 = trunc i32 %1760 to i8
  %1773 = call i8 @llvm.ctpop.i8(i8 %1772)
  %1774 = and i8 %1773, 1
  %1775 = icmp eq i8 %1774, 0
  store i1 %1775, i1* %pf
  store volatile i64 37335, i64* @assembly_address
  %1776 = load i1* %cf
  %1777 = icmp eq i1 %1776, false
  br i1 %1777, label %block_91fa, label %block_91d9

block_91d9:                                       ; preds = %block_91c9
  store volatile i64 37337, i64* @assembly_address
  %1778 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1779 = zext i32 %1778 to i64
  store i64 %1779, i64* %rax
  store volatile i64 37343, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 37346, i64* @assembly_address
  %1780 = load i64* %rdx
  %1781 = trunc i64 %1780 to i32
  store i32 %1781, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 37352, i64* @assembly_address
  %1782 = load i64* %rax
  %1783 = trunc i64 %1782 to i32
  %1784 = zext i32 %1783 to i64
  store i64 %1784, i64* %rdx
  store volatile i64 37354, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 37361, i64* @assembly_address
  %1785 = load i64* %rdx
  %1786 = load i64* %rax
  %1787 = mul i64 %1786, 1
  %1788 = add i64 %1785, %1787
  %1789 = inttoptr i64 %1788 to i8*
  %1790 = load i8* %1789
  %1791 = zext i8 %1790 to i64
  store i64 %1791, i64* %rax
  store volatile i64 37365, i64* @assembly_address
  %1792 = load i64* %rax
  %1793 = trunc i64 %1792 to i8
  %1794 = zext i8 %1793 to i64
  store i64 %1794, i64* %rax
  store volatile i64 37368, i64* @assembly_address
  br label %block_9213

block_91fa:                                       ; preds = %block_91c9
  store volatile i64 37370, i64* @assembly_address
  %1795 = load i64** %stack_var_-1348
  %1796 = ptrtoint i64* %1795 to i32
  %1797 = zext i32 %1796 to i64
  store i64 %1797, i64* %rax
  store volatile i64 37376, i64* @assembly_address
  %1798 = load i64* %rax
  %1799 = trunc i64 %1798 to i32
  store i32 %1799, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 37382, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 37387, i64* @assembly_address
  %1800 = load i64* %rdi
  %1801 = trunc i64 %1800 to i32
  %1802 = call i64 @fill_inbuf(i32 %1801)
  store i64 %1802, i64* %rax
  store i64 %1802, i64* %rax
  store volatile i64 37392, i64* @assembly_address
  %1803 = load i64* %rax
  %1804 = trunc i64 %1803 to i8
  %1805 = zext i8 %1804 to i64
  store i64 %1805, i64* %rax
  br label %block_9213

block_9213:                                       ; preds = %block_91fa, %block_91d9
  store volatile i64 37395, i64* @assembly_address
  %1806 = load i64* %rbx
  %1807 = trunc i64 %1806 to i32
  %1808 = zext i32 %1807 to i64
  store i64 %1808, i64* %rcx
  store volatile i64 37397, i64* @assembly_address
  %1809 = load i64* %rax
  %1810 = load i64* %rcx
  %1811 = trunc i64 %1810 to i8
  %1812 = zext i8 %1811 to i64
  %1813 = and i64 %1812, 63
  %1814 = load i1* %of
  %1815 = icmp eq i64 %1813, 0
  br i1 %1815, label %1832, label %1816

; <label>:1816                                    ; preds = %block_9213
  %1817 = shl i64 %1809, %1813
  %1818 = icmp eq i64 %1817, 0
  store i1 %1818, i1* %zf
  %1819 = icmp slt i64 %1817, 0
  store i1 %1819, i1* %sf
  %1820 = trunc i64 %1817 to i8
  %1821 = call i8 @llvm.ctpop.i8(i8 %1820)
  %1822 = and i8 %1821, 1
  %1823 = icmp eq i8 %1822, 0
  store i1 %1823, i1* %pf
  store i64 %1817, i64* %rax
  %1824 = sub i64 %1813, 1
  %1825 = shl i64 %1809, %1824
  %1826 = lshr i64 %1825, 63
  %1827 = trunc i64 %1826 to i1
  store i1 %1827, i1* %cf
  %1828 = lshr i64 %1817, 63
  %1829 = icmp ne i64 %1828, %1826
  %1830 = icmp eq i64 %1813, 1
  %1831 = select i1 %1830, i1 %1829, i1 %1814
  store i1 %1831, i1* %of
  br label %1832

; <label>:1832                                    ; preds = %block_9213, %1816
  store volatile i64 37400, i64* @assembly_address
  %1833 = load i64* %r12
  %1834 = load i64* %rax
  %1835 = or i64 %1833, %1834
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1836 = icmp eq i64 %1835, 0
  store i1 %1836, i1* %zf
  %1837 = icmp slt i64 %1835, 0
  store i1 %1837, i1* %sf
  %1838 = trunc i64 %1835 to i8
  %1839 = call i8 @llvm.ctpop.i8(i8 %1838)
  %1840 = and i8 %1839, 1
  %1841 = icmp eq i8 %1840, 0
  store i1 %1841, i1* %pf
  store i64 %1835, i64* %r12
  store volatile i64 37403, i64* @assembly_address
  %1842 = load i64* %rbx
  %1843 = trunc i64 %1842 to i32
  %1844 = add i32 %1843, 8
  %1845 = and i32 %1843, 15
  %1846 = add i32 %1845, 8
  %1847 = icmp ugt i32 %1846, 15
  %1848 = icmp ult i32 %1844, %1843
  %1849 = xor i32 %1843, %1844
  %1850 = xor i32 8, %1844
  %1851 = and i32 %1849, %1850
  %1852 = icmp slt i32 %1851, 0
  store i1 %1847, i1* %az
  store i1 %1848, i1* %cf
  store i1 %1852, i1* %of
  %1853 = icmp eq i32 %1844, 0
  store i1 %1853, i1* %zf
  %1854 = icmp slt i32 %1844, 0
  store i1 %1854, i1* %sf
  %1855 = trunc i32 %1844 to i8
  %1856 = call i8 @llvm.ctpop.i8(i8 %1855)
  %1857 = and i8 %1856, 1
  %1858 = icmp eq i8 %1857, 0
  store i1 %1858, i1* %pf
  %1859 = zext i32 %1844 to i64
  store i64 %1859, i64* %rbx
  br label %block_921e

block_921e:                                       ; preds = %1832, %block_91c7
  store volatile i64 37406, i64* @assembly_address
  %1860 = load i64* %rbx
  %1861 = trunc i64 %1860 to i32
  %1862 = sub i32 %1861, 2
  %1863 = and i32 %1861, 15
  %1864 = sub i32 %1863, 2
  %1865 = icmp ugt i32 %1864, 15
  %1866 = icmp ult i32 %1861, 2
  %1867 = xor i32 %1861, 2
  %1868 = xor i32 %1861, %1862
  %1869 = and i32 %1867, %1868
  %1870 = icmp slt i32 %1869, 0
  store i1 %1865, i1* %az
  store i1 %1866, i1* %cf
  store i1 %1870, i1* %of
  %1871 = icmp eq i32 %1862, 0
  store i1 %1871, i1* %zf
  %1872 = icmp slt i32 %1862, 0
  store i1 %1872, i1* %sf
  %1873 = trunc i32 %1862 to i8
  %1874 = call i8 @llvm.ctpop.i8(i8 %1873)
  %1875 = and i8 %1874, 1
  %1876 = icmp eq i8 %1875, 0
  store i1 %1876, i1* %pf
  store volatile i64 37409, i64* @assembly_address
  %1877 = load i1* %cf
  %1878 = load i1* %zf
  %1879 = or i1 %1877, %1878
  br i1 %1879, label %block_91c9, label %block_9223

block_9223:                                       ; preds = %block_921e
  store volatile i64 37411, i64* @assembly_address
  %1880 = load i64* %r12
  %1881 = trunc i64 %1880 to i32
  %1882 = zext i32 %1881 to i64
  store i64 %1882, i64* %rax
  store volatile i64 37414, i64* @assembly_address
  %1883 = load i64* %rax
  %1884 = trunc i64 %1883 to i32
  %1885 = and i32 %1884, 7
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1886 = icmp eq i32 %1885, 0
  store i1 %1886, i1* %zf
  %1887 = icmp slt i32 %1885, 0
  store i1 %1887, i1* %sf
  %1888 = trunc i32 %1885 to i8
  %1889 = call i8 @llvm.ctpop.i8(i8 %1888)
  %1890 = and i8 %1889, 1
  %1891 = icmp eq i8 %1890, 0
  store i1 %1891, i1* %pf
  %1892 = zext i32 %1885 to i64
  store i64 %1892, i64* %rax
  store volatile i64 37417, i64* @assembly_address
  %1893 = load i64* %rax
  %1894 = trunc i64 %1893 to i32
  %1895 = add i32 %1894, 3
  %1896 = and i32 %1894, 15
  %1897 = add i32 %1896, 3
  %1898 = icmp ugt i32 %1897, 15
  %1899 = icmp ult i32 %1895, %1894
  %1900 = xor i32 %1894, %1895
  %1901 = xor i32 3, %1895
  %1902 = and i32 %1900, %1901
  %1903 = icmp slt i32 %1902, 0
  store i1 %1898, i1* %az
  store i1 %1899, i1* %cf
  store i1 %1903, i1* %of
  %1904 = icmp eq i32 %1895, 0
  store i1 %1904, i1* %zf
  %1905 = icmp slt i32 %1895, 0
  store i1 %1905, i1* %sf
  %1906 = trunc i32 %1895 to i8
  %1907 = call i8 @llvm.ctpop.i8(i8 %1906)
  %1908 = and i8 %1907, 1
  %1909 = icmp eq i8 %1908, 0
  store i1 %1909, i1* %pf
  %1910 = zext i32 %1895 to i64
  store i64 %1910, i64* %rax
  store volatile i64 37420, i64* @assembly_address
  %1911 = load i64* %rax
  %1912 = trunc i64 %1911 to i32
  store i32 %1912, i32* %stack_var_-1356
  store volatile i64 37426, i64* @assembly_address
  %1913 = load i64* %r12
  %1914 = load i1* %of
  %1915 = lshr i64 %1913, 3
  %1916 = icmp eq i64 %1915, 0
  store i1 %1916, i1* %zf
  %1917 = icmp slt i64 %1915, 0
  store i1 %1917, i1* %sf
  %1918 = trunc i64 %1915 to i8
  %1919 = call i8 @llvm.ctpop.i8(i8 %1918)
  %1920 = and i8 %1919, 1
  %1921 = icmp eq i8 %1920, 0
  store i1 %1921, i1* %pf
  store i64 %1915, i64* %r12
  %1922 = and i64 4, %1913
  %1923 = icmp ne i64 %1922, 0
  store i1 %1923, i1* %cf
  %1924 = icmp slt i64 %1913, 0
  %1925 = select i1 false, i1 %1924, i1 %1914
  store i1 %1925, i1* %of
  store volatile i64 37430, i64* @assembly_address
  %1926 = load i64* %rbx
  %1927 = trunc i64 %1926 to i32
  %1928 = sub i32 %1927, 3
  %1929 = and i32 %1927, 15
  %1930 = sub i32 %1929, 3
  %1931 = icmp ugt i32 %1930, 15
  %1932 = icmp ult i32 %1927, 3
  %1933 = xor i32 %1927, 3
  %1934 = xor i32 %1927, %1928
  %1935 = and i32 %1933, %1934
  %1936 = icmp slt i32 %1935, 0
  store i1 %1931, i1* %az
  store i1 %1932, i1* %cf
  store i1 %1936, i1* %of
  %1937 = icmp eq i32 %1928, 0
  store i1 %1937, i1* %zf
  %1938 = icmp slt i32 %1928, 0
  store i1 %1938, i1* %sf
  %1939 = trunc i32 %1928 to i8
  %1940 = call i8 @llvm.ctpop.i8(i8 %1939)
  %1941 = and i8 %1940, 1
  %1942 = icmp eq i8 %1941, 0
  store i1 %1942, i1* %pf
  %1943 = zext i32 %1928 to i64
  store i64 %1943, i64* %rbx
  store volatile i64 37433, i64* @assembly_address
  %1944 = load i32* %stack_var_-1360
  %1945 = zext i32 %1944 to i64
  store i64 %1945, i64* %rdx
  store volatile i64 37439, i64* @assembly_address
  %1946 = load i32* %stack_var_-1356
  %1947 = zext i32 %1946 to i64
  store i64 %1947, i64* %rax
  store volatile i64 37445, i64* @assembly_address
  %1948 = load i64* %rax
  %1949 = trunc i64 %1948 to i32
  %1950 = load i64* %rdx
  %1951 = trunc i64 %1950 to i32
  %1952 = add i32 %1949, %1951
  %1953 = and i32 %1949, 15
  %1954 = and i32 %1951, 15
  %1955 = add i32 %1953, %1954
  %1956 = icmp ugt i32 %1955, 15
  %1957 = icmp ult i32 %1952, %1949
  %1958 = xor i32 %1949, %1952
  %1959 = xor i32 %1951, %1952
  %1960 = and i32 %1958, %1959
  %1961 = icmp slt i32 %1960, 0
  store i1 %1956, i1* %az
  store i1 %1957, i1* %cf
  store i1 %1961, i1* %of
  %1962 = icmp eq i32 %1952, 0
  store i1 %1962, i1* %zf
  %1963 = icmp slt i32 %1952, 0
  store i1 %1963, i1* %sf
  %1964 = trunc i32 %1952 to i8
  %1965 = call i8 @llvm.ctpop.i8(i8 %1964)
  %1966 = and i8 %1965, 1
  %1967 = icmp eq i8 %1966, 0
  store i1 %1967, i1* %pf
  %1968 = zext i32 %1952 to i64
  store i64 %1968, i64* %rax
  store volatile i64 37447, i64* @assembly_address
  %1969 = load i32* %stack_var_-1332
  %1970 = load i64* %rax
  %1971 = trunc i64 %1970 to i32
  %1972 = sub i32 %1969, %1971
  %1973 = and i32 %1969, 15
  %1974 = and i32 %1971, 15
  %1975 = sub i32 %1973, %1974
  %1976 = icmp ugt i32 %1975, 15
  %1977 = icmp ult i32 %1969, %1971
  %1978 = xor i32 %1969, %1971
  %1979 = xor i32 %1969, %1972
  %1980 = and i32 %1978, %1979
  %1981 = icmp slt i32 %1980, 0
  store i1 %1976, i1* %az
  store i1 %1977, i1* %cf
  store i1 %1981, i1* %of
  %1982 = icmp eq i32 %1972, 0
  store i1 %1982, i1* %zf
  %1983 = icmp slt i32 %1972, 0
  store i1 %1983, i1* %sf
  %1984 = trunc i32 %1972 to i8
  %1985 = call i8 @llvm.ctpop.i8(i8 %1984)
  %1986 = and i8 %1985, 1
  %1987 = icmp eq i8 %1986, 0
  store i1 %1987, i1* %pf
  store volatile i64 37453, i64* @assembly_address
  %1988 = load i1* %cf
  %1989 = icmp eq i1 %1988, false
  br i1 %1989, label %block_9275, label %block_924f

block_924f:                                       ; preds = %block_9223
  store volatile i64 37455, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 37460, i64* @assembly_address
  br label %block_94ff

block_9259:                                       ; preds = %block_9275
  store volatile i64 37465, i64* @assembly_address
  %1990 = load i32* %stack_var_-1360
  %1991 = zext i32 %1990 to i64
  store i64 %1991, i64* %rax
  store volatile i64 37471, i64* @assembly_address
  %1992 = load i64* %rax
  %1993 = add i64 %1992, 1
  %1994 = trunc i64 %1993 to i32
  %1995 = zext i32 %1994 to i64
  store i64 %1995, i64* %rdx
  store volatile i64 37474, i64* @assembly_address
  %1996 = load i64* %rdx
  %1997 = trunc i64 %1996 to i32
  store i32 %1997, i32* %stack_var_-1360
  store volatile i64 37480, i64* @assembly_address
  %1998 = load i64* %rax
  %1999 = trunc i64 %1998 to i32
  %2000 = sext i32 %1999 to i64
  store i64 %2000, i64* %rax
  store volatile i64 37482, i64* @assembly_address
  %2001 = load i64* %rbp
  %2002 = load i64* %rax
  %2003 = mul i64 %2002, 4
  %2004 = add i64 %2001, -1296
  %2005 = add i64 %2004, %2003
  %2006 = inttoptr i64 %2005 to i32*
  store i32 0, i32* %2006
  br label %block_9275

block_9275:                                       ; preds = %block_9259, %block_9223
  store volatile i64 37493, i64* @assembly_address
  %2007 = load i32* %stack_var_-1356
  %2008 = zext i32 %2007 to i64
  store i64 %2008, i64* %rax
  store volatile i64 37499, i64* @assembly_address
  %2009 = load i64* %rax
  %2010 = add i64 %2009, -1
  %2011 = trunc i64 %2010 to i32
  %2012 = zext i32 %2011 to i64
  store i64 %2012, i64* %rdx
  store volatile i64 37502, i64* @assembly_address
  %2013 = load i64* %rdx
  %2014 = trunc i64 %2013 to i32
  store i32 %2014, i32* %stack_var_-1356
  store volatile i64 37508, i64* @assembly_address
  %2015 = load i64* %rax
  %2016 = trunc i64 %2015 to i32
  %2017 = load i64* %rax
  %2018 = trunc i64 %2017 to i32
  %2019 = and i32 %2016, %2018
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2020 = icmp eq i32 %2019, 0
  store i1 %2020, i1* %zf
  %2021 = icmp slt i32 %2019, 0
  store i1 %2021, i1* %sf
  %2022 = trunc i32 %2019 to i8
  %2023 = call i8 @llvm.ctpop.i8(i8 %2022)
  %2024 = and i8 %2023, 1
  %2025 = icmp eq i8 %2024, 0
  store i1 %2025, i1* %pf
  store volatile i64 37510, i64* @assembly_address
  %2026 = load i1* %zf
  %2027 = icmp eq i1 %2026, false
  br i1 %2027, label %block_9259, label %block_9288

block_9288:                                       ; preds = %block_9275
  store volatile i64 37512, i64* @assembly_address
  store i32 0, i32* %stack_var_-1352
  store volatile i64 37522, i64* @assembly_address
  br label %block_9360

block_9297:                                       ; preds = %block_92ec
  store volatile i64 37527, i64* @assembly_address
  %2028 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %2029 = zext i32 %2028 to i64
  store i64 %2029, i64* %rdx
  store volatile i64 37533, i64* @assembly_address
  %2030 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %2031 = zext i32 %2030 to i64
  store i64 %2031, i64* %rax
  store volatile i64 37539, i64* @assembly_address
  %2032 = load i64* %rdx
  %2033 = trunc i64 %2032 to i32
  %2034 = load i64* %rax
  %2035 = trunc i64 %2034 to i32
  %2036 = sub i32 %2033, %2035
  %2037 = and i32 %2033, 15
  %2038 = and i32 %2035, 15
  %2039 = sub i32 %2037, %2038
  %2040 = icmp ugt i32 %2039, 15
  %2041 = icmp ult i32 %2033, %2035
  %2042 = xor i32 %2033, %2035
  %2043 = xor i32 %2033, %2036
  %2044 = and i32 %2042, %2043
  %2045 = icmp slt i32 %2044, 0
  store i1 %2040, i1* %az
  store i1 %2041, i1* %cf
  store i1 %2045, i1* %of
  %2046 = icmp eq i32 %2036, 0
  store i1 %2046, i1* %zf
  %2047 = icmp slt i32 %2036, 0
  store i1 %2047, i1* %sf
  %2048 = trunc i32 %2036 to i8
  %2049 = call i8 @llvm.ctpop.i8(i8 %2048)
  %2050 = and i8 %2049, 1
  %2051 = icmp eq i8 %2050, 0
  store i1 %2051, i1* %pf
  store volatile i64 37541, i64* @assembly_address
  %2052 = load i1* %cf
  %2053 = icmp eq i1 %2052, false
  br i1 %2053, label %block_92c8, label %block_92a7

block_92a7:                                       ; preds = %block_9297
  store volatile i64 37543, i64* @assembly_address
  %2054 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %2055 = zext i32 %2054 to i64
  store i64 %2055, i64* %rax
  store volatile i64 37549, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 37552, i64* @assembly_address
  %2056 = load i64* %rdx
  %2057 = trunc i64 %2056 to i32
  store i32 %2057, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 37558, i64* @assembly_address
  %2058 = load i64* %rax
  %2059 = trunc i64 %2058 to i32
  %2060 = zext i32 %2059 to i64
  store i64 %2060, i64* %rdx
  store volatile i64 37560, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 37567, i64* @assembly_address
  %2061 = load i64* %rdx
  %2062 = load i64* %rax
  %2063 = mul i64 %2062, 1
  %2064 = add i64 %2061, %2063
  %2065 = inttoptr i64 %2064 to i8*
  %2066 = load i8* %2065
  %2067 = zext i8 %2066 to i64
  store i64 %2067, i64* %rax
  store volatile i64 37571, i64* @assembly_address
  %2068 = load i64* %rax
  %2069 = trunc i64 %2068 to i8
  %2070 = zext i8 %2069 to i64
  store i64 %2070, i64* %rax
  store volatile i64 37574, i64* @assembly_address
  br label %block_92e1

block_92c8:                                       ; preds = %block_9297
  store volatile i64 37576, i64* @assembly_address
  %2071 = load i64** %stack_var_-1348
  %2072 = ptrtoint i64* %2071 to i32
  %2073 = zext i32 %2072 to i64
  store i64 %2073, i64* %rax
  store volatile i64 37582, i64* @assembly_address
  %2074 = load i64* %rax
  %2075 = trunc i64 %2074 to i32
  store i32 %2075, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 37588, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 37593, i64* @assembly_address
  %2076 = load i64* %rdi
  %2077 = trunc i64 %2076 to i32
  %2078 = call i64 @fill_inbuf(i32 %2077)
  store i64 %2078, i64* %rax
  store i64 %2078, i64* %rax
  store volatile i64 37598, i64* @assembly_address
  %2079 = load i64* %rax
  %2080 = trunc i64 %2079 to i8
  %2081 = zext i8 %2080 to i64
  store i64 %2081, i64* %rax
  br label %block_92e1

block_92e1:                                       ; preds = %block_92c8, %block_92a7
  store volatile i64 37601, i64* @assembly_address
  %2082 = load i64* %rbx
  %2083 = trunc i64 %2082 to i32
  %2084 = zext i32 %2083 to i64
  store i64 %2084, i64* %rcx
  store volatile i64 37603, i64* @assembly_address
  %2085 = load i64* %rax
  %2086 = load i64* %rcx
  %2087 = trunc i64 %2086 to i8
  %2088 = zext i8 %2087 to i64
  %2089 = and i64 %2088, 63
  %2090 = load i1* %of
  %2091 = icmp eq i64 %2089, 0
  br i1 %2091, label %2108, label %2092

; <label>:2092                                    ; preds = %block_92e1
  %2093 = shl i64 %2085, %2089
  %2094 = icmp eq i64 %2093, 0
  store i1 %2094, i1* %zf
  %2095 = icmp slt i64 %2093, 0
  store i1 %2095, i1* %sf
  %2096 = trunc i64 %2093 to i8
  %2097 = call i8 @llvm.ctpop.i8(i8 %2096)
  %2098 = and i8 %2097, 1
  %2099 = icmp eq i8 %2098, 0
  store i1 %2099, i1* %pf
  store i64 %2093, i64* %rax
  %2100 = sub i64 %2089, 1
  %2101 = shl i64 %2085, %2100
  %2102 = lshr i64 %2101, 63
  %2103 = trunc i64 %2102 to i1
  store i1 %2103, i1* %cf
  %2104 = lshr i64 %2093, 63
  %2105 = icmp ne i64 %2104, %2102
  %2106 = icmp eq i64 %2089, 1
  %2107 = select i1 %2106, i1 %2105, i1 %2090
  store i1 %2107, i1* %of
  br label %2108

; <label>:2108                                    ; preds = %block_92e1, %2092
  store volatile i64 37606, i64* @assembly_address
  %2109 = load i64* %r12
  %2110 = load i64* %rax
  %2111 = or i64 %2109, %2110
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2112 = icmp eq i64 %2111, 0
  store i1 %2112, i1* %zf
  %2113 = icmp slt i64 %2111, 0
  store i1 %2113, i1* %sf
  %2114 = trunc i64 %2111 to i8
  %2115 = call i8 @llvm.ctpop.i8(i8 %2114)
  %2116 = and i8 %2115, 1
  %2117 = icmp eq i8 %2116, 0
  store i1 %2117, i1* %pf
  store i64 %2111, i64* %r12
  store volatile i64 37609, i64* @assembly_address
  %2118 = load i64* %rbx
  %2119 = trunc i64 %2118 to i32
  %2120 = add i32 %2119, 8
  %2121 = and i32 %2119, 15
  %2122 = add i32 %2121, 8
  %2123 = icmp ugt i32 %2122, 15
  %2124 = icmp ult i32 %2120, %2119
  %2125 = xor i32 %2119, %2120
  %2126 = xor i32 8, %2120
  %2127 = and i32 %2125, %2126
  %2128 = icmp slt i32 %2127, 0
  store i1 %2123, i1* %az
  store i1 %2124, i1* %cf
  store i1 %2128, i1* %of
  %2129 = icmp eq i32 %2120, 0
  store i1 %2129, i1* %zf
  %2130 = icmp slt i32 %2120, 0
  store i1 %2130, i1* %sf
  %2131 = trunc i32 %2120 to i8
  %2132 = call i8 @llvm.ctpop.i8(i8 %2131)
  %2133 = and i8 %2132, 1
  %2134 = icmp eq i8 %2133, 0
  store i1 %2134, i1* %pf
  %2135 = zext i32 %2120 to i64
  store i64 %2135, i64* %rbx
  br label %block_92ec

block_92ec:                                       ; preds = %2108, %block_91ba
  store volatile i64 37612, i64* @assembly_address
  %2136 = load i64* %rbx
  %2137 = trunc i64 %2136 to i32
  %2138 = sub i32 %2137, 6
  %2139 = and i32 %2137, 15
  %2140 = sub i32 %2139, 6
  %2141 = icmp ugt i32 %2140, 15
  %2142 = icmp ult i32 %2137, 6
  %2143 = xor i32 %2137, 6
  %2144 = xor i32 %2137, %2138
  %2145 = and i32 %2143, %2144
  %2146 = icmp slt i32 %2145, 0
  store i1 %2141, i1* %az
  store i1 %2142, i1* %cf
  store i1 %2146, i1* %of
  %2147 = icmp eq i32 %2138, 0
  store i1 %2147, i1* %zf
  %2148 = icmp slt i32 %2138, 0
  store i1 %2148, i1* %sf
  %2149 = trunc i32 %2138 to i8
  %2150 = call i8 @llvm.ctpop.i8(i8 %2149)
  %2151 = and i8 %2150, 1
  %2152 = icmp eq i8 %2151, 0
  store i1 %2152, i1* %pf
  store volatile i64 37615, i64* @assembly_address
  %2153 = load i1* %cf
  %2154 = load i1* %zf
  %2155 = or i1 %2153, %2154
  br i1 %2155, label %block_9297, label %block_92f1

block_92f1:                                       ; preds = %block_92ec
  store volatile i64 37617, i64* @assembly_address
  %2156 = load i64* %r12
  %2157 = trunc i64 %2156 to i32
  %2158 = zext i32 %2157 to i64
  store i64 %2158, i64* %rax
  store volatile i64 37620, i64* @assembly_address
  %2159 = load i64* %rax
  %2160 = trunc i64 %2159 to i32
  %2161 = and i32 %2160, 127
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2162 = icmp eq i32 %2161, 0
  store i1 %2162, i1* %zf
  %2163 = icmp slt i32 %2161, 0
  store i1 %2163, i1* %sf
  %2164 = trunc i32 %2161 to i8
  %2165 = call i8 @llvm.ctpop.i8(i8 %2164)
  %2166 = and i8 %2165, 1
  %2167 = icmp eq i8 %2166, 0
  store i1 %2167, i1* %pf
  %2168 = zext i32 %2161 to i64
  store i64 %2168, i64* %rax
  store volatile i64 37623, i64* @assembly_address
  %2169 = load i64* %rax
  %2170 = trunc i64 %2169 to i32
  %2171 = add i32 %2170, 11
  %2172 = and i32 %2170, 15
  %2173 = add i32 %2172, 11
  %2174 = icmp ugt i32 %2173, 15
  %2175 = icmp ult i32 %2171, %2170
  %2176 = xor i32 %2170, %2171
  %2177 = xor i32 11, %2171
  %2178 = and i32 %2176, %2177
  %2179 = icmp slt i32 %2178, 0
  store i1 %2174, i1* %az
  store i1 %2175, i1* %cf
  store i1 %2179, i1* %of
  %2180 = icmp eq i32 %2171, 0
  store i1 %2180, i1* %zf
  %2181 = icmp slt i32 %2171, 0
  store i1 %2181, i1* %sf
  %2182 = trunc i32 %2171 to i8
  %2183 = call i8 @llvm.ctpop.i8(i8 %2182)
  %2184 = and i8 %2183, 1
  %2185 = icmp eq i8 %2184, 0
  store i1 %2185, i1* %pf
  %2186 = zext i32 %2171 to i64
  store i64 %2186, i64* %rax
  store volatile i64 37626, i64* @assembly_address
  %2187 = load i64* %rax
  %2188 = trunc i64 %2187 to i32
  store i32 %2188, i32* %stack_var_-1356
  store volatile i64 37632, i64* @assembly_address
  %2189 = load i64* %r12
  %2190 = load i1* %of
  %2191 = lshr i64 %2189, 7
  %2192 = icmp eq i64 %2191, 0
  store i1 %2192, i1* %zf
  %2193 = icmp slt i64 %2191, 0
  store i1 %2193, i1* %sf
  %2194 = trunc i64 %2191 to i8
  %2195 = call i8 @llvm.ctpop.i8(i8 %2194)
  %2196 = and i8 %2195, 1
  %2197 = icmp eq i8 %2196, 0
  store i1 %2197, i1* %pf
  store i64 %2191, i64* %r12
  %2198 = and i64 64, %2189
  %2199 = icmp ne i64 %2198, 0
  store i1 %2199, i1* %cf
  %2200 = icmp slt i64 %2189, 0
  %2201 = select i1 false, i1 %2200, i1 %2190
  store i1 %2201, i1* %of
  store volatile i64 37636, i64* @assembly_address
  %2202 = load i64* %rbx
  %2203 = trunc i64 %2202 to i32
  %2204 = sub i32 %2203, 7
  %2205 = and i32 %2203, 15
  %2206 = sub i32 %2205, 7
  %2207 = icmp ugt i32 %2206, 15
  %2208 = icmp ult i32 %2203, 7
  %2209 = xor i32 %2203, 7
  %2210 = xor i32 %2203, %2204
  %2211 = and i32 %2209, %2210
  %2212 = icmp slt i32 %2211, 0
  store i1 %2207, i1* %az
  store i1 %2208, i1* %cf
  store i1 %2212, i1* %of
  %2213 = icmp eq i32 %2204, 0
  store i1 %2213, i1* %zf
  %2214 = icmp slt i32 %2204, 0
  store i1 %2214, i1* %sf
  %2215 = trunc i32 %2204 to i8
  %2216 = call i8 @llvm.ctpop.i8(i8 %2215)
  %2217 = and i8 %2216, 1
  %2218 = icmp eq i8 %2217, 0
  store i1 %2218, i1* %pf
  %2219 = zext i32 %2204 to i64
  store i64 %2219, i64* %rbx
  store volatile i64 37639, i64* @assembly_address
  %2220 = load i32* %stack_var_-1360
  %2221 = zext i32 %2220 to i64
  store i64 %2221, i64* %rdx
  store volatile i64 37645, i64* @assembly_address
  %2222 = load i32* %stack_var_-1356
  %2223 = zext i32 %2222 to i64
  store i64 %2223, i64* %rax
  store volatile i64 37651, i64* @assembly_address
  %2224 = load i64* %rax
  %2225 = trunc i64 %2224 to i32
  %2226 = load i64* %rdx
  %2227 = trunc i64 %2226 to i32
  %2228 = add i32 %2225, %2227
  %2229 = and i32 %2225, 15
  %2230 = and i32 %2227, 15
  %2231 = add i32 %2229, %2230
  %2232 = icmp ugt i32 %2231, 15
  %2233 = icmp ult i32 %2228, %2225
  %2234 = xor i32 %2225, %2228
  %2235 = xor i32 %2227, %2228
  %2236 = and i32 %2234, %2235
  %2237 = icmp slt i32 %2236, 0
  store i1 %2232, i1* %az
  store i1 %2233, i1* %cf
  store i1 %2237, i1* %of
  %2238 = icmp eq i32 %2228, 0
  store i1 %2238, i1* %zf
  %2239 = icmp slt i32 %2228, 0
  store i1 %2239, i1* %sf
  %2240 = trunc i32 %2228 to i8
  %2241 = call i8 @llvm.ctpop.i8(i8 %2240)
  %2242 = and i8 %2241, 1
  %2243 = icmp eq i8 %2242, 0
  store i1 %2243, i1* %pf
  %2244 = zext i32 %2228 to i64
  store i64 %2244, i64* %rax
  store volatile i64 37653, i64* @assembly_address
  %2245 = load i32* %stack_var_-1332
  %2246 = load i64* %rax
  %2247 = trunc i64 %2246 to i32
  %2248 = sub i32 %2245, %2247
  %2249 = and i32 %2245, 15
  %2250 = and i32 %2247, 15
  %2251 = sub i32 %2249, %2250
  %2252 = icmp ugt i32 %2251, 15
  %2253 = icmp ult i32 %2245, %2247
  %2254 = xor i32 %2245, %2247
  %2255 = xor i32 %2245, %2248
  %2256 = and i32 %2254, %2255
  %2257 = icmp slt i32 %2256, 0
  store i1 %2252, i1* %az
  store i1 %2253, i1* %cf
  store i1 %2257, i1* %of
  %2258 = icmp eq i32 %2248, 0
  store i1 %2258, i1* %zf
  %2259 = icmp slt i32 %2248, 0
  store i1 %2259, i1* %sf
  %2260 = trunc i32 %2248 to i8
  %2261 = call i8 @llvm.ctpop.i8(i8 %2260)
  %2262 = and i8 %2261, 1
  %2263 = icmp eq i8 %2262, 0
  store i1 %2263, i1* %pf
  store volatile i64 37659, i64* @assembly_address
  %2264 = load i1* %cf
  %2265 = icmp eq i1 %2264, false
  br i1 %2265, label %block_9343, label %block_931d

block_931d:                                       ; preds = %block_92f1
  store volatile i64 37661, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 37666, i64* @assembly_address
  br label %block_94ff

block_9327:                                       ; preds = %block_9343
  store volatile i64 37671, i64* @assembly_address
  %2266 = load i32* %stack_var_-1360
  %2267 = zext i32 %2266 to i64
  store i64 %2267, i64* %rax
  store volatile i64 37677, i64* @assembly_address
  %2268 = load i64* %rax
  %2269 = add i64 %2268, 1
  %2270 = trunc i64 %2269 to i32
  %2271 = zext i32 %2270 to i64
  store i64 %2271, i64* %rdx
  store volatile i64 37680, i64* @assembly_address
  %2272 = load i64* %rdx
  %2273 = trunc i64 %2272 to i32
  store i32 %2273, i32* %stack_var_-1360
  store volatile i64 37686, i64* @assembly_address
  %2274 = load i64* %rax
  %2275 = trunc i64 %2274 to i32
  %2276 = sext i32 %2275 to i64
  store i64 %2276, i64* %rax
  store volatile i64 37688, i64* @assembly_address
  %2277 = load i64* %rbp
  %2278 = load i64* %rax
  %2279 = mul i64 %2278, 4
  %2280 = add i64 %2277, -1296
  %2281 = add i64 %2280, %2279
  %2282 = inttoptr i64 %2281 to i32*
  store i32 0, i32* %2282
  br label %block_9343

block_9343:                                       ; preds = %block_9327, %block_92f1
  store volatile i64 37699, i64* @assembly_address
  %2283 = load i32* %stack_var_-1356
  %2284 = zext i32 %2283 to i64
  store i64 %2284, i64* %rax
  store volatile i64 37705, i64* @assembly_address
  %2285 = load i64* %rax
  %2286 = add i64 %2285, -1
  %2287 = trunc i64 %2286 to i32
  %2288 = zext i32 %2287 to i64
  store i64 %2288, i64* %rdx
  store volatile i64 37708, i64* @assembly_address
  %2289 = load i64* %rdx
  %2290 = trunc i64 %2289 to i32
  store i32 %2290, i32* %stack_var_-1356
  store volatile i64 37714, i64* @assembly_address
  %2291 = load i64* %rax
  %2292 = trunc i64 %2291 to i32
  %2293 = load i64* %rax
  %2294 = trunc i64 %2293 to i32
  %2295 = and i32 %2292, %2294
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2296 = icmp eq i32 %2295, 0
  store i1 %2296, i1* %zf
  %2297 = icmp slt i32 %2295, 0
  store i1 %2297, i1* %sf
  %2298 = trunc i32 %2295 to i8
  %2299 = call i8 @llvm.ctpop.i8(i8 %2298)
  %2300 = and i8 %2299, 1
  %2301 = icmp eq i8 %2300, 0
  store i1 %2301, i1* %pf
  store volatile i64 37716, i64* @assembly_address
  %2302 = load i1* %zf
  %2303 = icmp eq i1 %2302, false
  br i1 %2303, label %block_9327, label %block_9356

block_9356:                                       ; preds = %block_9343
  store volatile i64 37718, i64* @assembly_address
  store i32 0, i32* %stack_var_-1352
  br label %block_9360

block_9360:                                       ; preds = %block_9356, %block_9288, %block_91b5, %block_90b6, %block_8f81
  store volatile i64 37728, i64* @assembly_address
  %2304 = load i32* %stack_var_-1360
  %2305 = zext i32 %2304 to i64
  store i64 %2305, i64* %rax
  store volatile i64 37734, i64* @assembly_address
  %2306 = load i32* %stack_var_-1332
  %2307 = load i64* %rax
  %2308 = trunc i64 %2307 to i32
  %2309 = sub i32 %2306, %2308
  %2310 = and i32 %2306, 15
  %2311 = and i32 %2308, 15
  %2312 = sub i32 %2310, %2311
  %2313 = icmp ugt i32 %2312, 15
  %2314 = icmp ult i32 %2306, %2308
  %2315 = xor i32 %2306, %2308
  %2316 = xor i32 %2306, %2309
  %2317 = and i32 %2315, %2316
  %2318 = icmp slt i32 %2317, 0
  store i1 %2313, i1* %az
  store i1 %2314, i1* %cf
  store i1 %2318, i1* %of
  %2319 = icmp eq i32 %2309, 0
  store i1 %2319, i1* %zf
  %2320 = icmp slt i32 %2309, 0
  store i1 %2320, i1* %sf
  %2321 = trunc i32 %2309 to i8
  %2322 = call i8 @llvm.ctpop.i8(i8 %2321)
  %2323 = and i8 %2322, 1
  %2324 = icmp eq i8 %2323, 0
  store i1 %2324, i1* %pf
  store volatile i64 37740, i64* @assembly_address
  %2325 = load i1* %cf
  %2326 = load i1* %zf
  %2327 = or i1 %2325, %2326
  %2328 = icmp ne i1 %2327, true
  br i1 %2328, label %block_9023, label %block_9372

block_9372:                                       ; preds = %block_9360
  store volatile i64 37746, i64* @assembly_address
  %2329 = load i64* %stack_var_-1320
  store i64 %2329, i64* %rax
  store volatile i64 37753, i64* @assembly_address
  %2330 = load i64* %rax
  store i64 %2330, i64* %rdi
  store volatile i64 37756, i64* @assembly_address
  %2331 = load i64* %rdi
  %2332 = inttoptr i64 %2331 to i64*
  %2333 = call i64 @huft_free(i64* %2332)
  store i64 %2333, i64* %rax
  store i64 %2333, i64* %rax
  store volatile i64 37761, i64* @assembly_address
  %2334 = load i64* %r12
  store i64 %2334, i64* @global_var_216f98
  store volatile i64 37768, i64* @assembly_address
  %2335 = load i64* %rbx
  %2336 = trunc i64 %2335 to i32
  store i32 %2336, i32* bitcast (i64* @global_var_216fa0 to i32*)
  store volatile i64 37774, i64* @assembly_address
  %2337 = load i32* bitcast ([2 x i8]* @global_var_216304 to i32*)
  %2338 = zext i32 %2337 to i64
  store i64 %2338, i64* %rax
  store volatile i64 37780, i64* @assembly_address
  %2339 = load i64* %rax
  %2340 = trunc i64 %2339 to i32
  %2341 = sext i32 %2340 to i64
  %2342 = inttoptr i64 %2341 to i8*
  store i8* %2342, i8** %stack_var_-1368
  store volatile i64 37786, i64* @assembly_address
  %2343 = ptrtoint i64* %stack_var_-1320 to i64
  store i64 %2343, i64* %rcx
  store volatile i64 37793, i64* @assembly_address
  %2344 = load i32* %stack_var_-1344
  %2345 = zext i32 %2344 to i64
  store i64 %2345, i64* %rsi
  store volatile i64 37799, i64* @assembly_address
  %2346 = ptrtoint i64* %stack_var_-1304 to i64
  store i64 %2346, i64* %rax
  store volatile i64 37806, i64* @assembly_address
  %2347 = load i64* %rsp
  %2348 = sub i64 %2347, 8
  %2349 = and i64 %2347, 15
  %2350 = sub i64 %2349, 8
  %2351 = icmp ugt i64 %2350, 15
  %2352 = icmp ult i64 %2347, 8
  %2353 = xor i64 %2347, 8
  %2354 = xor i64 %2347, %2348
  %2355 = and i64 %2353, %2354
  %2356 = icmp slt i64 %2355, 0
  store i1 %2351, i1* %az
  store i1 %2352, i1* %cf
  store i1 %2356, i1* %of
  %2357 = icmp eq i64 %2348, 0
  store i1 %2357, i1* %zf
  %2358 = icmp slt i64 %2348, 0
  store i1 %2358, i1* %sf
  %2359 = trunc i64 %2348 to i8
  %2360 = call i8 @llvm.ctpop.i8(i8 %2359)
  %2361 = and i8 %2360, 1
  %2362 = icmp eq i8 %2361, 0
  store i1 %2362, i1* %pf
  %2363 = ptrtoint i64* %stack_var_-1376 to i64
  store i64 %2363, i64* %rsp
  store volatile i64 37810, i64* @assembly_address
  %2364 = ptrtoint i8** %stack_var_-1368 to i64
  store i64 %2364, i64* %rdx
  store volatile i64 37817, i64* @assembly_address
  %2365 = bitcast i8** %stack_var_-1368 to i32*
  store i32* %2365, i32** %stack_var_-1384
  %2366 = ptrtoint i32** %stack_var_-1384 to i64
  store i64 %2366, i64* %rsp
  store volatile i64 37818, i64* @assembly_address
  %2367 = ptrtoint i64* %stack_var_-1320 to i64
  store i64 %2367, i64* %r9
  store volatile i64 37821, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216220 to i64), i64* %r8
  store volatile i64 37828, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2161e0 to i64), i64* %rcx
  store volatile i64 37835, i64* @assembly_address
  store i64 257, i64* %rdx
  store volatile i64 37840, i64* @assembly_address
  %2368 = ptrtoint i64* %stack_var_-1304 to i64
  store i64 %2368, i64* %rdi
  store volatile i64 37843, i64* @assembly_address
  %2369 = load i64* %rdi
  %2370 = inttoptr i64 %2369 to i64*
  %2371 = load i64* %rsi
  %2372 = load i64* %rdx
  %2373 = load i64* %rcx
  %2374 = inttoptr i64 %2373 to i64*
  %2375 = load i64* %r8
  %2376 = inttoptr i64 %2375 to i64*
  %2377 = load i64* %r9
  %2378 = inttoptr i64 %2377 to i64*
  %2379 = load i32** %stack_var_-1384
  %2380 = bitcast i64* %2370 to i32*
  %2381 = call i64 @huft_build(i32* %2380, i64 %2371, i64 %2372, i64* %2374, i64* %2376, i64* %2378, i32* %2379)
  store i64 %2381, i64* %rax
  store i64 %2381, i64* %rax
  store volatile i64 37848, i64* @assembly_address
  %2382 = load i64* %rsp
  %2383 = add i64 %2382, 16
  %2384 = and i64 %2382, 15
  %2385 = icmp ugt i64 %2384, 15
  %2386 = icmp ult i64 %2383, %2382
  %2387 = xor i64 %2382, %2383
  %2388 = xor i64 16, %2383
  %2389 = and i64 %2387, %2388
  %2390 = icmp slt i64 %2389, 0
  store i1 %2385, i1* %az
  store i1 %2386, i1* %cf
  store i1 %2390, i1* %of
  %2391 = icmp eq i64 %2383, 0
  store i1 %2391, i1* %zf
  %2392 = icmp slt i64 %2383, 0
  store i1 %2392, i1* %sf
  %2393 = trunc i64 %2383 to i8
  %2394 = call i8 @llvm.ctpop.i8(i8 %2393)
  %2395 = and i8 %2394, 1
  %2396 = icmp eq i8 %2395, 0
  store i1 %2396, i1* %pf
  %2397 = ptrtoint i8** %stack_var_-1368 to i64
  store i64 %2397, i64* %rsp
  store volatile i64 37852, i64* @assembly_address
  %2398 = load i64* %rax
  %2399 = trunc i64 %2398 to i32
  store i32 %2399, i32* %stack_var_-1360
  store volatile i64 37858, i64* @assembly_address
  %2400 = load i32* %stack_var_-1360
  %2401 = and i32 %2400, 15
  %2402 = icmp ugt i32 %2401, 15
  %2403 = icmp ult i32 %2400, 0
  %2404 = xor i32 %2400, 0
  %2405 = and i32 %2404, 0
  %2406 = icmp slt i32 %2405, 0
  store i1 %2402, i1* %az
  store i1 %2403, i1* %cf
  store i1 %2406, i1* %of
  %2407 = icmp eq i32 %2400, 0
  store i1 %2407, i1* %zf
  %2408 = icmp slt i32 %2400, 0
  store i1 %2408, i1* %sf
  %2409 = trunc i32 %2400 to i8
  %2410 = call i8 @llvm.ctpop.i8(i8 %2409)
  %2411 = and i8 %2410, 1
  %2412 = icmp eq i8 %2411, 0
  store i1 %2412, i1* %pf
  store volatile i64 37865, i64* @assembly_address
  %2413 = load i1* %zf
  br i1 %2413, label %block_940e, label %block_93eb

block_93eb:                                       ; preds = %block_9372
  store volatile i64 37867, i64* @assembly_address
  %2414 = load i32* %stack_var_-1360
  %2415 = sub i32 %2414, 1
  %2416 = and i32 %2414, 15
  %2417 = sub i32 %2416, 1
  %2418 = icmp ugt i32 %2417, 15
  %2419 = icmp ult i32 %2414, 1
  %2420 = xor i32 %2414, 1
  %2421 = xor i32 %2414, %2415
  %2422 = and i32 %2420, %2421
  %2423 = icmp slt i32 %2422, 0
  store i1 %2418, i1* %az
  store i1 %2419, i1* %cf
  store i1 %2423, i1* %of
  %2424 = icmp eq i32 %2415, 0
  store i1 %2424, i1* %zf
  %2425 = icmp slt i32 %2415, 0
  store i1 %2425, i1* %sf
  %2426 = trunc i32 %2415 to i8
  %2427 = call i8 @llvm.ctpop.i8(i8 %2426)
  %2428 = and i8 %2427, 1
  %2429 = icmp eq i8 %2428, 0
  store i1 %2429, i1* %pf
  store volatile i64 37874, i64* @assembly_address
  %2430 = load i1* %zf
  %2431 = icmp eq i1 %2430, false
  br i1 %2431, label %block_9403, label %block_93f4

block_93f4:                                       ; preds = %block_93eb
  store volatile i64 37876, i64* @assembly_address
  %2432 = load i64* %stack_var_-1320
  store i64 %2432, i64* %rax
  store volatile i64 37883, i64* @assembly_address
  %2433 = load i64* %rax
  store i64 %2433, i64* %rdi
  store volatile i64 37886, i64* @assembly_address
  %2434 = load i64* %rdi
  %2435 = inttoptr i64 %2434 to i64*
  %2436 = call i64 @huft_free(i64* %2435)
  store i64 %2436, i64* %rax
  store i64 %2436, i64* %rax
  br label %block_9403

block_9403:                                       ; preds = %block_93f4, %block_93eb
  store volatile i64 37891, i64* @assembly_address
  %2437 = load i32* %stack_var_-1360
  %2438 = zext i32 %2437 to i64
  store i64 %2438, i64* %rax
  store volatile i64 37897, i64* @assembly_address
  br label %block_94ff

block_940e:                                       ; preds = %block_9372
  store volatile i64 37902, i64* @assembly_address
  %2439 = load i32* bitcast (i64* @global_var_216308 to i32*)
  %2440 = zext i32 %2439 to i64
  store i64 %2440, i64* %rax
  store volatile i64 37908, i64* @assembly_address
  %2441 = load i64* %rax
  %2442 = trunc i64 %2441 to i32
  store i32 %2442, i32* %stack_var_-1364
  store volatile i64 37914, i64* @assembly_address
  %2443 = load i32* %stack_var_-1344
  %2444 = zext i32 %2443 to i64
  store i64 %2444, i64* %rax
  store volatile i64 37920, i64* @assembly_address
  %2445 = load i64* %rax
  %2446 = mul i64 %2445, 4
  store i64 %2446, i64* %rdx
  store volatile i64 37928, i64* @assembly_address
  %2447 = ptrtoint i64* %stack_var_-1304 to i64
  store i64 %2447, i64* %rax
  store volatile i64 37935, i64* @assembly_address
  %2448 = load i64* %rax
  %2449 = load i64* %rdx
  %2450 = mul i64 %2449, 1
  %2451 = add i64 %2448, %2450
  store i64 %2451, i64* %rdi
  store volatile i64 37939, i64* @assembly_address
  %2452 = ptrtoint i8** %stack_var_-1312 to i64
  store i64 %2452, i64* %rcx
  store volatile i64 37946, i64* @assembly_address
  %2453 = load i32* %stack_var_-1340
  %2454 = zext i32 %2453 to i64
  store i64 %2454, i64* %rax
  store volatile i64 37952, i64* @assembly_address
  %2455 = load i64* %rsp
  %2456 = sub i64 %2455, 8
  %2457 = and i64 %2455, 15
  %2458 = sub i64 %2457, 8
  %2459 = icmp ugt i64 %2458, 15
  %2460 = icmp ult i64 %2455, 8
  %2461 = xor i64 %2455, 8
  %2462 = xor i64 %2455, %2456
  %2463 = and i64 %2461, %2462
  %2464 = icmp slt i64 %2463, 0
  store i1 %2459, i1* %az
  store i1 %2460, i1* %cf
  store i1 %2464, i1* %of
  %2465 = icmp eq i64 %2456, 0
  store i1 %2465, i1* %zf
  %2466 = icmp slt i64 %2456, 0
  store i1 %2466, i1* %sf
  %2467 = trunc i64 %2456 to i8
  %2468 = call i8 @llvm.ctpop.i8(i8 %2467)
  %2469 = and i8 %2468, 1
  %2470 = icmp eq i8 %2469, 0
  store i1 %2470, i1* %pf
  %2471 = ptrtoint i64* %stack_var_-1376 to i64
  store i64 %2471, i64* %rsp
  store volatile i64 37956, i64* @assembly_address
  %2472 = ptrtoint i32* %stack_var_-1364 to i64
  store i64 %2472, i64* %rdx
  store volatile i64 37963, i64* @assembly_address
  store i32* %stack_var_-1364, i32** %stack_var_-1384
  %2473 = ptrtoint i32** %stack_var_-1384 to i64
  store i64 %2473, i64* %rsp
  store volatile i64 37964, i64* @assembly_address
  %2474 = ptrtoint i8** %stack_var_-1312 to i64
  store i64 %2474, i64* %r9
  store volatile i64 37967, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2162a0 to i64), i64* %r8
  store volatile i64 37974, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216260 to i64), i64* %rcx
  store volatile i64 37981, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 37986, i64* @assembly_address
  %2475 = load i64* %rax
  %2476 = trunc i64 %2475 to i32
  %2477 = zext i32 %2476 to i64
  store i64 %2477, i64* %rsi
  store volatile i64 37988, i64* @assembly_address
  %2478 = load i64* %rdi
  %2479 = inttoptr i64 %2478 to i64*
  %2480 = load i64* %rsi
  %2481 = load i64* %rdx
  %2482 = load i64* %rcx
  %2483 = inttoptr i64 %2482 to i64*
  %2484 = load i64* %r8
  %2485 = inttoptr i64 %2484 to i64*
  %2486 = load i64* %r9
  %2487 = inttoptr i64 %2486 to i64*
  %2488 = load i32** %stack_var_-1384
  %2489 = bitcast i64* %2479 to i32*
  %2490 = call i64 @huft_build(i32* %2489, i64 %2480, i64 %2481, i64* %2483, i64* %2485, i64* %2487, i32* %2488)
  store i64 %2490, i64* %rax
  store i64 %2490, i64* %rax
  store volatile i64 37993, i64* @assembly_address
  %2491 = load i64* %rsp
  %2492 = add i64 %2491, 16
  %2493 = and i64 %2491, 15
  %2494 = icmp ugt i64 %2493, 15
  %2495 = icmp ult i64 %2492, %2491
  %2496 = xor i64 %2491, %2492
  %2497 = xor i64 16, %2492
  %2498 = and i64 %2496, %2497
  %2499 = icmp slt i64 %2498, 0
  store i1 %2494, i1* %az
  store i1 %2495, i1* %cf
  store i1 %2499, i1* %of
  %2500 = icmp eq i64 %2492, 0
  store i1 %2500, i1* %zf
  %2501 = icmp slt i64 %2492, 0
  store i1 %2501, i1* %sf
  %2502 = trunc i64 %2492 to i8
  %2503 = call i8 @llvm.ctpop.i8(i8 %2502)
  %2504 = and i8 %2503, 1
  %2505 = icmp eq i8 %2504, 0
  store i1 %2505, i1* %pf
  %2506 = ptrtoint i8** %stack_var_-1368 to i64
  store i64 %2506, i64* %rsp
  store volatile i64 37997, i64* @assembly_address
  %2507 = load i64* %rax
  %2508 = trunc i64 %2507 to i32
  store i32 %2508, i32* %stack_var_-1360
  store volatile i64 38003, i64* @assembly_address
  %2509 = load i32* %stack_var_-1360
  %2510 = and i32 %2509, 15
  %2511 = icmp ugt i32 %2510, 15
  %2512 = icmp ult i32 %2509, 0
  %2513 = xor i32 %2509, 0
  %2514 = and i32 %2513, 0
  %2515 = icmp slt i32 %2514, 0
  store i1 %2511, i1* %az
  store i1 %2512, i1* %cf
  store i1 %2515, i1* %of
  %2516 = icmp eq i32 %2509, 0
  store i1 %2516, i1* %zf
  %2517 = icmp slt i32 %2509, 0
  store i1 %2517, i1* %sf
  %2518 = trunc i32 %2509 to i8
  %2519 = call i8 @llvm.ctpop.i8(i8 %2518)
  %2520 = and i8 %2519, 1
  %2521 = icmp eq i8 %2520, 0
  store i1 %2521, i1* %pf
  store volatile i64 38010, i64* @assembly_address
  %2522 = load i1* %zf
  br i1 %2522, label %block_94ab, label %block_947c

block_947c:                                       ; preds = %block_940e
  store volatile i64 38012, i64* @assembly_address
  %2523 = load i32* %stack_var_-1360
  %2524 = sub i32 %2523, 1
  %2525 = and i32 %2523, 15
  %2526 = sub i32 %2525, 1
  %2527 = icmp ugt i32 %2526, 15
  %2528 = icmp ult i32 %2523, 1
  %2529 = xor i32 %2523, 1
  %2530 = xor i32 %2523, %2524
  %2531 = and i32 %2529, %2530
  %2532 = icmp slt i32 %2531, 0
  store i1 %2527, i1* %az
  store i1 %2528, i1* %cf
  store i1 %2532, i1* %of
  %2533 = icmp eq i32 %2524, 0
  store i1 %2533, i1* %zf
  %2534 = icmp slt i32 %2524, 0
  store i1 %2534, i1* %sf
  %2535 = trunc i32 %2524 to i8
  %2536 = call i8 @llvm.ctpop.i8(i8 %2535)
  %2537 = and i8 %2536, 1
  %2538 = icmp eq i8 %2537, 0
  store i1 %2538, i1* %pf
  store volatile i64 38019, i64* @assembly_address
  %2539 = load i1* %zf
  %2540 = icmp eq i1 %2539, false
  br i1 %2540, label %block_9494, label %block_9485

block_9485:                                       ; preds = %block_947c
  store volatile i64 38021, i64* @assembly_address
  %2541 = load i8** %stack_var_-1312
  %2542 = ptrtoint i8* %2541 to i64
  store i64 %2542, i64* %rax
  store volatile i64 38028, i64* @assembly_address
  %2543 = load i64* %rax
  store i64 %2543, i64* %rdi
  store volatile i64 38031, i64* @assembly_address
  %2544 = load i64* %rdi
  %2545 = inttoptr i64 %2544 to i64*
  %2546 = call i64 @huft_free(i64* %2545)
  store i64 %2546, i64* %rax
  store i64 %2546, i64* %rax
  br label %block_9494

block_9494:                                       ; preds = %block_9485, %block_947c
  store volatile i64 38036, i64* @assembly_address
  %2547 = load i64* %stack_var_-1320
  store i64 %2547, i64* %rax
  store volatile i64 38043, i64* @assembly_address
  %2548 = load i64* %rax
  store i64 %2548, i64* %rdi
  store volatile i64 38046, i64* @assembly_address
  %2549 = load i64* %rdi
  %2550 = inttoptr i64 %2549 to i64*
  %2551 = call i64 @huft_free(i64* %2550)
  store i64 %2551, i64* %rax
  store i64 %2551, i64* %rax
  store volatile i64 38051, i64* @assembly_address
  %2552 = load i32* %stack_var_-1360
  %2553 = zext i32 %2552 to i64
  store i64 %2553, i64* %rax
  store volatile i64 38057, i64* @assembly_address
  br label %block_94ff

block_94ab:                                       ; preds = %block_940e
  store volatile i64 38059, i64* @assembly_address
  %2554 = load i32* %stack_var_-1364
  %2555 = zext i32 %2554 to i64
  store i64 %2555, i64* %rcx
  store volatile i64 38065, i64* @assembly_address
  %2556 = load i8** %stack_var_-1368
  %2557 = ptrtoint i8* %2556 to i64
  %2558 = trunc i64 %2557 to i32
  %2559 = zext i32 %2558 to i64
  store i64 %2559, i64* %rdx
  store volatile i64 38071, i64* @assembly_address
  %2560 = load i8** %stack_var_-1312
  %2561 = ptrtoint i8* %2560 to i64
  store i64 %2561, i64* %rsi
  store volatile i64 38078, i64* @assembly_address
  %2562 = load i64* %stack_var_-1320
  store i64 %2562, i64* %rax
  store volatile i64 38085, i64* @assembly_address
  %2563 = load i64* %rax
  store i64 %2563, i64* %rdi
  store volatile i64 38088, i64* @assembly_address
  %2564 = load i64* %rdi
  %2565 = load i64* %rsi
  %2566 = load i64* %rdx
  %2567 = trunc i64 %2566 to i32
  %2568 = load i64* %rcx
  %2569 = trunc i64 %2568 to i32
  %2570 = inttoptr i64 %2564 to i8*
  %2571 = call i64 @inflate_codes(i8* %2570, i64 %2565, i32 %2567, i32 %2569)
  store i64 %2571, i64* %rax
  store i64 %2571, i64* %rax
  store volatile i64 38093, i64* @assembly_address
  %2572 = load i64* %rax
  %2573 = trunc i64 %2572 to i32
  %2574 = load i64* %rax
  %2575 = trunc i64 %2574 to i32
  %2576 = and i32 %2573, %2575
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2577 = icmp eq i32 %2576, 0
  store i1 %2577, i1* %zf
  %2578 = icmp slt i32 %2576, 0
  store i1 %2578, i1* %sf
  %2579 = trunc i32 %2576 to i8
  %2580 = call i8 @llvm.ctpop.i8(i8 %2579)
  %2581 = and i8 %2580, 1
  %2582 = icmp eq i8 %2581, 0
  store i1 %2582, i1* %pf
  store volatile i64 38095, i64* @assembly_address
  %2583 = load i1* %zf
  %2584 = icmp eq i1 %2583, false
  %2585 = zext i1 %2584 to i8
  %2586 = zext i8 %2585 to i64
  %2587 = load i64* %rax
  %2588 = and i64 %2587, -256
  %2589 = or i64 %2588, %2586
  store i64 %2589, i64* %rax
  store volatile i64 38098, i64* @assembly_address
  %2590 = load i64* %rax
  %2591 = trunc i64 %2590 to i8
  %2592 = zext i8 %2591 to i64
  store i64 %2592, i64* %rax
  store volatile i64 38101, i64* @assembly_address
  %2593 = load i64* %rax
  %2594 = trunc i64 %2593 to i32
  store i32 %2594, i32* %stack_var_-1324
  store volatile i64 38107, i64* @assembly_address
  %2595 = load i64* %stack_var_-1320
  store i64 %2595, i64* %rax
  store volatile i64 38114, i64* @assembly_address
  %2596 = load i64* %rax
  store i64 %2596, i64* %rdi
  store volatile i64 38117, i64* @assembly_address
  %2597 = load i64* %rdi
  %2598 = inttoptr i64 %2597 to i64*
  %2599 = call i64 @huft_free(i64* %2598)
  store i64 %2599, i64* %rax
  store i64 %2599, i64* %rax
  store volatile i64 38122, i64* @assembly_address
  %2600 = load i8** %stack_var_-1312
  %2601 = ptrtoint i8* %2600 to i64
  store i64 %2601, i64* %rax
  store volatile i64 38129, i64* @assembly_address
  %2602 = load i64* %rax
  store i64 %2602, i64* %rdi
  store volatile i64 38132, i64* @assembly_address
  %2603 = load i64* %rdi
  %2604 = inttoptr i64 %2603 to i64*
  %2605 = call i64 @huft_free(i64* %2604)
  store i64 %2605, i64* %rax
  store i64 %2605, i64* %rax
  store volatile i64 38137, i64* @assembly_address
  %2606 = load i32* %stack_var_-1324
  %2607 = zext i32 %2606 to i64
  store i64 %2607, i64* %rax
  br label %block_94ff

block_94ff:                                       ; preds = %block_94ab, %block_9494, %block_9403, %block_931d, %block_924f, %block_917a, %block_9080, %block_8f77, %block_8f60, %block_8e02
  store volatile i64 38143, i64* @assembly_address
  %2608 = load i64* %stack_var_-32
  store i64 %2608, i64* %rsi
  store volatile i64 38147, i64* @assembly_address
  %2609 = load i64* %rsi
  %2610 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %2611 = xor i64 %2609, %2610
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2612 = icmp eq i64 %2611, 0
  store i1 %2612, i1* %zf
  %2613 = icmp slt i64 %2611, 0
  store i1 %2613, i1* %sf
  %2614 = trunc i64 %2611 to i8
  %2615 = call i8 @llvm.ctpop.i8(i8 %2614)
  %2616 = and i8 %2615, 1
  %2617 = icmp eq i8 %2616, 0
  store i1 %2617, i1* %pf
  store i64 %2611, i64* %rsi
  store volatile i64 38156, i64* @assembly_address
  %2618 = load i1* %zf
  br i1 %2618, label %block_9513, label %block_950e

block_950e:                                       ; preds = %block_94ff
  store volatile i64 38158, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_9513:                                       ; preds = %block_94ff
  store volatile i64 38163, i64* @assembly_address
  %2619 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %2619, i64* %rsp
  store volatile i64 38167, i64* @assembly_address
  %2620 = load i64* %stack_var_-24
  store i64 %2620, i64* %rbx
  %2621 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %2621, i64* %rsp
  store volatile i64 38168, i64* @assembly_address
  %2622 = load i64* %stack_var_-16
  store i64 %2622, i64* %r12
  %2623 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2623, i64* %rsp
  store volatile i64 38170, i64* @assembly_address
  %2624 = load i64* %stack_var_-8
  store i64 %2624, i64* %rbp
  %2625 = ptrtoint i64* %stack_var_0 to i64
  store i64 %2625, i64* %rsp
  store volatile i64 38171, i64* @assembly_address
  %2626 = load i64* %rax
  %2627 = load i64* %rax
  ret i64 %2627
}

define i64 @inflate_block(i32* %arg1) {
block_951c:
  %r12 = alloca i64
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = bitcast i32* %arg1 to i64*
  %1 = ptrtoint i64* %0 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i64*
  %2 = alloca i32
  %stack_var_-48 = alloca i32*
  %3 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 38172, i64* @assembly_address
  %4 = load i64* %rbp
  store i64 %4, i64* %stack_var_-8
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rsp
  store volatile i64 38173, i64* @assembly_address
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rbp
  store volatile i64 38176, i64* @assembly_address
  %7 = load i64* %r12
  store i64 %7, i64* %stack_var_-16
  %8 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %8, i64* %rsp
  store volatile i64 38178, i64* @assembly_address
  %9 = load i64* %rbx
  store i64 %9, i64* %stack_var_-24
  %10 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %10, i64* %rsp
  store volatile i64 38179, i64* @assembly_address
  %11 = load i64* %rsp
  %12 = sub i64 %11, 32
  %13 = and i64 %11, 15
  %14 = icmp ugt i64 %13, 15
  %15 = icmp ult i64 %11, 32
  %16 = xor i64 %11, 32
  %17 = xor i64 %11, %12
  %18 = and i64 %16, %17
  %19 = icmp slt i64 %18, 0
  store i1 %14, i1* %az
  store i1 %15, i1* %cf
  store i1 %19, i1* %of
  %20 = icmp eq i64 %12, 0
  store i1 %20, i1* %zf
  %21 = icmp slt i64 %12, 0
  store i1 %21, i1* %sf
  %22 = trunc i64 %12 to i8
  %23 = call i8 @llvm.ctpop.i8(i8 %22)
  %24 = and i8 %23, 1
  %25 = icmp eq i8 %24, 0
  store i1 %25, i1* %pf
  %26 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %26, i64* %rsp
  store volatile i64 38183, i64* @assembly_address
  %27 = load i64* %rdi
  %28 = inttoptr i64 %27 to i32*
  store i32* %28, i32** %stack_var_-48
  store volatile i64 38187, i64* @assembly_address
  %29 = load i64* @global_var_216f98
  store i64 %29, i64* %r12
  store volatile i64 38194, i64* @assembly_address
  %30 = load i32* bitcast (i64* @global_var_216fa0 to i32*)
  %31 = zext i32 %30 to i64
  store i64 %31, i64* %rbx
  store volatile i64 38200, i64* @assembly_address
  %32 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %33 = zext i32 %32 to i64
  store i64 %33, i64* %rax
  store volatile i64 38206, i64* @assembly_address
  %34 = load i64* %rax
  %35 = trunc i64 %34 to i32
  %36 = inttoptr i32 %35 to i64*
  store i64* %36, i64** %stack_var_-32
  store volatile i64 38209, i64* @assembly_address
  br label %block_9595

block_9543:                                       ; preds = %block_9595
  store volatile i64 38211, i64* @assembly_address
  %37 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %38 = zext i32 %37 to i64
  store i64 %38, i64* %rdx
  store volatile i64 38217, i64* @assembly_address
  %39 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %40 = zext i32 %39 to i64
  store i64 %40, i64* %rax
  store volatile i64 38223, i64* @assembly_address
  %41 = load i64* %rdx
  %42 = trunc i64 %41 to i32
  %43 = load i64* %rax
  %44 = trunc i64 %43 to i32
  %45 = sub i32 %42, %44
  %46 = and i32 %42, 15
  %47 = and i32 %44, 15
  %48 = sub i32 %46, %47
  %49 = icmp ugt i32 %48, 15
  %50 = icmp ult i32 %42, %44
  %51 = xor i32 %42, %44
  %52 = xor i32 %42, %45
  %53 = and i32 %51, %52
  %54 = icmp slt i32 %53, 0
  store i1 %49, i1* %az
  store i1 %50, i1* %cf
  store i1 %54, i1* %of
  %55 = icmp eq i32 %45, 0
  store i1 %55, i1* %zf
  %56 = icmp slt i32 %45, 0
  store i1 %56, i1* %sf
  %57 = trunc i32 %45 to i8
  %58 = call i8 @llvm.ctpop.i8(i8 %57)
  %59 = and i8 %58, 1
  %60 = icmp eq i8 %59, 0
  store i1 %60, i1* %pf
  store volatile i64 38225, i64* @assembly_address
  %61 = load i1* %cf
  %62 = icmp eq i1 %61, false
  br i1 %62, label %block_9574, label %block_9553

block_9553:                                       ; preds = %block_9543
  store volatile i64 38227, i64* @assembly_address
  %63 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %64 = zext i32 %63 to i64
  store i64 %64, i64* %rax
  store volatile i64 38233, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 38236, i64* @assembly_address
  %65 = load i64* %rdx
  %66 = trunc i64 %65 to i32
  store i32 %66, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 38242, i64* @assembly_address
  %67 = load i64* %rax
  %68 = trunc i64 %67 to i32
  %69 = zext i32 %68 to i64
  store i64 %69, i64* %rdx
  store volatile i64 38244, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 38251, i64* @assembly_address
  %70 = load i64* %rdx
  %71 = load i64* %rax
  %72 = mul i64 %71, 1
  %73 = add i64 %70, %72
  %74 = inttoptr i64 %73 to i8*
  %75 = load i8* %74
  %76 = zext i8 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 38255, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i8
  %79 = zext i8 %78 to i64
  store i64 %79, i64* %rax
  store volatile i64 38258, i64* @assembly_address
  br label %block_958a

block_9574:                                       ; preds = %block_9543
  store volatile i64 38260, i64* @assembly_address
  %80 = load i64** %stack_var_-32
  %81 = ptrtoint i64* %80 to i32
  %82 = zext i32 %81 to i64
  store i64 %82, i64* %rax
  store volatile i64 38263, i64* @assembly_address
  %83 = load i64* %rax
  %84 = trunc i64 %83 to i32
  store i32 %84, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 38269, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 38274, i64* @assembly_address
  %85 = load i64* %rdi
  %86 = trunc i64 %85 to i32
  %87 = call i64 @fill_inbuf(i32 %86)
  store i64 %87, i64* %rax
  store i64 %87, i64* %rax
  store volatile i64 38279, i64* @assembly_address
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i8
  %90 = zext i8 %89 to i64
  store i64 %90, i64* %rax
  br label %block_958a

block_958a:                                       ; preds = %block_9574, %block_9553
  store volatile i64 38282, i64* @assembly_address
  %91 = load i64* %rbx
  %92 = trunc i64 %91 to i32
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rcx
  store volatile i64 38284, i64* @assembly_address
  %94 = load i64* %rax
  %95 = load i64* %rcx
  %96 = trunc i64 %95 to i8
  %97 = zext i8 %96 to i64
  %98 = and i64 %97, 63
  %99 = load i1* %of
  %100 = icmp eq i64 %98, 0
  br i1 %100, label %117, label %101

; <label>:101                                     ; preds = %block_958a
  %102 = shl i64 %94, %98
  %103 = icmp eq i64 %102, 0
  store i1 %103, i1* %zf
  %104 = icmp slt i64 %102, 0
  store i1 %104, i1* %sf
  %105 = trunc i64 %102 to i8
  %106 = call i8 @llvm.ctpop.i8(i8 %105)
  %107 = and i8 %106, 1
  %108 = icmp eq i8 %107, 0
  store i1 %108, i1* %pf
  store i64 %102, i64* %rax
  %109 = sub i64 %98, 1
  %110 = shl i64 %94, %109
  %111 = lshr i64 %110, 63
  %112 = trunc i64 %111 to i1
  store i1 %112, i1* %cf
  %113 = lshr i64 %102, 63
  %114 = icmp ne i64 %113, %111
  %115 = icmp eq i64 %98, 1
  %116 = select i1 %115, i1 %114, i1 %99
  store i1 %116, i1* %of
  br label %117

; <label>:117                                     ; preds = %block_958a, %101
  store volatile i64 38287, i64* @assembly_address
  %118 = load i64* %r12
  %119 = load i64* %rax
  %120 = or i64 %118, %119
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %121 = icmp eq i64 %120, 0
  store i1 %121, i1* %zf
  %122 = icmp slt i64 %120, 0
  store i1 %122, i1* %sf
  %123 = trunc i64 %120 to i8
  %124 = call i8 @llvm.ctpop.i8(i8 %123)
  %125 = and i8 %124, 1
  %126 = icmp eq i8 %125, 0
  store i1 %126, i1* %pf
  store i64 %120, i64* %r12
  store volatile i64 38290, i64* @assembly_address
  %127 = load i64* %rbx
  %128 = trunc i64 %127 to i32
  %129 = add i32 %128, 8
  %130 = and i32 %128, 15
  %131 = add i32 %130, 8
  %132 = icmp ugt i32 %131, 15
  %133 = icmp ult i32 %129, %128
  %134 = xor i32 %128, %129
  %135 = xor i32 8, %129
  %136 = and i32 %134, %135
  %137 = icmp slt i32 %136, 0
  store i1 %132, i1* %az
  store i1 %133, i1* %cf
  store i1 %137, i1* %of
  %138 = icmp eq i32 %129, 0
  store i1 %138, i1* %zf
  %139 = icmp slt i32 %129, 0
  store i1 %139, i1* %sf
  %140 = trunc i32 %129 to i8
  %141 = call i8 @llvm.ctpop.i8(i8 %140)
  %142 = and i8 %141, 1
  %143 = icmp eq i8 %142, 0
  store i1 %143, i1* %pf
  %144 = zext i32 %129 to i64
  store i64 %144, i64* %rbx
  br label %block_9595

block_9595:                                       ; preds = %117, %block_951c
  store volatile i64 38293, i64* @assembly_address
  %145 = load i64* %rbx
  %146 = trunc i64 %145 to i32
  %147 = load i64* %rbx
  %148 = trunc i64 %147 to i32
  %149 = and i32 %146, %148
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %150 = icmp eq i32 %149, 0
  store i1 %150, i1* %zf
  %151 = icmp slt i32 %149, 0
  store i1 %151, i1* %sf
  %152 = trunc i32 %149 to i8
  %153 = call i8 @llvm.ctpop.i8(i8 %152)
  %154 = and i8 %153, 1
  %155 = icmp eq i8 %154, 0
  store i1 %155, i1* %pf
  store volatile i64 38295, i64* @assembly_address
  %156 = load i1* %zf
  br i1 %156, label %block_9543, label %block_9599

block_9599:                                       ; preds = %block_9595
  store volatile i64 38297, i64* @assembly_address
  %157 = load i64* %r12
  %158 = trunc i64 %157 to i32
  %159 = zext i32 %158 to i64
  store i64 %159, i64* %rax
  store volatile i64 38300, i64* @assembly_address
  %160 = load i64* %rax
  %161 = trunc i64 %160 to i32
  %162 = and i32 %161, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %163 = icmp eq i32 %162, 0
  store i1 %163, i1* %zf
  %164 = icmp slt i32 %162, 0
  store i1 %164, i1* %sf
  %165 = trunc i32 %162 to i8
  %166 = call i8 @llvm.ctpop.i8(i8 %165)
  %167 = and i8 %166, 1
  %168 = icmp eq i8 %167, 0
  store i1 %168, i1* %pf
  %169 = zext i32 %162 to i64
  store i64 %169, i64* %rax
  store volatile i64 38303, i64* @assembly_address
  %170 = load i64* %rax
  %171 = trunc i64 %170 to i32
  %172 = zext i32 %171 to i64
  store i64 %172, i64* %rdx
  store volatile i64 38305, i64* @assembly_address
  %173 = load i32** %stack_var_-48
  %174 = ptrtoint i32* %173 to i64
  store i64 %174, i64* %rax
  store volatile i64 38309, i64* @assembly_address
  %175 = load i64* %rdx
  %176 = trunc i64 %175 to i32
  %177 = load i64* %rax
  %178 = inttoptr i64 %177 to i32*
  store i32 %176, i32* %178
  store volatile i64 38311, i64* @assembly_address
  %179 = load i64* %r12
  %180 = load i1* %of
  %181 = lshr i64 %179, 1
  %182 = icmp eq i64 %181, 0
  store i1 %182, i1* %zf
  %183 = icmp slt i64 %181, 0
  store i1 %183, i1* %sf
  %184 = trunc i64 %181 to i8
  %185 = call i8 @llvm.ctpop.i8(i8 %184)
  %186 = and i8 %185, 1
  %187 = icmp eq i8 %186, 0
  store i1 %187, i1* %pf
  store i64 %181, i64* %r12
  %188 = and i64 1, %179
  %189 = icmp ne i64 %188, 0
  store i1 %189, i1* %cf
  %190 = icmp slt i64 %179, 0
  %191 = select i1 true, i1 %190, i1 %180
  store i1 %191, i1* %of
  store volatile i64 38314, i64* @assembly_address
  %192 = load i64* %rbx
  %193 = trunc i64 %192 to i32
  %194 = sub i32 %193, 1
  %195 = and i32 %193, 15
  %196 = sub i32 %195, 1
  %197 = icmp ugt i32 %196, 15
  %198 = icmp ult i32 %193, 1
  %199 = xor i32 %193, 1
  %200 = xor i32 %193, %194
  %201 = and i32 %199, %200
  %202 = icmp slt i32 %201, 0
  store i1 %197, i1* %az
  store i1 %198, i1* %cf
  store i1 %202, i1* %of
  %203 = icmp eq i32 %194, 0
  store i1 %203, i1* %zf
  %204 = icmp slt i32 %194, 0
  store i1 %204, i1* %sf
  %205 = trunc i32 %194 to i8
  %206 = call i8 @llvm.ctpop.i8(i8 %205)
  %207 = and i8 %206, 1
  %208 = icmp eq i8 %207, 0
  store i1 %208, i1* %pf
  %209 = zext i32 %194 to i64
  store i64 %209, i64* %rbx
  store volatile i64 38317, i64* @assembly_address
  br label %block_9601

block_95af:                                       ; preds = %block_9601
  store volatile i64 38319, i64* @assembly_address
  %210 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %211 = zext i32 %210 to i64
  store i64 %211, i64* %rdx
  store volatile i64 38325, i64* @assembly_address
  %212 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %213 = zext i32 %212 to i64
  store i64 %213, i64* %rax
  store volatile i64 38331, i64* @assembly_address
  %214 = load i64* %rdx
  %215 = trunc i64 %214 to i32
  %216 = load i64* %rax
  %217 = trunc i64 %216 to i32
  %218 = sub i32 %215, %217
  %219 = and i32 %215, 15
  %220 = and i32 %217, 15
  %221 = sub i32 %219, %220
  %222 = icmp ugt i32 %221, 15
  %223 = icmp ult i32 %215, %217
  %224 = xor i32 %215, %217
  %225 = xor i32 %215, %218
  %226 = and i32 %224, %225
  %227 = icmp slt i32 %226, 0
  store i1 %222, i1* %az
  store i1 %223, i1* %cf
  store i1 %227, i1* %of
  %228 = icmp eq i32 %218, 0
  store i1 %228, i1* %zf
  %229 = icmp slt i32 %218, 0
  store i1 %229, i1* %sf
  %230 = trunc i32 %218 to i8
  %231 = call i8 @llvm.ctpop.i8(i8 %230)
  %232 = and i8 %231, 1
  %233 = icmp eq i8 %232, 0
  store i1 %233, i1* %pf
  store volatile i64 38333, i64* @assembly_address
  %234 = load i1* %cf
  %235 = icmp eq i1 %234, false
  br i1 %235, label %block_95e0, label %block_95bf

block_95bf:                                       ; preds = %block_95af
  store volatile i64 38335, i64* @assembly_address
  %236 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %237 = zext i32 %236 to i64
  store i64 %237, i64* %rax
  store volatile i64 38341, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 38344, i64* @assembly_address
  %238 = load i64* %rdx
  %239 = trunc i64 %238 to i32
  store i32 %239, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 38350, i64* @assembly_address
  %240 = load i64* %rax
  %241 = trunc i64 %240 to i32
  %242 = zext i32 %241 to i64
  store i64 %242, i64* %rdx
  store volatile i64 38352, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 38359, i64* @assembly_address
  %243 = load i64* %rdx
  %244 = load i64* %rax
  %245 = mul i64 %244, 1
  %246 = add i64 %243, %245
  %247 = inttoptr i64 %246 to i8*
  %248 = load i8* %247
  %249 = zext i8 %248 to i64
  store i64 %249, i64* %rax
  store volatile i64 38363, i64* @assembly_address
  %250 = load i64* %rax
  %251 = trunc i64 %250 to i8
  %252 = zext i8 %251 to i64
  store i64 %252, i64* %rax
  store volatile i64 38366, i64* @assembly_address
  br label %block_95f6

block_95e0:                                       ; preds = %block_95af
  store volatile i64 38368, i64* @assembly_address
  %253 = load i64** %stack_var_-32
  %254 = ptrtoint i64* %253 to i32
  %255 = zext i32 %254 to i64
  store i64 %255, i64* %rax
  store volatile i64 38371, i64* @assembly_address
  %256 = load i64* %rax
  %257 = trunc i64 %256 to i32
  store i32 %257, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 38377, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 38382, i64* @assembly_address
  %258 = load i64* %rdi
  %259 = trunc i64 %258 to i32
  %260 = call i64 @fill_inbuf(i32 %259)
  store i64 %260, i64* %rax
  store i64 %260, i64* %rax
  store volatile i64 38387, i64* @assembly_address
  %261 = load i64* %rax
  %262 = trunc i64 %261 to i8
  %263 = zext i8 %262 to i64
  store i64 %263, i64* %rax
  br label %block_95f6

block_95f6:                                       ; preds = %block_95e0, %block_95bf
  store volatile i64 38390, i64* @assembly_address
  %264 = load i64* %rbx
  %265 = trunc i64 %264 to i32
  %266 = zext i32 %265 to i64
  store i64 %266, i64* %rcx
  store volatile i64 38392, i64* @assembly_address
  %267 = load i64* %rax
  %268 = load i64* %rcx
  %269 = trunc i64 %268 to i8
  %270 = zext i8 %269 to i64
  %271 = and i64 %270, 63
  %272 = load i1* %of
  %273 = icmp eq i64 %271, 0
  br i1 %273, label %290, label %274

; <label>:274                                     ; preds = %block_95f6
  %275 = shl i64 %267, %271
  %276 = icmp eq i64 %275, 0
  store i1 %276, i1* %zf
  %277 = icmp slt i64 %275, 0
  store i1 %277, i1* %sf
  %278 = trunc i64 %275 to i8
  %279 = call i8 @llvm.ctpop.i8(i8 %278)
  %280 = and i8 %279, 1
  %281 = icmp eq i8 %280, 0
  store i1 %281, i1* %pf
  store i64 %275, i64* %rax
  %282 = sub i64 %271, 1
  %283 = shl i64 %267, %282
  %284 = lshr i64 %283, 63
  %285 = trunc i64 %284 to i1
  store i1 %285, i1* %cf
  %286 = lshr i64 %275, 63
  %287 = icmp ne i64 %286, %284
  %288 = icmp eq i64 %271, 1
  %289 = select i1 %288, i1 %287, i1 %272
  store i1 %289, i1* %of
  br label %290

; <label>:290                                     ; preds = %block_95f6, %274
  store volatile i64 38395, i64* @assembly_address
  %291 = load i64* %r12
  %292 = load i64* %rax
  %293 = or i64 %291, %292
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %294 = icmp eq i64 %293, 0
  store i1 %294, i1* %zf
  %295 = icmp slt i64 %293, 0
  store i1 %295, i1* %sf
  %296 = trunc i64 %293 to i8
  %297 = call i8 @llvm.ctpop.i8(i8 %296)
  %298 = and i8 %297, 1
  %299 = icmp eq i8 %298, 0
  store i1 %299, i1* %pf
  store i64 %293, i64* %r12
  store volatile i64 38398, i64* @assembly_address
  %300 = load i64* %rbx
  %301 = trunc i64 %300 to i32
  %302 = add i32 %301, 8
  %303 = and i32 %301, 15
  %304 = add i32 %303, 8
  %305 = icmp ugt i32 %304, 15
  %306 = icmp ult i32 %302, %301
  %307 = xor i32 %301, %302
  %308 = xor i32 8, %302
  %309 = and i32 %307, %308
  %310 = icmp slt i32 %309, 0
  store i1 %305, i1* %az
  store i1 %306, i1* %cf
  store i1 %310, i1* %of
  %311 = icmp eq i32 %302, 0
  store i1 %311, i1* %zf
  %312 = icmp slt i32 %302, 0
  store i1 %312, i1* %sf
  %313 = trunc i32 %302 to i8
  %314 = call i8 @llvm.ctpop.i8(i8 %313)
  %315 = and i8 %314, 1
  %316 = icmp eq i8 %315, 0
  store i1 %316, i1* %pf
  %317 = zext i32 %302 to i64
  store i64 %317, i64* %rbx
  br label %block_9601

block_9601:                                       ; preds = %290, %block_9599
  store volatile i64 38401, i64* @assembly_address
  %318 = load i64* %rbx
  %319 = trunc i64 %318 to i32
  %320 = sub i32 %319, 1
  %321 = and i32 %319, 15
  %322 = sub i32 %321, 1
  %323 = icmp ugt i32 %322, 15
  %324 = icmp ult i32 %319, 1
  %325 = xor i32 %319, 1
  %326 = xor i32 %319, %320
  %327 = and i32 %325, %326
  %328 = icmp slt i32 %327, 0
  store i1 %323, i1* %az
  store i1 %324, i1* %cf
  store i1 %328, i1* %of
  %329 = icmp eq i32 %320, 0
  store i1 %329, i1* %zf
  %330 = icmp slt i32 %320, 0
  store i1 %330, i1* %sf
  %331 = trunc i32 %320 to i8
  %332 = call i8 @llvm.ctpop.i8(i8 %331)
  %333 = and i8 %332, 1
  %334 = icmp eq i8 %333, 0
  store i1 %334, i1* %pf
  store volatile i64 38404, i64* @assembly_address
  %335 = load i1* %cf
  %336 = load i1* %zf
  %337 = or i1 %335, %336
  br i1 %337, label %block_95af, label %block_9606

block_9606:                                       ; preds = %block_9601
  store volatile i64 38406, i64* @assembly_address
  %338 = load i64* %r12
  %339 = trunc i64 %338 to i32
  %340 = zext i32 %339 to i64
  store i64 %340, i64* %rax
  store volatile i64 38409, i64* @assembly_address
  %341 = load i64* %rax
  %342 = trunc i64 %341 to i32
  %343 = and i32 %342, 3
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %344 = icmp eq i32 %343, 0
  store i1 %344, i1* %zf
  %345 = icmp slt i32 %343, 0
  store i1 %345, i1* %sf
  %346 = trunc i32 %343 to i8
  %347 = call i8 @llvm.ctpop.i8(i8 %346)
  %348 = and i8 %347, 1
  %349 = icmp eq i8 %348, 0
  store i1 %349, i1* %pf
  %350 = zext i32 %343 to i64
  store i64 %350, i64* %rax
  store volatile i64 38412, i64* @assembly_address
  %351 = load i64* %rax
  %352 = trunc i64 %351 to i32
  store i32 %352, i32* %stack_var_-28
  store volatile i64 38415, i64* @assembly_address
  %353 = load i64* %r12
  %354 = load i1* %of
  %355 = lshr i64 %353, 2
  %356 = icmp eq i64 %355, 0
  store i1 %356, i1* %zf
  %357 = icmp slt i64 %355, 0
  store i1 %357, i1* %sf
  %358 = trunc i64 %355 to i8
  %359 = call i8 @llvm.ctpop.i8(i8 %358)
  %360 = and i8 %359, 1
  %361 = icmp eq i8 %360, 0
  store i1 %361, i1* %pf
  store i64 %355, i64* %r12
  %362 = and i64 2, %353
  %363 = icmp ne i64 %362, 0
  store i1 %363, i1* %cf
  %364 = icmp slt i64 %353, 0
  %365 = select i1 false, i1 %364, i1 %354
  store i1 %365, i1* %of
  store volatile i64 38419, i64* @assembly_address
  %366 = load i64* %rbx
  %367 = trunc i64 %366 to i32
  %368 = sub i32 %367, 2
  %369 = and i32 %367, 15
  %370 = sub i32 %369, 2
  %371 = icmp ugt i32 %370, 15
  %372 = icmp ult i32 %367, 2
  %373 = xor i32 %367, 2
  %374 = xor i32 %367, %368
  %375 = and i32 %373, %374
  %376 = icmp slt i32 %375, 0
  store i1 %371, i1* %az
  store i1 %372, i1* %cf
  store i1 %376, i1* %of
  %377 = icmp eq i32 %368, 0
  store i1 %377, i1* %zf
  %378 = icmp slt i32 %368, 0
  store i1 %378, i1* %sf
  %379 = trunc i32 %368 to i8
  %380 = call i8 @llvm.ctpop.i8(i8 %379)
  %381 = and i8 %380, 1
  %382 = icmp eq i8 %381, 0
  store i1 %382, i1* %pf
  %383 = zext i32 %368 to i64
  store i64 %383, i64* %rbx
  store volatile i64 38422, i64* @assembly_address
  %384 = load i64* %r12
  store i64 %384, i64* @global_var_216f98
  store volatile i64 38429, i64* @assembly_address
  %385 = load i64* %rbx
  %386 = trunc i64 %385 to i32
  store i32 %386, i32* bitcast (i64* @global_var_216fa0 to i32*)
  store volatile i64 38435, i64* @assembly_address
  %387 = load i32* %stack_var_-28
  %388 = sub i32 %387, 2
  %389 = and i32 %387, 15
  %390 = sub i32 %389, 2
  %391 = icmp ugt i32 %390, 15
  %392 = icmp ult i32 %387, 2
  %393 = xor i32 %387, 2
  %394 = xor i32 %387, %388
  %395 = and i32 %393, %394
  %396 = icmp slt i32 %395, 0
  store i1 %391, i1* %az
  store i1 %392, i1* %cf
  store i1 %396, i1* %of
  %397 = icmp eq i32 %388, 0
  store i1 %397, i1* %zf
  %398 = icmp slt i32 %388, 0
  store i1 %398, i1* %sf
  %399 = trunc i32 %388 to i8
  %400 = call i8 @llvm.ctpop.i8(i8 %399)
  %401 = and i8 %400, 1
  %402 = icmp eq i8 %401, 0
  store i1 %402, i1* %pf
  store volatile i64 38439, i64* @assembly_address
  %403 = load i1* %zf
  %404 = icmp eq i1 %403, false
  br i1 %404, label %block_9630, label %block_9629

block_9629:                                       ; preds = %block_9606
  store volatile i64 38441, i64* @assembly_address
  %405 = call i64 @inflate_dynamic()
  store i64 %405, i64* %rax
  store i64 %405, i64* %rax
  store i64 %405, i64* %rax
  store volatile i64 38446, i64* @assembly_address
  br label %block_964f

block_9630:                                       ; preds = %block_9606
  store volatile i64 38448, i64* @assembly_address
  %406 = load i32* %stack_var_-28
  %407 = and i32 %406, 15
  %408 = icmp ugt i32 %407, 15
  %409 = icmp ult i32 %406, 0
  %410 = xor i32 %406, 0
  %411 = and i32 %410, 0
  %412 = icmp slt i32 %411, 0
  store i1 %408, i1* %az
  store i1 %409, i1* %cf
  store i1 %412, i1* %of
  %413 = icmp eq i32 %406, 0
  store i1 %413, i1* %zf
  %414 = icmp slt i32 %406, 0
  store i1 %414, i1* %sf
  %415 = trunc i32 %406 to i8
  %416 = call i8 @llvm.ctpop.i8(i8 %415)
  %417 = and i8 %416, 1
  %418 = icmp eq i8 %417, 0
  store i1 %418, i1* %pf
  store volatile i64 38452, i64* @assembly_address
  %419 = load i1* %zf
  %420 = icmp eq i1 %419, false
  br i1 %420, label %block_963d, label %block_9636

block_9636:                                       ; preds = %block_9630
  store volatile i64 38454, i64* @assembly_address
  %421 = call i64 @inflate_stored()
  store i64 %421, i64* %rax
  store i64 %421, i64* %rax
  store i64 %421, i64* %rax
  store volatile i64 38459, i64* @assembly_address
  br label %block_964f

block_963d:                                       ; preds = %block_9630
  store volatile i64 38461, i64* @assembly_address
  %422 = load i32* %stack_var_-28
  %423 = sub i32 %422, 1
  %424 = and i32 %422, 15
  %425 = sub i32 %424, 1
  %426 = icmp ugt i32 %425, 15
  %427 = icmp ult i32 %422, 1
  %428 = xor i32 %422, 1
  %429 = xor i32 %422, %423
  %430 = and i32 %428, %429
  %431 = icmp slt i32 %430, 0
  store i1 %426, i1* %az
  store i1 %427, i1* %cf
  store i1 %431, i1* %of
  %432 = icmp eq i32 %423, 0
  store i1 %432, i1* %zf
  %433 = icmp slt i32 %423, 0
  store i1 %433, i1* %sf
  %434 = trunc i32 %423 to i8
  %435 = call i8 @llvm.ctpop.i8(i8 %434)
  %436 = and i8 %435, 1
  %437 = icmp eq i8 %436, 0
  store i1 %437, i1* %pf
  store volatile i64 38465, i64* @assembly_address
  %438 = load i1* %zf
  %439 = icmp eq i1 %438, false
  br i1 %439, label %block_964a, label %block_9643

block_9643:                                       ; preds = %block_963d
  store volatile i64 38467, i64* @assembly_address
  %440 = call i64 @inflate_fixed()
  store i64 %440, i64* %rax
  store i64 %440, i64* %rax
  store i64 %440, i64* %rax
  store volatile i64 38472, i64* @assembly_address
  br label %block_964f

block_964a:                                       ; preds = %block_963d
  store volatile i64 38474, i64* @assembly_address
  store i64 2, i64* %rax
  br label %block_964f

block_964f:                                       ; preds = %block_964a, %block_9643, %block_9636, %block_9629
  store volatile i64 38479, i64* @assembly_address
  %441 = load i64* %rsp
  %442 = add i64 %441, 32
  %443 = and i64 %441, 15
  %444 = icmp ugt i64 %443, 15
  %445 = icmp ult i64 %442, %441
  %446 = xor i64 %441, %442
  %447 = xor i64 32, %442
  %448 = and i64 %446, %447
  %449 = icmp slt i64 %448, 0
  store i1 %444, i1* %az
  store i1 %445, i1* %cf
  store i1 %449, i1* %of
  %450 = icmp eq i64 %442, 0
  store i1 %450, i1* %zf
  %451 = icmp slt i64 %442, 0
  store i1 %451, i1* %sf
  %452 = trunc i64 %442 to i8
  %453 = call i8 @llvm.ctpop.i8(i8 %452)
  %454 = and i8 %453, 1
  %455 = icmp eq i8 %454, 0
  store i1 %455, i1* %pf
  %456 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %456, i64* %rsp
  store volatile i64 38483, i64* @assembly_address
  %457 = load i64* %stack_var_-24
  store i64 %457, i64* %rbx
  %458 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %458, i64* %rsp
  store volatile i64 38484, i64* @assembly_address
  %459 = load i64* %stack_var_-16
  store i64 %459, i64* %r12
  %460 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %460, i64* %rsp
  store volatile i64 38486, i64* @assembly_address
  %461 = load i64* %stack_var_-8
  store i64 %461, i64* %rbp
  %462 = ptrtoint i64* %stack_var_0 to i64
  store i64 %462, i64* %rsp
  store volatile i64 38487, i64* @assembly_address
  %463 = load i64* %rax
  ret i64 %463
}

declare i64 @200(i64*)

define i32 @inflate(%z_stream_s* %strm, i32 %flush) {
block_9658:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %flush to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint %z_stream_s* %strm to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-20 = alloca i32
  %stack_var_-28 = alloca i32
  %2 = alloca i64
  %stack_var_-24 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 38488, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 38489, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 38492, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 38496, i64* @assembly_address
  %22 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %22, i64* %rax
  store volatile i64 38505, i64* @assembly_address
  %23 = load i64* %rax
  store i64 %23, i64* %stack_var_-16
  store volatile i64 38509, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %24 = icmp eq i32 0, 0
  store i1 %24, i1* %zf
  %25 = icmp slt i32 0, 0
  store i1 %25, i1* %sf
  %26 = trunc i32 0 to i8
  %27 = call i8 @llvm.ctpop.i8(i8 %26)
  %28 = and i8 %27, 1
  %29 = icmp eq i8 %28, 0
  store i1 %29, i1* %pf
  %30 = zext i32 0 to i64
  store i64 %30, i64* %rax
  store volatile i64 38511, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 38521, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216fa0 to i32*)
  store volatile i64 38531, i64* @assembly_address
  store i64 0, i64* @global_var_216f98
  store volatile i64 38542, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  br label %block_9695

block_9695:                                       ; preds = %block_96cd, %block_9658
  store volatile i64 38549, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_216fa4 to i32*)
  store volatile i64 38559, i64* @assembly_address
  %31 = ptrtoint i32* %stack_var_-28 to i64
  store i64 %31, i64* %rax
  store volatile i64 38563, i64* @assembly_address
  %32 = ptrtoint i32* %stack_var_-28 to i64
  store i64 %32, i64* %rdi
  store volatile i64 38566, i64* @assembly_address
  %33 = load i64* %rdi
  %34 = inttoptr i64 %33 to i64*
  %35 = bitcast i64* %34 to i32*
  %36 = call i64 @inflate_block(i32* %35)
  store i64 %36, i64* %rax
  store i64 %36, i64* %rax
  store volatile i64 38571, i64* @assembly_address
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i32
  store i32 %38, i32* %stack_var_-20
  store volatile i64 38574, i64* @assembly_address
  %39 = load i32* %stack_var_-20
  %40 = and i32 %39, 15
  %41 = icmp ugt i32 %40, 15
  %42 = icmp ult i32 %39, 0
  %43 = xor i32 %39, 0
  %44 = and i32 %43, 0
  %45 = icmp slt i32 %44, 0
  store i1 %41, i1* %az
  store i1 %42, i1* %cf
  store i1 %45, i1* %of
  %46 = icmp eq i32 %39, 0
  store i1 %46, i1* %zf
  %47 = icmp slt i32 %39, 0
  store i1 %47, i1* %sf
  %48 = trunc i32 %39 to i8
  %49 = call i8 @llvm.ctpop.i8(i8 %48)
  %50 = and i8 %49, 1
  %51 = icmp eq i8 %50, 0
  store i1 %51, i1* %pf
  store volatile i64 38578, i64* @assembly_address
  %52 = load i1* %zf
  br i1 %52, label %block_96b9, label %block_96b4

block_96b4:                                       ; preds = %block_9695
  store volatile i64 38580, i64* @assembly_address
  %53 = load i32* %stack_var_-20
  %54 = zext i32 %53 to i64
  store i64 %54, i64* %rax
  store volatile i64 38583, i64* @assembly_address
  br label %block_9715

block_96b9:                                       ; preds = %block_9695
  store volatile i64 38585, i64* @assembly_address
  %55 = load i32* bitcast (i64* @global_var_216fa4 to i32*)
  %56 = zext i32 %55 to i64
  store i64 %56, i64* %rax
  store volatile i64 38591, i64* @assembly_address
  %57 = load i32* %stack_var_-24
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i32
  %60 = sub i32 %57, %59
  %61 = and i32 %57, 15
  %62 = and i32 %59, 15
  %63 = sub i32 %61, %62
  %64 = icmp ugt i32 %63, 15
  %65 = icmp ult i32 %57, %59
  %66 = xor i32 %57, %59
  %67 = xor i32 %57, %60
  %68 = and i32 %66, %67
  %69 = icmp slt i32 %68, 0
  store i1 %64, i1* %az
  store i1 %65, i1* %cf
  store i1 %69, i1* %of
  %70 = icmp eq i32 %60, 0
  store i1 %70, i1* %zf
  %71 = icmp slt i32 %60, 0
  store i1 %71, i1* %sf
  %72 = trunc i32 %60 to i8
  %73 = call i8 @llvm.ctpop.i8(i8 %72)
  %74 = and i8 %73, 1
  %75 = icmp eq i8 %74, 0
  store i1 %75, i1* %pf
  store volatile i64 38594, i64* @assembly_address
  %76 = load i1* %cf
  %77 = icmp eq i1 %76, false
  br i1 %77, label %block_96cd, label %block_96c4

block_96c4:                                       ; preds = %block_96b9
  store volatile i64 38596, i64* @assembly_address
  %78 = load i32* bitcast (i64* @global_var_216fa4 to i32*)
  %79 = zext i32 %78 to i64
  store i64 %79, i64* %rax
  store volatile i64 38602, i64* @assembly_address
  %80 = load i64* %rax
  %81 = trunc i64 %80 to i32
  store i32 %81, i32* %stack_var_-24
  br label %block_96cd

block_96cd:                                       ; preds = %block_96c4, %block_96b9
  store volatile i64 38605, i64* @assembly_address
  %82 = load i32* %stack_var_-28
  %83 = sext i32 %82 to i64
  %84 = trunc i64 %83 to i32
  %85 = zext i32 %84 to i64
  store i64 %85, i64* %rax
  store volatile i64 38608, i64* @assembly_address
  %86 = load i64* %rax
  %87 = trunc i64 %86 to i32
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  %90 = and i32 %87, %89
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %91 = icmp eq i32 %90, 0
  store i1 %91, i1* %zf
  %92 = icmp slt i32 %90, 0
  store i1 %92, i1* %sf
  %93 = trunc i32 %90 to i8
  %94 = call i8 @llvm.ctpop.i8(i8 %93)
  %95 = and i8 %94, 1
  %96 = icmp eq i8 %95, 0
  store i1 %96, i1* %pf
  store volatile i64 38610, i64* @assembly_address
  %97 = load i1* %zf
  br i1 %97, label %block_9695, label %block_96d4

block_96d4:                                       ; preds = %block_96cd
  store volatile i64 38612, i64* @assembly_address
  br label %block_96f4

block_96d6:                                       ; preds = %block_96f4
  store volatile i64 38614, i64* @assembly_address
  %98 = load i32* bitcast (i64* @global_var_216fa0 to i32*)
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rax
  store volatile i64 38620, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i32
  %102 = sub i32 %101, 8
  %103 = and i32 %101, 15
  %104 = sub i32 %103, 8
  %105 = icmp ugt i32 %104, 15
  %106 = icmp ult i32 %101, 8
  %107 = xor i32 %101, 8
  %108 = xor i32 %101, %102
  %109 = and i32 %107, %108
  %110 = icmp slt i32 %109, 0
  store i1 %105, i1* %az
  store i1 %106, i1* %cf
  store i1 %110, i1* %of
  %111 = icmp eq i32 %102, 0
  store i1 %111, i1* %zf
  %112 = icmp slt i32 %102, 0
  store i1 %112, i1* %sf
  %113 = trunc i32 %102 to i8
  %114 = call i8 @llvm.ctpop.i8(i8 %113)
  %115 = and i8 %114, 1
  %116 = icmp eq i8 %115, 0
  store i1 %116, i1* %pf
  %117 = zext i32 %102 to i64
  store i64 %117, i64* %rax
  store volatile i64 38623, i64* @assembly_address
  %118 = load i64* %rax
  %119 = trunc i64 %118 to i32
  store i32 %119, i32* bitcast (i64* @global_var_216fa0 to i32*)
  store volatile i64 38629, i64* @assembly_address
  %120 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %121 = zext i32 %120 to i64
  store i64 %121, i64* %rax
  store volatile i64 38635, i64* @assembly_address
  %122 = load i64* %rax
  %123 = trunc i64 %122 to i32
  %124 = sub i32 %123, 1
  %125 = and i32 %123, 15
  %126 = sub i32 %125, 1
  %127 = icmp ugt i32 %126, 15
  %128 = icmp ult i32 %123, 1
  %129 = xor i32 %123, 1
  %130 = xor i32 %123, %124
  %131 = and i32 %129, %130
  %132 = icmp slt i32 %131, 0
  store i1 %127, i1* %az
  store i1 %128, i1* %cf
  store i1 %132, i1* %of
  %133 = icmp eq i32 %124, 0
  store i1 %133, i1* %zf
  %134 = icmp slt i32 %124, 0
  store i1 %134, i1* %sf
  %135 = trunc i32 %124 to i8
  %136 = call i8 @llvm.ctpop.i8(i8 %135)
  %137 = and i8 %136, 1
  %138 = icmp eq i8 %137, 0
  store i1 %138, i1* %pf
  %139 = zext i32 %124 to i64
  store i64 %139, i64* %rax
  store volatile i64 38638, i64* @assembly_address
  %140 = load i64* %rax
  %141 = trunc i64 %140 to i32
  store i32 %141, i32* bitcast (i64* @global_var_24a884 to i32*)
  br label %block_96f4

block_96f4:                                       ; preds = %block_96d6, %block_96d4
  store volatile i64 38644, i64* @assembly_address
  %142 = load i32* bitcast (i64* @global_var_216fa0 to i32*)
  %143 = zext i32 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 38650, i64* @assembly_address
  %144 = load i64* %rax
  %145 = trunc i64 %144 to i32
  %146 = sub i32 %145, 7
  %147 = and i32 %145, 15
  %148 = sub i32 %147, 7
  %149 = icmp ugt i32 %148, 15
  %150 = icmp ult i32 %145, 7
  %151 = xor i32 %145, 7
  %152 = xor i32 %145, %146
  %153 = and i32 %151, %152
  %154 = icmp slt i32 %153, 0
  store i1 %149, i1* %az
  store i1 %150, i1* %cf
  store i1 %154, i1* %of
  %155 = icmp eq i32 %146, 0
  store i1 %155, i1* %zf
  %156 = icmp slt i32 %146, 0
  store i1 %156, i1* %sf
  %157 = trunc i32 %146 to i8
  %158 = call i8 @llvm.ctpop.i8(i8 %157)
  %159 = and i8 %158, 1
  %160 = icmp eq i8 %159, 0
  store i1 %160, i1* %pf
  store volatile i64 38653, i64* @assembly_address
  %161 = load i1* %cf
  %162 = load i1* %zf
  %163 = or i1 %161, %162
  %164 = icmp ne i1 %163, true
  br i1 %164, label %block_96d6, label %block_96ff

block_96ff:                                       ; preds = %block_96f4
  store volatile i64 38655, i64* @assembly_address
  %165 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %166 = zext i32 %165 to i64
  store i64 %166, i64* %rax
  store volatile i64 38661, i64* @assembly_address
  %167 = load i64* %rax
  %168 = trunc i64 %167 to i32
  store i32 %168, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 38667, i64* @assembly_address
  %169 = call i64 @flush_window()
  store i64 %169, i64* %rax
  store i64 %169, i64* %rax
  store i64 %169, i64* %rax
  store volatile i64 38672, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_9715

block_9715:                                       ; preds = %block_96ff, %block_96b4
  store volatile i64 38677, i64* @assembly_address
  %170 = load i64* %stack_var_-16
  store i64 %170, i64* %rdx
  store volatile i64 38681, i64* @assembly_address
  %171 = load i64* %rdx
  %172 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %173 = xor i64 %171, %172
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %174 = icmp eq i64 %173, 0
  store i1 %174, i1* %zf
  %175 = icmp slt i64 %173, 0
  store i1 %175, i1* %sf
  %176 = trunc i64 %173 to i8
  %177 = call i8 @llvm.ctpop.i8(i8 %176)
  %178 = and i8 %177, 1
  %179 = icmp eq i8 %178, 0
  store i1 %179, i1* %pf
  store i64 %173, i64* %rdx
  store volatile i64 38690, i64* @assembly_address
  %180 = load i1* %zf
  br i1 %180, label %block_9729, label %block_9724

block_9724:                                       ; preds = %block_9715
  store volatile i64 38692, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_9729:                                       ; preds = %block_9715
  store volatile i64 38697, i64* @assembly_address
  %181 = load i64* %stack_var_-8
  store i64 %181, i64* %rbp
  %182 = ptrtoint i64* %stack_var_0 to i64
  store i64 %182, i64* %rsp
  store volatile i64 38698, i64* @assembly_address
  %183 = load i64* %rax
  %184 = trunc i64 %183 to i32
  ret i32 %184
}

define i64 @lzw(i32 %arg1, i64 %arg2) {
block_972b:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i32
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 38699, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 38700, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 38703, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 38707, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-12
  store volatile i64 38710, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-16
  store volatile i64 38713, i64* @assembly_address
  %24 = load i32* bitcast (i64* @global_var_216fa8 to i32*)
  %25 = zext i32 %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 38719, i64* @assembly_address
  %26 = load i64* %rax
  %27 = trunc i64 %26 to i32
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  %30 = and i32 %27, %29
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %31 = icmp eq i32 %30, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i32 %30, 0
  store i1 %32, i1* %sf
  %33 = trunc i32 %30 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  store volatile i64 38721, i64* @assembly_address
  %37 = load i1* %zf
  br i1 %37, label %block_974a, label %block_9743

block_9743:                                       ; preds = %block_972b
  store volatile i64 38723, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 38728, i64* @assembly_address
  br label %block_978b

block_974a:                                       ; preds = %block_972b
  store volatile i64 38730, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_216fa8 to i32*)
  store volatile i64 38740, i64* @assembly_address
  %38 = load i64* @global_var_216580
  store i64 %38, i64* %rax
  store volatile i64 38747, i64* @assembly_address
  %39 = load i64* %rax
  store i64 %39, i64* %rcx
  store volatile i64 38750, i64* @assembly_address
  store i64 43, i64* %rdx
  store volatile i64 38755, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 38760, i64* @assembly_address
  store i64 ptrtoint ([44 x i8]* @global_var_12090 to i64), i64* %rdi
  store volatile i64 38767, i64* @assembly_address
  %40 = load i64* %rdi
  %41 = inttoptr i64 %40 to i64*
  %42 = load i64* %rsi
  %43 = trunc i64 %42 to i32
  %44 = load i64* %rdx
  %45 = trunc i64 %44 to i32
  %46 = load i64* %rcx
  %47 = inttoptr i64 %46 to %_IO_FILE*
  %48 = call i32 @fwrite(i64* %41, i32 %43, i32 %45, %_IO_FILE* %47)
  %49 = sext i32 %48 to i64
  store i64 %49, i64* %rax
  %50 = sext i32 %48 to i64
  store i64 %50, i64* %rax
  store volatile i64 38772, i64* @assembly_address
  %51 = load i32* %stack_var_-12
  %52 = zext i32 %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 38775, i64* @assembly_address
  %53 = load i64* %rax
  %54 = trunc i64 %53 to i32
  %55 = load i32* %stack_var_-16
  %56 = sub i32 %54, %55
  %57 = and i32 %54, 15
  %58 = and i32 %55, 15
  %59 = sub i32 %57, %58
  %60 = icmp ugt i32 %59, 15
  %61 = icmp ult i32 %54, %55
  %62 = xor i32 %54, %55
  %63 = xor i32 %54, %56
  %64 = and i32 %62, %63
  %65 = icmp slt i32 %64, 0
  store i1 %60, i1* %az
  store i1 %61, i1* %cf
  store i1 %65, i1* %of
  %66 = icmp eq i32 %56, 0
  store i1 %66, i1* %zf
  %67 = icmp slt i32 %56, 0
  store i1 %67, i1* %sf
  %68 = trunc i32 %56 to i8
  %69 = call i8 @llvm.ctpop.i8(i8 %68)
  %70 = and i8 %69, 1
  %71 = icmp eq i8 %70, 0
  store i1 %71, i1* %pf
  store volatile i64 38778, i64* @assembly_address
  %72 = load i1* %zf
  br i1 %72, label %block_9786, label %block_977c

block_977c:                                       ; preds = %block_974a
  store volatile i64 38780, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_9786

block_9786:                                       ; preds = %block_977c, %block_974a
  store volatile i64 38790, i64* @assembly_address
  store i64 1, i64* %rax
  br label %block_978b

block_978b:                                       ; preds = %block_9786, %block_9743
  store volatile i64 38795, i64* @assembly_address
  %73 = load i64* %stack_var_-8
  store i64 %73, i64* %rbp
  %74 = ptrtoint i64* %stack_var_0 to i64
  store i64 %74, i64* %rsp
  store volatile i64 38796, i64* @assembly_address
  %75 = load i64* %rax
  ret i64 %75
}

declare i64 @201(i64, i32)

declare i64 @202(i64, i64)

define i64 @ct_init(i16* %arg1, i8* %arg2) {
block_978d:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i16* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-24 = alloca i32
  %stack_var_-12 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-48 = alloca i16*
  %2 = alloca i64
  %stack_var_-56 = alloca i8*
  %3 = alloca i64
  %stack_var_-8 = alloca i64
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i32
  %9 = alloca i32
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i32
  %13 = alloca i32
  %14 = alloca i32
  %15 = alloca i32
  %16 = alloca i32
  %17 = alloca i32
  %18 = alloca i64
  %19 = alloca i32
  %20 = alloca i32
  %21 = alloca i32
  %22 = alloca i64
  %23 = alloca i32
  %24 = alloca i32
  %25 = alloca i32
  %26 = alloca i64
  %27 = alloca i32
  store volatile i64 38797, i64* @assembly_address
  %28 = load i64* %rbp
  store i64 %28, i64* %stack_var_-8
  %29 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %29, i64* %rsp
  store volatile i64 38798, i64* @assembly_address
  %30 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %30, i64* %rbp
  store volatile i64 38801, i64* @assembly_address
  %31 = load i64* %rsp
  %32 = sub i64 %31, 48
  %33 = and i64 %31, 15
  %34 = icmp ugt i64 %33, 15
  %35 = icmp ult i64 %31, 48
  %36 = xor i64 %31, 48
  %37 = xor i64 %31, %32
  %38 = and i64 %36, %37
  %39 = icmp slt i64 %38, 0
  store i1 %34, i1* %az
  store i1 %35, i1* %cf
  store i1 %39, i1* %of
  %40 = icmp eq i64 %32, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i64 %32, 0
  store i1 %41, i1* %sf
  %42 = trunc i64 %32 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  %46 = ptrtoint i8** %stack_var_-56 to i64
  store i64 %46, i64* %rsp
  store volatile i64 38805, i64* @assembly_address
  %47 = load i64* %rdi
  %48 = inttoptr i64 %47 to i16*
  store i16* %48, i16** %stack_var_-48
  store volatile i64 38809, i64* @assembly_address
  %49 = load i64* %rsi
  %50 = inttoptr i64 %49 to i8*
  store i8* %50, i8** %stack_var_-56
  store volatile i64 38813, i64* @assembly_address
  %51 = load i16** %stack_var_-48
  %52 = ptrtoint i16* %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 38817, i64* @assembly_address
  %53 = load i64* %rax
  store i64 %53, i64* @global_var_219ef0
  store volatile i64 38824, i64* @assembly_address
  %54 = load i8** %stack_var_-56
  %55 = ptrtoint i8* %54 to i64
  store i64 %55, i64* %rax
  store volatile i64 38828, i64* @assembly_address
  %56 = load i64* %rax
  store i64 %56, i64* @global_var_219ef8
  store volatile i64 38835, i64* @assembly_address
  store i64 0, i64* @global_var_219ee8
  store volatile i64 38846, i64* @assembly_address
  %57 = load i64* @global_var_219ee8
  store i64 %57, i64* %rax
  store volatile i64 38853, i64* @assembly_address
  %58 = load i64* %rax
  store i64 %58, i64* @global_var_219ee0
  store volatile i64 38860, i64* @assembly_address
  %59 = load i16* bitcast (i64* @global_var_217e42 to i16*)
  %60 = zext i16 %59 to i64
  store i64 %60, i64* %rax
  store volatile i64 38867, i64* @assembly_address
  %61 = load i64* %rax
  %62 = trunc i64 %61 to i16
  %63 = load i64* %rax
  %64 = trunc i64 %63 to i16
  %65 = and i16 %62, %64
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %66 = icmp eq i16 %65, 0
  store i1 %66, i1* %zf
  %67 = icmp slt i16 %65, 0
  store i1 %67, i1* %sf
  %68 = trunc i16 %65 to i8
  %69 = call i8 @llvm.ctpop.i8(i8 %68)
  %70 = and i8 %69, 1
  %71 = icmp eq i8 %70, 0
  store i1 %71, i1* %pf
  store volatile i64 38870, i64* @assembly_address
  %72 = load i1* %zf
  %73 = icmp eq i1 %72, false
  br i1 %73, label %block_9b13, label %block_97dc

block_97dc:                                       ; preds = %block_978d
  store volatile i64 38876, i64* @assembly_address
  store i32 0, i32* %stack_var_-20
  store volatile i64 38883, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 38890, i64* @assembly_address
  br label %block_9859

block_97ec:                                       ; preds = %block_9859
  store volatile i64 38892, i64* @assembly_address
  %74 = load i32* %stack_var_-16
  %75 = zext i32 %74 to i64
  store i64 %75, i64* %rax
  store volatile i64 38895, i64* @assembly_address
  %76 = load i64* %rax
  %77 = trunc i64 %76 to i32
  %78 = sext i32 %77 to i64
  store i64 %78, i64* %rax
  store volatile i64 38897, i64* @assembly_address
  %79 = load i64* %rax
  %80 = mul i64 %79, 4
  store i64 %80, i64* %rcx
  store volatile i64 38905, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218dc0 to i64), i64* %rax
  store volatile i64 38912, i64* @assembly_address
  %81 = load i32* %stack_var_-20
  %82 = zext i32 %81 to i64
  store i64 %82, i64* %rdx
  store volatile i64 38915, i64* @assembly_address
  %83 = load i64* %rdx
  %84 = trunc i64 %83 to i32
  %85 = load i64* %rcx
  %86 = load i64* %rax
  %87 = mul i64 %86, 1
  %88 = add i64 %85, %87
  %89 = inttoptr i64 %88 to i32*
  store i32 %84, i32* %89
  store volatile i64 38918, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 38925, i64* @assembly_address
  br label %block_982e

block_980f:                                       ; preds = %171
  store volatile i64 38927, i64* @assembly_address
  %90 = load i32* %stack_var_-20
  %91 = zext i32 %90 to i64
  store i64 %91, i64* %rax
  store volatile i64 38930, i64* @assembly_address
  %92 = load i64* %rax
  %93 = add i64 %92, 1
  %94 = trunc i64 %93 to i32
  %95 = zext i32 %94 to i64
  store i64 %95, i64* %rdx
  store volatile i64 38933, i64* @assembly_address
  %96 = load i64* %rdx
  %97 = trunc i64 %96 to i32
  store i32 %97, i32* %stack_var_-20
  store volatile i64 38936, i64* @assembly_address
  %98 = load i32* %stack_var_-16
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rdx
  store volatile i64 38939, i64* @assembly_address
  %100 = load i64* %rdx
  %101 = trunc i64 %100 to i32
  %102 = zext i32 %101 to i64
  store i64 %102, i64* %rcx
  store volatile i64 38941, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  %105 = sext i32 %104 to i64
  store i64 %105, i64* %rdx
  store volatile i64 38944, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218ac0 to i64), i64* %rax
  store volatile i64 38951, i64* @assembly_address
  %106 = load i64* %rcx
  %107 = trunc i64 %106 to i8
  %108 = load i64* %rdx
  %109 = load i64* %rax
  %110 = mul i64 %109, 1
  %111 = add i64 %108, %110
  %112 = inttoptr i64 %111 to i8*
  store i8 %107, i8* %112
  store volatile i64 38954, i64* @assembly_address
  %113 = load i32* %stack_var_-28
  %114 = add i32 %113, 1
  %115 = and i32 %113, 15
  %116 = add i32 %115, 1
  %117 = icmp ugt i32 %116, 15
  %118 = icmp ult i32 %114, %113
  %119 = xor i32 %113, %114
  %120 = xor i32 1, %114
  %121 = and i32 %119, %120
  %122 = icmp slt i32 %121, 0
  store i1 %117, i1* %az
  store i1 %118, i1* %cf
  store i1 %122, i1* %of
  %123 = icmp eq i32 %114, 0
  store i1 %123, i1* %zf
  %124 = icmp slt i32 %114, 0
  store i1 %124, i1* %sf
  %125 = trunc i32 %114 to i8
  %126 = call i8 @llvm.ctpop.i8(i8 %125)
  %127 = and i8 %126, 1
  %128 = icmp eq i8 %127, 0
  store i1 %128, i1* %pf
  store i32 %114, i32* %stack_var_-28
  br label %block_982e

block_982e:                                       ; preds = %block_980f, %block_97ec
  store volatile i64 38958, i64* @assembly_address
  %129 = load i32* %stack_var_-16
  %130 = zext i32 %129 to i64
  store i64 %130, i64* %rax
  store volatile i64 38961, i64* @assembly_address
  %131 = load i64* %rax
  %132 = trunc i64 %131 to i32
  %133 = sext i32 %132 to i64
  store i64 %133, i64* %rax
  store volatile i64 38963, i64* @assembly_address
  %134 = load i64* %rax
  %135 = mul i64 %134, 4
  store i64 %135, i64* %rdx
  store volatile i64 38971, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216320 to i64), i64* %rax
  store volatile i64 38978, i64* @assembly_address
  %136 = load i64* %rdx
  %137 = load i64* %rax
  %138 = mul i64 %137, 1
  %139 = add i64 %136, %138
  %140 = inttoptr i64 %139 to i32*
  %141 = load i32* %140
  %142 = zext i32 %141 to i64
  store i64 %142, i64* %rax
  store volatile i64 38981, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 38986, i64* @assembly_address
  %143 = load i64* %rax
  %144 = trunc i64 %143 to i32
  %145 = zext i32 %144 to i64
  store i64 %145, i64* %rcx
  store volatile i64 38988, i64* @assembly_address
  %146 = load i64* %rdx
  %147 = trunc i64 %146 to i32
  %148 = load i64* %rcx
  %149 = trunc i64 %148 to i8
  %150 = zext i8 %149 to i32
  %151 = and i32 %150, 31
  %152 = load i1* %of
  %153 = icmp eq i32 %151, 0
  br i1 %153, label %171, label %154

; <label>:154                                     ; preds = %block_982e
  %155 = shl i32 %147, %151
  %156 = icmp eq i32 %155, 0
  store i1 %156, i1* %zf
  %157 = icmp slt i32 %155, 0
  store i1 %157, i1* %sf
  %158 = trunc i32 %155 to i8
  %159 = call i8 @llvm.ctpop.i8(i8 %158)
  %160 = and i8 %159, 1
  %161 = icmp eq i8 %160, 0
  store i1 %161, i1* %pf
  %162 = zext i32 %155 to i64
  store i64 %162, i64* %rdx
  %163 = sub i32 %151, 1
  %164 = shl i32 %147, %163
  %165 = lshr i32 %164, 31
  %166 = trunc i32 %165 to i1
  store i1 %166, i1* %cf
  %167 = lshr i32 %155, 31
  %168 = icmp ne i32 %167, %165
  %169 = icmp eq i32 %151, 1
  %170 = select i1 %169, i1 %168, i1 %152
  store i1 %170, i1* %of
  br label %171

; <label>:171                                     ; preds = %block_982e, %154
  store volatile i64 38990, i64* @assembly_address
  %172 = load i64* %rdx
  %173 = trunc i64 %172 to i32
  %174 = zext i32 %173 to i64
  store i64 %174, i64* %rax
  store volatile i64 38992, i64* @assembly_address
  %175 = load i32* %stack_var_-28
  %176 = load i64* %rax
  %177 = trunc i64 %176 to i32
  store i32 %175, i32* %27
  store i64 %176, i64* %26
  %178 = sub i32 %175, %177
  %179 = and i32 %175, 15
  %180 = and i32 %177, 15
  %181 = sub i32 %179, %180
  %182 = icmp ugt i32 %181, 15
  %183 = icmp ult i32 %175, %177
  %184 = xor i32 %175, %177
  %185 = xor i32 %175, %178
  %186 = and i32 %184, %185
  %187 = icmp slt i32 %186, 0
  store i1 %182, i1* %az
  store i1 %183, i1* %cf
  store i1 %187, i1* %of
  %188 = icmp eq i32 %178, 0
  store i1 %188, i1* %zf
  %189 = icmp slt i32 %178, 0
  store i1 %189, i1* %sf
  %190 = trunc i32 %178 to i8
  %191 = call i8 @llvm.ctpop.i8(i8 %190)
  %192 = and i8 %191, 1
  %193 = icmp eq i8 %192, 0
  store i1 %193, i1* %pf
  store volatile i64 38995, i64* @assembly_address
  %194 = load i32* %27
  %195 = load i64* %26
  %196 = sext i32 %194 to i64
  %197 = icmp slt i64 %196, %195
  br i1 %197, label %block_980f, label %block_9855

block_9855:                                       ; preds = %171
  store volatile i64 38997, i64* @assembly_address
  %198 = load i32* %stack_var_-16
  %199 = add i32 %198, 1
  %200 = and i32 %198, 15
  %201 = add i32 %200, 1
  %202 = icmp ugt i32 %201, 15
  %203 = icmp ult i32 %199, %198
  %204 = xor i32 %198, %199
  %205 = xor i32 1, %199
  %206 = and i32 %204, %205
  %207 = icmp slt i32 %206, 0
  store i1 %202, i1* %az
  store i1 %203, i1* %cf
  store i1 %207, i1* %of
  %208 = icmp eq i32 %199, 0
  store i1 %208, i1* %zf
  %209 = icmp slt i32 %199, 0
  store i1 %209, i1* %sf
  %210 = trunc i32 %199 to i8
  %211 = call i8 @llvm.ctpop.i8(i8 %210)
  %212 = and i8 %211, 1
  %213 = icmp eq i8 %212, 0
  store i1 %213, i1* %pf
  store i32 %199, i32* %stack_var_-16
  br label %block_9859

block_9859:                                       ; preds = %block_9855, %block_97dc
  store volatile i64 39001, i64* @assembly_address
  %214 = load i32* %stack_var_-16
  store i32 %214, i32* %25
  store i32 27, i32* %24
  %215 = sub i32 %214, 27
  %216 = and i32 %214, 15
  %217 = sub i32 %216, 11
  %218 = icmp ugt i32 %217, 15
  %219 = icmp ult i32 %214, 27
  %220 = xor i32 %214, 27
  %221 = xor i32 %214, %215
  %222 = and i32 %220, %221
  %223 = icmp slt i32 %222, 0
  store i1 %218, i1* %az
  store i1 %219, i1* %cf
  store i1 %223, i1* %of
  %224 = icmp eq i32 %215, 0
  store i1 %224, i1* %zf
  %225 = icmp slt i32 %215, 0
  store i1 %225, i1* %sf
  %226 = trunc i32 %215 to i8
  %227 = call i8 @llvm.ctpop.i8(i8 %226)
  %228 = and i8 %227, 1
  %229 = icmp eq i8 %228, 0
  store i1 %229, i1* %pf
  store volatile i64 39005, i64* @assembly_address
  %230 = load i32* %25
  %231 = load i32* %24
  %232 = icmp sle i32 %230, %231
  br i1 %232, label %block_97ec, label %block_985f

block_985f:                                       ; preds = %block_9859
  store volatile i64 39007, i64* @assembly_address
  %233 = load i32* %stack_var_-20
  %234 = zext i32 %233 to i64
  store i64 %234, i64* %rax
  store volatile i64 39010, i64* @assembly_address
  %235 = load i64* %rax
  %236 = add i64 %235, -1
  %237 = trunc i64 %236 to i32
  %238 = zext i32 %237 to i64
  store i64 %238, i64* %rdx
  store volatile i64 39013, i64* @assembly_address
  %239 = load i32* %stack_var_-16
  %240 = zext i32 %239 to i64
  store i64 %240, i64* %rax
  store volatile i64 39016, i64* @assembly_address
  %241 = load i64* %rax
  %242 = trunc i64 %241 to i32
  %243 = zext i32 %242 to i64
  store i64 %243, i64* %rcx
  store volatile i64 39018, i64* @assembly_address
  %244 = load i64* %rdx
  %245 = trunc i64 %244 to i32
  %246 = sext i32 %245 to i64
  store i64 %246, i64* %rdx
  store volatile i64 39021, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218ac0 to i64), i64* %rax
  store volatile i64 39028, i64* @assembly_address
  %247 = load i64* %rcx
  %248 = trunc i64 %247 to i8
  %249 = load i64* %rdx
  %250 = load i64* %rax
  %251 = mul i64 %250, 1
  %252 = add i64 %249, %251
  %253 = inttoptr i64 %252 to i8*
  store i8 %248, i8* %253
  store volatile i64 39031, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 39038, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 39045, i64* @assembly_address
  br label %block_98f4

block_9887:                                       ; preds = %block_98f4
  store volatile i64 39047, i64* @assembly_address
  %254 = load i32* %stack_var_-16
  %255 = zext i32 %254 to i64
  store i64 %255, i64* %rax
  store volatile i64 39050, i64* @assembly_address
  %256 = load i64* %rax
  %257 = trunc i64 %256 to i32
  %258 = sext i32 %257 to i64
  store i64 %258, i64* %rax
  store volatile i64 39052, i64* @assembly_address
  %259 = load i64* %rax
  %260 = mul i64 %259, 4
  store i64 %260, i64* %rcx
  store volatile i64 39060, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218e40 to i64), i64* %rax
  store volatile i64 39067, i64* @assembly_address
  %261 = load i32* %stack_var_-12
  %262 = zext i32 %261 to i64
  store i64 %262, i64* %rdx
  store volatile i64 39070, i64* @assembly_address
  %263 = load i64* %rdx
  %264 = trunc i64 %263 to i32
  %265 = load i64* %rcx
  %266 = load i64* %rax
  %267 = mul i64 %266, 1
  %268 = add i64 %265, %267
  %269 = inttoptr i64 %268 to i32*
  store i32 %264, i32* %269
  store volatile i64 39073, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 39080, i64* @assembly_address
  br label %block_98c9

block_98aa:                                       ; preds = %351
  store volatile i64 39082, i64* @assembly_address
  %270 = load i32* %stack_var_-12
  %271 = zext i32 %270 to i64
  store i64 %271, i64* %rax
  store volatile i64 39085, i64* @assembly_address
  %272 = load i64* %rax
  %273 = add i64 %272, 1
  %274 = trunc i64 %273 to i32
  %275 = zext i32 %274 to i64
  store i64 %275, i64* %rdx
  store volatile i64 39088, i64* @assembly_address
  %276 = load i64* %rdx
  %277 = trunc i64 %276 to i32
  store i32 %277, i32* %stack_var_-12
  store volatile i64 39091, i64* @assembly_address
  %278 = load i32* %stack_var_-16
  %279 = zext i32 %278 to i64
  store i64 %279, i64* %rdx
  store volatile i64 39094, i64* @assembly_address
  %280 = load i64* %rdx
  %281 = trunc i64 %280 to i32
  %282 = zext i32 %281 to i64
  store i64 %282, i64* %rcx
  store volatile i64 39096, i64* @assembly_address
  %283 = load i64* %rax
  %284 = trunc i64 %283 to i32
  %285 = sext i32 %284 to i64
  store i64 %285, i64* %rdx
  store volatile i64 39099, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218bc0 to i64), i64* %rax
  store volatile i64 39106, i64* @assembly_address
  %286 = load i64* %rcx
  %287 = trunc i64 %286 to i8
  %288 = load i64* %rdx
  %289 = load i64* %rax
  %290 = mul i64 %289, 1
  %291 = add i64 %288, %290
  %292 = inttoptr i64 %291 to i8*
  store i8 %287, i8* %292
  store volatile i64 39109, i64* @assembly_address
  %293 = load i32* %stack_var_-28
  %294 = add i32 %293, 1
  %295 = and i32 %293, 15
  %296 = add i32 %295, 1
  %297 = icmp ugt i32 %296, 15
  %298 = icmp ult i32 %294, %293
  %299 = xor i32 %293, %294
  %300 = xor i32 1, %294
  %301 = and i32 %299, %300
  %302 = icmp slt i32 %301, 0
  store i1 %297, i1* %az
  store i1 %298, i1* %cf
  store i1 %302, i1* %of
  %303 = icmp eq i32 %294, 0
  store i1 %303, i1* %zf
  %304 = icmp slt i32 %294, 0
  store i1 %304, i1* %sf
  %305 = trunc i32 %294 to i8
  %306 = call i8 @llvm.ctpop.i8(i8 %305)
  %307 = and i8 %306, 1
  %308 = icmp eq i8 %307, 0
  store i1 %308, i1* %pf
  store i32 %294, i32* %stack_var_-28
  br label %block_98c9

block_98c9:                                       ; preds = %block_98aa, %block_9887
  store volatile i64 39113, i64* @assembly_address
  %309 = load i32* %stack_var_-16
  %310 = zext i32 %309 to i64
  store i64 %310, i64* %rax
  store volatile i64 39116, i64* @assembly_address
  %311 = load i64* %rax
  %312 = trunc i64 %311 to i32
  %313 = sext i32 %312 to i64
  store i64 %313, i64* %rax
  store volatile i64 39118, i64* @assembly_address
  %314 = load i64* %rax
  %315 = mul i64 %314, 4
  store i64 %315, i64* %rdx
  store volatile i64 39126, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2163a0 to i64), i64* %rax
  store volatile i64 39133, i64* @assembly_address
  %316 = load i64* %rdx
  %317 = load i64* %rax
  %318 = mul i64 %317, 1
  %319 = add i64 %316, %318
  %320 = inttoptr i64 %319 to i32*
  %321 = load i32* %320
  %322 = zext i32 %321 to i64
  store i64 %322, i64* %rax
  store volatile i64 39136, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 39141, i64* @assembly_address
  %323 = load i64* %rax
  %324 = trunc i64 %323 to i32
  %325 = zext i32 %324 to i64
  store i64 %325, i64* %rcx
  store volatile i64 39143, i64* @assembly_address
  %326 = load i64* %rdx
  %327 = trunc i64 %326 to i32
  %328 = load i64* %rcx
  %329 = trunc i64 %328 to i8
  %330 = zext i8 %329 to i32
  %331 = and i32 %330, 31
  %332 = load i1* %of
  %333 = icmp eq i32 %331, 0
  br i1 %333, label %351, label %334

; <label>:334                                     ; preds = %block_98c9
  %335 = shl i32 %327, %331
  %336 = icmp eq i32 %335, 0
  store i1 %336, i1* %zf
  %337 = icmp slt i32 %335, 0
  store i1 %337, i1* %sf
  %338 = trunc i32 %335 to i8
  %339 = call i8 @llvm.ctpop.i8(i8 %338)
  %340 = and i8 %339, 1
  %341 = icmp eq i8 %340, 0
  store i1 %341, i1* %pf
  %342 = zext i32 %335 to i64
  store i64 %342, i64* %rdx
  %343 = sub i32 %331, 1
  %344 = shl i32 %327, %343
  %345 = lshr i32 %344, 31
  %346 = trunc i32 %345 to i1
  store i1 %346, i1* %cf
  %347 = lshr i32 %335, 31
  %348 = icmp ne i32 %347, %345
  %349 = icmp eq i32 %331, 1
  %350 = select i1 %349, i1 %348, i1 %332
  store i1 %350, i1* %of
  br label %351

; <label>:351                                     ; preds = %block_98c9, %334
  store volatile i64 39145, i64* @assembly_address
  %352 = load i64* %rdx
  %353 = trunc i64 %352 to i32
  %354 = zext i32 %353 to i64
  store i64 %354, i64* %rax
  store volatile i64 39147, i64* @assembly_address
  %355 = load i32* %stack_var_-28
  %356 = load i64* %rax
  %357 = trunc i64 %356 to i32
  store i32 %355, i32* %23
  store i64 %356, i64* %22
  %358 = sub i32 %355, %357
  %359 = and i32 %355, 15
  %360 = and i32 %357, 15
  %361 = sub i32 %359, %360
  %362 = icmp ugt i32 %361, 15
  %363 = icmp ult i32 %355, %357
  %364 = xor i32 %355, %357
  %365 = xor i32 %355, %358
  %366 = and i32 %364, %365
  %367 = icmp slt i32 %366, 0
  store i1 %362, i1* %az
  store i1 %363, i1* %cf
  store i1 %367, i1* %of
  %368 = icmp eq i32 %358, 0
  store i1 %368, i1* %zf
  %369 = icmp slt i32 %358, 0
  store i1 %369, i1* %sf
  %370 = trunc i32 %358 to i8
  %371 = call i8 @llvm.ctpop.i8(i8 %370)
  %372 = and i8 %371, 1
  %373 = icmp eq i8 %372, 0
  store i1 %373, i1* %pf
  store volatile i64 39150, i64* @assembly_address
  %374 = load i32* %23
  %375 = load i64* %22
  %376 = sext i32 %374 to i64
  %377 = icmp slt i64 %376, %375
  br i1 %377, label %block_98aa, label %block_98f0

block_98f0:                                       ; preds = %351
  store volatile i64 39152, i64* @assembly_address
  %378 = load i32* %stack_var_-16
  %379 = add i32 %378, 1
  %380 = and i32 %378, 15
  %381 = add i32 %380, 1
  %382 = icmp ugt i32 %381, 15
  %383 = icmp ult i32 %379, %378
  %384 = xor i32 %378, %379
  %385 = xor i32 1, %379
  %386 = and i32 %384, %385
  %387 = icmp slt i32 %386, 0
  store i1 %382, i1* %az
  store i1 %383, i1* %cf
  store i1 %387, i1* %of
  %388 = icmp eq i32 %379, 0
  store i1 %388, i1* %zf
  %389 = icmp slt i32 %379, 0
  store i1 %389, i1* %sf
  %390 = trunc i32 %379 to i8
  %391 = call i8 @llvm.ctpop.i8(i8 %390)
  %392 = and i8 %391, 1
  %393 = icmp eq i8 %392, 0
  store i1 %393, i1* %pf
  store i32 %379, i32* %stack_var_-16
  br label %block_98f4

block_98f4:                                       ; preds = %block_98f0, %block_985f
  store volatile i64 39156, i64* @assembly_address
  %394 = load i32* %stack_var_-16
  store i32 %394, i32* %21
  store i32 15, i32* %20
  %395 = sub i32 %394, 15
  %396 = and i32 %394, 15
  %397 = sub i32 %396, 15
  %398 = icmp ugt i32 %397, 15
  %399 = icmp ult i32 %394, 15
  %400 = xor i32 %394, 15
  %401 = xor i32 %394, %395
  %402 = and i32 %400, %401
  %403 = icmp slt i32 %402, 0
  store i1 %398, i1* %az
  store i1 %399, i1* %cf
  store i1 %403, i1* %of
  %404 = icmp eq i32 %395, 0
  store i1 %404, i1* %zf
  %405 = icmp slt i32 %395, 0
  store i1 %405, i1* %sf
  %406 = trunc i32 %395 to i8
  %407 = call i8 @llvm.ctpop.i8(i8 %406)
  %408 = and i8 %407, 1
  %409 = icmp eq i8 %408, 0
  store i1 %409, i1* %pf
  store volatile i64 39160, i64* @assembly_address
  %410 = load i32* %21
  %411 = load i32* %20
  %412 = icmp sle i32 %410, %411
  br i1 %412, label %block_9887, label %block_98fa

block_98fa:                                       ; preds = %block_98f4
  store volatile i64 39162, i64* @assembly_address
  %413 = load i32* %stack_var_-12
  %414 = load i1* %of
  %415 = ashr i32 %413, 7
  %416 = icmp eq i32 %415, 0
  store i1 %416, i1* %zf
  %417 = icmp slt i32 %415, 0
  store i1 %417, i1* %sf
  %418 = trunc i32 %415 to i8
  %419 = call i8 @llvm.ctpop.i8(i8 %418)
  %420 = and i8 %419, 1
  %421 = icmp eq i8 %420, 0
  store i1 %421, i1* %pf
  store i32 %415, i32* %stack_var_-12
  %422 = and i32 64, %413
  %423 = icmp ne i32 %422, 0
  store i1 %423, i1* %cf
  %424 = select i1 false, i1 false, i1 %414
  store i1 %424, i1* %of
  store volatile i64 39166, i64* @assembly_address
  br label %block_997b

block_9900:                                       ; preds = %block_997b
  store volatile i64 39168, i64* @assembly_address
  %425 = load i32* %stack_var_-12
  %426 = zext i32 %425 to i64
  store i64 %426, i64* %rax
  store volatile i64 39171, i64* @assembly_address
  %427 = load i64* %rax
  %428 = trunc i64 %427 to i32
  %429 = load i1* %of
  %430 = shl i32 %428, 7
  %431 = icmp eq i32 %430, 0
  store i1 %431, i1* %zf
  %432 = icmp slt i32 %430, 0
  store i1 %432, i1* %sf
  %433 = trunc i32 %430 to i8
  %434 = call i8 @llvm.ctpop.i8(i8 %433)
  %435 = and i8 %434, 1
  %436 = icmp eq i8 %435, 0
  store i1 %436, i1* %pf
  %437 = zext i32 %430 to i64
  store i64 %437, i64* %rax
  %438 = shl i32 %428, 6
  %439 = lshr i32 %438, 31
  %440 = trunc i32 %439 to i1
  store i1 %440, i1* %cf
  %441 = lshr i32 %430, 31
  %442 = icmp ne i32 %441, %439
  %443 = select i1 false, i1 %442, i1 %429
  store i1 %443, i1* %of
  store volatile i64 39174, i64* @assembly_address
  %444 = load i64* %rax
  %445 = trunc i64 %444 to i32
  %446 = zext i32 %445 to i64
  store i64 %446, i64* %rcx
  store volatile i64 39176, i64* @assembly_address
  %447 = load i32* %stack_var_-16
  %448 = zext i32 %447 to i64
  store i64 %448, i64* %rax
  store volatile i64 39179, i64* @assembly_address
  %449 = load i64* %rax
  %450 = trunc i64 %449 to i32
  %451 = sext i32 %450 to i64
  store i64 %451, i64* %rax
  store volatile i64 39181, i64* @assembly_address
  %452 = load i64* %rax
  %453 = mul i64 %452, 4
  store i64 %453, i64* %rdx
  store volatile i64 39189, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218e40 to i64), i64* %rax
  store volatile i64 39196, i64* @assembly_address
  %454 = load i64* %rcx
  %455 = trunc i64 %454 to i32
  %456 = load i64* %rdx
  %457 = load i64* %rax
  %458 = mul i64 %457, 1
  %459 = add i64 %456, %458
  %460 = inttoptr i64 %459 to i32*
  store i32 %455, i32* %460
  store volatile i64 39199, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 39206, i64* @assembly_address
  br label %block_994d

block_9928:                                       ; preds = %564
  store volatile i64 39208, i64* @assembly_address
  %461 = load i32* %stack_var_-12
  %462 = zext i32 %461 to i64
  store i64 %462, i64* %rax
  store volatile i64 39211, i64* @assembly_address
  %463 = load i64* %rax
  %464 = add i64 %463, 1
  %465 = trunc i64 %464 to i32
  %466 = zext i32 %465 to i64
  store i64 %466, i64* %rdx
  store volatile i64 39214, i64* @assembly_address
  %467 = load i64* %rdx
  %468 = trunc i64 %467 to i32
  store i32 %468, i32* %stack_var_-12
  store volatile i64 39217, i64* @assembly_address
  %469 = load i64* %rax
  %470 = add i64 %469, 256
  %471 = trunc i64 %470 to i32
  %472 = zext i32 %471 to i64
  store i64 %472, i64* %rdx
  store volatile i64 39223, i64* @assembly_address
  %473 = load i32* %stack_var_-16
  %474 = zext i32 %473 to i64
  store i64 %474, i64* %rax
  store volatile i64 39226, i64* @assembly_address
  %475 = load i64* %rax
  %476 = trunc i64 %475 to i32
  %477 = zext i32 %476 to i64
  store i64 %477, i64* %rcx
  store volatile i64 39228, i64* @assembly_address
  %478 = load i64* %rdx
  %479 = trunc i64 %478 to i32
  %480 = sext i32 %479 to i64
  store i64 %480, i64* %rdx
  store volatile i64 39231, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218bc0 to i64), i64* %rax
  store volatile i64 39238, i64* @assembly_address
  %481 = load i64* %rcx
  %482 = trunc i64 %481 to i8
  %483 = load i64* %rdx
  %484 = load i64* %rax
  %485 = mul i64 %484, 1
  %486 = add i64 %483, %485
  %487 = inttoptr i64 %486 to i8*
  store i8 %482, i8* %487
  store volatile i64 39241, i64* @assembly_address
  %488 = load i32* %stack_var_-28
  %489 = add i32 %488, 1
  %490 = and i32 %488, 15
  %491 = add i32 %490, 1
  %492 = icmp ugt i32 %491, 15
  %493 = icmp ult i32 %489, %488
  %494 = xor i32 %488, %489
  %495 = xor i32 1, %489
  %496 = and i32 %494, %495
  %497 = icmp slt i32 %496, 0
  store i1 %492, i1* %az
  store i1 %493, i1* %cf
  store i1 %497, i1* %of
  %498 = icmp eq i32 %489, 0
  store i1 %498, i1* %zf
  %499 = icmp slt i32 %489, 0
  store i1 %499, i1* %sf
  %500 = trunc i32 %489 to i8
  %501 = call i8 @llvm.ctpop.i8(i8 %500)
  %502 = and i8 %501, 1
  %503 = icmp eq i8 %502, 0
  store i1 %503, i1* %pf
  store i32 %489, i32* %stack_var_-28
  br label %block_994d

block_994d:                                       ; preds = %block_9928, %block_9900
  store volatile i64 39245, i64* @assembly_address
  %504 = load i32* %stack_var_-16
  %505 = zext i32 %504 to i64
  store i64 %505, i64* %rax
  store volatile i64 39248, i64* @assembly_address
  %506 = load i64* %rax
  %507 = trunc i64 %506 to i32
  %508 = sext i32 %507 to i64
  store i64 %508, i64* %rax
  store volatile i64 39250, i64* @assembly_address
  %509 = load i64* %rax
  %510 = mul i64 %509, 4
  store i64 %510, i64* %rdx
  store volatile i64 39258, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2163a0 to i64), i64* %rax
  store volatile i64 39265, i64* @assembly_address
  %511 = load i64* %rdx
  %512 = load i64* %rax
  %513 = mul i64 %512, 1
  %514 = add i64 %511, %513
  %515 = inttoptr i64 %514 to i32*
  %516 = load i32* %515
  %517 = zext i32 %516 to i64
  store i64 %517, i64* %rax
  store volatile i64 39268, i64* @assembly_address
  %518 = load i64* %rax
  %519 = trunc i64 %518 to i32
  %520 = sub i32 %519, 7
  %521 = and i32 %519, 15
  %522 = sub i32 %521, 7
  %523 = icmp ugt i32 %522, 15
  %524 = icmp ult i32 %519, 7
  %525 = xor i32 %519, 7
  %526 = xor i32 %519, %520
  %527 = and i32 %525, %526
  %528 = icmp slt i32 %527, 0
  store i1 %523, i1* %az
  store i1 %524, i1* %cf
  store i1 %528, i1* %of
  %529 = icmp eq i32 %520, 0
  store i1 %529, i1* %zf
  %530 = icmp slt i32 %520, 0
  store i1 %530, i1* %sf
  %531 = trunc i32 %520 to i8
  %532 = call i8 @llvm.ctpop.i8(i8 %531)
  %533 = and i8 %532, 1
  %534 = icmp eq i8 %533, 0
  store i1 %534, i1* %pf
  %535 = zext i32 %520 to i64
  store i64 %535, i64* %rax
  store volatile i64 39271, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 39276, i64* @assembly_address
  %536 = load i64* %rax
  %537 = trunc i64 %536 to i32
  %538 = zext i32 %537 to i64
  store i64 %538, i64* %rcx
  store volatile i64 39278, i64* @assembly_address
  %539 = load i64* %rdx
  %540 = trunc i64 %539 to i32
  %541 = load i64* %rcx
  %542 = trunc i64 %541 to i8
  %543 = zext i8 %542 to i32
  %544 = and i32 %543, 31
  %545 = load i1* %of
  %546 = icmp eq i32 %544, 0
  br i1 %546, label %564, label %547

; <label>:547                                     ; preds = %block_994d
  %548 = shl i32 %540, %544
  %549 = icmp eq i32 %548, 0
  store i1 %549, i1* %zf
  %550 = icmp slt i32 %548, 0
  store i1 %550, i1* %sf
  %551 = trunc i32 %548 to i8
  %552 = call i8 @llvm.ctpop.i8(i8 %551)
  %553 = and i8 %552, 1
  %554 = icmp eq i8 %553, 0
  store i1 %554, i1* %pf
  %555 = zext i32 %548 to i64
  store i64 %555, i64* %rdx
  %556 = sub i32 %544, 1
  %557 = shl i32 %540, %556
  %558 = lshr i32 %557, 31
  %559 = trunc i32 %558 to i1
  store i1 %559, i1* %cf
  %560 = lshr i32 %548, 31
  %561 = icmp ne i32 %560, %558
  %562 = icmp eq i32 %544, 1
  %563 = select i1 %562, i1 %561, i1 %545
  store i1 %563, i1* %of
  br label %564

; <label>:564                                     ; preds = %block_994d, %547
  store volatile i64 39280, i64* @assembly_address
  %565 = load i64* %rdx
  %566 = trunc i64 %565 to i32
  %567 = zext i32 %566 to i64
  store i64 %567, i64* %rax
  store volatile i64 39282, i64* @assembly_address
  %568 = load i32* %stack_var_-28
  %569 = load i64* %rax
  %570 = trunc i64 %569 to i32
  store i32 %568, i32* %19
  store i64 %569, i64* %18
  %571 = sub i32 %568, %570
  %572 = and i32 %568, 15
  %573 = and i32 %570, 15
  %574 = sub i32 %572, %573
  %575 = icmp ugt i32 %574, 15
  %576 = icmp ult i32 %568, %570
  %577 = xor i32 %568, %570
  %578 = xor i32 %568, %571
  %579 = and i32 %577, %578
  %580 = icmp slt i32 %579, 0
  store i1 %575, i1* %az
  store i1 %576, i1* %cf
  store i1 %580, i1* %of
  %581 = icmp eq i32 %571, 0
  store i1 %581, i1* %zf
  %582 = icmp slt i32 %571, 0
  store i1 %582, i1* %sf
  %583 = trunc i32 %571 to i8
  %584 = call i8 @llvm.ctpop.i8(i8 %583)
  %585 = and i8 %584, 1
  %586 = icmp eq i8 %585, 0
  store i1 %586, i1* %pf
  store volatile i64 39285, i64* @assembly_address
  %587 = load i32* %19
  %588 = load i64* %18
  %589 = sext i32 %587 to i64
  %590 = icmp slt i64 %589, %588
  br i1 %590, label %block_9928, label %block_9977

block_9977:                                       ; preds = %564
  store volatile i64 39287, i64* @assembly_address
  %591 = load i32* %stack_var_-16
  %592 = add i32 %591, 1
  %593 = and i32 %591, 15
  %594 = add i32 %593, 1
  %595 = icmp ugt i32 %594, 15
  %596 = icmp ult i32 %592, %591
  %597 = xor i32 %591, %592
  %598 = xor i32 1, %592
  %599 = and i32 %597, %598
  %600 = icmp slt i32 %599, 0
  store i1 %595, i1* %az
  store i1 %596, i1* %cf
  store i1 %600, i1* %of
  %601 = icmp eq i32 %592, 0
  store i1 %601, i1* %zf
  %602 = icmp slt i32 %592, 0
  store i1 %602, i1* %sf
  %603 = trunc i32 %592 to i8
  %604 = call i8 @llvm.ctpop.i8(i8 %603)
  %605 = and i8 %604, 1
  %606 = icmp eq i8 %605, 0
  store i1 %606, i1* %pf
  store i32 %592, i32* %stack_var_-16
  br label %block_997b

block_997b:                                       ; preds = %block_9977, %block_98fa
  store volatile i64 39291, i64* @assembly_address
  %607 = load i32* %stack_var_-16
  store i32 %607, i32* %17
  store i32 29, i32* %16
  %608 = sub i32 %607, 29
  %609 = and i32 %607, 15
  %610 = sub i32 %609, 13
  %611 = icmp ugt i32 %610, 15
  %612 = icmp ult i32 %607, 29
  %613 = xor i32 %607, 29
  %614 = xor i32 %607, %608
  %615 = and i32 %613, %614
  %616 = icmp slt i32 %615, 0
  store i1 %611, i1* %az
  store i1 %612, i1* %cf
  store i1 %616, i1* %of
  %617 = icmp eq i32 %608, 0
  store i1 %617, i1* %zf
  %618 = icmp slt i32 %608, 0
  store i1 %618, i1* %sf
  %619 = trunc i32 %608 to i8
  %620 = call i8 @llvm.ctpop.i8(i8 %619)
  %621 = and i8 %620, 1
  %622 = icmp eq i8 %621, 0
  store i1 %622, i1* %pf
  store volatile i64 39295, i64* @assembly_address
  %623 = load i32* %17
  %624 = load i32* %16
  %625 = icmp sle i32 %623, %624
  br i1 %625, label %block_9900, label %block_9985

block_9985:                                       ; preds = %block_997b
  store volatile i64 39301, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 39308, i64* @assembly_address
  br label %block_99a8

block_998e:                                       ; preds = %block_99a8
  store volatile i64 39310, i64* @assembly_address
  %626 = load i32* %stack_var_-24
  %627 = zext i32 %626 to i64
  store i64 %627, i64* %rax
  store volatile i64 39313, i64* @assembly_address
  %628 = load i64* %rax
  %629 = trunc i64 %628 to i32
  %630 = sext i32 %629 to i64
  store i64 %630, i64* %rax
  store volatile i64 39315, i64* @assembly_address
  %631 = load i64* %rax
  %632 = load i64* %rax
  %633 = mul i64 %632, 1
  %634 = add i64 %631, %633
  store i64 %634, i64* %rdx
  store volatile i64 39319, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 39326, i64* @assembly_address
  %635 = load i64* %rdx
  %636 = load i64* %rax
  %637 = mul i64 %636, 1
  %638 = add i64 %635, %637
  %639 = inttoptr i64 %638 to i16*
  store i16 0, i16* %639
  store volatile i64 39332, i64* @assembly_address
  %640 = load i32* %stack_var_-24
  %641 = add i32 %640, 1
  %642 = and i32 %640, 15
  %643 = add i32 %642, 1
  %644 = icmp ugt i32 %643, 15
  %645 = icmp ult i32 %641, %640
  %646 = xor i32 %640, %641
  %647 = xor i32 1, %641
  %648 = and i32 %646, %647
  %649 = icmp slt i32 %648, 0
  store i1 %644, i1* %az
  store i1 %645, i1* %cf
  store i1 %649, i1* %of
  %650 = icmp eq i32 %641, 0
  store i1 %650, i1* %zf
  %651 = icmp slt i32 %641, 0
  store i1 %651, i1* %sf
  %652 = trunc i32 %641 to i8
  %653 = call i8 @llvm.ctpop.i8(i8 %652)
  %654 = and i8 %653, 1
  %655 = icmp eq i8 %654, 0
  store i1 %655, i1* %pf
  store i32 %641, i32* %stack_var_-24
  br label %block_99a8

block_99a8:                                       ; preds = %block_998e, %block_9985
  store volatile i64 39336, i64* @assembly_address
  %656 = load i32* %stack_var_-24
  store i32 %656, i32* %15
  store i32 15, i32* %14
  %657 = sub i32 %656, 15
  %658 = and i32 %656, 15
  %659 = sub i32 %658, 15
  %660 = icmp ugt i32 %659, 15
  %661 = icmp ult i32 %656, 15
  %662 = xor i32 %656, 15
  %663 = xor i32 %656, %657
  %664 = and i32 %662, %663
  %665 = icmp slt i32 %664, 0
  store i1 %660, i1* %az
  store i1 %661, i1* %cf
  store i1 %665, i1* %of
  %666 = icmp eq i32 %657, 0
  store i1 %666, i1* %zf
  %667 = icmp slt i32 %657, 0
  store i1 %667, i1* %sf
  %668 = trunc i32 %657 to i8
  %669 = call i8 @llvm.ctpop.i8(i8 %668)
  %670 = and i8 %669, 1
  %671 = icmp eq i8 %670, 0
  store i1 %671, i1* %pf
  store volatile i64 39340, i64* @assembly_address
  %672 = load i32* %15
  %673 = load i32* %14
  %674 = icmp sle i32 %672, %673
  br i1 %674, label %block_998e, label %block_99ae

block_99ae:                                       ; preds = %block_99a8
  store volatile i64 39342, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 39349, i64* @assembly_address
  br label %block_99e8

block_99b7:                                       ; preds = %block_99e8
  store volatile i64 39351, i64* @assembly_address
  %675 = load i32* %stack_var_-28
  %676 = zext i32 %675 to i64
  store i64 %676, i64* %rax
  store volatile i64 39354, i64* @assembly_address
  %677 = load i64* %rax
  %678 = add i64 %677, 1
  %679 = trunc i64 %678 to i32
  %680 = zext i32 %679 to i64
  store i64 %680, i64* %rdx
  store volatile i64 39357, i64* @assembly_address
  %681 = load i64* %rdx
  %682 = trunc i64 %681 to i32
  store i32 %682, i32* %stack_var_-28
  store volatile i64 39360, i64* @assembly_address
  %683 = load i64* %rax
  %684 = trunc i64 %683 to i32
  %685 = sext i32 %684 to i64
  store i64 %685, i64* %rax
  store volatile i64 39362, i64* @assembly_address
  %686 = load i64* %rax
  %687 = mul i64 %686, 4
  store i64 %687, i64* %rdx
  store volatile i64 39370, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2179c2 to i64), i64* %rax
  store volatile i64 39377, i64* @assembly_address
  %688 = load i64* %rdx
  %689 = load i64* %rax
  %690 = mul i64 %689, 1
  %691 = add i64 %688, %690
  %692 = inttoptr i64 %691 to i16*
  store i16 8, i16* %692
  store volatile i64 39383, i64* @assembly_address
  %693 = load i16* bitcast (i64* @global_var_217f70 to i16*)
  %694 = zext i16 %693 to i64
  store i64 %694, i64* %rax
  store volatile i64 39390, i64* @assembly_address
  %695 = load i64* %rax
  %696 = trunc i64 %695 to i32
  %697 = add i32 %696, 1
  %698 = and i32 %696, 15
  %699 = add i32 %698, 1
  %700 = icmp ugt i32 %699, 15
  %701 = icmp ult i32 %697, %696
  %702 = xor i32 %696, %697
  %703 = xor i32 1, %697
  %704 = and i32 %702, %703
  %705 = icmp slt i32 %704, 0
  store i1 %700, i1* %az
  store i1 %701, i1* %cf
  store i1 %705, i1* %of
  %706 = icmp eq i32 %697, 0
  store i1 %706, i1* %zf
  %707 = icmp slt i32 %697, 0
  store i1 %707, i1* %sf
  %708 = trunc i32 %697 to i8
  %709 = call i8 @llvm.ctpop.i8(i8 %708)
  %710 = and i8 %709, 1
  %711 = icmp eq i8 %710, 0
  store i1 %711, i1* %pf
  store i64 ptrtoint (i64* @global_var_217f71 to i64), i64* %rax
  store volatile i64 39393, i64* @assembly_address
  %712 = load i64* %rax
  %713 = trunc i64 %712 to i16
  store i16 %713, i16* bitcast (i64* @global_var_217f70 to i16*)
  br label %block_99e8

block_99e8:                                       ; preds = %block_99b7, %block_99ae
  store volatile i64 39400, i64* @assembly_address
  %714 = load i32* %stack_var_-28
  store i32 %714, i32* %13
  store i32 143, i32* %12
  %715 = sub i32 %714, 143
  %716 = and i32 %714, 15
  %717 = sub i32 %716, 15
  %718 = icmp ugt i32 %717, 15
  %719 = icmp ult i32 %714, 143
  %720 = xor i32 %714, 143
  %721 = xor i32 %714, %715
  %722 = and i32 %720, %721
  %723 = icmp slt i32 %722, 0
  store i1 %718, i1* %az
  store i1 %719, i1* %cf
  store i1 %723, i1* %of
  %724 = icmp eq i32 %715, 0
  store i1 %724, i1* %zf
  %725 = icmp slt i32 %715, 0
  store i1 %725, i1* %sf
  %726 = trunc i32 %715 to i8
  %727 = call i8 @llvm.ctpop.i8(i8 %726)
  %728 = and i8 %727, 1
  %729 = icmp eq i8 %728, 0
  store i1 %729, i1* %pf
  store volatile i64 39407, i64* @assembly_address
  %730 = load i32* %13
  %731 = load i32* %12
  %732 = icmp sle i32 %730, %731
  br i1 %732, label %block_99b7, label %block_99f1

block_99f1:                                       ; preds = %block_99e8
  store volatile i64 39409, i64* @assembly_address
  br label %block_9a24

block_99f3:                                       ; preds = %block_9a24
  store volatile i64 39411, i64* @assembly_address
  %733 = load i32* %stack_var_-28
  %734 = zext i32 %733 to i64
  store i64 %734, i64* %rax
  store volatile i64 39414, i64* @assembly_address
  %735 = load i64* %rax
  %736 = add i64 %735, 1
  %737 = trunc i64 %736 to i32
  %738 = zext i32 %737 to i64
  store i64 %738, i64* %rdx
  store volatile i64 39417, i64* @assembly_address
  %739 = load i64* %rdx
  %740 = trunc i64 %739 to i32
  store i32 %740, i32* %stack_var_-28
  store volatile i64 39420, i64* @assembly_address
  %741 = load i64* %rax
  %742 = trunc i64 %741 to i32
  %743 = sext i32 %742 to i64
  store i64 %743, i64* %rax
  store volatile i64 39422, i64* @assembly_address
  %744 = load i64* %rax
  %745 = mul i64 %744, 4
  store i64 %745, i64* %rdx
  store volatile i64 39430, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2179c2 to i64), i64* %rax
  store volatile i64 39437, i64* @assembly_address
  %746 = load i64* %rdx
  %747 = load i64* %rax
  %748 = mul i64 %747, 1
  %749 = add i64 %746, %748
  %750 = inttoptr i64 %749 to i16*
  store i16 9, i16* %750
  store volatile i64 39443, i64* @assembly_address
  %751 = load i16* bitcast (i64* @global_var_217f72 to i16*)
  %752 = zext i16 %751 to i64
  store i64 %752, i64* %rax
  store volatile i64 39450, i64* @assembly_address
  %753 = load i64* %rax
  %754 = trunc i64 %753 to i32
  %755 = add i32 %754, 1
  %756 = and i32 %754, 15
  %757 = add i32 %756, 1
  %758 = icmp ugt i32 %757, 15
  %759 = icmp ult i32 %755, %754
  %760 = xor i32 %754, %755
  %761 = xor i32 1, %755
  %762 = and i32 %760, %761
  %763 = icmp slt i32 %762, 0
  store i1 %758, i1* %az
  store i1 %759, i1* %cf
  store i1 %763, i1* %of
  %764 = icmp eq i32 %755, 0
  store i1 %764, i1* %zf
  %765 = icmp slt i32 %755, 0
  store i1 %765, i1* %sf
  %766 = trunc i32 %755 to i8
  %767 = call i8 @llvm.ctpop.i8(i8 %766)
  %768 = and i8 %767, 1
  %769 = icmp eq i8 %768, 0
  store i1 %769, i1* %pf
  store i64 ptrtoint (i64* @global_var_217f73 to i64), i64* %rax
  store volatile i64 39453, i64* @assembly_address
  %770 = load i64* %rax
  %771 = trunc i64 %770 to i16
  store i16 %771, i16* bitcast (i64* @global_var_217f72 to i16*)
  br label %block_9a24

block_9a24:                                       ; preds = %block_99f3, %block_99f1
  store volatile i64 39460, i64* @assembly_address
  %772 = load i32* %stack_var_-28
  store i32 %772, i32* %11
  store i32 255, i32* %10
  %773 = sub i32 %772, 255
  %774 = and i32 %772, 15
  %775 = sub i32 %774, 15
  %776 = icmp ugt i32 %775, 15
  %777 = icmp ult i32 %772, 255
  %778 = xor i32 %772, 255
  %779 = xor i32 %772, %773
  %780 = and i32 %778, %779
  %781 = icmp slt i32 %780, 0
  store i1 %776, i1* %az
  store i1 %777, i1* %cf
  store i1 %781, i1* %of
  %782 = icmp eq i32 %773, 0
  store i1 %782, i1* %zf
  %783 = icmp slt i32 %773, 0
  store i1 %783, i1* %sf
  %784 = trunc i32 %773 to i8
  %785 = call i8 @llvm.ctpop.i8(i8 %784)
  %786 = and i8 %785, 1
  %787 = icmp eq i8 %786, 0
  store i1 %787, i1* %pf
  store volatile i64 39467, i64* @assembly_address
  %788 = load i32* %11
  %789 = load i32* %10
  %790 = icmp sle i32 %788, %789
  br i1 %790, label %block_99f3, label %block_9a2d

block_9a2d:                                       ; preds = %block_9a24
  store volatile i64 39469, i64* @assembly_address
  br label %block_9a60

block_9a2f:                                       ; preds = %block_9a60
  store volatile i64 39471, i64* @assembly_address
  %791 = load i32* %stack_var_-28
  %792 = zext i32 %791 to i64
  store i64 %792, i64* %rax
  store volatile i64 39474, i64* @assembly_address
  %793 = load i64* %rax
  %794 = add i64 %793, 1
  %795 = trunc i64 %794 to i32
  %796 = zext i32 %795 to i64
  store i64 %796, i64* %rdx
  store volatile i64 39477, i64* @assembly_address
  %797 = load i64* %rdx
  %798 = trunc i64 %797 to i32
  store i32 %798, i32* %stack_var_-28
  store volatile i64 39480, i64* @assembly_address
  %799 = load i64* %rax
  %800 = trunc i64 %799 to i32
  %801 = sext i32 %800 to i64
  store i64 %801, i64* %rax
  store volatile i64 39482, i64* @assembly_address
  %802 = load i64* %rax
  %803 = mul i64 %802, 4
  store i64 %803, i64* %rdx
  store volatile i64 39490, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2179c2 to i64), i64* %rax
  store volatile i64 39497, i64* @assembly_address
  %804 = load i64* %rdx
  %805 = load i64* %rax
  %806 = mul i64 %805, 1
  %807 = add i64 %804, %806
  %808 = inttoptr i64 %807 to i16*
  store i16 7, i16* %808
  store volatile i64 39503, i64* @assembly_address
  %809 = load i16* bitcast (i64* @global_var_217f6e to i16*)
  %810 = zext i16 %809 to i64
  store i64 %810, i64* %rax
  store volatile i64 39510, i64* @assembly_address
  %811 = load i64* %rax
  %812 = trunc i64 %811 to i32
  %813 = add i32 %812, 1
  %814 = and i32 %812, 15
  %815 = add i32 %814, 1
  %816 = icmp ugt i32 %815, 15
  %817 = icmp ult i32 %813, %812
  %818 = xor i32 %812, %813
  %819 = xor i32 1, %813
  %820 = and i32 %818, %819
  %821 = icmp slt i32 %820, 0
  store i1 %816, i1* %az
  store i1 %817, i1* %cf
  store i1 %821, i1* %of
  %822 = icmp eq i32 %813, 0
  store i1 %822, i1* %zf
  %823 = icmp slt i32 %813, 0
  store i1 %823, i1* %sf
  %824 = trunc i32 %813 to i8
  %825 = call i8 @llvm.ctpop.i8(i8 %824)
  %826 = and i8 %825, 1
  %827 = icmp eq i8 %826, 0
  store i1 %827, i1* %pf
  store i64 ptrtoint (i64* @global_var_217f6f to i64), i64* %rax
  store volatile i64 39513, i64* @assembly_address
  %828 = load i64* %rax
  %829 = trunc i64 %828 to i16
  store i16 %829, i16* bitcast (i64* @global_var_217f6e to i16*)
  br label %block_9a60

block_9a60:                                       ; preds = %block_9a2f, %block_9a2d
  store volatile i64 39520, i64* @assembly_address
  %830 = load i32* %stack_var_-28
  store i32 %830, i32* %9
  store i32 279, i32* %8
  %831 = sub i32 %830, 279
  %832 = and i32 %830, 15
  %833 = sub i32 %832, 7
  %834 = icmp ugt i32 %833, 15
  %835 = icmp ult i32 %830, 279
  %836 = xor i32 %830, 279
  %837 = xor i32 %830, %831
  %838 = and i32 %836, %837
  %839 = icmp slt i32 %838, 0
  store i1 %834, i1* %az
  store i1 %835, i1* %cf
  store i1 %839, i1* %of
  %840 = icmp eq i32 %831, 0
  store i1 %840, i1* %zf
  %841 = icmp slt i32 %831, 0
  store i1 %841, i1* %sf
  %842 = trunc i32 %831 to i8
  %843 = call i8 @llvm.ctpop.i8(i8 %842)
  %844 = and i8 %843, 1
  %845 = icmp eq i8 %844, 0
  store i1 %845, i1* %pf
  store volatile i64 39527, i64* @assembly_address
  %846 = load i32* %9
  %847 = load i32* %8
  %848 = icmp sle i32 %846, %847
  br i1 %848, label %block_9a2f, label %block_9a69

block_9a69:                                       ; preds = %block_9a60
  store volatile i64 39529, i64* @assembly_address
  br label %block_9a9c

block_9a6b:                                       ; preds = %block_9a9c
  store volatile i64 39531, i64* @assembly_address
  %849 = load i32* %stack_var_-28
  %850 = zext i32 %849 to i64
  store i64 %850, i64* %rax
  store volatile i64 39534, i64* @assembly_address
  %851 = load i64* %rax
  %852 = add i64 %851, 1
  %853 = trunc i64 %852 to i32
  %854 = zext i32 %853 to i64
  store i64 %854, i64* %rdx
  store volatile i64 39537, i64* @assembly_address
  %855 = load i64* %rdx
  %856 = trunc i64 %855 to i32
  store i32 %856, i32* %stack_var_-28
  store volatile i64 39540, i64* @assembly_address
  %857 = load i64* %rax
  %858 = trunc i64 %857 to i32
  %859 = sext i32 %858 to i64
  store i64 %859, i64* %rax
  store volatile i64 39542, i64* @assembly_address
  %860 = load i64* %rax
  %861 = mul i64 %860, 4
  store i64 %861, i64* %rdx
  store volatile i64 39550, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2179c2 to i64), i64* %rax
  store volatile i64 39557, i64* @assembly_address
  %862 = load i64* %rdx
  %863 = load i64* %rax
  %864 = mul i64 %863, 1
  %865 = add i64 %862, %864
  %866 = inttoptr i64 %865 to i16*
  store i16 8, i16* %866
  store volatile i64 39563, i64* @assembly_address
  %867 = load i16* bitcast (i64* @global_var_217f70 to i16*)
  %868 = zext i16 %867 to i64
  store i64 %868, i64* %rax
  store volatile i64 39570, i64* @assembly_address
  %869 = load i64* %rax
  %870 = trunc i64 %869 to i32
  %871 = add i32 %870, 1
  %872 = and i32 %870, 15
  %873 = add i32 %872, 1
  %874 = icmp ugt i32 %873, 15
  %875 = icmp ult i32 %871, %870
  %876 = xor i32 %870, %871
  %877 = xor i32 1, %871
  %878 = and i32 %876, %877
  %879 = icmp slt i32 %878, 0
  store i1 %874, i1* %az
  store i1 %875, i1* %cf
  store i1 %879, i1* %of
  %880 = icmp eq i32 %871, 0
  store i1 %880, i1* %zf
  %881 = icmp slt i32 %871, 0
  store i1 %881, i1* %sf
  %882 = trunc i32 %871 to i8
  %883 = call i8 @llvm.ctpop.i8(i8 %882)
  %884 = and i8 %883, 1
  %885 = icmp eq i8 %884, 0
  store i1 %885, i1* %pf
  store i64 ptrtoint (i64* @global_var_217f71 to i64), i64* %rax
  store volatile i64 39573, i64* @assembly_address
  %886 = load i64* %rax
  %887 = trunc i64 %886 to i16
  store i16 %887, i16* bitcast (i64* @global_var_217f70 to i16*)
  br label %block_9a9c

block_9a9c:                                       ; preds = %block_9a6b, %block_9a69
  store volatile i64 39580, i64* @assembly_address
  %888 = load i32* %stack_var_-28
  store i32 %888, i32* %7
  store i32 287, i32* %6
  %889 = sub i32 %888, 287
  %890 = and i32 %888, 15
  %891 = sub i32 %890, 15
  %892 = icmp ugt i32 %891, 15
  %893 = icmp ult i32 %888, 287
  %894 = xor i32 %888, 287
  %895 = xor i32 %888, %889
  %896 = and i32 %894, %895
  %897 = icmp slt i32 %896, 0
  store i1 %892, i1* %az
  store i1 %893, i1* %cf
  store i1 %897, i1* %of
  %898 = icmp eq i32 %889, 0
  store i1 %898, i1* %zf
  %899 = icmp slt i32 %889, 0
  store i1 %899, i1* %sf
  %900 = trunc i32 %889 to i8
  %901 = call i8 @llvm.ctpop.i8(i8 %900)
  %902 = and i8 %901, 1
  %903 = icmp eq i8 %902, 0
  store i1 %903, i1* %pf
  store volatile i64 39587, i64* @assembly_address
  %904 = load i32* %7
  %905 = load i32* %6
  %906 = icmp sle i32 %904, %905
  br i1 %906, label %block_9a6b, label %block_9aa5

block_9aa5:                                       ; preds = %block_9a9c
  store volatile i64 39589, i64* @assembly_address
  store i64 287, i64* %rsi
  store volatile i64 39594, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2179c0 to i64), i64* %rdi
  store volatile i64 39601, i64* @assembly_address
  %907 = load i64* %rdi
  %908 = inttoptr i64 %907 to i64*
  %909 = load i64* %rsi
  %910 = bitcast i64* %908 to i16*
  %911 = call i64 @gen_codes(i16* %910, i64 %909)
  store i64 %911, i64* %rax
  store i64 %911, i64* %rax
  store volatile i64 39606, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 39613, i64* @assembly_address
  br label %block_9b06

block_9abf:                                       ; preds = %block_9b06
  store volatile i64 39615, i64* @assembly_address
  %912 = load i32* %stack_var_-28
  %913 = zext i32 %912 to i64
  store i64 %913, i64* %rax
  store volatile i64 39618, i64* @assembly_address
  %914 = load i64* %rax
  %915 = trunc i64 %914 to i32
  %916 = sext i32 %915 to i64
  store i64 %916, i64* %rax
  store volatile i64 39620, i64* @assembly_address
  %917 = load i64* %rax
  %918 = mul i64 %917, 4
  store i64 %918, i64* %rdx
  store volatile i64 39628, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217e42 to i64), i64* %rax
  store volatile i64 39635, i64* @assembly_address
  %919 = load i64* %rdx
  %920 = load i64* %rax
  %921 = mul i64 %920, 1
  %922 = add i64 %919, %921
  %923 = inttoptr i64 %922 to i16*
  store i16 5, i16* %923
  store volatile i64 39641, i64* @assembly_address
  %924 = load i32* %stack_var_-28
  %925 = zext i32 %924 to i64
  store i64 %925, i64* %rax
  store volatile i64 39644, i64* @assembly_address
  store i64 5, i64* %rsi
  store volatile i64 39649, i64* @assembly_address
  %926 = load i64* %rax
  %927 = trunc i64 %926 to i32
  %928 = zext i32 %927 to i64
  store i64 %928, i64* %rdi
  store volatile i64 39651, i64* @assembly_address
  %929 = load i64* %rdi
  %930 = load i64* %rsi
  %931 = trunc i64 %929 to i32
  %932 = call i64 @bi_reverse(i32 %931, i64 %930)
  store i64 %932, i64* %rax
  store i64 %932, i64* %rax
  store volatile i64 39656, i64* @assembly_address
  %933 = load i64* %rax
  %934 = trunc i64 %933 to i32
  %935 = zext i32 %934 to i64
  store i64 %935, i64* %rcx
  store volatile i64 39658, i64* @assembly_address
  %936 = load i32* %stack_var_-28
  %937 = zext i32 %936 to i64
  store i64 %937, i64* %rax
  store volatile i64 39661, i64* @assembly_address
  %938 = load i64* %rax
  %939 = trunc i64 %938 to i32
  %940 = sext i32 %939 to i64
  store i64 %940, i64* %rax
  store volatile i64 39663, i64* @assembly_address
  %941 = load i64* %rax
  %942 = mul i64 %941, 4
  store i64 %942, i64* %rdx
  store volatile i64 39671, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217e40 to i64), i64* %rax
  store volatile i64 39678, i64* @assembly_address
  %943 = load i64* %rcx
  %944 = trunc i64 %943 to i16
  %945 = load i64* %rdx
  %946 = load i64* %rax
  %947 = mul i64 %946, 1
  %948 = add i64 %945, %947
  %949 = inttoptr i64 %948 to i16*
  store i16 %944, i16* %949
  store volatile i64 39682, i64* @assembly_address
  %950 = load i32* %stack_var_-28
  %951 = add i32 %950, 1
  %952 = and i32 %950, 15
  %953 = add i32 %952, 1
  %954 = icmp ugt i32 %953, 15
  %955 = icmp ult i32 %951, %950
  %956 = xor i32 %950, %951
  %957 = xor i32 1, %951
  %958 = and i32 %956, %957
  %959 = icmp slt i32 %958, 0
  store i1 %954, i1* %az
  store i1 %955, i1* %cf
  store i1 %959, i1* %of
  %960 = icmp eq i32 %951, 0
  store i1 %960, i1* %zf
  %961 = icmp slt i32 %951, 0
  store i1 %961, i1* %sf
  %962 = trunc i32 %951 to i8
  %963 = call i8 @llvm.ctpop.i8(i8 %962)
  %964 = and i8 %963, 1
  %965 = icmp eq i8 %964, 0
  store i1 %965, i1* %pf
  store i32 %951, i32* %stack_var_-28
  br label %block_9b06

block_9b06:                                       ; preds = %block_9abf, %block_9aa5
  store volatile i64 39686, i64* @assembly_address
  %966 = load i32* %stack_var_-28
  store i32 %966, i32* %5
  store i32 29, i32* %4
  %967 = sub i32 %966, 29
  %968 = and i32 %966, 15
  %969 = sub i32 %968, 13
  %970 = icmp ugt i32 %969, 15
  %971 = icmp ult i32 %966, 29
  %972 = xor i32 %966, 29
  %973 = xor i32 %966, %967
  %974 = and i32 %972, %973
  %975 = icmp slt i32 %974, 0
  store i1 %970, i1* %az
  store i1 %971, i1* %cf
  store i1 %975, i1* %of
  %976 = icmp eq i32 %967, 0
  store i1 %976, i1* %zf
  %977 = icmp slt i32 %967, 0
  store i1 %977, i1* %sf
  %978 = trunc i32 %967 to i8
  %979 = call i8 @llvm.ctpop.i8(i8 %978)
  %980 = and i8 %979, 1
  %981 = icmp eq i8 %980, 0
  store i1 %981, i1* %pf
  store volatile i64 39690, i64* @assembly_address
  %982 = load i32* %5
  %983 = load i32* %4
  %984 = icmp sle i32 %982, %983
  br i1 %984, label %block_9abf, label %block_9b0c

block_9b0c:                                       ; preds = %block_9b06
  store volatile i64 39692, i64* @assembly_address
  %985 = call i64 @init_block()
  store i64 %985, i64* %rax
  store i64 %985, i64* %rax
  store i64 %985, i64* %rax
  store volatile i64 39697, i64* @assembly_address
  br label %block_9b14

block_9b13:                                       ; preds = %block_978d
  store volatile i64 39699, i64* @assembly_address
  br label %block_9b14

block_9b14:                                       ; preds = %block_9b13, %block_9b0c
  store volatile i64 39700, i64* @assembly_address
  %986 = load i64* %stack_var_-8
  store i64 %986, i64* %rbp
  %987 = ptrtoint i64* %stack_var_0 to i64
  store i64 %987, i64* %rsp
  store volatile i64 39701, i64* @assembly_address
  %988 = load i64* %rax
  ret i64 %988
}

define i64 @init_block() {
block_9b16:
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-8 = alloca i64
  %0 = alloca i32
  %1 = alloca i32
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i32
  store volatile i64 39702, i64* @assembly_address
  %6 = load i64* %rbp
  store i64 %6, i64* %stack_var_-8
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rsp
  store volatile i64 39703, i64* @assembly_address
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rbp
  store volatile i64 39706, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 39713, i64* @assembly_address
  br label %block_9b41

block_9b23:                                       ; preds = %block_9b41
  store volatile i64 39715, i64* @assembly_address
  %9 = load i32* %stack_var_-12
  %10 = zext i32 %9 to i64
  store i64 %10, i64* %rax
  store volatile i64 39718, i64* @assembly_address
  %11 = load i64* %rax
  %12 = trunc i64 %11 to i32
  %13 = sext i32 %12 to i64
  store i64 %13, i64* %rax
  store volatile i64 39720, i64* @assembly_address
  %14 = load i64* %rax
  %15 = mul i64 %14, 4
  store i64 %15, i64* %rdx
  store volatile i64 39728, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rax
  store volatile i64 39735, i64* @assembly_address
  %16 = load i64* %rdx
  %17 = load i64* %rax
  %18 = mul i64 %17, 1
  %19 = add i64 %16, %18
  %20 = inttoptr i64 %19 to i16*
  store i16 0, i16* %20
  store volatile i64 39741, i64* @assembly_address
  %21 = load i32* %stack_var_-12
  %22 = add i32 %21, 1
  %23 = and i32 %21, 15
  %24 = add i32 %23, 1
  %25 = icmp ugt i32 %24, 15
  %26 = icmp ult i32 %22, %21
  %27 = xor i32 %21, %22
  %28 = xor i32 1, %22
  %29 = and i32 %27, %28
  %30 = icmp slt i32 %29, 0
  store i1 %25, i1* %az
  store i1 %26, i1* %cf
  store i1 %30, i1* %of
  %31 = icmp eq i32 %22, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i32 %22, 0
  store i1 %32, i1* %sf
  %33 = trunc i32 %22 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  store i32 %22, i32* %stack_var_-12
  br label %block_9b41

block_9b41:                                       ; preds = %block_9b23, %block_9b16
  store volatile i64 39745, i64* @assembly_address
  %37 = load i32* %stack_var_-12
  store i32 %37, i32* %5
  store i32 285, i32* %4
  %38 = sub i32 %37, 285
  %39 = and i32 %37, 15
  %40 = sub i32 %39, 13
  %41 = icmp ugt i32 %40, 15
  %42 = icmp ult i32 %37, 285
  %43 = xor i32 %37, 285
  %44 = xor i32 %37, %38
  %45 = and i32 %43, %44
  %46 = icmp slt i32 %45, 0
  store i1 %41, i1* %az
  store i1 %42, i1* %cf
  store i1 %46, i1* %of
  %47 = icmp eq i32 %38, 0
  store i1 %47, i1* %zf
  %48 = icmp slt i32 %38, 0
  store i1 %48, i1* %sf
  %49 = trunc i32 %38 to i8
  %50 = call i8 @llvm.ctpop.i8(i8 %49)
  %51 = and i8 %50, 1
  %52 = icmp eq i8 %51, 0
  store i1 %52, i1* %pf
  store volatile i64 39752, i64* @assembly_address
  %53 = load i32* %5
  %54 = load i32* %4
  %55 = icmp sle i32 %53, %54
  br i1 %55, label %block_9b23, label %block_9b4a

block_9b4a:                                       ; preds = %block_9b41
  store volatile i64 39754, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 39761, i64* @assembly_address
  br label %block_9b71

block_9b53:                                       ; preds = %block_9b71
  store volatile i64 39763, i64* @assembly_address
  %56 = load i32* %stack_var_-12
  %57 = zext i32 %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 39766, i64* @assembly_address
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i32
  %60 = sext i32 %59 to i64
  store i64 %60, i64* %rax
  store volatile i64 39768, i64* @assembly_address
  %61 = load i64* %rax
  %62 = mul i64 %61, 4
  store i64 %62, i64* %rdx
  store volatile i64 39776, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2178c0 to i64), i64* %rax
  store volatile i64 39783, i64* @assembly_address
  %63 = load i64* %rdx
  %64 = load i64* %rax
  %65 = mul i64 %64, 1
  %66 = add i64 %63, %65
  %67 = inttoptr i64 %66 to i16*
  store i16 0, i16* %67
  store volatile i64 39789, i64* @assembly_address
  %68 = load i32* %stack_var_-12
  %69 = add i32 %68, 1
  %70 = and i32 %68, 15
  %71 = add i32 %70, 1
  %72 = icmp ugt i32 %71, 15
  %73 = icmp ult i32 %69, %68
  %74 = xor i32 %68, %69
  %75 = xor i32 1, %69
  %76 = and i32 %74, %75
  %77 = icmp slt i32 %76, 0
  store i1 %72, i1* %az
  store i1 %73, i1* %cf
  store i1 %77, i1* %of
  %78 = icmp eq i32 %69, 0
  store i1 %78, i1* %zf
  %79 = icmp slt i32 %69, 0
  store i1 %79, i1* %sf
  %80 = trunc i32 %69 to i8
  %81 = call i8 @llvm.ctpop.i8(i8 %80)
  %82 = and i8 %81, 1
  %83 = icmp eq i8 %82, 0
  store i1 %83, i1* %pf
  store i32 %69, i32* %stack_var_-12
  br label %block_9b71

block_9b71:                                       ; preds = %block_9b53, %block_9b4a
  store volatile i64 39793, i64* @assembly_address
  %84 = load i32* %stack_var_-12
  store i32 %84, i32* %3
  store i32 29, i32* %2
  %85 = sub i32 %84, 29
  %86 = and i32 %84, 15
  %87 = sub i32 %86, 13
  %88 = icmp ugt i32 %87, 15
  %89 = icmp ult i32 %84, 29
  %90 = xor i32 %84, 29
  %91 = xor i32 %84, %85
  %92 = and i32 %90, %91
  %93 = icmp slt i32 %92, 0
  store i1 %88, i1* %az
  store i1 %89, i1* %cf
  store i1 %93, i1* %of
  %94 = icmp eq i32 %85, 0
  store i1 %94, i1* %zf
  %95 = icmp slt i32 %85, 0
  store i1 %95, i1* %sf
  %96 = trunc i32 %85 to i8
  %97 = call i8 @llvm.ctpop.i8(i8 %96)
  %98 = and i8 %97, 1
  %99 = icmp eq i8 %98, 0
  store i1 %99, i1* %pf
  store volatile i64 39797, i64* @assembly_address
  %100 = load i32* %3
  %101 = load i32* %2
  %102 = icmp sle i32 %100, %101
  br i1 %102, label %block_9b53, label %block_9b77

block_9b77:                                       ; preds = %block_9b71
  store volatile i64 39799, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 39806, i64* @assembly_address
  br label %block_9b9e

block_9b80:                                       ; preds = %block_9b9e
  store volatile i64 39808, i64* @assembly_address
  %103 = load i32* %stack_var_-12
  %104 = zext i32 %103 to i64
  store i64 %104, i64* %rax
  store volatile i64 39811, i64* @assembly_address
  %105 = load i64* %rax
  %106 = trunc i64 %105 to i32
  %107 = sext i32 %106 to i64
  store i64 %107, i64* %rax
  store volatile i64 39813, i64* @assembly_address
  %108 = load i64* %rax
  %109 = mul i64 %108, 4
  store i64 %109, i64* %rdx
  store volatile i64 39821, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec0 to i64), i64* %rax
  store volatile i64 39828, i64* @assembly_address
  %110 = load i64* %rdx
  %111 = load i64* %rax
  %112 = mul i64 %111, 1
  %113 = add i64 %110, %112
  %114 = inttoptr i64 %113 to i16*
  store i16 0, i16* %114
  store volatile i64 39834, i64* @assembly_address
  %115 = load i32* %stack_var_-12
  %116 = add i32 %115, 1
  %117 = and i32 %115, 15
  %118 = add i32 %117, 1
  %119 = icmp ugt i32 %118, 15
  %120 = icmp ult i32 %116, %115
  %121 = xor i32 %115, %116
  %122 = xor i32 1, %116
  %123 = and i32 %121, %122
  %124 = icmp slt i32 %123, 0
  store i1 %119, i1* %az
  store i1 %120, i1* %cf
  store i1 %124, i1* %of
  %125 = icmp eq i32 %116, 0
  store i1 %125, i1* %zf
  %126 = icmp slt i32 %116, 0
  store i1 %126, i1* %sf
  %127 = trunc i32 %116 to i8
  %128 = call i8 @llvm.ctpop.i8(i8 %127)
  %129 = and i8 %128, 1
  %130 = icmp eq i8 %129, 0
  store i1 %130, i1* %pf
  store i32 %116, i32* %stack_var_-12
  br label %block_9b9e

block_9b9e:                                       ; preds = %block_9b80, %block_9b77
  store volatile i64 39838, i64* @assembly_address
  %131 = load i32* %stack_var_-12
  store i32 %131, i32* %1
  store i32 18, i32* %0
  %132 = sub i32 %131, 18
  %133 = and i32 %131, 15
  %134 = sub i32 %133, 2
  %135 = icmp ugt i32 %134, 15
  %136 = icmp ult i32 %131, 18
  %137 = xor i32 %131, 18
  %138 = xor i32 %131, %132
  %139 = and i32 %137, %138
  %140 = icmp slt i32 %139, 0
  store i1 %135, i1* %az
  store i1 %136, i1* %cf
  store i1 %140, i1* %of
  %141 = icmp eq i32 %132, 0
  store i1 %141, i1* %zf
  %142 = icmp slt i32 %132, 0
  store i1 %142, i1* %sf
  %143 = trunc i32 %132 to i8
  %144 = call i8 @llvm.ctpop.i8(i8 %143)
  %145 = and i8 %144, 1
  %146 = icmp eq i8 %145, 0
  store i1 %146, i1* %pf
  store volatile i64 39842, i64* @assembly_address
  %147 = load i32* %1
  %148 = load i32* %0
  %149 = icmp sle i32 %147, %148
  br i1 %149, label %block_9b80, label %block_9ba4

block_9ba4:                                       ; preds = %block_9b9e
  store volatile i64 39844, i64* @assembly_address
  store i16 1, i16* bitcast (i64* @global_var_2173c0 to i16*)
  store volatile i64 39853, i64* @assembly_address
  store i64 0, i64* @global_var_219ed8
  store volatile i64 39864, i64* @assembly_address
  %150 = load i64* @global_var_219ed8
  store i64 %150, i64* %rax
  store volatile i64 39871, i64* @assembly_address
  %151 = load i64* %rax
  store i64 %151, i64* @global_var_219ed0
  store volatile i64 39878, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_219ec8 to i32*)
  store volatile i64 39888, i64* @assembly_address
  %152 = load i32* bitcast (i64* @global_var_219ec8 to i32*)
  %153 = zext i32 %152 to i64
  store i64 %153, i64* %rax
  store volatile i64 39894, i64* @assembly_address
  %154 = load i64* %rax
  %155 = trunc i64 %154 to i32
  store i32 %155, i32* bitcast (i64* @global_var_219ec4 to i32*)
  store volatile i64 39900, i64* @assembly_address
  %156 = load i32* bitcast (i64* @global_var_219ec4 to i32*)
  %157 = zext i32 %156 to i64
  store i64 %157, i64* %rax
  store volatile i64 39906, i64* @assembly_address
  %158 = load i64* %rax
  %159 = trunc i64 %158 to i32
  store i32 %159, i32* bitcast (i64* @global_var_219ec0 to i32*)
  store volatile i64 39912, i64* @assembly_address
  store i8 0, i8* bitcast (i64* @global_var_219ecc to i8*)
  store volatile i64 39919, i64* @assembly_address
  store i8 1, i8* bitcast (i64* @global_var_219ecd to i8*)
  store volatile i64 39926, i64* @assembly_address
  store volatile i64 39927, i64* @assembly_address
  %160 = load i64* %stack_var_-8
  store i64 %160, i64* %rbp
  %161 = ptrtoint i64* %stack_var_0 to i64
  store i64 %161, i64* %rsp
  store volatile i64 39928, i64* @assembly_address
  %162 = load i64* %rax
  %163 = load i64* %rax
  ret i64 %163
}

define i64 @pqdownheap(i16* %arg1, i64 %arg2) {
block_9bf9:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i16* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i32
  %stack_var_-12 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-32 = alloca i16*
  %1 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i64
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i64
  %7 = alloca i32
  store volatile i64 39929, i64* @assembly_address
  %8 = load i64* %rbp
  store i64 %8, i64* %stack_var_-8
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rsp
  store volatile i64 39930, i64* @assembly_address
  %10 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %10, i64* %rbp
  store volatile i64 39933, i64* @assembly_address
  %11 = load i64* %rdi
  %12 = inttoptr i64 %11 to i16*
  store i16* %12, i16** %stack_var_-32
  store volatile i64 39937, i64* @assembly_address
  %13 = load i64* %rsi
  %14 = trunc i64 %13 to i32
  store i32 %14, i32* %stack_var_-36
  store volatile i64 39940, i64* @assembly_address
  %15 = load i32* %stack_var_-36
  %16 = zext i32 %15 to i64
  store i64 %16, i64* %rax
  store volatile i64 39943, i64* @assembly_address
  %17 = load i64* %rax
  %18 = trunc i64 %17 to i32
  %19 = sext i32 %18 to i64
  store i64 %19, i64* %rax
  store volatile i64 39945, i64* @assembly_address
  %20 = load i64* %rax
  %21 = mul i64 %20, 4
  store i64 %21, i64* %rdx
  store volatile i64 39953, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 39960, i64* @assembly_address
  %22 = load i64* %rdx
  %23 = load i64* %rax
  %24 = mul i64 %23, 1
  %25 = add i64 %22, %24
  %26 = inttoptr i64 %25 to i32*
  %27 = load i32* %26
  %28 = zext i32 %27 to i64
  store i64 %28, i64* %rax
  store volatile i64 39963, i64* @assembly_address
  %29 = load i64* %rax
  %30 = trunc i64 %29 to i32
  store i32 %30, i32* %stack_var_-12
  store volatile i64 39966, i64* @assembly_address
  %31 = load i32* %stack_var_-36
  %32 = zext i32 %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 39969, i64* @assembly_address
  %33 = load i64* %rax
  %34 = trunc i64 %33 to i32
  %35 = load i64* %rax
  %36 = trunc i64 %35 to i32
  %37 = add i32 %34, %36
  %38 = and i32 %34, 15
  %39 = and i32 %36, 15
  %40 = add i32 %38, %39
  %41 = icmp ugt i32 %40, 15
  %42 = icmp ult i32 %37, %34
  %43 = xor i32 %34, %37
  %44 = xor i32 %36, %37
  %45 = and i32 %43, %44
  %46 = icmp slt i32 %45, 0
  store i1 %41, i1* %az
  store i1 %42, i1* %cf
  store i1 %46, i1* %of
  %47 = icmp eq i32 %37, 0
  store i1 %47, i1* %zf
  %48 = icmp slt i32 %37, 0
  store i1 %48, i1* %sf
  %49 = trunc i32 %37 to i8
  %50 = call i8 @llvm.ctpop.i8(i8 %49)
  %51 = and i8 %50, 1
  %52 = icmp eq i8 %51, 0
  store i1 %52, i1* %pf
  %53 = zext i32 %37 to i64
  store i64 %53, i64* %rax
  store volatile i64 39971, i64* @assembly_address
  %54 = load i64* %rax
  %55 = trunc i64 %54 to i32
  store i32 %55, i32* %stack_var_-16
  store volatile i64 39974, i64* @assembly_address
  br label %block_9e52

block_9c2b:                                       ; preds = %block_9e52
  store volatile i64 39979, i64* @assembly_address
  %56 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %57 = zext i32 %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 39985, i64* @assembly_address
  %58 = load i32* %stack_var_-16
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  store i32 %58, i32* %7
  %61 = trunc i64 %59 to i32
  store i32 %61, i32* %5
  %62 = sub i32 %58, %60
  %63 = and i32 %58, 15
  %64 = and i32 %60, 15
  %65 = sub i32 %63, %64
  %66 = icmp ugt i32 %65, 15
  %67 = icmp ult i32 %58, %60
  %68 = xor i32 %58, %60
  %69 = xor i32 %58, %62
  %70 = and i32 %68, %69
  %71 = icmp slt i32 %70, 0
  store i1 %66, i1* %az
  store i1 %67, i1* %cf
  store i1 %71, i1* %of
  %72 = icmp eq i32 %62, 0
  store i1 %72, i1* %zf
  %73 = icmp slt i32 %62, 0
  store i1 %73, i1* %sf
  %74 = trunc i32 %62 to i8
  %75 = call i8 @llvm.ctpop.i8(i8 %74)
  %76 = and i8 %75, 1
  %77 = icmp eq i8 %76, 0
  store i1 %77, i1* %pf
  store volatile i64 39988, i64* @assembly_address
  %78 = load i32* %7
  %79 = load i32* %5
  %80 = sext i32 %79 to i64
  %81 = sext i32 %78 to i64
  %82 = icmp sge i64 %81, %80
  br i1 %82, label %block_9d4f, label %block_9c3a

block_9c3a:                                       ; preds = %block_9c2b
  store volatile i64 39994, i64* @assembly_address
  %83 = load i32* %stack_var_-16
  %84 = zext i32 %83 to i64
  store i64 %84, i64* %rax
  store volatile i64 39997, i64* @assembly_address
  %85 = load i64* %rax
  %86 = trunc i64 %85 to i32
  %87 = add i32 %86, 1
  %88 = and i32 %86, 15
  %89 = add i32 %88, 1
  %90 = icmp ugt i32 %89, 15
  %91 = icmp ult i32 %87, %86
  %92 = xor i32 %86, %87
  %93 = xor i32 1, %87
  %94 = and i32 %92, %93
  %95 = icmp slt i32 %94, 0
  store i1 %90, i1* %az
  store i1 %91, i1* %cf
  store i1 %95, i1* %of
  %96 = icmp eq i32 %87, 0
  store i1 %96, i1* %zf
  %97 = icmp slt i32 %87, 0
  store i1 %97, i1* %sf
  %98 = trunc i32 %87 to i8
  %99 = call i8 @llvm.ctpop.i8(i8 %98)
  %100 = and i8 %99, 1
  %101 = icmp eq i8 %100, 0
  store i1 %101, i1* %pf
  %102 = zext i32 %87 to i64
  store i64 %102, i64* %rax
  store volatile i64 40000, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  %105 = sext i32 %104 to i64
  store i64 %105, i64* %rax
  store volatile i64 40002, i64* @assembly_address
  %106 = load i64* %rax
  %107 = mul i64 %106, 4
  store i64 %107, i64* %rdx
  store volatile i64 40010, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40017, i64* @assembly_address
  %108 = load i64* %rdx
  %109 = load i64* %rax
  %110 = mul i64 %109, 1
  %111 = add i64 %108, %110
  %112 = inttoptr i64 %111 to i32*
  %113 = load i32* %112
  %114 = zext i32 %113 to i64
  store i64 %114, i64* %rax
  store volatile i64 40020, i64* @assembly_address
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i32
  %117 = sext i32 %116 to i64
  store i64 %117, i64* %rax
  store volatile i64 40022, i64* @assembly_address
  %118 = load i64* %rax
  %119 = mul i64 %118, 4
  store i64 %119, i64* %rdx
  store volatile i64 40030, i64* @assembly_address
  %120 = load i16** %stack_var_-32
  %121 = ptrtoint i16* %120 to i64
  store i64 %121, i64* %rax
  store volatile i64 40034, i64* @assembly_address
  %122 = load i64* %rax
  %123 = load i64* %rdx
  %124 = add i64 %122, %123
  %125 = and i64 %122, 15
  %126 = and i64 %123, 15
  %127 = add i64 %125, %126
  %128 = icmp ugt i64 %127, 15
  %129 = icmp ult i64 %124, %122
  %130 = xor i64 %122, %124
  %131 = xor i64 %123, %124
  %132 = and i64 %130, %131
  %133 = icmp slt i64 %132, 0
  store i1 %128, i1* %az
  store i1 %129, i1* %cf
  store i1 %133, i1* %of
  %134 = icmp eq i64 %124, 0
  store i1 %134, i1* %zf
  %135 = icmp slt i64 %124, 0
  store i1 %135, i1* %sf
  %136 = trunc i64 %124 to i8
  %137 = call i8 @llvm.ctpop.i8(i8 %136)
  %138 = and i8 %137, 1
  %139 = icmp eq i8 %138, 0
  store i1 %139, i1* %pf
  store i64 %124, i64* %rax
  store volatile i64 40037, i64* @assembly_address
  %140 = load i64* %rax
  %141 = inttoptr i64 %140 to i16*
  %142 = load i16* %141
  %143 = zext i16 %142 to i64
  store i64 %143, i64* %rdx
  store volatile i64 40040, i64* @assembly_address
  %144 = load i32* %stack_var_-16
  %145 = zext i32 %144 to i64
  store i64 %145, i64* %rax
  store volatile i64 40043, i64* @assembly_address
  %146 = load i64* %rax
  %147 = trunc i64 %146 to i32
  %148 = sext i32 %147 to i64
  store i64 %148, i64* %rax
  store volatile i64 40045, i64* @assembly_address
  %149 = load i64* %rax
  %150 = mul i64 %149, 4
  store i64 %150, i64* %rcx
  store volatile i64 40053, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40060, i64* @assembly_address
  %151 = load i64* %rcx
  %152 = load i64* %rax
  %153 = mul i64 %152, 1
  %154 = add i64 %151, %153
  %155 = inttoptr i64 %154 to i32*
  %156 = load i32* %155
  %157 = zext i32 %156 to i64
  store i64 %157, i64* %rax
  store volatile i64 40063, i64* @assembly_address
  %158 = load i64* %rax
  %159 = trunc i64 %158 to i32
  %160 = sext i32 %159 to i64
  store i64 %160, i64* %rax
  store volatile i64 40065, i64* @assembly_address
  %161 = load i64* %rax
  %162 = mul i64 %161, 4
  store i64 %162, i64* %rcx
  store volatile i64 40073, i64* @assembly_address
  %163 = load i16** %stack_var_-32
  %164 = ptrtoint i16* %163 to i64
  store i64 %164, i64* %rax
  store volatile i64 40077, i64* @assembly_address
  %165 = load i64* %rax
  %166 = load i64* %rcx
  %167 = add i64 %165, %166
  %168 = and i64 %165, 15
  %169 = and i64 %166, 15
  %170 = add i64 %168, %169
  %171 = icmp ugt i64 %170, 15
  %172 = icmp ult i64 %167, %165
  %173 = xor i64 %165, %167
  %174 = xor i64 %166, %167
  %175 = and i64 %173, %174
  %176 = icmp slt i64 %175, 0
  store i1 %171, i1* %az
  store i1 %172, i1* %cf
  store i1 %176, i1* %of
  %177 = icmp eq i64 %167, 0
  store i1 %177, i1* %zf
  %178 = icmp slt i64 %167, 0
  store i1 %178, i1* %sf
  %179 = trunc i64 %167 to i8
  %180 = call i8 @llvm.ctpop.i8(i8 %179)
  %181 = and i8 %180, 1
  %182 = icmp eq i8 %181, 0
  store i1 %182, i1* %pf
  store i64 %167, i64* %rax
  store volatile i64 40080, i64* @assembly_address
  %183 = load i64* %rax
  %184 = inttoptr i64 %183 to i16*
  %185 = load i16* %184
  %186 = zext i16 %185 to i64
  store i64 %186, i64* %rax
  store volatile i64 40083, i64* @assembly_address
  %187 = load i64* %rdx
  %188 = trunc i64 %187 to i16
  %189 = load i64* %rax
  %190 = trunc i64 %189 to i16
  %191 = sub i16 %188, %190
  %192 = and i16 %188, 15
  %193 = and i16 %190, 15
  %194 = sub i16 %192, %193
  %195 = icmp ugt i16 %194, 15
  %196 = icmp ult i16 %188, %190
  %197 = xor i16 %188, %190
  %198 = xor i16 %188, %191
  %199 = and i16 %197, %198
  %200 = icmp slt i16 %199, 0
  store i1 %195, i1* %az
  store i1 %196, i1* %cf
  store i1 %200, i1* %of
  %201 = icmp eq i16 %191, 0
  store i1 %201, i1* %zf
  %202 = icmp slt i16 %191, 0
  store i1 %202, i1* %sf
  %203 = trunc i16 %191 to i8
  %204 = call i8 @llvm.ctpop.i8(i8 %203)
  %205 = and i8 %204, 1
  %206 = icmp eq i8 %205, 0
  store i1 %206, i1* %pf
  store volatile i64 40086, i64* @assembly_address
  %207 = load i1* %cf
  br i1 %207, label %block_9d4b, label %block_9c9c

block_9c9c:                                       ; preds = %block_9c3a
  store volatile i64 40092, i64* @assembly_address
  %208 = load i32* %stack_var_-16
  %209 = zext i32 %208 to i64
  store i64 %209, i64* %rax
  store volatile i64 40095, i64* @assembly_address
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  %212 = add i32 %211, 1
  %213 = and i32 %211, 15
  %214 = add i32 %213, 1
  %215 = icmp ugt i32 %214, 15
  %216 = icmp ult i32 %212, %211
  %217 = xor i32 %211, %212
  %218 = xor i32 1, %212
  %219 = and i32 %217, %218
  %220 = icmp slt i32 %219, 0
  store i1 %215, i1* %az
  store i1 %216, i1* %cf
  store i1 %220, i1* %of
  %221 = icmp eq i32 %212, 0
  store i1 %221, i1* %zf
  %222 = icmp slt i32 %212, 0
  store i1 %222, i1* %sf
  %223 = trunc i32 %212 to i8
  %224 = call i8 @llvm.ctpop.i8(i8 %223)
  %225 = and i8 %224, 1
  %226 = icmp eq i8 %225, 0
  store i1 %226, i1* %pf
  %227 = zext i32 %212 to i64
  store i64 %227, i64* %rax
  store volatile i64 40098, i64* @assembly_address
  %228 = load i64* %rax
  %229 = trunc i64 %228 to i32
  %230 = sext i32 %229 to i64
  store i64 %230, i64* %rax
  store volatile i64 40100, i64* @assembly_address
  %231 = load i64* %rax
  %232 = mul i64 %231, 4
  store i64 %232, i64* %rdx
  store volatile i64 40108, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40115, i64* @assembly_address
  %233 = load i64* %rdx
  %234 = load i64* %rax
  %235 = mul i64 %234, 1
  %236 = add i64 %233, %235
  %237 = inttoptr i64 %236 to i32*
  %238 = load i32* %237
  %239 = zext i32 %238 to i64
  store i64 %239, i64* %rax
  store volatile i64 40118, i64* @assembly_address
  %240 = load i64* %rax
  %241 = trunc i64 %240 to i32
  %242 = sext i32 %241 to i64
  store i64 %242, i64* %rax
  store volatile i64 40120, i64* @assembly_address
  %243 = load i64* %rax
  %244 = mul i64 %243, 4
  store i64 %244, i64* %rdx
  store volatile i64 40128, i64* @assembly_address
  %245 = load i16** %stack_var_-32
  %246 = ptrtoint i16* %245 to i64
  store i64 %246, i64* %rax
  store volatile i64 40132, i64* @assembly_address
  %247 = load i64* %rax
  %248 = load i64* %rdx
  %249 = add i64 %247, %248
  %250 = and i64 %247, 15
  %251 = and i64 %248, 15
  %252 = add i64 %250, %251
  %253 = icmp ugt i64 %252, 15
  %254 = icmp ult i64 %249, %247
  %255 = xor i64 %247, %249
  %256 = xor i64 %248, %249
  %257 = and i64 %255, %256
  %258 = icmp slt i64 %257, 0
  store i1 %253, i1* %az
  store i1 %254, i1* %cf
  store i1 %258, i1* %of
  %259 = icmp eq i64 %249, 0
  store i1 %259, i1* %zf
  %260 = icmp slt i64 %249, 0
  store i1 %260, i1* %sf
  %261 = trunc i64 %249 to i8
  %262 = call i8 @llvm.ctpop.i8(i8 %261)
  %263 = and i8 %262, 1
  %264 = icmp eq i8 %263, 0
  store i1 %264, i1* %pf
  store i64 %249, i64* %rax
  store volatile i64 40135, i64* @assembly_address
  %265 = load i64* %rax
  %266 = inttoptr i64 %265 to i16*
  %267 = load i16* %266
  %268 = zext i16 %267 to i64
  store i64 %268, i64* %rdx
  store volatile i64 40138, i64* @assembly_address
  %269 = load i32* %stack_var_-16
  %270 = zext i32 %269 to i64
  store i64 %270, i64* %rax
  store volatile i64 40141, i64* @assembly_address
  %271 = load i64* %rax
  %272 = trunc i64 %271 to i32
  %273 = sext i32 %272 to i64
  store i64 %273, i64* %rax
  store volatile i64 40143, i64* @assembly_address
  %274 = load i64* %rax
  %275 = mul i64 %274, 4
  store i64 %275, i64* %rcx
  store volatile i64 40151, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40158, i64* @assembly_address
  %276 = load i64* %rcx
  %277 = load i64* %rax
  %278 = mul i64 %277, 1
  %279 = add i64 %276, %278
  %280 = inttoptr i64 %279 to i32*
  %281 = load i32* %280
  %282 = zext i32 %281 to i64
  store i64 %282, i64* %rax
  store volatile i64 40161, i64* @assembly_address
  %283 = load i64* %rax
  %284 = trunc i64 %283 to i32
  %285 = sext i32 %284 to i64
  store i64 %285, i64* %rax
  store volatile i64 40163, i64* @assembly_address
  %286 = load i64* %rax
  %287 = mul i64 %286, 4
  store i64 %287, i64* %rcx
  store volatile i64 40171, i64* @assembly_address
  %288 = load i16** %stack_var_-32
  %289 = ptrtoint i16* %288 to i64
  store i64 %289, i64* %rax
  store volatile i64 40175, i64* @assembly_address
  %290 = load i64* %rax
  %291 = load i64* %rcx
  %292 = add i64 %290, %291
  %293 = and i64 %290, 15
  %294 = and i64 %291, 15
  %295 = add i64 %293, %294
  %296 = icmp ugt i64 %295, 15
  %297 = icmp ult i64 %292, %290
  %298 = xor i64 %290, %292
  %299 = xor i64 %291, %292
  %300 = and i64 %298, %299
  %301 = icmp slt i64 %300, 0
  store i1 %296, i1* %az
  store i1 %297, i1* %cf
  store i1 %301, i1* %of
  %302 = icmp eq i64 %292, 0
  store i1 %302, i1* %zf
  %303 = icmp slt i64 %292, 0
  store i1 %303, i1* %sf
  %304 = trunc i64 %292 to i8
  %305 = call i8 @llvm.ctpop.i8(i8 %304)
  %306 = and i8 %305, 1
  %307 = icmp eq i8 %306, 0
  store i1 %307, i1* %pf
  store i64 %292, i64* %rax
  store volatile i64 40178, i64* @assembly_address
  %308 = load i64* %rax
  %309 = inttoptr i64 %308 to i16*
  %310 = load i16* %309
  %311 = zext i16 %310 to i64
  store i64 %311, i64* %rax
  store volatile i64 40181, i64* @assembly_address
  %312 = load i64* %rdx
  %313 = trunc i64 %312 to i16
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i16
  %316 = sub i16 %313, %315
  %317 = and i16 %313, 15
  %318 = and i16 %315, 15
  %319 = sub i16 %317, %318
  %320 = icmp ugt i16 %319, 15
  %321 = icmp ult i16 %313, %315
  %322 = xor i16 %313, %315
  %323 = xor i16 %313, %316
  %324 = and i16 %322, %323
  %325 = icmp slt i16 %324, 0
  store i1 %320, i1* %az
  store i1 %321, i1* %cf
  store i1 %325, i1* %of
  %326 = icmp eq i16 %316, 0
  store i1 %326, i1* %zf
  %327 = icmp slt i16 %316, 0
  store i1 %327, i1* %sf
  %328 = trunc i16 %316 to i8
  %329 = call i8 @llvm.ctpop.i8(i8 %328)
  %330 = and i8 %329, 1
  %331 = icmp eq i8 %330, 0
  store i1 %331, i1* %pf
  store volatile i64 40184, i64* @assembly_address
  %332 = load i1* %zf
  %333 = icmp eq i1 %332, false
  br i1 %333, label %block_9d4f, label %block_9cfa

block_9cfa:                                       ; preds = %block_9c9c
  store volatile i64 40186, i64* @assembly_address
  %334 = load i32* %stack_var_-16
  %335 = zext i32 %334 to i64
  store i64 %335, i64* %rax
  store volatile i64 40189, i64* @assembly_address
  %336 = load i64* %rax
  %337 = trunc i64 %336 to i32
  %338 = add i32 %337, 1
  %339 = and i32 %337, 15
  %340 = add i32 %339, 1
  %341 = icmp ugt i32 %340, 15
  %342 = icmp ult i32 %338, %337
  %343 = xor i32 %337, %338
  %344 = xor i32 1, %338
  %345 = and i32 %343, %344
  %346 = icmp slt i32 %345, 0
  store i1 %341, i1* %az
  store i1 %342, i1* %cf
  store i1 %346, i1* %of
  %347 = icmp eq i32 %338, 0
  store i1 %347, i1* %zf
  %348 = icmp slt i32 %338, 0
  store i1 %348, i1* %sf
  %349 = trunc i32 %338 to i8
  %350 = call i8 @llvm.ctpop.i8(i8 %349)
  %351 = and i8 %350, 1
  %352 = icmp eq i8 %351, 0
  store i1 %352, i1* %pf
  %353 = zext i32 %338 to i64
  store i64 %353, i64* %rax
  store volatile i64 40192, i64* @assembly_address
  %354 = load i64* %rax
  %355 = trunc i64 %354 to i32
  %356 = sext i32 %355 to i64
  store i64 %356, i64* %rax
  store volatile i64 40194, i64* @assembly_address
  %357 = load i64* %rax
  %358 = mul i64 %357, 4
  store i64 %358, i64* %rdx
  store volatile i64 40202, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40209, i64* @assembly_address
  %359 = load i64* %rdx
  %360 = load i64* %rax
  %361 = mul i64 %360, 1
  %362 = add i64 %359, %361
  %363 = inttoptr i64 %362 to i32*
  %364 = load i32* %363
  %365 = zext i32 %364 to i64
  store i64 %365, i64* %rax
  store volatile i64 40212, i64* @assembly_address
  %366 = load i64* %rax
  %367 = trunc i64 %366 to i32
  %368 = sext i32 %367 to i64
  store i64 %368, i64* %rdx
  store volatile i64 40215, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 40222, i64* @assembly_address
  %369 = load i64* %rdx
  %370 = load i64* %rax
  %371 = mul i64 %370, 1
  %372 = add i64 %369, %371
  %373 = inttoptr i64 %372 to i8*
  %374 = load i8* %373
  %375 = zext i8 %374 to i64
  store i64 %375, i64* %rcx
  store volatile i64 40226, i64* @assembly_address
  %376 = load i32* %stack_var_-16
  %377 = zext i32 %376 to i64
  store i64 %377, i64* %rax
  store volatile i64 40229, i64* @assembly_address
  %378 = load i64* %rax
  %379 = trunc i64 %378 to i32
  %380 = sext i32 %379 to i64
  store i64 %380, i64* %rax
  store volatile i64 40231, i64* @assembly_address
  %381 = load i64* %rax
  %382 = mul i64 %381, 4
  store i64 %382, i64* %rdx
  store volatile i64 40239, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40246, i64* @assembly_address
  %383 = load i64* %rdx
  %384 = load i64* %rax
  %385 = mul i64 %384, 1
  %386 = add i64 %383, %385
  %387 = inttoptr i64 %386 to i32*
  %388 = load i32* %387
  %389 = zext i32 %388 to i64
  store i64 %389, i64* %rax
  store volatile i64 40249, i64* @assembly_address
  %390 = load i64* %rax
  %391 = trunc i64 %390 to i32
  %392 = sext i32 %391 to i64
  store i64 %392, i64* %rdx
  store volatile i64 40252, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 40259, i64* @assembly_address
  %393 = load i64* %rdx
  %394 = load i64* %rax
  %395 = mul i64 %394, 1
  %396 = add i64 %393, %395
  %397 = inttoptr i64 %396 to i8*
  %398 = load i8* %397
  %399 = zext i8 %398 to i64
  store i64 %399, i64* %rax
  store volatile i64 40263, i64* @assembly_address
  %400 = load i64* %rcx
  %401 = trunc i64 %400 to i8
  %402 = load i64* %rax
  %403 = trunc i64 %402 to i8
  %404 = sub i8 %401, %403
  %405 = and i8 %401, 15
  %406 = and i8 %403, 15
  %407 = sub i8 %405, %406
  %408 = icmp ugt i8 %407, 15
  %409 = icmp ult i8 %401, %403
  %410 = xor i8 %401, %403
  %411 = xor i8 %401, %404
  %412 = and i8 %410, %411
  %413 = icmp slt i8 %412, 0
  store i1 %408, i1* %az
  store i1 %409, i1* %cf
  store i1 %413, i1* %of
  %414 = icmp eq i8 %404, 0
  store i1 %414, i1* %zf
  %415 = icmp slt i8 %404, 0
  store i1 %415, i1* %sf
  %416 = call i8 @llvm.ctpop.i8(i8 %404)
  %417 = and i8 %416, 1
  %418 = icmp eq i8 %417, 0
  store i1 %418, i1* %pf
  store volatile i64 40265, i64* @assembly_address
  %419 = load i1* %cf
  %420 = load i1* %zf
  %421 = or i1 %419, %420
  %422 = icmp ne i1 %421, true
  br i1 %422, label %block_9d4f, label %block_9d4b

block_9d4b:                                       ; preds = %block_9cfa, %block_9c3a
  store volatile i64 40267, i64* @assembly_address
  %423 = load i32* %stack_var_-16
  %424 = add i32 %423, 1
  %425 = and i32 %423, 15
  %426 = add i32 %425, 1
  %427 = icmp ugt i32 %426, 15
  %428 = icmp ult i32 %424, %423
  %429 = xor i32 %423, %424
  %430 = xor i32 1, %424
  %431 = and i32 %429, %430
  %432 = icmp slt i32 %431, 0
  store i1 %427, i1* %az
  store i1 %428, i1* %cf
  store i1 %432, i1* %of
  %433 = icmp eq i32 %424, 0
  store i1 %433, i1* %zf
  %434 = icmp slt i32 %424, 0
  store i1 %434, i1* %sf
  %435 = trunc i32 %424 to i8
  %436 = call i8 @llvm.ctpop.i8(i8 %435)
  %437 = and i8 %436, 1
  %438 = icmp eq i8 %437, 0
  store i1 %438, i1* %pf
  store i32 %424, i32* %stack_var_-16
  br label %block_9d4f

block_9d4f:                                       ; preds = %block_9d4b, %block_9cfa, %block_9c9c, %block_9c2b
  store volatile i64 40271, i64* @assembly_address
  %439 = load i32* %stack_var_-12
  %440 = zext i32 %439 to i64
  store i64 %440, i64* %rax
  store volatile i64 40274, i64* @assembly_address
  %441 = load i64* %rax
  %442 = trunc i64 %441 to i32
  %443 = sext i32 %442 to i64
  store i64 %443, i64* %rax
  store volatile i64 40276, i64* @assembly_address
  %444 = load i64* %rax
  %445 = mul i64 %444, 4
  store i64 %445, i64* %rdx
  store volatile i64 40284, i64* @assembly_address
  %446 = load i16** %stack_var_-32
  %447 = ptrtoint i16* %446 to i64
  store i64 %447, i64* %rax
  store volatile i64 40288, i64* @assembly_address
  %448 = load i64* %rax
  %449 = load i64* %rdx
  %450 = add i64 %448, %449
  %451 = and i64 %448, 15
  %452 = and i64 %449, 15
  %453 = add i64 %451, %452
  %454 = icmp ugt i64 %453, 15
  %455 = icmp ult i64 %450, %448
  %456 = xor i64 %448, %450
  %457 = xor i64 %449, %450
  %458 = and i64 %456, %457
  %459 = icmp slt i64 %458, 0
  store i1 %454, i1* %az
  store i1 %455, i1* %cf
  store i1 %459, i1* %of
  %460 = icmp eq i64 %450, 0
  store i1 %460, i1* %zf
  %461 = icmp slt i64 %450, 0
  store i1 %461, i1* %sf
  %462 = trunc i64 %450 to i8
  %463 = call i8 @llvm.ctpop.i8(i8 %462)
  %464 = and i8 %463, 1
  %465 = icmp eq i8 %464, 0
  store i1 %465, i1* %pf
  store i64 %450, i64* %rax
  store volatile i64 40291, i64* @assembly_address
  %466 = load i64* %rax
  %467 = inttoptr i64 %466 to i16*
  %468 = load i16* %467
  %469 = zext i16 %468 to i64
  store i64 %469, i64* %rdx
  store volatile i64 40294, i64* @assembly_address
  %470 = load i32* %stack_var_-16
  %471 = zext i32 %470 to i64
  store i64 %471, i64* %rax
  store volatile i64 40297, i64* @assembly_address
  %472 = load i64* %rax
  %473 = trunc i64 %472 to i32
  %474 = sext i32 %473 to i64
  store i64 %474, i64* %rax
  store volatile i64 40299, i64* @assembly_address
  %475 = load i64* %rax
  %476 = mul i64 %475, 4
  store i64 %476, i64* %rcx
  store volatile i64 40307, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40314, i64* @assembly_address
  %477 = load i64* %rcx
  %478 = load i64* %rax
  %479 = mul i64 %478, 1
  %480 = add i64 %477, %479
  %481 = inttoptr i64 %480 to i32*
  %482 = load i32* %481
  %483 = zext i32 %482 to i64
  store i64 %483, i64* %rax
  store volatile i64 40317, i64* @assembly_address
  %484 = load i64* %rax
  %485 = trunc i64 %484 to i32
  %486 = sext i32 %485 to i64
  store i64 %486, i64* %rax
  store volatile i64 40319, i64* @assembly_address
  %487 = load i64* %rax
  %488 = mul i64 %487, 4
  store i64 %488, i64* %rcx
  store volatile i64 40327, i64* @assembly_address
  %489 = load i16** %stack_var_-32
  %490 = ptrtoint i16* %489 to i64
  store i64 %490, i64* %rax
  store volatile i64 40331, i64* @assembly_address
  %491 = load i64* %rax
  %492 = load i64* %rcx
  %493 = add i64 %491, %492
  %494 = and i64 %491, 15
  %495 = and i64 %492, 15
  %496 = add i64 %494, %495
  %497 = icmp ugt i64 %496, 15
  %498 = icmp ult i64 %493, %491
  %499 = xor i64 %491, %493
  %500 = xor i64 %492, %493
  %501 = and i64 %499, %500
  %502 = icmp slt i64 %501, 0
  store i1 %497, i1* %az
  store i1 %498, i1* %cf
  store i1 %502, i1* %of
  %503 = icmp eq i64 %493, 0
  store i1 %503, i1* %zf
  %504 = icmp slt i64 %493, 0
  store i1 %504, i1* %sf
  %505 = trunc i64 %493 to i8
  %506 = call i8 @llvm.ctpop.i8(i8 %505)
  %507 = and i8 %506, 1
  %508 = icmp eq i8 %507, 0
  store i1 %508, i1* %pf
  store i64 %493, i64* %rax
  store volatile i64 40334, i64* @assembly_address
  %509 = load i64* %rax
  %510 = inttoptr i64 %509 to i16*
  %511 = load i16* %510
  %512 = zext i16 %511 to i64
  store i64 %512, i64* %rax
  store volatile i64 40337, i64* @assembly_address
  %513 = load i64* %rdx
  %514 = trunc i64 %513 to i16
  %515 = load i64* %rax
  %516 = trunc i64 %515 to i16
  %517 = sub i16 %514, %516
  %518 = and i16 %514, 15
  %519 = and i16 %516, 15
  %520 = sub i16 %518, %519
  %521 = icmp ugt i16 %520, 15
  %522 = icmp ult i16 %514, %516
  %523 = xor i16 %514, %516
  %524 = xor i16 %514, %517
  %525 = and i16 %523, %524
  %526 = icmp slt i16 %525, 0
  store i1 %521, i1* %az
  store i1 %522, i1* %cf
  store i1 %526, i1* %of
  %527 = icmp eq i16 %517, 0
  store i1 %527, i1* %zf
  %528 = icmp slt i16 %517, 0
  store i1 %528, i1* %sf
  %529 = trunc i16 %517 to i8
  %530 = call i8 @llvm.ctpop.i8(i8 %529)
  %531 = and i8 %530, 1
  %532 = icmp eq i8 %531, 0
  store i1 %532, i1* %pf
  store volatile i64 40340, i64* @assembly_address
  %533 = load i1* %cf
  br i1 %533, label %block_9e61, label %block_9d9a

block_9d9a:                                       ; preds = %block_9d4f
  store volatile i64 40346, i64* @assembly_address
  %534 = load i32* %stack_var_-12
  %535 = zext i32 %534 to i64
  store i64 %535, i64* %rax
  store volatile i64 40349, i64* @assembly_address
  %536 = load i64* %rax
  %537 = trunc i64 %536 to i32
  %538 = sext i32 %537 to i64
  store i64 %538, i64* %rax
  store volatile i64 40351, i64* @assembly_address
  %539 = load i64* %rax
  %540 = mul i64 %539, 4
  store i64 %540, i64* %rdx
  store volatile i64 40359, i64* @assembly_address
  %541 = load i16** %stack_var_-32
  %542 = ptrtoint i16* %541 to i64
  store i64 %542, i64* %rax
  store volatile i64 40363, i64* @assembly_address
  %543 = load i64* %rax
  %544 = load i64* %rdx
  %545 = add i64 %543, %544
  %546 = and i64 %543, 15
  %547 = and i64 %544, 15
  %548 = add i64 %546, %547
  %549 = icmp ugt i64 %548, 15
  %550 = icmp ult i64 %545, %543
  %551 = xor i64 %543, %545
  %552 = xor i64 %544, %545
  %553 = and i64 %551, %552
  %554 = icmp slt i64 %553, 0
  store i1 %549, i1* %az
  store i1 %550, i1* %cf
  store i1 %554, i1* %of
  %555 = icmp eq i64 %545, 0
  store i1 %555, i1* %zf
  %556 = icmp slt i64 %545, 0
  store i1 %556, i1* %sf
  %557 = trunc i64 %545 to i8
  %558 = call i8 @llvm.ctpop.i8(i8 %557)
  %559 = and i8 %558, 1
  %560 = icmp eq i8 %559, 0
  store i1 %560, i1* %pf
  store i64 %545, i64* %rax
  store volatile i64 40366, i64* @assembly_address
  %561 = load i64* %rax
  %562 = inttoptr i64 %561 to i16*
  %563 = load i16* %562
  %564 = zext i16 %563 to i64
  store i64 %564, i64* %rdx
  store volatile i64 40369, i64* @assembly_address
  %565 = load i32* %stack_var_-16
  %566 = zext i32 %565 to i64
  store i64 %566, i64* %rax
  store volatile i64 40372, i64* @assembly_address
  %567 = load i64* %rax
  %568 = trunc i64 %567 to i32
  %569 = sext i32 %568 to i64
  store i64 %569, i64* %rax
  store volatile i64 40374, i64* @assembly_address
  %570 = load i64* %rax
  %571 = mul i64 %570, 4
  store i64 %571, i64* %rcx
  store volatile i64 40382, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40389, i64* @assembly_address
  %572 = load i64* %rcx
  %573 = load i64* %rax
  %574 = mul i64 %573, 1
  %575 = add i64 %572, %574
  %576 = inttoptr i64 %575 to i32*
  %577 = load i32* %576
  %578 = zext i32 %577 to i64
  store i64 %578, i64* %rax
  store volatile i64 40392, i64* @assembly_address
  %579 = load i64* %rax
  %580 = trunc i64 %579 to i32
  %581 = sext i32 %580 to i64
  store i64 %581, i64* %rax
  store volatile i64 40394, i64* @assembly_address
  %582 = load i64* %rax
  %583 = mul i64 %582, 4
  store i64 %583, i64* %rcx
  store volatile i64 40402, i64* @assembly_address
  %584 = load i16** %stack_var_-32
  %585 = ptrtoint i16* %584 to i64
  store i64 %585, i64* %rax
  store volatile i64 40406, i64* @assembly_address
  %586 = load i64* %rax
  %587 = load i64* %rcx
  %588 = add i64 %586, %587
  %589 = and i64 %586, 15
  %590 = and i64 %587, 15
  %591 = add i64 %589, %590
  %592 = icmp ugt i64 %591, 15
  %593 = icmp ult i64 %588, %586
  %594 = xor i64 %586, %588
  %595 = xor i64 %587, %588
  %596 = and i64 %594, %595
  %597 = icmp slt i64 %596, 0
  store i1 %592, i1* %az
  store i1 %593, i1* %cf
  store i1 %597, i1* %of
  %598 = icmp eq i64 %588, 0
  store i1 %598, i1* %zf
  %599 = icmp slt i64 %588, 0
  store i1 %599, i1* %sf
  %600 = trunc i64 %588 to i8
  %601 = call i8 @llvm.ctpop.i8(i8 %600)
  %602 = and i8 %601, 1
  %603 = icmp eq i8 %602, 0
  store i1 %603, i1* %pf
  store i64 %588, i64* %rax
  store volatile i64 40409, i64* @assembly_address
  %604 = load i64* %rax
  %605 = inttoptr i64 %604 to i16*
  %606 = load i16* %605
  %607 = zext i16 %606 to i64
  store i64 %607, i64* %rax
  store volatile i64 40412, i64* @assembly_address
  %608 = load i64* %rdx
  %609 = trunc i64 %608 to i16
  %610 = load i64* %rax
  %611 = trunc i64 %610 to i16
  %612 = sub i16 %609, %611
  %613 = and i16 %609, 15
  %614 = and i16 %611, 15
  %615 = sub i16 %613, %614
  %616 = icmp ugt i16 %615, 15
  %617 = icmp ult i16 %609, %611
  %618 = xor i16 %609, %611
  %619 = xor i16 %609, %612
  %620 = and i16 %618, %619
  %621 = icmp slt i16 %620, 0
  store i1 %616, i1* %az
  store i1 %617, i1* %cf
  store i1 %621, i1* %of
  %622 = icmp eq i16 %612, 0
  store i1 %622, i1* %zf
  %623 = icmp slt i16 %612, 0
  store i1 %623, i1* %sf
  %624 = trunc i16 %612 to i8
  %625 = call i8 @llvm.ctpop.i8(i8 %624)
  %626 = and i8 %625, 1
  %627 = icmp eq i8 %626, 0
  store i1 %627, i1* %pf
  store volatile i64 40415, i64* @assembly_address
  %628 = load i1* %zf
  %629 = icmp eq i1 %628, false
  br i1 %629, label %block_9e1b, label %block_9de1

block_9de1:                                       ; preds = %block_9d9a
  store volatile i64 40417, i64* @assembly_address
  %630 = load i32* %stack_var_-12
  %631 = zext i32 %630 to i64
  store i64 %631, i64* %rax
  store volatile i64 40420, i64* @assembly_address
  %632 = load i64* %rax
  %633 = trunc i64 %632 to i32
  %634 = sext i32 %633 to i64
  store i64 %634, i64* %rdx
  store volatile i64 40423, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 40430, i64* @assembly_address
  %635 = load i64* %rdx
  %636 = load i64* %rax
  %637 = mul i64 %636, 1
  %638 = add i64 %635, %637
  %639 = inttoptr i64 %638 to i8*
  %640 = load i8* %639
  %641 = zext i8 %640 to i64
  store i64 %641, i64* %rcx
  store volatile i64 40434, i64* @assembly_address
  %642 = load i32* %stack_var_-16
  %643 = zext i32 %642 to i64
  store i64 %643, i64* %rax
  store volatile i64 40437, i64* @assembly_address
  %644 = load i64* %rax
  %645 = trunc i64 %644 to i32
  %646 = sext i32 %645 to i64
  store i64 %646, i64* %rax
  store volatile i64 40439, i64* @assembly_address
  %647 = load i64* %rax
  %648 = mul i64 %647, 4
  store i64 %648, i64* %rdx
  store volatile i64 40447, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40454, i64* @assembly_address
  %649 = load i64* %rdx
  %650 = load i64* %rax
  %651 = mul i64 %650, 1
  %652 = add i64 %649, %651
  %653 = inttoptr i64 %652 to i32*
  %654 = load i32* %653
  %655 = zext i32 %654 to i64
  store i64 %655, i64* %rax
  store volatile i64 40457, i64* @assembly_address
  %656 = load i64* %rax
  %657 = trunc i64 %656 to i32
  %658 = sext i32 %657 to i64
  store i64 %658, i64* %rdx
  store volatile i64 40460, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 40467, i64* @assembly_address
  %659 = load i64* %rdx
  %660 = load i64* %rax
  %661 = mul i64 %660, 1
  %662 = add i64 %659, %661
  %663 = inttoptr i64 %662 to i8*
  %664 = load i8* %663
  %665 = zext i8 %664 to i64
  store i64 %665, i64* %rax
  store volatile i64 40471, i64* @assembly_address
  %666 = load i64* %rcx
  %667 = trunc i64 %666 to i8
  %668 = load i64* %rax
  %669 = trunc i64 %668 to i8
  %670 = sub i8 %667, %669
  %671 = and i8 %667, 15
  %672 = and i8 %669, 15
  %673 = sub i8 %671, %672
  %674 = icmp ugt i8 %673, 15
  %675 = icmp ult i8 %667, %669
  %676 = xor i8 %667, %669
  %677 = xor i8 %667, %670
  %678 = and i8 %676, %677
  %679 = icmp slt i8 %678, 0
  store i1 %674, i1* %az
  store i1 %675, i1* %cf
  store i1 %679, i1* %of
  %680 = icmp eq i8 %670, 0
  store i1 %680, i1* %zf
  %681 = icmp slt i8 %670, 0
  store i1 %681, i1* %sf
  %682 = call i8 @llvm.ctpop.i8(i8 %670)
  %683 = and i8 %682, 1
  %684 = icmp eq i8 %683, 0
  store i1 %684, i1* %pf
  store volatile i64 40473, i64* @assembly_address
  %685 = load i1* %cf
  %686 = load i1* %zf
  %687 = or i1 %685, %686
  br i1 %687, label %block_9e61, label %block_9e1b

block_9e1b:                                       ; preds = %block_9de1, %block_9d9a
  store volatile i64 40475, i64* @assembly_address
  %688 = load i32* %stack_var_-16
  %689 = zext i32 %688 to i64
  store i64 %689, i64* %rax
  store volatile i64 40478, i64* @assembly_address
  %690 = load i64* %rax
  %691 = trunc i64 %690 to i32
  %692 = sext i32 %691 to i64
  store i64 %692, i64* %rax
  store volatile i64 40480, i64* @assembly_address
  %693 = load i64* %rax
  %694 = mul i64 %693, 4
  store i64 %694, i64* %rdx
  store volatile i64 40488, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40495, i64* @assembly_address
  %695 = load i64* %rdx
  %696 = load i64* %rax
  %697 = mul i64 %696, 1
  %698 = add i64 %695, %697
  %699 = inttoptr i64 %698 to i32*
  %700 = load i32* %699
  %701 = zext i32 %700 to i64
  store i64 %701, i64* %rdx
  store volatile i64 40498, i64* @assembly_address
  %702 = load i32* %stack_var_-36
  %703 = zext i32 %702 to i64
  store i64 %703, i64* %rax
  store volatile i64 40501, i64* @assembly_address
  %704 = load i64* %rax
  %705 = trunc i64 %704 to i32
  %706 = sext i32 %705 to i64
  store i64 %706, i64* %rax
  store volatile i64 40503, i64* @assembly_address
  %707 = load i64* %rax
  %708 = mul i64 %707, 4
  store i64 %708, i64* %rcx
  store volatile i64 40511, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40518, i64* @assembly_address
  %709 = load i64* %rdx
  %710 = trunc i64 %709 to i32
  %711 = load i64* %rcx
  %712 = load i64* %rax
  %713 = mul i64 %712, 1
  %714 = add i64 %711, %713
  %715 = inttoptr i64 %714 to i32*
  store i32 %710, i32* %715
  store volatile i64 40521, i64* @assembly_address
  %716 = load i32* %stack_var_-16
  %717 = zext i32 %716 to i64
  store i64 %717, i64* %rax
  store volatile i64 40524, i64* @assembly_address
  %718 = load i64* %rax
  %719 = trunc i64 %718 to i32
  store i32 %719, i32* %stack_var_-36
  store volatile i64 40527, i64* @assembly_address
  %720 = load i32* %stack_var_-16
  %721 = load i1* %of
  %722 = shl i32 %720, 1
  %723 = icmp eq i32 %722, 0
  store i1 %723, i1* %zf
  %724 = icmp slt i32 %722, 0
  store i1 %724, i1* %sf
  %725 = trunc i32 %722 to i8
  %726 = call i8 @llvm.ctpop.i8(i8 %725)
  %727 = and i8 %726, 1
  %728 = icmp eq i8 %727, 0
  store i1 %728, i1* %pf
  store i32 %722, i32* %stack_var_-16
  %729 = shl i32 %720, 0
  %730 = lshr i32 %729, 31
  %731 = trunc i32 %730 to i1
  store i1 %731, i1* %cf
  %732 = lshr i32 %722, 31
  %733 = icmp ne i32 %732, %730
  %734 = select i1 true, i1 %733, i1 %721
  store i1 %734, i1* %of
  br label %block_9e52

block_9e52:                                       ; preds = %block_9e1b, %block_9bf9
  store volatile i64 40530, i64* @assembly_address
  %735 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %736 = zext i32 %735 to i64
  store i64 %736, i64* %rax
  store volatile i64 40536, i64* @assembly_address
  %737 = load i32* %stack_var_-16
  %738 = load i64* %rax
  %739 = trunc i64 %738 to i32
  store i32 %737, i32* %4
  %740 = trunc i64 %738 to i32
  store i32 %740, i32* %2
  %741 = sub i32 %737, %739
  %742 = and i32 %737, 15
  %743 = and i32 %739, 15
  %744 = sub i32 %742, %743
  %745 = icmp ugt i32 %744, 15
  %746 = icmp ult i32 %737, %739
  %747 = xor i32 %737, %739
  %748 = xor i32 %737, %741
  %749 = and i32 %747, %748
  %750 = icmp slt i32 %749, 0
  store i1 %745, i1* %az
  store i1 %746, i1* %cf
  store i1 %750, i1* %of
  %751 = icmp eq i32 %741, 0
  store i1 %751, i1* %zf
  %752 = icmp slt i32 %741, 0
  store i1 %752, i1* %sf
  %753 = trunc i32 %741 to i8
  %754 = call i8 @llvm.ctpop.i8(i8 %753)
  %755 = and i8 %754, 1
  %756 = icmp eq i8 %755, 0
  store i1 %756, i1* %pf
  store volatile i64 40539, i64* @assembly_address
  %757 = load i32* %4
  %758 = load i32* %2
  %759 = sext i32 %758 to i64
  %760 = sext i32 %757 to i64
  %761 = icmp sle i64 %760, %759
  br i1 %761, label %block_9c2b, label %block_9e61

block_9e61:                                       ; preds = %block_9de1, %block_9d4f, %block_9e52
  store volatile i64 40545, i64* @assembly_address
  %762 = load i32* %stack_var_-36
  %763 = zext i32 %762 to i64
  store i64 %763, i64* %rax
  store volatile i64 40548, i64* @assembly_address
  %764 = load i64* %rax
  %765 = trunc i64 %764 to i32
  %766 = sext i32 %765 to i64
  store i64 %766, i64* %rax
  store volatile i64 40550, i64* @assembly_address
  %767 = load i64* %rax
  %768 = mul i64 %767, 4
  store i64 %768, i64* %rcx
  store volatile i64 40558, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40565, i64* @assembly_address
  %769 = load i32* %stack_var_-12
  %770 = zext i32 %769 to i64
  store i64 %770, i64* %rdx
  store volatile i64 40568, i64* @assembly_address
  %771 = load i64* %rdx
  %772 = trunc i64 %771 to i32
  %773 = load i64* %rcx
  %774 = load i64* %rax
  %775 = mul i64 %774, 1
  %776 = add i64 %773, %775
  %777 = inttoptr i64 %776 to i32*
  store i32 %772, i32* %777
  store volatile i64 40571, i64* @assembly_address
  store volatile i64 40572, i64* @assembly_address
  %778 = load i64* %stack_var_-8
  store i64 %778, i64* %rbp
  %779 = ptrtoint i64* %stack_var_0 to i64
  store i64 %779, i64* %rsp
  store volatile i64 40573, i64* @assembly_address
  %780 = load i64* %rax
  ret i64 %780
}

declare i64 @203(i64, i32)

declare i64 @204(i64, i64)

define i64 @gen_bitlen(i64* %arg1) {
block_9e7e:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-36 = alloca i32
  %stack_var_-70 = alloca i128
  %1 = alloca i16
  %stack_var_-56 = alloca i32
  %stack_var_-64 = alloca i32
  %stack_var_-68 = alloca i32
  %stack_var_-60 = alloca i128
  %2 = alloca i32
  %stack_var_-52 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-40 = alloca i32
  %stack_var_-44 = alloca i32
  %stack_var_-48 = alloca i32
  %stack_var_-24 = alloca i32*
  %3 = alloca i64
  %stack_var_-32 = alloca i16*
  %4 = alloca i64
  %stack_var_-80 = alloca i64
  %stack_var_-8 = alloca i64
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i64
  %8 = alloca i8*
  %9 = alloca i32
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i32
  %13 = alloca i64
  %14 = alloca i32
  %15 = alloca i32
  %16 = alloca i64
  %17 = alloca i128
  %18 = alloca i32
  %19 = alloca i128
  %20 = alloca i64
  %21 = alloca i32
  %22 = alloca i128
  %23 = alloca i32
  store volatile i64 40574, i64* @assembly_address
  %24 = load i64* %rbp
  store i64 %24, i64* %stack_var_-8
  %25 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %25, i64* %rsp
  store volatile i64 40575, i64* @assembly_address
  %26 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %26, i64* %rbp
  store volatile i64 40578, i64* @assembly_address
  %27 = load i64* %rdi
  store i64 %27, i64* %stack_var_-80
  store volatile i64 40582, i64* @assembly_address
  %28 = load i64* %stack_var_-80
  store i64 %28, i64* %rax
  store volatile i64 40586, i64* @assembly_address
  %29 = load i64* %rax
  %30 = inttoptr i64 %29 to i64*
  %31 = load i64* %30
  store i64 %31, i64* %rax
  store volatile i64 40589, i64* @assembly_address
  %32 = load i64* %rax
  %33 = inttoptr i64 %32 to i16*
  store i16* %33, i16** %stack_var_-32
  store volatile i64 40593, i64* @assembly_address
  %34 = load i64* %stack_var_-80
  store i64 %34, i64* %rax
  store volatile i64 40597, i64* @assembly_address
  %35 = load i64* %rax
  %36 = add i64 %35, 16
  %37 = inttoptr i64 %36 to i64*
  %38 = load i64* %37
  store i64 %38, i64* %rax
  store volatile i64 40601, i64* @assembly_address
  %39 = load i64* %rax
  %40 = inttoptr i64 %39 to i32*
  store i32* %40, i32** %stack_var_-24
  store volatile i64 40605, i64* @assembly_address
  %41 = load i64* %stack_var_-80
  store i64 %41, i64* %rax
  store volatile i64 40609, i64* @assembly_address
  %42 = load i64* %rax
  %43 = add i64 %42, 24
  %44 = inttoptr i64 %43 to i32*
  %45 = load i32* %44
  %46 = zext i32 %45 to i64
  store i64 %46, i64* %rax
  store volatile i64 40612, i64* @assembly_address
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i32
  store i32 %48, i32* %stack_var_-48
  store volatile i64 40615, i64* @assembly_address
  %49 = load i64* %stack_var_-80
  store i64 %49, i64* %rax
  store volatile i64 40619, i64* @assembly_address
  %50 = load i64* %rax
  %51 = add i64 %50, 36
  %52 = inttoptr i64 %51 to i32*
  %53 = load i32* %52
  %54 = zext i32 %53 to i64
  store i64 %54, i64* %rax
  store volatile i64 40622, i64* @assembly_address
  %55 = load i64* %rax
  %56 = trunc i64 %55 to i32
  store i32 %56, i32* %stack_var_-44
  store volatile i64 40625, i64* @assembly_address
  %57 = load i64* %stack_var_-80
  store i64 %57, i64* %rax
  store volatile i64 40629, i64* @assembly_address
  %58 = load i64* %rax
  %59 = add i64 %58, 32
  %60 = inttoptr i64 %59 to i32*
  %61 = load i32* %60
  %62 = zext i32 %61 to i64
  store i64 %62, i64* %rax
  store volatile i64 40632, i64* @assembly_address
  %63 = load i64* %rax
  %64 = trunc i64 %63 to i32
  store i32 %64, i32* %stack_var_-40
  store volatile i64 40635, i64* @assembly_address
  %65 = load i64* %stack_var_-80
  store i64 %65, i64* %rax
  store volatile i64 40639, i64* @assembly_address
  %66 = load i64* %rax
  %67 = add i64 %66, 8
  %68 = inttoptr i64 %67 to i64*
  %69 = load i64* %68
  store i64 %69, i64* %rax
  store volatile i64 40643, i64* @assembly_address
  %70 = load i64* %rax
  store i64 %70, i64* %stack_var_-16
  store volatile i64 40647, i64* @assembly_address
  store i32 0, i32* %stack_var_-52
  store volatile i64 40654, i64* @assembly_address
  %71 = sext i32 0 to i128
  store i128 %71, i128* %stack_var_-60
  store volatile i64 40661, i64* @assembly_address
  br label %block_9ef1

block_9ed7:                                       ; preds = %block_9ef1
  store volatile i64 40663, i64* @assembly_address
  %72 = load i128* %stack_var_-60
  %73 = trunc i128 %72 to i32
  %74 = zext i32 %73 to i64
  store i64 %74, i64* %rax
  store volatile i64 40666, i64* @assembly_address
  %75 = load i64* %rax
  %76 = trunc i64 %75 to i32
  %77 = sext i32 %76 to i64
  store i64 %77, i64* %rax
  store volatile i64 40668, i64* @assembly_address
  %78 = load i64* %rax
  %79 = load i64* %rax
  %80 = mul i64 %79, 1
  %81 = add i64 %78, %80
  store i64 %81, i64* %rdx
  store volatile i64 40672, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 40679, i64* @assembly_address
  %82 = load i64* %rdx
  %83 = load i64* %rax
  %84 = mul i64 %83, 1
  %85 = add i64 %82, %84
  %86 = inttoptr i64 %85 to i16*
  store i16 0, i16* %86
  store volatile i64 40685, i64* @assembly_address
  %87 = load i128* %stack_var_-60
  %88 = trunc i128 %87 to i32
  %89 = add i32 %88, 1
  %90 = and i32 %88, 15
  %91 = add i32 %90, 1
  %92 = icmp ugt i32 %91, 15
  %93 = icmp ult i32 %89, %88
  %94 = xor i32 %88, %89
  %95 = xor i32 1, %89
  %96 = and i32 %94, %95
  %97 = icmp slt i32 %96, 0
  store i1 %92, i1* %az
  store i1 %93, i1* %cf
  store i1 %97, i1* %of
  %98 = icmp eq i32 %89, 0
  store i1 %98, i1* %zf
  %99 = icmp slt i32 %89, 0
  store i1 %99, i1* %sf
  %100 = trunc i32 %89 to i8
  %101 = call i8 @llvm.ctpop.i8(i8 %100)
  %102 = and i8 %101, 1
  %103 = icmp eq i8 %102, 0
  store i1 %103, i1* %pf
  %104 = sext i32 %89 to i128
  store i128 %104, i128* %stack_var_-60
  br label %block_9ef1

block_9ef1:                                       ; preds = %block_9ed7, %block_9e7e
  store volatile i64 40689, i64* @assembly_address
  %105 = load i128* %stack_var_-60
  %106 = trunc i128 %105 to i32
  %107 = sext i32 %106 to i128
  store i128 %107, i128* %22
  store i32 15, i32* %21
  %108 = sub i32 %106, 15
  %109 = and i32 %106, 15
  %110 = sub i32 %109, 15
  %111 = icmp ugt i32 %110, 15
  %112 = icmp ult i32 %106, 15
  %113 = xor i32 %106, 15
  %114 = xor i32 %106, %108
  %115 = and i32 %113, %114
  %116 = icmp slt i32 %115, 0
  store i1 %111, i1* %az
  store i1 %112, i1* %cf
  store i1 %116, i1* %of
  %117 = icmp eq i32 %108, 0
  store i1 %117, i1* %zf
  %118 = icmp slt i32 %108, 0
  store i1 %118, i1* %sf
  %119 = trunc i32 %108 to i8
  %120 = call i8 @llvm.ctpop.i8(i8 %119)
  %121 = and i8 %120, 1
  %122 = icmp eq i8 %121, 0
  store i1 %122, i1* %pf
  store volatile i64 40693, i64* @assembly_address
  %123 = load i128* %22
  %124 = trunc i128 %123 to i32
  %125 = load i32* %21
  %126 = icmp sle i32 %124, %125
  br i1 %126, label %block_9ed7, label %block_9ef7

block_9ef7:                                       ; preds = %block_9ef1
  store volatile i64 40695, i64* @assembly_address
  %127 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %128 = zext i32 %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 40701, i64* @assembly_address
  %129 = load i64* %rax
  %130 = trunc i64 %129 to i32
  %131 = sext i32 %130 to i64
  store i64 %131, i64* %rax
  store volatile i64 40703, i64* @assembly_address
  %132 = load i64* %rax
  %133 = mul i64 %132, 4
  store i64 %133, i64* %rdx
  store volatile i64 40711, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40718, i64* @assembly_address
  %134 = load i64* %rdx
  %135 = load i64* %rax
  %136 = mul i64 %135, 1
  %137 = add i64 %134, %136
  %138 = inttoptr i64 %137 to i32*
  %139 = load i32* %138
  %140 = zext i32 %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 40721, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  %143 = sext i32 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 40723, i64* @assembly_address
  %144 = load i64* %rax
  %145 = mul i64 %144, 4
  store i64 %145, i64* %rdx
  store volatile i64 40731, i64* @assembly_address
  %146 = load i16** %stack_var_-32
  %147 = ptrtoint i16* %146 to i64
  store i64 %147, i64* %rax
  store volatile i64 40735, i64* @assembly_address
  %148 = load i64* %rax
  %149 = load i64* %rdx
  %150 = add i64 %148, %149
  %151 = and i64 %148, 15
  %152 = and i64 %149, 15
  %153 = add i64 %151, %152
  %154 = icmp ugt i64 %153, 15
  %155 = icmp ult i64 %150, %148
  %156 = xor i64 %148, %150
  %157 = xor i64 %149, %150
  %158 = and i64 %156, %157
  %159 = icmp slt i64 %158, 0
  store i1 %154, i1* %az
  store i1 %155, i1* %cf
  store i1 %159, i1* %of
  %160 = icmp eq i64 %150, 0
  store i1 %160, i1* %zf
  %161 = icmp slt i64 %150, 0
  store i1 %161, i1* %sf
  %162 = trunc i64 %150 to i8
  %163 = call i8 @llvm.ctpop.i8(i8 %162)
  %164 = and i8 %163, 1
  %165 = icmp eq i8 %164, 0
  store i1 %165, i1* %pf
  store i64 %150, i64* %rax
  store volatile i64 40738, i64* @assembly_address
  %166 = load i64* %rax
  %167 = add i64 %166, 2
  %168 = inttoptr i64 %167 to i16*
  store i16 0, i16* %168
  store volatile i64 40744, i64* @assembly_address
  %169 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %170 = zext i32 %169 to i64
  store i64 %170, i64* %rax
  store volatile i64 40750, i64* @assembly_address
  %171 = load i64* %rax
  %172 = trunc i64 %171 to i32
  %173 = add i32 %172, 1
  %174 = and i32 %172, 15
  %175 = add i32 %174, 1
  %176 = icmp ugt i32 %175, 15
  %177 = icmp ult i32 %173, %172
  %178 = xor i32 %172, %173
  %179 = xor i32 1, %173
  %180 = and i32 %178, %179
  %181 = icmp slt i32 %180, 0
  store i1 %176, i1* %az
  store i1 %177, i1* %cf
  store i1 %181, i1* %of
  %182 = icmp eq i32 %173, 0
  store i1 %182, i1* %zf
  %183 = icmp slt i32 %173, 0
  store i1 %183, i1* %sf
  %184 = trunc i32 %173 to i8
  %185 = call i8 @llvm.ctpop.i8(i8 %184)
  %186 = and i8 %185, 1
  %187 = icmp eq i8 %186, 0
  store i1 %187, i1* %pf
  store i64 ptrtoint (i64* @global_var_218879 to i64), i64* %rax
  store volatile i64 40753, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  store i32 %189, i32* %stack_var_-68
  store volatile i64 40756, i64* @assembly_address
  br label %block_a0a0

block_9f39:                                       ; preds = %block_a0a0
  store volatile i64 40761, i64* @assembly_address
  %190 = load i32* %stack_var_-68
  %191 = zext i32 %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 40764, i64* @assembly_address
  %192 = load i64* %rax
  %193 = trunc i64 %192 to i32
  %194 = sext i32 %193 to i64
  store i64 %194, i64* %rax
  store volatile i64 40766, i64* @assembly_address
  %195 = load i64* %rax
  %196 = mul i64 %195, 4
  store i64 %196, i64* %rdx
  store volatile i64 40774, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 40781, i64* @assembly_address
  %197 = load i64* %rdx
  %198 = load i64* %rax
  %199 = mul i64 %198, 1
  %200 = add i64 %197, %199
  %201 = inttoptr i64 %200 to i32*
  %202 = load i32* %201
  %203 = zext i32 %202 to i64
  store i64 %203, i64* %rax
  store volatile i64 40784, i64* @assembly_address
  %204 = load i64* %rax
  %205 = trunc i64 %204 to i32
  store i32 %205, i32* %stack_var_-64
  store volatile i64 40787, i64* @assembly_address
  %206 = load i32* %stack_var_-64
  %207 = zext i32 %206 to i64
  store i64 %207, i64* %rax
  store volatile i64 40790, i64* @assembly_address
  %208 = load i64* %rax
  %209 = trunc i64 %208 to i32
  %210 = sext i32 %209 to i64
  store i64 %210, i64* %rax
  store volatile i64 40792, i64* @assembly_address
  %211 = load i64* %rax
  %212 = mul i64 %211, 4
  store i64 %212, i64* %rdx
  store volatile i64 40800, i64* @assembly_address
  %213 = load i16** %stack_var_-32
  %214 = ptrtoint i16* %213 to i64
  store i64 %214, i64* %rax
  store volatile i64 40804, i64* @assembly_address
  %215 = load i64* %rax
  %216 = load i64* %rdx
  %217 = add i64 %215, %216
  %218 = and i64 %215, 15
  %219 = and i64 %216, 15
  %220 = add i64 %218, %219
  %221 = icmp ugt i64 %220, 15
  %222 = icmp ult i64 %217, %215
  %223 = xor i64 %215, %217
  %224 = xor i64 %216, %217
  %225 = and i64 %223, %224
  %226 = icmp slt i64 %225, 0
  store i1 %221, i1* %az
  store i1 %222, i1* %cf
  store i1 %226, i1* %of
  %227 = icmp eq i64 %217, 0
  store i1 %227, i1* %zf
  %228 = icmp slt i64 %217, 0
  store i1 %228, i1* %sf
  %229 = trunc i64 %217 to i8
  %230 = call i8 @llvm.ctpop.i8(i8 %229)
  %231 = and i8 %230, 1
  %232 = icmp eq i8 %231, 0
  store i1 %232, i1* %pf
  store i64 %217, i64* %rax
  store volatile i64 40807, i64* @assembly_address
  %233 = load i64* %rax
  %234 = add i64 %233, 2
  %235 = inttoptr i64 %234 to i16*
  %236 = load i16* %235
  %237 = zext i16 %236 to i64
  store i64 %237, i64* %rax
  store volatile i64 40811, i64* @assembly_address
  %238 = load i64* %rax
  %239 = trunc i64 %238 to i16
  %240 = zext i16 %239 to i64
  store i64 %240, i64* %rax
  store volatile i64 40814, i64* @assembly_address
  %241 = load i64* %rax
  %242 = mul i64 %241, 4
  store i64 %242, i64* %rdx
  store volatile i64 40822, i64* @assembly_address
  %243 = load i16** %stack_var_-32
  %244 = ptrtoint i16* %243 to i64
  store i64 %244, i64* %rax
  store volatile i64 40826, i64* @assembly_address
  %245 = load i64* %rax
  %246 = load i64* %rdx
  %247 = add i64 %245, %246
  %248 = and i64 %245, 15
  %249 = and i64 %246, 15
  %250 = add i64 %248, %249
  %251 = icmp ugt i64 %250, 15
  %252 = icmp ult i64 %247, %245
  %253 = xor i64 %245, %247
  %254 = xor i64 %246, %247
  %255 = and i64 %253, %254
  %256 = icmp slt i64 %255, 0
  store i1 %251, i1* %az
  store i1 %252, i1* %cf
  store i1 %256, i1* %of
  %257 = icmp eq i64 %247, 0
  store i1 %257, i1* %zf
  %258 = icmp slt i64 %247, 0
  store i1 %258, i1* %sf
  %259 = trunc i64 %247 to i8
  %260 = call i8 @llvm.ctpop.i8(i8 %259)
  %261 = and i8 %260, 1
  %262 = icmp eq i8 %261, 0
  store i1 %262, i1* %pf
  store i64 %247, i64* %rax
  store volatile i64 40829, i64* @assembly_address
  %263 = load i64* %rax
  %264 = add i64 %263, 2
  %265 = inttoptr i64 %264 to i16*
  %266 = load i16* %265
  %267 = zext i16 %266 to i64
  store i64 %267, i64* %rax
  store volatile i64 40833, i64* @assembly_address
  %268 = load i64* %rax
  %269 = trunc i64 %268 to i16
  %270 = zext i16 %269 to i64
  store i64 %270, i64* %rax
  store volatile i64 40836, i64* @assembly_address
  %271 = load i64* %rax
  %272 = trunc i64 %271 to i32
  %273 = add i32 %272, 1
  %274 = and i32 %272, 15
  %275 = add i32 %274, 1
  %276 = icmp ugt i32 %275, 15
  %277 = icmp ult i32 %273, %272
  %278 = xor i32 %272, %273
  %279 = xor i32 1, %273
  %280 = and i32 %278, %279
  %281 = icmp slt i32 %280, 0
  store i1 %276, i1* %az
  store i1 %277, i1* %cf
  store i1 %281, i1* %of
  %282 = icmp eq i32 %273, 0
  store i1 %282, i1* %zf
  %283 = icmp slt i32 %273, 0
  store i1 %283, i1* %sf
  %284 = trunc i32 %273 to i8
  %285 = call i8 @llvm.ctpop.i8(i8 %284)
  %286 = and i8 %285, 1
  %287 = icmp eq i8 %286, 0
  store i1 %287, i1* %pf
  %288 = zext i32 %273 to i64
  store i64 %288, i64* %rax
  store volatile i64 40839, i64* @assembly_address
  %289 = load i64* %rax
  %290 = trunc i64 %289 to i32
  %291 = sext i32 %290 to i128
  store i128 %291, i128* %stack_var_-60
  store volatile i64 40842, i64* @assembly_address
  %292 = load i128* %stack_var_-60
  %293 = trunc i128 %292 to i32
  %294 = zext i32 %293 to i64
  store i64 %294, i64* %rax
  store volatile i64 40845, i64* @assembly_address
  %295 = load i64* %rax
  %296 = trunc i64 %295 to i32
  %297 = load i32* %stack_var_-40
  %298 = sext i64 %295 to i128
  store i128 %298, i128* %19
  %299 = sext i32 %297 to i128
  store i128 %299, i128* %17
  %300 = sub i32 %296, %297
  %301 = and i32 %296, 15
  %302 = and i32 %297, 15
  %303 = sub i32 %301, %302
  %304 = icmp ugt i32 %303, 15
  %305 = icmp ult i32 %296, %297
  %306 = xor i32 %296, %297
  %307 = xor i32 %296, %300
  %308 = and i32 %306, %307
  %309 = icmp slt i32 %308, 0
  store i1 %304, i1* %az
  store i1 %305, i1* %cf
  store i1 %309, i1* %of
  %310 = icmp eq i32 %300, 0
  store i1 %310, i1* %zf
  %311 = icmp slt i32 %300, 0
  store i1 %311, i1* %sf
  %312 = trunc i32 %300 to i8
  %313 = call i8 @llvm.ctpop.i8(i8 %312)
  %314 = and i8 %313, 1
  %315 = icmp eq i8 %314, 0
  store i1 %315, i1* %pf
  store volatile i64 40848, i64* @assembly_address
  %316 = load i128* %19
  %317 = trunc i128 %316 to i64
  %318 = load i128* %17
  %319 = trunc i128 %318 to i32
  %320 = trunc i64 %317 to i32
  %321 = icmp sle i32 %320, %319
  br i1 %321, label %block_9f9c, label %block_9f92

block_9f92:                                       ; preds = %block_9f39
  store volatile i64 40850, i64* @assembly_address
  %322 = load i32* %stack_var_-40
  %323 = zext i32 %322 to i64
  store i64 %323, i64* %rax
  store volatile i64 40853, i64* @assembly_address
  %324 = load i64* %rax
  %325 = trunc i64 %324 to i32
  %326 = sext i32 %325 to i128
  store i128 %326, i128* %stack_var_-60
  store volatile i64 40856, i64* @assembly_address
  %327 = load i32* %stack_var_-52
  %328 = add i32 %327, 1
  %329 = and i32 %327, 15
  %330 = add i32 %329, 1
  %331 = icmp ugt i32 %330, 15
  %332 = icmp ult i32 %328, %327
  %333 = xor i32 %327, %328
  %334 = xor i32 1, %328
  %335 = and i32 %333, %334
  %336 = icmp slt i32 %335, 0
  store i1 %331, i1* %az
  store i1 %332, i1* %cf
  store i1 %336, i1* %of
  %337 = icmp eq i32 %328, 0
  store i1 %337, i1* %zf
  %338 = icmp slt i32 %328, 0
  store i1 %338, i1* %sf
  %339 = trunc i32 %328 to i8
  %340 = call i8 @llvm.ctpop.i8(i8 %339)
  %341 = and i8 %340, 1
  %342 = icmp eq i8 %341, 0
  store i1 %342, i1* %pf
  store i32 %328, i32* %stack_var_-52
  br label %block_9f9c

block_9f9c:                                       ; preds = %block_9f92, %block_9f39
  store volatile i64 40860, i64* @assembly_address
  %343 = load i32* %stack_var_-64
  %344 = zext i32 %343 to i64
  store i64 %344, i64* %rax
  store volatile i64 40863, i64* @assembly_address
  %345 = load i64* %rax
  %346 = trunc i64 %345 to i32
  %347 = sext i32 %346 to i64
  store i64 %347, i64* %rax
  store volatile i64 40865, i64* @assembly_address
  %348 = load i64* %rax
  %349 = mul i64 %348, 4
  store i64 %349, i64* %rdx
  store volatile i64 40873, i64* @assembly_address
  %350 = load i16** %stack_var_-32
  %351 = ptrtoint i16* %350 to i64
  store i64 %351, i64* %rax
  store volatile i64 40877, i64* @assembly_address
  %352 = load i64* %rax
  %353 = load i64* %rdx
  %354 = add i64 %352, %353
  %355 = and i64 %352, 15
  %356 = and i64 %353, 15
  %357 = add i64 %355, %356
  %358 = icmp ugt i64 %357, 15
  %359 = icmp ult i64 %354, %352
  %360 = xor i64 %352, %354
  %361 = xor i64 %353, %354
  %362 = and i64 %360, %361
  %363 = icmp slt i64 %362, 0
  store i1 %358, i1* %az
  store i1 %359, i1* %cf
  store i1 %363, i1* %of
  %364 = icmp eq i64 %354, 0
  store i1 %364, i1* %zf
  %365 = icmp slt i64 %354, 0
  store i1 %365, i1* %sf
  %366 = trunc i64 %354 to i8
  %367 = call i8 @llvm.ctpop.i8(i8 %366)
  %368 = and i8 %367, 1
  %369 = icmp eq i8 %368, 0
  store i1 %369, i1* %pf
  store i64 %354, i64* %rax
  store volatile i64 40880, i64* @assembly_address
  %370 = load i128* %stack_var_-60
  %371 = trunc i128 %370 to i32
  %372 = zext i32 %371 to i64
  store i64 %372, i64* %rdx
  store volatile i64 40883, i64* @assembly_address
  %373 = load i64* %rdx
  %374 = trunc i64 %373 to i16
  %375 = load i64* %rax
  %376 = add i64 %375, 2
  %377 = inttoptr i64 %376 to i16*
  store i16 %374, i16* %377
  store volatile i64 40887, i64* @assembly_address
  %378 = load i32* %stack_var_-64
  %379 = zext i32 %378 to i64
  store i64 %379, i64* %rax
  store volatile i64 40890, i64* @assembly_address
  %380 = load i64* %rax
  %381 = trunc i64 %380 to i32
  %382 = load i32* %stack_var_-44
  %383 = trunc i64 %380 to i32
  store i32 %383, i32* %15
  store i32 %382, i32* %14
  %384 = sub i32 %381, %382
  %385 = and i32 %381, 15
  %386 = and i32 %382, 15
  %387 = sub i32 %385, %386
  %388 = icmp ugt i32 %387, 15
  %389 = icmp ult i32 %381, %382
  %390 = xor i32 %381, %382
  %391 = xor i32 %381, %384
  %392 = and i32 %390, %391
  %393 = icmp slt i32 %392, 0
  store i1 %388, i1* %az
  store i1 %389, i1* %cf
  store i1 %393, i1* %of
  %394 = icmp eq i32 %384, 0
  store i1 %394, i1* %zf
  %395 = icmp slt i32 %384, 0
  store i1 %395, i1* %sf
  %396 = trunc i32 %384 to i8
  %397 = call i8 @llvm.ctpop.i8(i8 %396)
  %398 = and i8 %397, 1
  %399 = icmp eq i8 %398, 0
  store i1 %399, i1* %pf
  store volatile i64 40893, i64* @assembly_address
  %400 = load i32* %15
  %401 = sext i32 %400 to i64
  %402 = load i32* %14
  %403 = trunc i64 %401 to i32
  %404 = icmp sgt i32 %403, %402
  br i1 %404, label %block_a09b, label %block_9fc3

block_9fc3:                                       ; preds = %block_9f9c
  store volatile i64 40899, i64* @assembly_address
  %405 = load i128* %stack_var_-60
  %406 = trunc i128 %405 to i32
  %407 = zext i32 %406 to i64
  store i64 %407, i64* %rax
  store volatile i64 40902, i64* @assembly_address
  %408 = load i64* %rax
  %409 = trunc i64 %408 to i32
  %410 = sext i32 %409 to i64
  store i64 %410, i64* %rax
  store volatile i64 40904, i64* @assembly_address
  %411 = load i64* %rax
  %412 = load i64* %rax
  %413 = mul i64 %412, 1
  %414 = add i64 %411, %413
  store i64 %414, i64* %rdx
  store volatile i64 40908, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 40915, i64* @assembly_address
  %415 = load i64* %rdx
  %416 = load i64* %rax
  %417 = mul i64 %416, 1
  %418 = add i64 %415, %417
  %419 = inttoptr i64 %418 to i16*
  %420 = load i16* %419
  %421 = zext i16 %420 to i64
  store i64 %421, i64* %rax
  store volatile i64 40919, i64* @assembly_address
  %422 = load i64* %rax
  %423 = add i64 %422, 1
  %424 = trunc i64 %423 to i32
  %425 = zext i32 %424 to i64
  store i64 %425, i64* %rcx
  store volatile i64 40922, i64* @assembly_address
  %426 = load i128* %stack_var_-60
  %427 = trunc i128 %426 to i32
  %428 = zext i32 %427 to i64
  store i64 %428, i64* %rax
  store volatile i64 40925, i64* @assembly_address
  %429 = load i64* %rax
  %430 = trunc i64 %429 to i32
  %431 = sext i32 %430 to i64
  store i64 %431, i64* %rax
  store volatile i64 40927, i64* @assembly_address
  %432 = load i64* %rax
  %433 = load i64* %rax
  %434 = mul i64 %433, 1
  %435 = add i64 %432, %434
  store i64 %435, i64* %rdx
  store volatile i64 40931, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 40938, i64* @assembly_address
  %436 = load i64* %rcx
  %437 = trunc i64 %436 to i16
  %438 = load i64* %rdx
  %439 = load i64* %rax
  %440 = mul i64 %439, 1
  %441 = add i64 %438, %440
  %442 = inttoptr i64 %441 to i16*
  store i16 %437, i16* %442
  store volatile i64 40942, i64* @assembly_address
  store i32 0, i32* %stack_var_-56
  store volatile i64 40949, i64* @assembly_address
  %443 = load i32* %stack_var_-64
  %444 = zext i32 %443 to i64
  store i64 %444, i64* %rax
  store volatile i64 40952, i64* @assembly_address
  %445 = load i64* %rax
  %446 = trunc i64 %445 to i32
  %447 = load i32* %stack_var_-48
  %448 = trunc i64 %445 to i32
  store i32 %448, i32* %12
  store i32 %447, i32* %11
  %449 = sub i32 %446, %447
  %450 = and i32 %446, 15
  %451 = and i32 %447, 15
  %452 = sub i32 %450, %451
  %453 = icmp ugt i32 %452, 15
  %454 = icmp ult i32 %446, %447
  %455 = xor i32 %446, %447
  %456 = xor i32 %446, %449
  %457 = and i32 %455, %456
  %458 = icmp slt i32 %457, 0
  store i1 %453, i1* %az
  store i1 %454, i1* %cf
  store i1 %458, i1* %of
  %459 = icmp eq i32 %449, 0
  store i1 %459, i1* %zf
  %460 = icmp slt i32 %449, 0
  store i1 %460, i1* %sf
  %461 = trunc i32 %449 to i8
  %462 = call i8 @llvm.ctpop.i8(i8 %461)
  %463 = and i8 %462, 1
  %464 = icmp eq i8 %463, 0
  store i1 %464, i1* %pf
  store volatile i64 40955, i64* @assembly_address
  %465 = load i32* %12
  %466 = sext i32 %465 to i64
  %467 = load i32* %11
  %468 = trunc i64 %466 to i32
  %469 = icmp slt i32 %468, %467
  br i1 %469, label %block_a019, label %block_9ffd

block_9ffd:                                       ; preds = %block_9fc3
  store volatile i64 40957, i64* @assembly_address
  %470 = load i32* %stack_var_-64
  %471 = zext i32 %470 to i64
  store i64 %471, i64* %rax
  store volatile i64 40960, i64* @assembly_address
  %472 = load i64* %rax
  %473 = trunc i64 %472 to i32
  %474 = load i32* %stack_var_-48
  %475 = sub i32 %473, %474
  %476 = and i32 %473, 15
  %477 = and i32 %474, 15
  %478 = sub i32 %476, %477
  %479 = icmp ugt i32 %478, 15
  %480 = icmp ult i32 %473, %474
  %481 = xor i32 %473, %474
  %482 = xor i32 %473, %475
  %483 = and i32 %481, %482
  %484 = icmp slt i32 %483, 0
  store i1 %479, i1* %az
  store i1 %480, i1* %cf
  store i1 %484, i1* %of
  %485 = icmp eq i32 %475, 0
  store i1 %485, i1* %zf
  %486 = icmp slt i32 %475, 0
  store i1 %486, i1* %sf
  %487 = trunc i32 %475 to i8
  %488 = call i8 @llvm.ctpop.i8(i8 %487)
  %489 = and i8 %488, 1
  %490 = icmp eq i8 %489, 0
  store i1 %490, i1* %pf
  %491 = zext i32 %475 to i64
  store i64 %491, i64* %rax
  store volatile i64 40963, i64* @assembly_address
  %492 = load i64* %rax
  %493 = trunc i64 %492 to i32
  %494 = sext i32 %493 to i64
  store i64 %494, i64* %rax
  store volatile i64 40965, i64* @assembly_address
  %495 = load i64* %rax
  %496 = mul i64 %495, 4
  store i64 %496, i64* %rdx
  store volatile i64 40973, i64* @assembly_address
  %497 = load i32** %stack_var_-24
  %498 = ptrtoint i32* %497 to i64
  store i64 %498, i64* %rax
  store volatile i64 40977, i64* @assembly_address
  %499 = load i64* %rax
  %500 = load i64* %rdx
  %501 = add i64 %499, %500
  %502 = and i64 %499, 15
  %503 = and i64 %500, 15
  %504 = add i64 %502, %503
  %505 = icmp ugt i64 %504, 15
  %506 = icmp ult i64 %501, %499
  %507 = xor i64 %499, %501
  %508 = xor i64 %500, %501
  %509 = and i64 %507, %508
  %510 = icmp slt i64 %509, 0
  store i1 %505, i1* %az
  store i1 %506, i1* %cf
  store i1 %510, i1* %of
  %511 = icmp eq i64 %501, 0
  store i1 %511, i1* %zf
  %512 = icmp slt i64 %501, 0
  store i1 %512, i1* %sf
  %513 = trunc i64 %501 to i8
  %514 = call i8 @llvm.ctpop.i8(i8 %513)
  %515 = and i8 %514, 1
  %516 = icmp eq i8 %515, 0
  store i1 %516, i1* %pf
  store i64 %501, i64* %rax
  store volatile i64 40980, i64* @assembly_address
  %517 = load i64* %rax
  %518 = inttoptr i64 %517 to i32*
  %519 = load i32* %518
  %520 = zext i32 %519 to i64
  store i64 %520, i64* %rax
  store volatile i64 40982, i64* @assembly_address
  %521 = load i64* %rax
  %522 = trunc i64 %521 to i32
  store i32 %522, i32* %stack_var_-56
  br label %block_a019

block_a019:                                       ; preds = %block_9ffd, %block_9fc3
  store volatile i64 40985, i64* @assembly_address
  %523 = load i32* %stack_var_-64
  %524 = zext i32 %523 to i64
  store i64 %524, i64* %rax
  store volatile i64 40988, i64* @assembly_address
  %525 = load i64* %rax
  %526 = trunc i64 %525 to i32
  %527 = sext i32 %526 to i64
  store i64 %527, i64* %rax
  store volatile i64 40990, i64* @assembly_address
  %528 = load i64* %rax
  %529 = mul i64 %528, 4
  store i64 %529, i64* %rdx
  store volatile i64 40998, i64* @assembly_address
  %530 = load i16** %stack_var_-32
  %531 = ptrtoint i16* %530 to i64
  store i64 %531, i64* %rax
  store volatile i64 41002, i64* @assembly_address
  %532 = load i64* %rax
  %533 = load i64* %rdx
  %534 = add i64 %532, %533
  %535 = and i64 %532, 15
  %536 = and i64 %533, 15
  %537 = add i64 %535, %536
  %538 = icmp ugt i64 %537, 15
  %539 = icmp ult i64 %534, %532
  %540 = xor i64 %532, %534
  %541 = xor i64 %533, %534
  %542 = and i64 %540, %541
  %543 = icmp slt i64 %542, 0
  store i1 %538, i1* %az
  store i1 %539, i1* %cf
  store i1 %543, i1* %of
  %544 = icmp eq i64 %534, 0
  store i1 %544, i1* %zf
  %545 = icmp slt i64 %534, 0
  store i1 %545, i1* %sf
  %546 = trunc i64 %534 to i8
  %547 = call i8 @llvm.ctpop.i8(i8 %546)
  %548 = and i8 %547, 1
  %549 = icmp eq i8 %548, 0
  store i1 %549, i1* %pf
  store i64 %534, i64* %rax
  store volatile i64 41005, i64* @assembly_address
  %550 = load i64* %rax
  %551 = inttoptr i64 %550 to i16*
  %552 = load i16* %551
  %553 = zext i16 %552 to i64
  store i64 %553, i64* %rax
  store volatile i64 41008, i64* @assembly_address
  %554 = load i64* %rax
  %555 = trunc i64 %554 to i16
  %556 = sext i16 %555 to i128
  store i128 %556, i128* %stack_var_-70
  store volatile i64 41012, i64* @assembly_address
  %557 = load i128* %stack_var_-70
  %558 = trunc i128 %557 to i16
  %559 = zext i16 %558 to i64
  store i64 %559, i64* %rdx
  store volatile i64 41016, i64* @assembly_address
  %560 = load i128* %stack_var_-60
  %561 = trunc i128 %560 to i32
  %562 = zext i32 %561 to i64
  store i64 %562, i64* %rcx
  store volatile i64 41019, i64* @assembly_address
  %563 = load i32* %stack_var_-56
  %564 = zext i32 %563 to i64
  store i64 %564, i64* %rax
  store volatile i64 41022, i64* @assembly_address
  %565 = load i64* %rax
  %566 = trunc i64 %565 to i32
  %567 = load i64* %rcx
  %568 = trunc i64 %567 to i32
  %569 = add i32 %566, %568
  %570 = and i32 %566, 15
  %571 = and i32 %568, 15
  %572 = add i32 %570, %571
  %573 = icmp ugt i32 %572, 15
  %574 = icmp ult i32 %569, %566
  %575 = xor i32 %566, %569
  %576 = xor i32 %568, %569
  %577 = and i32 %575, %576
  %578 = icmp slt i32 %577, 0
  store i1 %573, i1* %az
  store i1 %574, i1* %cf
  store i1 %578, i1* %of
  %579 = icmp eq i32 %569, 0
  store i1 %579, i1* %zf
  %580 = icmp slt i32 %569, 0
  store i1 %580, i1* %sf
  %581 = trunc i32 %569 to i8
  %582 = call i8 @llvm.ctpop.i8(i8 %581)
  %583 = and i8 %582, 1
  %584 = icmp eq i8 %583, 0
  store i1 %584, i1* %pf
  %585 = zext i32 %569 to i64
  store i64 %585, i64* %rax
  store volatile i64 41024, i64* @assembly_address
  %586 = load i64* %rax
  %587 = trunc i64 %586 to i32
  %588 = sext i32 %587 to i64
  store i64 %588, i64* %rax
  store volatile i64 41026, i64* @assembly_address
  %589 = load i64* %rdx
  %590 = load i64* %rax
  %591 = sext i64 %589 to i128
  %592 = sext i64 %590 to i128
  %593 = mul i128 %591, %592
  %594 = trunc i128 %593 to i64
  store i64 %594, i64* %rdx
  %595 = trunc i128 %593 to i64
  %596 = sext i64 %595 to i128
  %597 = icmp ne i128 %593, %596
  store i1 %597, i1* %of
  store i1 %597, i1* %cf
  store volatile i64 41030, i64* @assembly_address
  %598 = load i64* @global_var_219ed0
  store i64 %598, i64* %rax
  store volatile i64 41037, i64* @assembly_address
  %599 = load i64* %rax
  %600 = load i64* %rdx
  %601 = add i64 %599, %600
  %602 = and i64 %599, 15
  %603 = and i64 %600, 15
  %604 = add i64 %602, %603
  %605 = icmp ugt i64 %604, 15
  %606 = icmp ult i64 %601, %599
  %607 = xor i64 %599, %601
  %608 = xor i64 %600, %601
  %609 = and i64 %607, %608
  %610 = icmp slt i64 %609, 0
  store i1 %605, i1* %az
  store i1 %606, i1* %cf
  store i1 %610, i1* %of
  %611 = icmp eq i64 %601, 0
  store i1 %611, i1* %zf
  %612 = icmp slt i64 %601, 0
  store i1 %612, i1* %sf
  %613 = trunc i64 %601 to i8
  %614 = call i8 @llvm.ctpop.i8(i8 %613)
  %615 = and i8 %614, 1
  %616 = icmp eq i8 %615, 0
  store i1 %616, i1* %pf
  store i64 %601, i64* %rax
  store volatile i64 41040, i64* @assembly_address
  %617 = load i64* %rax
  store i64 %617, i64* @global_var_219ed0
  store volatile i64 41047, i64* @assembly_address
  %618 = load i64* %stack_var_-16
  %619 = and i64 %618, 15
  %620 = icmp ugt i64 %619, 15
  %621 = icmp ult i64 %618, 0
  %622 = xor i64 %618, 0
  %623 = and i64 %622, 0
  %624 = icmp slt i64 %623, 0
  store i1 %620, i1* %az
  store i1 %621, i1* %cf
  store i1 %624, i1* %of
  %625 = icmp eq i64 %618, 0
  store i1 %625, i1* %zf
  %626 = icmp slt i64 %618, 0
  store i1 %626, i1* %sf
  %627 = trunc i64 %618 to i8
  %628 = call i8 @llvm.ctpop.i8(i8 %627)
  %629 = and i8 %628, 1
  %630 = icmp eq i8 %629, 0
  store i1 %630, i1* %pf
  store volatile i64 41052, i64* @assembly_address
  %631 = load i1* %zf
  br i1 %631, label %block_a09c, label %block_a05e

block_a05e:                                       ; preds = %block_a019
  store volatile i64 41054, i64* @assembly_address
  %632 = load i128* %stack_var_-70
  %633 = trunc i128 %632 to i16
  %634 = zext i16 %633 to i64
  store i64 %634, i64* %rdx
  store volatile i64 41058, i64* @assembly_address
  %635 = load i32* %stack_var_-64
  %636 = zext i32 %635 to i64
  store i64 %636, i64* %rax
  store volatile i64 41061, i64* @assembly_address
  %637 = load i64* %rax
  %638 = trunc i64 %637 to i32
  %639 = sext i32 %638 to i64
  store i64 %639, i64* %rax
  store volatile i64 41063, i64* @assembly_address
  %640 = load i64* %rax
  %641 = mul i64 %640, 4
  store i64 %641, i64* %rcx
  store volatile i64 41071, i64* @assembly_address
  %642 = load i64* %stack_var_-16
  store i64 %642, i64* %rax
  store volatile i64 41075, i64* @assembly_address
  %643 = load i64* %rax
  %644 = load i64* %rcx
  %645 = add i64 %643, %644
  %646 = and i64 %643, 15
  %647 = and i64 %644, 15
  %648 = add i64 %646, %647
  %649 = icmp ugt i64 %648, 15
  %650 = icmp ult i64 %645, %643
  %651 = xor i64 %643, %645
  %652 = xor i64 %644, %645
  %653 = and i64 %651, %652
  %654 = icmp slt i64 %653, 0
  store i1 %649, i1* %az
  store i1 %650, i1* %cf
  store i1 %654, i1* %of
  %655 = icmp eq i64 %645, 0
  store i1 %655, i1* %zf
  %656 = icmp slt i64 %645, 0
  store i1 %656, i1* %sf
  %657 = trunc i64 %645 to i8
  %658 = call i8 @llvm.ctpop.i8(i8 %657)
  %659 = and i8 %658, 1
  %660 = icmp eq i8 %659, 0
  store i1 %660, i1* %pf
  store i64 %645, i64* %rax
  store volatile i64 41078, i64* @assembly_address
  %661 = load i64* %rax
  %662 = add i64 %661, 2
  %663 = inttoptr i64 %662 to i16*
  %664 = load i16* %663
  %665 = zext i16 %664 to i64
  store i64 %665, i64* %rax
  store volatile i64 41082, i64* @assembly_address
  %666 = load i64* %rax
  %667 = trunc i64 %666 to i16
  %668 = zext i16 %667 to i64
  store i64 %668, i64* %rcx
  store volatile i64 41085, i64* @assembly_address
  %669 = load i32* %stack_var_-56
  %670 = zext i32 %669 to i64
  store i64 %670, i64* %rax
  store volatile i64 41088, i64* @assembly_address
  %671 = load i64* %rax
  %672 = trunc i64 %671 to i32
  %673 = load i64* %rcx
  %674 = trunc i64 %673 to i32
  %675 = add i32 %672, %674
  %676 = and i32 %672, 15
  %677 = and i32 %674, 15
  %678 = add i32 %676, %677
  %679 = icmp ugt i32 %678, 15
  %680 = icmp ult i32 %675, %672
  %681 = xor i32 %672, %675
  %682 = xor i32 %674, %675
  %683 = and i32 %681, %682
  %684 = icmp slt i32 %683, 0
  store i1 %679, i1* %az
  store i1 %680, i1* %cf
  store i1 %684, i1* %of
  %685 = icmp eq i32 %675, 0
  store i1 %685, i1* %zf
  %686 = icmp slt i32 %675, 0
  store i1 %686, i1* %sf
  %687 = trunc i32 %675 to i8
  %688 = call i8 @llvm.ctpop.i8(i8 %687)
  %689 = and i8 %688, 1
  %690 = icmp eq i8 %689, 0
  store i1 %690, i1* %pf
  %691 = zext i32 %675 to i64
  store i64 %691, i64* %rax
  store volatile i64 41090, i64* @assembly_address
  %692 = load i64* %rax
  %693 = trunc i64 %692 to i32
  %694 = sext i32 %693 to i64
  store i64 %694, i64* %rax
  store volatile i64 41092, i64* @assembly_address
  %695 = load i64* %rdx
  %696 = load i64* %rax
  %697 = sext i64 %695 to i128
  %698 = sext i64 %696 to i128
  %699 = mul i128 %697, %698
  %700 = trunc i128 %699 to i64
  store i64 %700, i64* %rdx
  %701 = trunc i128 %699 to i64
  %702 = sext i64 %701 to i128
  %703 = icmp ne i128 %699, %702
  store i1 %703, i1* %of
  store i1 %703, i1* %cf
  store volatile i64 41096, i64* @assembly_address
  %704 = load i64* @global_var_219ed8
  store i64 %704, i64* %rax
  store volatile i64 41103, i64* @assembly_address
  %705 = load i64* %rax
  %706 = load i64* %rdx
  %707 = add i64 %705, %706
  %708 = and i64 %705, 15
  %709 = and i64 %706, 15
  %710 = add i64 %708, %709
  %711 = icmp ugt i64 %710, 15
  %712 = icmp ult i64 %707, %705
  %713 = xor i64 %705, %707
  %714 = xor i64 %706, %707
  %715 = and i64 %713, %714
  %716 = icmp slt i64 %715, 0
  store i1 %711, i1* %az
  store i1 %712, i1* %cf
  store i1 %716, i1* %of
  %717 = icmp eq i64 %707, 0
  store i1 %717, i1* %zf
  %718 = icmp slt i64 %707, 0
  store i1 %718, i1* %sf
  %719 = trunc i64 %707 to i8
  %720 = call i8 @llvm.ctpop.i8(i8 %719)
  %721 = and i8 %720, 1
  %722 = icmp eq i8 %721, 0
  store i1 %722, i1* %pf
  store i64 %707, i64* %rax
  store volatile i64 41106, i64* @assembly_address
  %723 = load i64* %rax
  store i64 %723, i64* @global_var_219ed8
  store volatile i64 41113, i64* @assembly_address
  br label %block_a09c

block_a09b:                                       ; preds = %block_9f9c
  store volatile i64 41115, i64* @assembly_address
  br label %block_a09c

block_a09c:                                       ; preds = %block_a09b, %block_a05e, %block_a019
  store volatile i64 41116, i64* @assembly_address
  %724 = load i32* %stack_var_-68
  %725 = add i32 %724, 1
  %726 = and i32 %724, 15
  %727 = add i32 %726, 1
  %728 = icmp ugt i32 %727, 15
  %729 = icmp ult i32 %725, %724
  %730 = xor i32 %724, %725
  %731 = xor i32 1, %725
  %732 = and i32 %730, %731
  %733 = icmp slt i32 %732, 0
  store i1 %728, i1* %az
  store i1 %729, i1* %cf
  store i1 %733, i1* %of
  %734 = icmp eq i32 %725, 0
  store i1 %734, i1* %zf
  %735 = icmp slt i32 %725, 0
  store i1 %735, i1* %sf
  %736 = trunc i32 %725 to i8
  %737 = call i8 @llvm.ctpop.i8(i8 %736)
  %738 = and i8 %737, 1
  %739 = icmp eq i8 %738, 0
  store i1 %739, i1* %pf
  store i32 %725, i32* %stack_var_-68
  br label %block_a0a0

block_a0a0:                                       ; preds = %block_a09c, %block_9ef7
  store volatile i64 41120, i64* @assembly_address
  %740 = load i32* %stack_var_-68
  store i32 %740, i32* %10
  %741 = inttoptr i32 ptrtoint ([24 x i8]* @global_var_23c to i32) to i8*
  store i8* %741, i8** %8
  %742 = sub i32 %740, 572
  %743 = and i32 %740, 15
  %744 = sub i32 %743, 12
  %745 = icmp ugt i32 %744, 15
  %746 = icmp ult i32 %740, 572
  %747 = xor i32 %740, 572
  %748 = xor i32 %740, %742
  %749 = and i32 %747, %748
  %750 = icmp slt i32 %749, 0
  store i1 %745, i1* %az
  store i1 %746, i1* %cf
  store i1 %750, i1* %of
  %751 = icmp eq i32 %742, 0
  store i1 %751, i1* %zf
  %752 = icmp slt i32 %742, 0
  store i1 %752, i1* %sf
  %753 = trunc i32 %742 to i8
  %754 = call i8 @llvm.ctpop.i8(i8 %753)
  %755 = and i8 %754, 1
  %756 = icmp eq i8 %755, 0
  store i1 %756, i1* %pf
  store volatile i64 41127, i64* @assembly_address
  %757 = load i32* %10
  %758 = load i8** %8
  %759 = ptrtoint i8* %758 to i32
  %760 = icmp sle i32 %757, %759
  br i1 %760, label %block_9f39, label %block_a0ad

block_a0ad:                                       ; preds = %block_a0a0
  store volatile i64 41133, i64* @assembly_address
  %761 = load i32* %stack_var_-52
  %762 = and i32 %761, 15
  %763 = icmp ugt i32 %762, 15
  %764 = icmp ult i32 %761, 0
  %765 = xor i32 %761, 0
  %766 = and i32 %765, 0
  %767 = icmp slt i32 %766, 0
  store i1 %763, i1* %az
  store i1 %764, i1* %cf
  store i1 %767, i1* %of
  %768 = icmp eq i32 %761, 0
  store i1 %768, i1* %zf
  %769 = icmp slt i32 %761, 0
  store i1 %769, i1* %sf
  %770 = trunc i32 %761 to i8
  %771 = call i8 @llvm.ctpop.i8(i8 %770)
  %772 = and i8 %771, 1
  %773 = icmp eq i8 %772, 0
  store i1 %773, i1* %pf
  store volatile i64 41137, i64* @assembly_address
  %774 = load i1* %zf
  br i1 %774, label %block_a27b, label %block_a0b7

block_a0b7:                                       ; preds = %block_a0df, %block_a0ad
  store volatile i64 41143, i64* @assembly_address
  %775 = load i32* %stack_var_-40
  %776 = zext i32 %775 to i64
  store i64 %776, i64* %rax
  store volatile i64 41146, i64* @assembly_address
  %777 = load i64* %rax
  %778 = trunc i64 %777 to i32
  %779 = sub i32 %778, 1
  %780 = and i32 %778, 15
  %781 = sub i32 %780, 1
  %782 = icmp ugt i32 %781, 15
  %783 = icmp ult i32 %778, 1
  %784 = xor i32 %778, 1
  %785 = xor i32 %778, %779
  %786 = and i32 %784, %785
  %787 = icmp slt i32 %786, 0
  store i1 %782, i1* %az
  store i1 %783, i1* %cf
  store i1 %787, i1* %of
  %788 = icmp eq i32 %779, 0
  store i1 %788, i1* %zf
  %789 = icmp slt i32 %779, 0
  store i1 %789, i1* %sf
  %790 = trunc i32 %779 to i8
  %791 = call i8 @llvm.ctpop.i8(i8 %790)
  %792 = and i8 %791, 1
  %793 = icmp eq i8 %792, 0
  store i1 %793, i1* %pf
  %794 = zext i32 %779 to i64
  store i64 %794, i64* %rax
  store volatile i64 41149, i64* @assembly_address
  %795 = load i64* %rax
  %796 = trunc i64 %795 to i32
  %797 = sext i32 %796 to i128
  store i128 %797, i128* %stack_var_-60
  store volatile i64 41152, i64* @assembly_address
  br label %block_a0c6

block_a0c2:                                       ; preds = %block_a0c6
  store volatile i64 41154, i64* @assembly_address
  %798 = load i128* %stack_var_-60
  %799 = trunc i128 %798 to i32
  %800 = sub i32 %799, 1
  %801 = and i32 %799, 15
  %802 = sub i32 %801, 1
  %803 = icmp ugt i32 %802, 15
  %804 = icmp ult i32 %799, 1
  %805 = xor i32 %799, 1
  %806 = xor i32 %799, %800
  %807 = and i32 %805, %806
  %808 = icmp slt i32 %807, 0
  store i1 %803, i1* %az
  store i1 %804, i1* %cf
  store i1 %808, i1* %of
  %809 = icmp eq i32 %800, 0
  store i1 %809, i1* %zf
  %810 = icmp slt i32 %800, 0
  store i1 %810, i1* %sf
  %811 = trunc i32 %800 to i8
  %812 = call i8 @llvm.ctpop.i8(i8 %811)
  %813 = and i8 %812, 1
  %814 = icmp eq i8 %813, 0
  store i1 %814, i1* %pf
  %815 = sext i32 %800 to i128
  store i128 %815, i128* %stack_var_-60
  br label %block_a0c6

block_a0c6:                                       ; preds = %block_a0c2, %block_a0b7
  store volatile i64 41158, i64* @assembly_address
  %816 = load i128* %stack_var_-60
  %817 = trunc i128 %816 to i32
  %818 = zext i32 %817 to i64
  store i64 %818, i64* %rax
  store volatile i64 41161, i64* @assembly_address
  %819 = load i64* %rax
  %820 = trunc i64 %819 to i32
  %821 = sext i32 %820 to i64
  store i64 %821, i64* %rax
  store volatile i64 41163, i64* @assembly_address
  %822 = load i64* %rax
  %823 = load i64* %rax
  %824 = mul i64 %823, 1
  %825 = add i64 %822, %824
  store i64 %825, i64* %rdx
  store volatile i64 41167, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41174, i64* @assembly_address
  %826 = load i64* %rdx
  %827 = load i64* %rax
  %828 = mul i64 %827, 1
  %829 = add i64 %826, %828
  %830 = inttoptr i64 %829 to i16*
  %831 = load i16* %830
  %832 = zext i16 %831 to i64
  store i64 %832, i64* %rax
  store volatile i64 41178, i64* @assembly_address
  %833 = load i64* %rax
  %834 = trunc i64 %833 to i16
  %835 = load i64* %rax
  %836 = trunc i64 %835 to i16
  %837 = and i16 %834, %836
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %838 = icmp eq i16 %837, 0
  store i1 %838, i1* %zf
  %839 = icmp slt i16 %837, 0
  store i1 %839, i1* %sf
  %840 = trunc i16 %837 to i8
  %841 = call i8 @llvm.ctpop.i8(i8 %840)
  %842 = and i8 %841, 1
  %843 = icmp eq i8 %842, 0
  store i1 %843, i1* %pf
  store volatile i64 41181, i64* @assembly_address
  %844 = load i1* %zf
  br i1 %844, label %block_a0c2, label %block_a0df

block_a0df:                                       ; preds = %block_a0c6
  store volatile i64 41183, i64* @assembly_address
  %845 = load i128* %stack_var_-60
  %846 = trunc i128 %845 to i32
  %847 = zext i32 %846 to i64
  store i64 %847, i64* %rax
  store volatile i64 41186, i64* @assembly_address
  %848 = load i64* %rax
  %849 = trunc i64 %848 to i32
  %850 = sext i32 %849 to i64
  store i64 %850, i64* %rax
  store volatile i64 41188, i64* @assembly_address
  %851 = load i64* %rax
  %852 = load i64* %rax
  %853 = mul i64 %852, 1
  %854 = add i64 %851, %853
  store i64 %854, i64* %rdx
  store volatile i64 41192, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41199, i64* @assembly_address
  %855 = load i64* %rdx
  %856 = load i64* %rax
  %857 = mul i64 %856, 1
  %858 = add i64 %855, %857
  %859 = inttoptr i64 %858 to i16*
  %860 = load i16* %859
  %861 = zext i16 %860 to i64
  store i64 %861, i64* %rax
  store volatile i64 41203, i64* @assembly_address
  %862 = load i64* %rax
  %863 = add i64 %862, -1
  %864 = trunc i64 %863 to i32
  %865 = zext i32 %864 to i64
  store i64 %865, i64* %rcx
  store volatile i64 41206, i64* @assembly_address
  %866 = load i128* %stack_var_-60
  %867 = trunc i128 %866 to i32
  %868 = zext i32 %867 to i64
  store i64 %868, i64* %rax
  store volatile i64 41209, i64* @assembly_address
  %869 = load i64* %rax
  %870 = trunc i64 %869 to i32
  %871 = sext i32 %870 to i64
  store i64 %871, i64* %rax
  store volatile i64 41211, i64* @assembly_address
  %872 = load i64* %rax
  %873 = load i64* %rax
  %874 = mul i64 %873, 1
  %875 = add i64 %872, %874
  store i64 %875, i64* %rdx
  store volatile i64 41215, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41222, i64* @assembly_address
  %876 = load i64* %rcx
  %877 = trunc i64 %876 to i16
  %878 = load i64* %rdx
  %879 = load i64* %rax
  %880 = mul i64 %879, 1
  %881 = add i64 %878, %880
  %882 = inttoptr i64 %881 to i16*
  store i16 %877, i16* %882
  store volatile i64 41226, i64* @assembly_address
  %883 = load i128* %stack_var_-60
  %884 = trunc i128 %883 to i32
  %885 = zext i32 %884 to i64
  store i64 %885, i64* %rax
  store volatile i64 41229, i64* @assembly_address
  %886 = load i64* %rax
  %887 = trunc i64 %886 to i32
  %888 = add i32 %887, 1
  %889 = and i32 %887, 15
  %890 = add i32 %889, 1
  %891 = icmp ugt i32 %890, 15
  %892 = icmp ult i32 %888, %887
  %893 = xor i32 %887, %888
  %894 = xor i32 1, %888
  %895 = and i32 %893, %894
  %896 = icmp slt i32 %895, 0
  store i1 %891, i1* %az
  store i1 %892, i1* %cf
  store i1 %896, i1* %of
  %897 = icmp eq i32 %888, 0
  store i1 %897, i1* %zf
  %898 = icmp slt i32 %888, 0
  store i1 %898, i1* %sf
  %899 = trunc i32 %888 to i8
  %900 = call i8 @llvm.ctpop.i8(i8 %899)
  %901 = and i8 %900, 1
  %902 = icmp eq i8 %901, 0
  store i1 %902, i1* %pf
  %903 = zext i32 %888 to i64
  store i64 %903, i64* %rax
  store volatile i64 41232, i64* @assembly_address
  %904 = load i64* %rax
  %905 = trunc i64 %904 to i32
  %906 = sext i32 %905 to i64
  store i64 %906, i64* %rax
  store volatile i64 41234, i64* @assembly_address
  %907 = load i64* %rax
  %908 = load i64* %rax
  %909 = mul i64 %908, 1
  %910 = add i64 %907, %909
  store i64 %910, i64* %rdx
  store volatile i64 41238, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41245, i64* @assembly_address
  %911 = load i64* %rdx
  %912 = load i64* %rax
  %913 = mul i64 %912, 1
  %914 = add i64 %911, %913
  %915 = inttoptr i64 %914 to i16*
  %916 = load i16* %915
  %917 = zext i16 %916 to i64
  store i64 %917, i64* %rax
  store volatile i64 41249, i64* @assembly_address
  %918 = load i128* %stack_var_-60
  %919 = trunc i128 %918 to i32
  %920 = zext i32 %919 to i64
  store i64 %920, i64* %rdx
  store volatile i64 41252, i64* @assembly_address
  %921 = load i64* %rdx
  %922 = trunc i64 %921 to i32
  %923 = add i32 %922, 1
  %924 = and i32 %922, 15
  %925 = add i32 %924, 1
  %926 = icmp ugt i32 %925, 15
  %927 = icmp ult i32 %923, %922
  %928 = xor i32 %922, %923
  %929 = xor i32 1, %923
  %930 = and i32 %928, %929
  %931 = icmp slt i32 %930, 0
  store i1 %926, i1* %az
  store i1 %927, i1* %cf
  store i1 %931, i1* %of
  %932 = icmp eq i32 %923, 0
  store i1 %932, i1* %zf
  %933 = icmp slt i32 %923, 0
  store i1 %933, i1* %sf
  %934 = trunc i32 %923 to i8
  %935 = call i8 @llvm.ctpop.i8(i8 %934)
  %936 = and i8 %935, 1
  %937 = icmp eq i8 %936, 0
  store i1 %937, i1* %pf
  %938 = zext i32 %923 to i64
  store i64 %938, i64* %rdx
  store volatile i64 41255, i64* @assembly_address
  %939 = load i64* %rax
  %940 = add i64 %939, 2
  %941 = trunc i64 %940 to i32
  %942 = zext i32 %941 to i64
  store i64 %942, i64* %rcx
  store volatile i64 41258, i64* @assembly_address
  %943 = load i64* %rdx
  %944 = trunc i64 %943 to i32
  %945 = sext i32 %944 to i64
  store i64 %945, i64* %rax
  store volatile i64 41261, i64* @assembly_address
  %946 = load i64* %rax
  %947 = load i64* %rax
  %948 = mul i64 %947, 1
  %949 = add i64 %946, %948
  store i64 %949, i64* %rdx
  store volatile i64 41265, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41272, i64* @assembly_address
  %950 = load i64* %rcx
  %951 = trunc i64 %950 to i16
  %952 = load i64* %rdx
  %953 = load i64* %rax
  %954 = mul i64 %953, 1
  %955 = add i64 %952, %954
  %956 = inttoptr i64 %955 to i16*
  store i16 %951, i16* %956
  store volatile i64 41276, i64* @assembly_address
  %957 = load i32* %stack_var_-40
  %958 = zext i32 %957 to i64
  store i64 %958, i64* %rax
  store volatile i64 41279, i64* @assembly_address
  %959 = load i64* %rax
  %960 = trunc i64 %959 to i32
  %961 = sext i32 %960 to i64
  store i64 %961, i64* %rax
  store volatile i64 41281, i64* @assembly_address
  %962 = load i64* %rax
  %963 = load i64* %rax
  %964 = mul i64 %963, 1
  %965 = add i64 %962, %964
  store i64 %965, i64* %rdx
  store volatile i64 41285, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41292, i64* @assembly_address
  %966 = load i64* %rdx
  %967 = load i64* %rax
  %968 = mul i64 %967, 1
  %969 = add i64 %966, %968
  %970 = inttoptr i64 %969 to i16*
  %971 = load i16* %970
  %972 = zext i16 %971 to i64
  store i64 %972, i64* %rax
  store volatile i64 41296, i64* @assembly_address
  %973 = load i64* %rax
  %974 = add i64 %973, -1
  %975 = trunc i64 %974 to i32
  %976 = zext i32 %975 to i64
  store i64 %976, i64* %rcx
  store volatile i64 41299, i64* @assembly_address
  %977 = load i32* %stack_var_-40
  %978 = zext i32 %977 to i64
  store i64 %978, i64* %rax
  store volatile i64 41302, i64* @assembly_address
  %979 = load i64* %rax
  %980 = trunc i64 %979 to i32
  %981 = sext i32 %980 to i64
  store i64 %981, i64* %rax
  store volatile i64 41304, i64* @assembly_address
  %982 = load i64* %rax
  %983 = load i64* %rax
  %984 = mul i64 %983, 1
  %985 = add i64 %982, %984
  store i64 %985, i64* %rdx
  store volatile i64 41308, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41315, i64* @assembly_address
  %986 = load i64* %rcx
  %987 = trunc i64 %986 to i16
  %988 = load i64* %rdx
  %989 = load i64* %rax
  %990 = mul i64 %989, 1
  %991 = add i64 %988, %990
  %992 = inttoptr i64 %991 to i16*
  store i16 %987, i16* %992
  store volatile i64 41319, i64* @assembly_address
  %993 = load i32* %stack_var_-52
  %994 = sub i32 %993, 2
  %995 = and i32 %993, 15
  %996 = sub i32 %995, 2
  %997 = icmp ugt i32 %996, 15
  %998 = icmp ult i32 %993, 2
  %999 = xor i32 %993, 2
  %1000 = xor i32 %993, %994
  %1001 = and i32 %999, %1000
  %1002 = icmp slt i32 %1001, 0
  store i1 %997, i1* %az
  store i1 %998, i1* %cf
  store i1 %1002, i1* %of
  %1003 = icmp eq i32 %994, 0
  store i1 %1003, i1* %zf
  %1004 = icmp slt i32 %994, 0
  store i1 %1004, i1* %sf
  %1005 = trunc i32 %994 to i8
  %1006 = call i8 @llvm.ctpop.i8(i8 %1005)
  %1007 = and i8 %1006, 1
  %1008 = icmp eq i8 %1007, 0
  store i1 %1008, i1* %pf
  store i32 %994, i32* %stack_var_-52
  store volatile i64 41323, i64* @assembly_address
  %1009 = load i32* %stack_var_-52
  %1010 = and i32 %1009, 15
  %1011 = icmp ugt i32 %1010, 15
  %1012 = icmp ult i32 %1009, 0
  %1013 = xor i32 %1009, 0
  %1014 = and i32 %1013, 0
  %1015 = icmp slt i32 %1014, 0
  store i1 %1011, i1* %az
  store i1 %1012, i1* %cf
  store i1 %1015, i1* %of
  %1016 = icmp eq i32 %1009, 0
  store i1 %1016, i1* %zf
  %1017 = icmp slt i32 %1009, 0
  store i1 %1017, i1* %sf
  %1018 = trunc i32 %1009 to i8
  %1019 = call i8 @llvm.ctpop.i8(i8 %1018)
  %1020 = and i8 %1019, 1
  %1021 = icmp eq i8 %1020, 0
  store i1 %1021, i1* %pf
  store volatile i64 41327, i64* @assembly_address
  %1022 = load i1* %zf
  %1023 = load i1* %sf
  %1024 = load i1* %of
  %1025 = icmp eq i1 %1023, %1024
  %1026 = icmp eq i1 %1022, false
  %1027 = icmp eq i1 %1025, %1026
  br i1 %1027, label %block_a0b7, label %block_a175

block_a175:                                       ; preds = %block_a0df
  store volatile i64 41333, i64* @assembly_address
  %1028 = load i32* %stack_var_-40
  %1029 = zext i32 %1028 to i64
  store i64 %1029, i64* %rax
  store volatile i64 41336, i64* @assembly_address
  %1030 = load i64* %rax
  %1031 = trunc i64 %1030 to i32
  %1032 = sext i32 %1031 to i128
  store i128 %1032, i128* %stack_var_-60
  store volatile i64 41339, i64* @assembly_address
  br label %block_a26f

block_a180:                                       ; preds = %block_a26f
  store volatile i64 41344, i64* @assembly_address
  %1033 = load i128* %stack_var_-60
  %1034 = trunc i128 %1033 to i32
  %1035 = zext i32 %1034 to i64
  store i64 %1035, i64* %rax
  store volatile i64 41347, i64* @assembly_address
  %1036 = load i64* %rax
  %1037 = trunc i64 %1036 to i32
  %1038 = sext i32 %1037 to i64
  store i64 %1038, i64* %rax
  store volatile i64 41349, i64* @assembly_address
  %1039 = load i64* %rax
  %1040 = load i64* %rax
  %1041 = mul i64 %1040, 1
  %1042 = add i64 %1039, %1041
  store i64 %1042, i64* %rdx
  store volatile i64 41353, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41360, i64* @assembly_address
  %1043 = load i64* %rdx
  %1044 = load i64* %rax
  %1045 = mul i64 %1044, 1
  %1046 = add i64 %1043, %1045
  %1047 = inttoptr i64 %1046 to i16*
  %1048 = load i16* %1047
  %1049 = zext i16 %1048 to i64
  store i64 %1049, i64* %rax
  store volatile i64 41364, i64* @assembly_address
  %1050 = load i64* %rax
  %1051 = trunc i64 %1050 to i16
  %1052 = zext i16 %1051 to i64
  store i64 %1052, i64* %rax
  store volatile i64 41367, i64* @assembly_address
  %1053 = load i64* %rax
  %1054 = trunc i64 %1053 to i32
  store i32 %1054, i32* %stack_var_-64
  store volatile i64 41370, i64* @assembly_address
  br label %block_a261

block_a19f:                                       ; preds = %block_a261
  store volatile i64 41375, i64* @assembly_address
  %1055 = load i32* %stack_var_-68
  %1056 = sub i32 %1055, 1
  %1057 = and i32 %1055, 15
  %1058 = sub i32 %1057, 1
  %1059 = icmp ugt i32 %1058, 15
  %1060 = icmp ult i32 %1055, 1
  %1061 = xor i32 %1055, 1
  %1062 = xor i32 %1055, %1056
  %1063 = and i32 %1061, %1062
  %1064 = icmp slt i32 %1063, 0
  store i1 %1059, i1* %az
  store i1 %1060, i1* %cf
  store i1 %1064, i1* %of
  %1065 = icmp eq i32 %1056, 0
  store i1 %1065, i1* %zf
  %1066 = icmp slt i32 %1056, 0
  store i1 %1066, i1* %sf
  %1067 = trunc i32 %1056 to i8
  %1068 = call i8 @llvm.ctpop.i8(i8 %1067)
  %1069 = and i8 %1068, 1
  %1070 = icmp eq i8 %1069, 0
  store i1 %1070, i1* %pf
  store i32 %1056, i32* %stack_var_-68
  store volatile i64 41379, i64* @assembly_address
  %1071 = load i32* %stack_var_-68
  %1072 = zext i32 %1071 to i64
  store i64 %1072, i64* %rax
  store volatile i64 41382, i64* @assembly_address
  %1073 = load i64* %rax
  %1074 = trunc i64 %1073 to i32
  %1075 = sext i32 %1074 to i64
  store i64 %1075, i64* %rax
  store volatile i64 41384, i64* @assembly_address
  %1076 = load i64* %rax
  %1077 = mul i64 %1076, 4
  store i64 %1077, i64* %rdx
  store volatile i64 41392, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 41399, i64* @assembly_address
  %1078 = load i64* %rdx
  %1079 = load i64* %rax
  %1080 = mul i64 %1079, 1
  %1081 = add i64 %1078, %1080
  %1082 = inttoptr i64 %1081 to i32*
  %1083 = load i32* %1082
  %1084 = zext i32 %1083 to i64
  store i64 %1084, i64* %rax
  store volatile i64 41402, i64* @assembly_address
  %1085 = load i64* %rax
  %1086 = trunc i64 %1085 to i32
  store i32 %1086, i32* %stack_var_-36
  store volatile i64 41405, i64* @assembly_address
  %1087 = load i32* %stack_var_-36
  %1088 = zext i32 %1087 to i64
  store i64 %1088, i64* %rax
  store volatile i64 41408, i64* @assembly_address
  %1089 = load i64* %rax
  %1090 = trunc i64 %1089 to i32
  %1091 = load i32* %stack_var_-44
  %1092 = trunc i64 %1089 to i32
  store i32 %1092, i32* %6
  store i32 %1091, i32* %5
  %1093 = sub i32 %1090, %1091
  %1094 = and i32 %1090, 15
  %1095 = and i32 %1091, 15
  %1096 = sub i32 %1094, %1095
  %1097 = icmp ugt i32 %1096, 15
  %1098 = icmp ult i32 %1090, %1091
  %1099 = xor i32 %1090, %1091
  %1100 = xor i32 %1090, %1093
  %1101 = and i32 %1099, %1100
  %1102 = icmp slt i32 %1101, 0
  store i1 %1097, i1* %az
  store i1 %1098, i1* %cf
  store i1 %1102, i1* %of
  %1103 = icmp eq i32 %1093, 0
  store i1 %1103, i1* %zf
  %1104 = icmp slt i32 %1093, 0
  store i1 %1104, i1* %sf
  %1105 = trunc i32 %1093 to i8
  %1106 = call i8 @llvm.ctpop.i8(i8 %1105)
  %1107 = and i8 %1106, 1
  %1108 = icmp eq i8 %1107, 0
  store i1 %1108, i1* %pf
  store volatile i64 41411, i64* @assembly_address
  %1109 = load i32* %6
  %1110 = sext i32 %1109 to i64
  %1111 = load i32* %5
  %1112 = trunc i64 %1110 to i32
  %1113 = icmp sle i32 %1112, %1111
  br i1 %1113, label %block_a1ca, label %block_a1c5

block_a1c5:                                       ; preds = %block_a19f
  store volatile i64 41413, i64* @assembly_address
  br label %block_a261

block_a1ca:                                       ; preds = %block_a19f
  store volatile i64 41418, i64* @assembly_address
  %1114 = load i32* %stack_var_-36
  %1115 = zext i32 %1114 to i64
  store i64 %1115, i64* %rax
  store volatile i64 41421, i64* @assembly_address
  %1116 = load i64* %rax
  %1117 = trunc i64 %1116 to i32
  %1118 = sext i32 %1117 to i64
  store i64 %1118, i64* %rax
  store volatile i64 41423, i64* @assembly_address
  %1119 = load i64* %rax
  %1120 = mul i64 %1119, 4
  store i64 %1120, i64* %rdx
  store volatile i64 41431, i64* @assembly_address
  %1121 = load i16** %stack_var_-32
  %1122 = ptrtoint i16* %1121 to i64
  store i64 %1122, i64* %rax
  store volatile i64 41435, i64* @assembly_address
  %1123 = load i64* %rax
  %1124 = load i64* %rdx
  %1125 = add i64 %1123, %1124
  %1126 = and i64 %1123, 15
  %1127 = and i64 %1124, 15
  %1128 = add i64 %1126, %1127
  %1129 = icmp ugt i64 %1128, 15
  %1130 = icmp ult i64 %1125, %1123
  %1131 = xor i64 %1123, %1125
  %1132 = xor i64 %1124, %1125
  %1133 = and i64 %1131, %1132
  %1134 = icmp slt i64 %1133, 0
  store i1 %1129, i1* %az
  store i1 %1130, i1* %cf
  store i1 %1134, i1* %of
  %1135 = icmp eq i64 %1125, 0
  store i1 %1135, i1* %zf
  %1136 = icmp slt i64 %1125, 0
  store i1 %1136, i1* %sf
  %1137 = trunc i64 %1125 to i8
  %1138 = call i8 @llvm.ctpop.i8(i8 %1137)
  %1139 = and i8 %1138, 1
  %1140 = icmp eq i8 %1139, 0
  store i1 %1140, i1* %pf
  store i64 %1125, i64* %rax
  store volatile i64 41438, i64* @assembly_address
  %1141 = load i64* %rax
  %1142 = add i64 %1141, 2
  %1143 = inttoptr i64 %1142 to i16*
  %1144 = load i16* %1143
  %1145 = zext i16 %1144 to i64
  store i64 %1145, i64* %rax
  store volatile i64 41442, i64* @assembly_address
  %1146 = load i64* %rax
  %1147 = trunc i64 %1146 to i16
  %1148 = zext i16 %1147 to i64
  store i64 %1148, i64* %rdx
  store volatile i64 41445, i64* @assembly_address
  %1149 = load i128* %stack_var_-60
  %1150 = trunc i128 %1149 to i32
  %1151 = zext i32 %1150 to i64
  store i64 %1151, i64* %rax
  store volatile i64 41448, i64* @assembly_address
  %1152 = load i64* %rdx
  %1153 = trunc i64 %1152 to i32
  %1154 = load i64* %rax
  %1155 = trunc i64 %1154 to i32
  %1156 = sub i32 %1153, %1155
  %1157 = and i32 %1153, 15
  %1158 = and i32 %1155, 15
  %1159 = sub i32 %1157, %1158
  %1160 = icmp ugt i32 %1159, 15
  %1161 = icmp ult i32 %1153, %1155
  %1162 = xor i32 %1153, %1155
  %1163 = xor i32 %1153, %1156
  %1164 = and i32 %1162, %1163
  %1165 = icmp slt i32 %1164, 0
  store i1 %1160, i1* %az
  store i1 %1161, i1* %cf
  store i1 %1165, i1* %of
  %1166 = icmp eq i32 %1156, 0
  store i1 %1166, i1* %zf
  %1167 = icmp slt i32 %1156, 0
  store i1 %1167, i1* %sf
  %1168 = trunc i32 %1156 to i8
  %1169 = call i8 @llvm.ctpop.i8(i8 %1168)
  %1170 = and i8 %1169, 1
  %1171 = icmp eq i8 %1170, 0
  store i1 %1171, i1* %pf
  store volatile i64 41450, i64* @assembly_address
  %1172 = load i1* %zf
  br i1 %1172, label %block_a25d, label %block_a1ec

block_a1ec:                                       ; preds = %block_a1ca
  store volatile i64 41452, i64* @assembly_address
  %1173 = load i128* %stack_var_-60
  %1174 = trunc i128 %1173 to i32
  %1175 = zext i32 %1174 to i64
  store i64 %1175, i64* %rax
  store volatile i64 41455, i64* @assembly_address
  %1176 = load i64* %rax
  %1177 = trunc i64 %1176 to i32
  %1178 = sext i32 %1177 to i64
  store i64 %1178, i64* %rdx
  store volatile i64 41458, i64* @assembly_address
  %1179 = load i32* %stack_var_-36
  %1180 = zext i32 %1179 to i64
  store i64 %1180, i64* %rax
  store volatile i64 41461, i64* @assembly_address
  %1181 = load i64* %rax
  %1182 = trunc i64 %1181 to i32
  %1183 = sext i32 %1182 to i64
  store i64 %1183, i64* %rax
  store volatile i64 41463, i64* @assembly_address
  %1184 = load i64* %rax
  %1185 = mul i64 %1184, 4
  store i64 %1185, i64* %rcx
  store volatile i64 41471, i64* @assembly_address
  %1186 = load i16** %stack_var_-32
  %1187 = ptrtoint i16* %1186 to i64
  store i64 %1187, i64* %rax
  store volatile i64 41475, i64* @assembly_address
  %1188 = load i64* %rax
  %1189 = load i64* %rcx
  %1190 = add i64 %1188, %1189
  %1191 = and i64 %1188, 15
  %1192 = and i64 %1189, 15
  %1193 = add i64 %1191, %1192
  %1194 = icmp ugt i64 %1193, 15
  %1195 = icmp ult i64 %1190, %1188
  %1196 = xor i64 %1188, %1190
  %1197 = xor i64 %1189, %1190
  %1198 = and i64 %1196, %1197
  %1199 = icmp slt i64 %1198, 0
  store i1 %1194, i1* %az
  store i1 %1195, i1* %cf
  store i1 %1199, i1* %of
  %1200 = icmp eq i64 %1190, 0
  store i1 %1200, i1* %zf
  %1201 = icmp slt i64 %1190, 0
  store i1 %1201, i1* %sf
  %1202 = trunc i64 %1190 to i8
  %1203 = call i8 @llvm.ctpop.i8(i8 %1202)
  %1204 = and i8 %1203, 1
  %1205 = icmp eq i8 %1204, 0
  store i1 %1205, i1* %pf
  store i64 %1190, i64* %rax
  store volatile i64 41478, i64* @assembly_address
  %1206 = load i64* %rax
  %1207 = add i64 %1206, 2
  %1208 = inttoptr i64 %1207 to i16*
  %1209 = load i16* %1208
  %1210 = zext i16 %1209 to i64
  store i64 %1210, i64* %rax
  store volatile i64 41482, i64* @assembly_address
  %1211 = load i64* %rax
  %1212 = trunc i64 %1211 to i16
  %1213 = zext i16 %1212 to i64
  store i64 %1213, i64* %rax
  store volatile i64 41485, i64* @assembly_address
  %1214 = load i64* %rdx
  %1215 = load i64* %rax
  %1216 = sub i64 %1214, %1215
  %1217 = and i64 %1214, 15
  %1218 = and i64 %1215, 15
  %1219 = sub i64 %1217, %1218
  %1220 = icmp ugt i64 %1219, 15
  %1221 = icmp ult i64 %1214, %1215
  %1222 = xor i64 %1214, %1215
  %1223 = xor i64 %1214, %1216
  %1224 = and i64 %1222, %1223
  %1225 = icmp slt i64 %1224, 0
  store i1 %1220, i1* %az
  store i1 %1221, i1* %cf
  store i1 %1225, i1* %of
  %1226 = icmp eq i64 %1216, 0
  store i1 %1226, i1* %zf
  %1227 = icmp slt i64 %1216, 0
  store i1 %1227, i1* %sf
  %1228 = trunc i64 %1216 to i8
  %1229 = call i8 @llvm.ctpop.i8(i8 %1228)
  %1230 = and i8 %1229, 1
  %1231 = icmp eq i8 %1230, 0
  store i1 %1231, i1* %pf
  store i64 %1216, i64* %rdx
  store volatile i64 41488, i64* @assembly_address
  %1232 = load i32* %stack_var_-36
  %1233 = zext i32 %1232 to i64
  store i64 %1233, i64* %rax
  store volatile i64 41491, i64* @assembly_address
  %1234 = load i64* %rax
  %1235 = trunc i64 %1234 to i32
  %1236 = sext i32 %1235 to i64
  store i64 %1236, i64* %rax
  store volatile i64 41493, i64* @assembly_address
  %1237 = load i64* %rax
  %1238 = mul i64 %1237, 4
  store i64 %1238, i64* %rcx
  store volatile i64 41501, i64* @assembly_address
  %1239 = load i16** %stack_var_-32
  %1240 = ptrtoint i16* %1239 to i64
  store i64 %1240, i64* %rax
  store volatile i64 41505, i64* @assembly_address
  %1241 = load i64* %rax
  %1242 = load i64* %rcx
  %1243 = add i64 %1241, %1242
  %1244 = and i64 %1241, 15
  %1245 = and i64 %1242, 15
  %1246 = add i64 %1244, %1245
  %1247 = icmp ugt i64 %1246, 15
  %1248 = icmp ult i64 %1243, %1241
  %1249 = xor i64 %1241, %1243
  %1250 = xor i64 %1242, %1243
  %1251 = and i64 %1249, %1250
  %1252 = icmp slt i64 %1251, 0
  store i1 %1247, i1* %az
  store i1 %1248, i1* %cf
  store i1 %1252, i1* %of
  %1253 = icmp eq i64 %1243, 0
  store i1 %1253, i1* %zf
  %1254 = icmp slt i64 %1243, 0
  store i1 %1254, i1* %sf
  %1255 = trunc i64 %1243 to i8
  %1256 = call i8 @llvm.ctpop.i8(i8 %1255)
  %1257 = and i8 %1256, 1
  %1258 = icmp eq i8 %1257, 0
  store i1 %1258, i1* %pf
  store i64 %1243, i64* %rax
  store volatile i64 41508, i64* @assembly_address
  %1259 = load i64* %rax
  %1260 = inttoptr i64 %1259 to i16*
  %1261 = load i16* %1260
  %1262 = zext i16 %1261 to i64
  store i64 %1262, i64* %rax
  store volatile i64 41511, i64* @assembly_address
  %1263 = load i64* %rax
  %1264 = trunc i64 %1263 to i16
  %1265 = zext i16 %1264 to i64
  store i64 %1265, i64* %rax
  store volatile i64 41514, i64* @assembly_address
  %1266 = load i64* %rax
  %1267 = load i64* %rdx
  %1268 = sext i64 %1266 to i128
  %1269 = sext i64 %1267 to i128
  %1270 = mul i128 %1268, %1269
  %1271 = trunc i128 %1270 to i64
  store i64 %1271, i64* %rax
  %1272 = trunc i128 %1270 to i64
  %1273 = sext i64 %1272 to i128
  %1274 = icmp ne i128 %1270, %1273
  store i1 %1274, i1* %of
  store i1 %1274, i1* %cf
  store volatile i64 41518, i64* @assembly_address
  %1275 = load i64* %rax
  store i64 %1275, i64* %rdx
  store volatile i64 41521, i64* @assembly_address
  %1276 = load i64* @global_var_219ed0
  store i64 %1276, i64* %rax
  store volatile i64 41528, i64* @assembly_address
  %1277 = load i64* %rax
  %1278 = load i64* %rdx
  %1279 = add i64 %1277, %1278
  %1280 = and i64 %1277, 15
  %1281 = and i64 %1278, 15
  %1282 = add i64 %1280, %1281
  %1283 = icmp ugt i64 %1282, 15
  %1284 = icmp ult i64 %1279, %1277
  %1285 = xor i64 %1277, %1279
  %1286 = xor i64 %1278, %1279
  %1287 = and i64 %1285, %1286
  %1288 = icmp slt i64 %1287, 0
  store i1 %1283, i1* %az
  store i1 %1284, i1* %cf
  store i1 %1288, i1* %of
  %1289 = icmp eq i64 %1279, 0
  store i1 %1289, i1* %zf
  %1290 = icmp slt i64 %1279, 0
  store i1 %1290, i1* %sf
  %1291 = trunc i64 %1279 to i8
  %1292 = call i8 @llvm.ctpop.i8(i8 %1291)
  %1293 = and i8 %1292, 1
  %1294 = icmp eq i8 %1293, 0
  store i1 %1294, i1* %pf
  store i64 %1279, i64* %rax
  store volatile i64 41531, i64* @assembly_address
  %1295 = load i64* %rax
  store i64 %1295, i64* @global_var_219ed0
  store volatile i64 41538, i64* @assembly_address
  %1296 = load i32* %stack_var_-36
  %1297 = zext i32 %1296 to i64
  store i64 %1297, i64* %rax
  store volatile i64 41541, i64* @assembly_address
  %1298 = load i64* %rax
  %1299 = trunc i64 %1298 to i32
  %1300 = sext i32 %1299 to i64
  store i64 %1300, i64* %rax
  store volatile i64 41543, i64* @assembly_address
  %1301 = load i64* %rax
  %1302 = mul i64 %1301, 4
  store i64 %1302, i64* %rdx
  store volatile i64 41551, i64* @assembly_address
  %1303 = load i16** %stack_var_-32
  %1304 = ptrtoint i16* %1303 to i64
  store i64 %1304, i64* %rax
  store volatile i64 41555, i64* @assembly_address
  %1305 = load i64* %rax
  %1306 = load i64* %rdx
  %1307 = add i64 %1305, %1306
  %1308 = and i64 %1305, 15
  %1309 = and i64 %1306, 15
  %1310 = add i64 %1308, %1309
  %1311 = icmp ugt i64 %1310, 15
  %1312 = icmp ult i64 %1307, %1305
  %1313 = xor i64 %1305, %1307
  %1314 = xor i64 %1306, %1307
  %1315 = and i64 %1313, %1314
  %1316 = icmp slt i64 %1315, 0
  store i1 %1311, i1* %az
  store i1 %1312, i1* %cf
  store i1 %1316, i1* %of
  %1317 = icmp eq i64 %1307, 0
  store i1 %1317, i1* %zf
  %1318 = icmp slt i64 %1307, 0
  store i1 %1318, i1* %sf
  %1319 = trunc i64 %1307 to i8
  %1320 = call i8 @llvm.ctpop.i8(i8 %1319)
  %1321 = and i8 %1320, 1
  %1322 = icmp eq i8 %1321, 0
  store i1 %1322, i1* %pf
  store i64 %1307, i64* %rax
  store volatile i64 41558, i64* @assembly_address
  %1323 = load i128* %stack_var_-60
  %1324 = trunc i128 %1323 to i32
  %1325 = zext i32 %1324 to i64
  store i64 %1325, i64* %rdx
  store volatile i64 41561, i64* @assembly_address
  %1326 = load i64* %rdx
  %1327 = trunc i64 %1326 to i16
  %1328 = load i64* %rax
  %1329 = add i64 %1328, 2
  %1330 = inttoptr i64 %1329 to i16*
  store i16 %1327, i16* %1330
  br label %block_a25d

block_a25d:                                       ; preds = %block_a1ec, %block_a1ca
  store volatile i64 41565, i64* @assembly_address
  %1331 = load i32* %stack_var_-64
  %1332 = sub i32 %1331, 1
  %1333 = and i32 %1331, 15
  %1334 = sub i32 %1333, 1
  %1335 = icmp ugt i32 %1334, 15
  %1336 = icmp ult i32 %1331, 1
  %1337 = xor i32 %1331, 1
  %1338 = xor i32 %1331, %1332
  %1339 = and i32 %1337, %1338
  %1340 = icmp slt i32 %1339, 0
  store i1 %1335, i1* %az
  store i1 %1336, i1* %cf
  store i1 %1340, i1* %of
  %1341 = icmp eq i32 %1332, 0
  store i1 %1341, i1* %zf
  %1342 = icmp slt i32 %1332, 0
  store i1 %1342, i1* %sf
  %1343 = trunc i32 %1332 to i8
  %1344 = call i8 @llvm.ctpop.i8(i8 %1343)
  %1345 = and i8 %1344, 1
  %1346 = icmp eq i8 %1345, 0
  store i1 %1346, i1* %pf
  store i32 %1332, i32* %stack_var_-64
  br label %block_a261

block_a261:                                       ; preds = %block_a25d, %block_a1c5, %block_a180
  store volatile i64 41569, i64* @assembly_address
  %1347 = load i32* %stack_var_-64
  %1348 = and i32 %1347, 15
  %1349 = icmp ugt i32 %1348, 15
  %1350 = icmp ult i32 %1347, 0
  %1351 = xor i32 %1347, 0
  %1352 = and i32 %1351, 0
  %1353 = icmp slt i32 %1352, 0
  store i1 %1349, i1* %az
  store i1 %1350, i1* %cf
  store i1 %1353, i1* %of
  %1354 = icmp eq i32 %1347, 0
  store i1 %1354, i1* %zf
  %1355 = icmp slt i32 %1347, 0
  store i1 %1355, i1* %sf
  %1356 = trunc i32 %1347 to i8
  %1357 = call i8 @llvm.ctpop.i8(i8 %1356)
  %1358 = and i8 %1357, 1
  %1359 = icmp eq i8 %1358, 0
  store i1 %1359, i1* %pf
  store volatile i64 41573, i64* @assembly_address
  %1360 = load i1* %zf
  %1361 = icmp eq i1 %1360, false
  br i1 %1361, label %block_a19f, label %block_a26b

block_a26b:                                       ; preds = %block_a261
  store volatile i64 41579, i64* @assembly_address
  %1362 = load i128* %stack_var_-60
  %1363 = trunc i128 %1362 to i32
  %1364 = sub i32 %1363, 1
  %1365 = and i32 %1363, 15
  %1366 = sub i32 %1365, 1
  %1367 = icmp ugt i32 %1366, 15
  %1368 = icmp ult i32 %1363, 1
  %1369 = xor i32 %1363, 1
  %1370 = xor i32 %1363, %1364
  %1371 = and i32 %1369, %1370
  %1372 = icmp slt i32 %1371, 0
  store i1 %1367, i1* %az
  store i1 %1368, i1* %cf
  store i1 %1372, i1* %of
  %1373 = icmp eq i32 %1364, 0
  store i1 %1373, i1* %zf
  %1374 = icmp slt i32 %1364, 0
  store i1 %1374, i1* %sf
  %1375 = trunc i32 %1364 to i8
  %1376 = call i8 @llvm.ctpop.i8(i8 %1375)
  %1377 = and i8 %1376, 1
  %1378 = icmp eq i8 %1377, 0
  store i1 %1378, i1* %pf
  %1379 = sext i32 %1364 to i128
  store i128 %1379, i128* %stack_var_-60
  br label %block_a26f

block_a26f:                                       ; preds = %block_a26b, %block_a175
  store volatile i64 41583, i64* @assembly_address
  %1380 = load i128* %stack_var_-60
  %1381 = trunc i128 %1380 to i32
  %1382 = and i32 %1381, 15
  %1383 = icmp ugt i32 %1382, 15
  %1384 = icmp ult i32 %1381, 0
  %1385 = xor i32 %1381, 0
  %1386 = and i32 %1385, 0
  %1387 = icmp slt i32 %1386, 0
  store i1 %1383, i1* %az
  store i1 %1384, i1* %cf
  store i1 %1387, i1* %of
  %1388 = icmp eq i32 %1381, 0
  store i1 %1388, i1* %zf
  %1389 = icmp slt i32 %1381, 0
  store i1 %1389, i1* %sf
  %1390 = trunc i32 %1381 to i8
  %1391 = call i8 @llvm.ctpop.i8(i8 %1390)
  %1392 = and i8 %1391, 1
  %1393 = icmp eq i8 %1392, 0
  store i1 %1393, i1* %pf
  store volatile i64 41587, i64* @assembly_address
  %1394 = load i1* %zf
  %1395 = icmp eq i1 %1394, false
  br i1 %1395, label %block_a180, label %block_a279

block_a279:                                       ; preds = %block_a26f
  store volatile i64 41593, i64* @assembly_address
  br label %block_a27c

block_a27b:                                       ; preds = %block_a0ad
  store volatile i64 41595, i64* @assembly_address
  br label %block_a27c

block_a27c:                                       ; preds = %block_a27b, %block_a279
  store volatile i64 41596, i64* @assembly_address
  %1396 = load i64* %stack_var_-8
  store i64 %1396, i64* %rbp
  %1397 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1397, i64* %rsp
  store volatile i64 41597, i64* @assembly_address
  %1398 = load i64* %rax
  ret i64 %1398
}

declare i64 @205(i64)

define i64 @gen_codes(i16* %arg1, i64 %arg2) {
block_a27e:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = bitcast i16* %arg1 to i64*
  %1 = ptrtoint i64* %0 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-60 = alloca i32
  %stack_var_-64 = alloca i32
  %stack_var_-68 = alloca i32
  %stack_var_-70 = alloca i32
  %2 = alloca i16
  %stack_var_-16 = alloca i64
  %stack_var_-84 = alloca i32
  %stack_var_-80 = alloca i16*
  %3 = alloca i64
  %stack_var_-88 = alloca i64
  %stack_var_-8 = alloca i64
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i64
  %7 = alloca i32
  %8 = alloca i32
  store volatile i64 41598, i64* @assembly_address
  %9 = load i64* %rbp
  store i64 %9, i64* %stack_var_-8
  %10 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %10, i64* %rsp
  store volatile i64 41599, i64* @assembly_address
  %11 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %11, i64* %rbp
  store volatile i64 41602, i64* @assembly_address
  %12 = load i64* %rsp
  %13 = sub i64 %12, 80
  %14 = and i64 %12, 15
  %15 = icmp ugt i64 %14, 15
  %16 = icmp ult i64 %12, 80
  %17 = xor i64 %12, 80
  %18 = xor i64 %12, %13
  %19 = and i64 %17, %18
  %20 = icmp slt i64 %19, 0
  store i1 %15, i1* %az
  store i1 %16, i1* %cf
  store i1 %20, i1* %of
  %21 = icmp eq i64 %13, 0
  store i1 %21, i1* %zf
  %22 = icmp slt i64 %13, 0
  store i1 %22, i1* %sf
  %23 = trunc i64 %13 to i8
  %24 = call i8 @llvm.ctpop.i8(i8 %23)
  %25 = and i8 %24, 1
  %26 = icmp eq i8 %25, 0
  store i1 %26, i1* %pf
  %27 = ptrtoint i64* %stack_var_-88 to i64
  store i64 %27, i64* %rsp
  store volatile i64 41606, i64* @assembly_address
  %28 = load i64* %rdi
  %29 = inttoptr i64 %28 to i16*
  store i16* %29, i16** %stack_var_-80
  store volatile i64 41610, i64* @assembly_address
  %30 = load i64* %rsi
  %31 = trunc i64 %30 to i32
  store i32 %31, i32* %stack_var_-84
  store volatile i64 41613, i64* @assembly_address
  %32 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %32, i64* %rax
  store volatile i64 41622, i64* @assembly_address
  %33 = load i64* %rax
  store i64 %33, i64* %stack_var_-16
  store volatile i64 41626, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %34 = icmp eq i32 0, 0
  store i1 %34, i1* %zf
  %35 = icmp slt i32 0, 0
  store i1 %35, i1* %sf
  %36 = trunc i32 0 to i8
  %37 = call i8 @llvm.ctpop.i8(i8 %36)
  %38 = and i8 %37, 1
  %39 = icmp eq i8 %38, 0
  store i1 %39, i1* %pf
  %40 = zext i32 0 to i64
  store i64 %40, i64* %rax
  store volatile i64 41628, i64* @assembly_address
  %41 = sext i16 0 to i32
  store i32 %41, i32* %stack_var_-70
  store volatile i64 41634, i64* @assembly_address
  store i32 1, i32* %stack_var_-68
  store volatile i64 41641, i64* @assembly_address
  br label %block_a2e0

block_a2ab:                                       ; preds = %block_a2e0
  store volatile i64 41643, i64* @assembly_address
  %42 = load i32* %stack_var_-68
  %43 = zext i32 %42 to i64
  store i64 %43, i64* %rax
  store volatile i64 41646, i64* @assembly_address
  %44 = load i64* %rax
  %45 = trunc i64 %44 to i32
  %46 = sub i32 %45, 1
  %47 = and i32 %45, 15
  %48 = sub i32 %47, 1
  %49 = icmp ugt i32 %48, 15
  %50 = icmp ult i32 %45, 1
  %51 = xor i32 %45, 1
  %52 = xor i32 %45, %46
  %53 = and i32 %51, %52
  %54 = icmp slt i32 %53, 0
  store i1 %49, i1* %az
  store i1 %50, i1* %cf
  store i1 %54, i1* %of
  %55 = icmp eq i32 %46, 0
  store i1 %55, i1* %zf
  %56 = icmp slt i32 %46, 0
  store i1 %56, i1* %sf
  %57 = trunc i32 %46 to i8
  %58 = call i8 @llvm.ctpop.i8(i8 %57)
  %59 = and i8 %58, 1
  %60 = icmp eq i8 %59, 0
  store i1 %60, i1* %pf
  %61 = zext i32 %46 to i64
  store i64 %61, i64* %rax
  store volatile i64 41649, i64* @assembly_address
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i32
  %64 = sext i32 %63 to i64
  store i64 %64, i64* %rax
  store volatile i64 41651, i64* @assembly_address
  %65 = load i64* %rax
  %66 = load i64* %rax
  %67 = mul i64 %66, 1
  %68 = add i64 %65, %67
  store i64 %68, i64* %rdx
  store volatile i64 41655, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f60 to i64), i64* %rax
  store volatile i64 41662, i64* @assembly_address
  %69 = load i64* %rdx
  %70 = load i64* %rax
  %71 = mul i64 %70, 1
  %72 = add i64 %69, %71
  %73 = inttoptr i64 %72 to i16*
  %74 = load i16* %73
  %75 = zext i16 %74 to i64
  store i64 %75, i64* %rdx
  store volatile i64 41666, i64* @assembly_address
  %76 = load i32* %stack_var_-70
  %77 = trunc i32 %76 to i16
  %78 = zext i16 %77 to i64
  store i64 %78, i64* %rax
  store volatile i64 41670, i64* @assembly_address
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  %81 = load i64* %rdx
  %82 = trunc i64 %81 to i32
  %83 = add i32 %80, %82
  %84 = and i32 %80, 15
  %85 = and i32 %82, 15
  %86 = add i32 %84, %85
  %87 = icmp ugt i32 %86, 15
  %88 = icmp ult i32 %83, %80
  %89 = xor i32 %80, %83
  %90 = xor i32 %82, %83
  %91 = and i32 %89, %90
  %92 = icmp slt i32 %91, 0
  store i1 %87, i1* %az
  store i1 %88, i1* %cf
  store i1 %92, i1* %of
  %93 = icmp eq i32 %83, 0
  store i1 %93, i1* %zf
  %94 = icmp slt i32 %83, 0
  store i1 %94, i1* %sf
  %95 = trunc i32 %83 to i8
  %96 = call i8 @llvm.ctpop.i8(i8 %95)
  %97 = and i8 %96, 1
  %98 = icmp eq i8 %97, 0
  store i1 %98, i1* %pf
  %99 = zext i32 %83 to i64
  store i64 %99, i64* %rax
  store volatile i64 41672, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i32
  %102 = load i64* %rax
  %103 = trunc i64 %102 to i32
  %104 = add i32 %101, %103
  %105 = and i32 %101, 15
  %106 = and i32 %103, 15
  %107 = add i32 %105, %106
  %108 = icmp ugt i32 %107, 15
  %109 = icmp ult i32 %104, %101
  %110 = xor i32 %101, %104
  %111 = xor i32 %103, %104
  %112 = and i32 %110, %111
  %113 = icmp slt i32 %112, 0
  store i1 %108, i1* %az
  store i1 %109, i1* %cf
  store i1 %113, i1* %of
  %114 = icmp eq i32 %104, 0
  store i1 %114, i1* %zf
  %115 = icmp slt i32 %104, 0
  store i1 %115, i1* %sf
  %116 = trunc i32 %104 to i8
  %117 = call i8 @llvm.ctpop.i8(i8 %116)
  %118 = and i8 %117, 1
  %119 = icmp eq i8 %118, 0
  store i1 %119, i1* %pf
  %120 = zext i32 %104 to i64
  store i64 %120, i64* %rax
  store volatile i64 41674, i64* @assembly_address
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i16
  %123 = sext i16 %122 to i32
  store i32 %123, i32* %stack_var_-70
  store volatile i64 41678, i64* @assembly_address
  %124 = load i32* %stack_var_-68
  %125 = zext i32 %124 to i64
  store i64 %125, i64* %rax
  store volatile i64 41681, i64* @assembly_address
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i32
  %128 = sext i32 %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 41683, i64* @assembly_address
  %129 = load i32* %stack_var_-70
  %130 = trunc i32 %129 to i16
  %131 = zext i16 %130 to i64
  store i64 %131, i64* %rdx
  store volatile i64 41687, i64* @assembly_address
  %132 = load i64* %rdx
  %133 = trunc i64 %132 to i16
  %134 = load i64* %rbp
  %135 = load i64* %rax
  %136 = mul i64 %135, 2
  %137 = add i64 %134, -48
  %138 = add i64 %137, %136
  %139 = inttoptr i64 %138 to i16*
  store i16 %133, i16* %139
  store volatile i64 41692, i64* @assembly_address
  %140 = load i32* %stack_var_-68
  %141 = add i32 %140, 1
  %142 = and i32 %140, 15
  %143 = add i32 %142, 1
  %144 = icmp ugt i32 %143, 15
  %145 = icmp ult i32 %141, %140
  %146 = xor i32 %140, %141
  %147 = xor i32 1, %141
  %148 = and i32 %146, %147
  %149 = icmp slt i32 %148, 0
  store i1 %144, i1* %az
  store i1 %145, i1* %cf
  store i1 %149, i1* %of
  %150 = icmp eq i32 %141, 0
  store i1 %150, i1* %zf
  %151 = icmp slt i32 %141, 0
  store i1 %151, i1* %sf
  %152 = trunc i32 %141 to i8
  %153 = call i8 @llvm.ctpop.i8(i8 %152)
  %154 = and i8 %153, 1
  %155 = icmp eq i8 %154, 0
  store i1 %155, i1* %pf
  store i32 %141, i32* %stack_var_-68
  br label %block_a2e0

block_a2e0:                                       ; preds = %block_a2ab, %block_a27e
  store volatile i64 41696, i64* @assembly_address
  %156 = load i32* %stack_var_-68
  store i32 %156, i32* %8
  store i32 15, i32* %7
  %157 = sub i32 %156, 15
  %158 = and i32 %156, 15
  %159 = sub i32 %158, 15
  %160 = icmp ugt i32 %159, 15
  %161 = icmp ult i32 %156, 15
  %162 = xor i32 %156, 15
  %163 = xor i32 %156, %157
  %164 = and i32 %162, %163
  %165 = icmp slt i32 %164, 0
  store i1 %160, i1* %az
  store i1 %161, i1* %cf
  store i1 %165, i1* %of
  %166 = icmp eq i32 %157, 0
  store i1 %166, i1* %zf
  %167 = icmp slt i32 %157, 0
  store i1 %167, i1* %sf
  %168 = trunc i32 %157 to i8
  %169 = call i8 @llvm.ctpop.i8(i8 %168)
  %170 = and i8 %169, 1
  %171 = icmp eq i8 %170, 0
  store i1 %171, i1* %pf
  store volatile i64 41700, i64* @assembly_address
  %172 = load i32* %8
  %173 = load i32* %7
  %174 = icmp sle i32 %172, %173
  br i1 %174, label %block_a2ab, label %block_a2e6

block_a2e6:                                       ; preds = %block_a2e0
  store volatile i64 41702, i64* @assembly_address
  store i32 0, i32* %stack_var_-64
  store volatile i64 41709, i64* @assembly_address
  br label %block_a35c

block_a2ef:                                       ; preds = %block_a35c
  store volatile i64 41711, i64* @assembly_address
  %175 = load i32* %stack_var_-64
  %176 = zext i32 %175 to i64
  store i64 %176, i64* %rax
  store volatile i64 41714, i64* @assembly_address
  %177 = load i64* %rax
  %178 = trunc i64 %177 to i32
  %179 = sext i32 %178 to i64
  store i64 %179, i64* %rax
  store volatile i64 41716, i64* @assembly_address
  %180 = load i64* %rax
  %181 = mul i64 %180, 4
  store i64 %181, i64* %rdx
  store volatile i64 41724, i64* @assembly_address
  %182 = load i16** %stack_var_-80
  %183 = ptrtoint i16* %182 to i64
  store i64 %183, i64* %rax
  store volatile i64 41728, i64* @assembly_address
  %184 = load i64* %rax
  %185 = load i64* %rdx
  %186 = add i64 %184, %185
  %187 = and i64 %184, 15
  %188 = and i64 %185, 15
  %189 = add i64 %187, %188
  %190 = icmp ugt i64 %189, 15
  %191 = icmp ult i64 %186, %184
  %192 = xor i64 %184, %186
  %193 = xor i64 %185, %186
  %194 = and i64 %192, %193
  %195 = icmp slt i64 %194, 0
  store i1 %190, i1* %az
  store i1 %191, i1* %cf
  store i1 %195, i1* %of
  %196 = icmp eq i64 %186, 0
  store i1 %196, i1* %zf
  %197 = icmp slt i64 %186, 0
  store i1 %197, i1* %sf
  %198 = trunc i64 %186 to i8
  %199 = call i8 @llvm.ctpop.i8(i8 %198)
  %200 = and i8 %199, 1
  %201 = icmp eq i8 %200, 0
  store i1 %201, i1* %pf
  store i64 %186, i64* %rax
  store volatile i64 41731, i64* @assembly_address
  %202 = load i64* %rax
  %203 = add i64 %202, 2
  %204 = inttoptr i64 %203 to i16*
  %205 = load i16* %204
  %206 = zext i16 %205 to i64
  store i64 %206, i64* %rax
  store volatile i64 41735, i64* @assembly_address
  %207 = load i64* %rax
  %208 = trunc i64 %207 to i16
  %209 = zext i16 %208 to i64
  store i64 %209, i64* %rax
  store volatile i64 41738, i64* @assembly_address
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  store i32 %211, i32* %stack_var_-60
  store volatile i64 41741, i64* @assembly_address
  %212 = load i32* %stack_var_-60
  %213 = and i32 %212, 15
  %214 = icmp ugt i32 %213, 15
  %215 = icmp ult i32 %212, 0
  %216 = xor i32 %212, 0
  %217 = and i32 %216, 0
  %218 = icmp slt i32 %217, 0
  store i1 %214, i1* %az
  store i1 %215, i1* %cf
  store i1 %218, i1* %of
  %219 = icmp eq i32 %212, 0
  store i1 %219, i1* %zf
  %220 = icmp slt i32 %212, 0
  store i1 %220, i1* %sf
  %221 = trunc i32 %212 to i8
  %222 = call i8 @llvm.ctpop.i8(i8 %221)
  %223 = and i8 %222, 1
  %224 = icmp eq i8 %223, 0
  store i1 %224, i1* %pf
  store volatile i64 41745, i64* @assembly_address
  %225 = load i1* %zf
  br i1 %225, label %block_a357, label %block_a313

block_a313:                                       ; preds = %block_a2ef
  store volatile i64 41747, i64* @assembly_address
  %226 = load i32* %stack_var_-60
  %227 = zext i32 %226 to i64
  store i64 %227, i64* %rax
  store volatile i64 41750, i64* @assembly_address
  %228 = load i64* %rax
  %229 = trunc i64 %228 to i32
  %230 = sext i32 %229 to i64
  store i64 %230, i64* %rax
  store volatile i64 41752, i64* @assembly_address
  %231 = load i64* %rbp
  %232 = load i64* %rax
  %233 = mul i64 %232, 2
  %234 = add i64 %231, -48
  %235 = add i64 %234, %233
  %236 = inttoptr i64 %235 to i16*
  %237 = load i16* %236
  %238 = zext i16 %237 to i64
  store i64 %238, i64* %rax
  store volatile i64 41757, i64* @assembly_address
  %239 = load i64* %rax
  %240 = add i64 %239, 1
  %241 = trunc i64 %240 to i32
  %242 = zext i32 %241 to i64
  store i64 %242, i64* %rcx
  store volatile i64 41760, i64* @assembly_address
  %243 = load i32* %stack_var_-60
  %244 = zext i32 %243 to i64
  store i64 %244, i64* %rdx
  store volatile i64 41763, i64* @assembly_address
  %245 = load i64* %rdx
  %246 = trunc i64 %245 to i32
  %247 = sext i32 %246 to i64
  store i64 %247, i64* %rdx
  store volatile i64 41766, i64* @assembly_address
  %248 = load i64* %rcx
  %249 = trunc i64 %248 to i16
  %250 = load i64* %rbp
  %251 = load i64* %rdx
  %252 = mul i64 %251, 2
  %253 = add i64 %250, -48
  %254 = add i64 %253, %252
  %255 = inttoptr i64 %254 to i16*
  store i16 %249, i16* %255
  store volatile i64 41771, i64* @assembly_address
  %256 = load i64* %rax
  %257 = trunc i64 %256 to i16
  %258 = zext i16 %257 to i64
  store i64 %258, i64* %rax
  store volatile i64 41774, i64* @assembly_address
  %259 = load i32* %stack_var_-60
  %260 = zext i32 %259 to i64
  store i64 %260, i64* %rdx
  store volatile i64 41777, i64* @assembly_address
  %261 = load i64* %rdx
  %262 = trunc i64 %261 to i32
  %263 = zext i32 %262 to i64
  store i64 %263, i64* %rsi
  store volatile i64 41779, i64* @assembly_address
  %264 = load i64* %rax
  %265 = trunc i64 %264 to i32
  %266 = zext i32 %265 to i64
  store i64 %266, i64* %rdi
  store volatile i64 41781, i64* @assembly_address
  %267 = load i64* %rdi
  %268 = load i64* %rsi
  %269 = trunc i64 %267 to i32
  %270 = call i64 @bi_reverse(i32 %269, i64 %268)
  store i64 %270, i64* %rax
  store i64 %270, i64* %rax
  store volatile i64 41786, i64* @assembly_address
  %271 = load i64* %rax
  %272 = trunc i64 %271 to i32
  %273 = zext i32 %272 to i64
  store i64 %273, i64* %rcx
  store volatile i64 41788, i64* @assembly_address
  %274 = load i32* %stack_var_-64
  %275 = zext i32 %274 to i64
  store i64 %275, i64* %rax
  store volatile i64 41791, i64* @assembly_address
  %276 = load i64* %rax
  %277 = trunc i64 %276 to i32
  %278 = sext i32 %277 to i64
  store i64 %278, i64* %rax
  store volatile i64 41793, i64* @assembly_address
  %279 = load i64* %rax
  %280 = mul i64 %279, 4
  store i64 %280, i64* %rdx
  store volatile i64 41801, i64* @assembly_address
  %281 = load i16** %stack_var_-80
  %282 = ptrtoint i16* %281 to i64
  store i64 %282, i64* %rax
  store volatile i64 41805, i64* @assembly_address
  %283 = load i64* %rax
  %284 = load i64* %rdx
  %285 = add i64 %283, %284
  %286 = and i64 %283, 15
  %287 = and i64 %284, 15
  %288 = add i64 %286, %287
  %289 = icmp ugt i64 %288, 15
  %290 = icmp ult i64 %285, %283
  %291 = xor i64 %283, %285
  %292 = xor i64 %284, %285
  %293 = and i64 %291, %292
  %294 = icmp slt i64 %293, 0
  store i1 %289, i1* %az
  store i1 %290, i1* %cf
  store i1 %294, i1* %of
  %295 = icmp eq i64 %285, 0
  store i1 %295, i1* %zf
  %296 = icmp slt i64 %285, 0
  store i1 %296, i1* %sf
  %297 = trunc i64 %285 to i8
  %298 = call i8 @llvm.ctpop.i8(i8 %297)
  %299 = and i8 %298, 1
  %300 = icmp eq i8 %299, 0
  store i1 %300, i1* %pf
  store i64 %285, i64* %rax
  store volatile i64 41808, i64* @assembly_address
  %301 = load i64* %rcx
  %302 = trunc i64 %301 to i32
  %303 = zext i32 %302 to i64
  store i64 %303, i64* %rdx
  store volatile i64 41810, i64* @assembly_address
  %304 = load i64* %rdx
  %305 = trunc i64 %304 to i16
  %306 = load i64* %rax
  %307 = inttoptr i64 %306 to i16*
  store i16 %305, i16* %307
  store volatile i64 41813, i64* @assembly_address
  br label %block_a358

block_a357:                                       ; preds = %block_a2ef
  store volatile i64 41815, i64* @assembly_address
  br label %block_a358

block_a358:                                       ; preds = %block_a357, %block_a313
  store volatile i64 41816, i64* @assembly_address
  %308 = load i32* %stack_var_-64
  %309 = add i32 %308, 1
  %310 = and i32 %308, 15
  %311 = add i32 %310, 1
  %312 = icmp ugt i32 %311, 15
  %313 = icmp ult i32 %309, %308
  %314 = xor i32 %308, %309
  %315 = xor i32 1, %309
  %316 = and i32 %314, %315
  %317 = icmp slt i32 %316, 0
  store i1 %312, i1* %az
  store i1 %313, i1* %cf
  store i1 %317, i1* %of
  %318 = icmp eq i32 %309, 0
  store i1 %318, i1* %zf
  %319 = icmp slt i32 %309, 0
  store i1 %319, i1* %sf
  %320 = trunc i32 %309 to i8
  %321 = call i8 @llvm.ctpop.i8(i8 %320)
  %322 = and i8 %321, 1
  %323 = icmp eq i8 %322, 0
  store i1 %323, i1* %pf
  store i32 %309, i32* %stack_var_-64
  br label %block_a35c

block_a35c:                                       ; preds = %block_a358, %block_a2e6
  store volatile i64 41820, i64* @assembly_address
  %324 = load i32* %stack_var_-64
  %325 = zext i32 %324 to i64
  store i64 %325, i64* %rax
  store volatile i64 41823, i64* @assembly_address
  %326 = load i64* %rax
  %327 = trunc i64 %326 to i32
  %328 = load i32* %stack_var_-84
  %329 = trunc i64 %326 to i32
  store i32 %329, i32* %5
  store i32 %328, i32* %4
  %330 = sub i32 %327, %328
  %331 = and i32 %327, 15
  %332 = and i32 %328, 15
  %333 = sub i32 %331, %332
  %334 = icmp ugt i32 %333, 15
  %335 = icmp ult i32 %327, %328
  %336 = xor i32 %327, %328
  %337 = xor i32 %327, %330
  %338 = and i32 %336, %337
  %339 = icmp slt i32 %338, 0
  store i1 %334, i1* %az
  store i1 %335, i1* %cf
  store i1 %339, i1* %of
  %340 = icmp eq i32 %330, 0
  store i1 %340, i1* %zf
  %341 = icmp slt i32 %330, 0
  store i1 %341, i1* %sf
  %342 = trunc i32 %330 to i8
  %343 = call i8 @llvm.ctpop.i8(i8 %342)
  %344 = and i8 %343, 1
  %345 = icmp eq i8 %344, 0
  store i1 %345, i1* %pf
  store volatile i64 41826, i64* @assembly_address
  %346 = load i32* %5
  %347 = sext i32 %346 to i64
  %348 = load i32* %4
  %349 = trunc i64 %347 to i32
  %350 = icmp sle i32 %349, %348
  br i1 %350, label %block_a2ef, label %block_a364

block_a364:                                       ; preds = %block_a35c
  store volatile i64 41828, i64* @assembly_address
  store volatile i64 41829, i64* @assembly_address
  %351 = load i64* %stack_var_-16
  store i64 %351, i64* %rax
  store volatile i64 41833, i64* @assembly_address
  %352 = load i64* %rax
  %353 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %354 = xor i64 %352, %353
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %355 = icmp eq i64 %354, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i64 %354, 0
  store i1 %356, i1* %sf
  %357 = trunc i64 %354 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  store i64 %354, i64* %rax
  store volatile i64 41842, i64* @assembly_address
  %361 = load i1* %zf
  br i1 %361, label %block_a379, label %block_a374

block_a374:                                       ; preds = %block_a364
  store volatile i64 41844, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_a379:                                       ; preds = %block_a364
  store volatile i64 41849, i64* @assembly_address
  %362 = load i64* %stack_var_-8
  store i64 %362, i64* %rbp
  %363 = ptrtoint i64* %stack_var_0 to i64
  store i64 %363, i64* %rsp
  store volatile i64 41850, i64* @assembly_address
  %364 = load i64* %rax
  ret i64 %364
}

declare i64 @206(i64*, i32)

declare i64 @207(i64*, i64)

define i64 @build_tree(i64** %arg1) {
block_a37b:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64** %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-48 = alloca i32
  %stack_var_-40 = alloca i32
  %stack_var_-44 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i16*
  %1 = alloca i64
  %stack_var_-64 = alloca i64
  %stack_var_-72 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i64
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i64
  %8 = alloca i32
  %9 = alloca i32
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i64
  store volatile i64 41851, i64* @assembly_address
  %13 = load i64* %rbp
  store i64 %13, i64* %stack_var_-8
  %14 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %14, i64* %rsp
  store volatile i64 41852, i64* @assembly_address
  %15 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %15, i64* %rbp
  store volatile i64 41855, i64* @assembly_address
  %16 = load i64* %rsp
  %17 = sub i64 %16, 64
  %18 = and i64 %16, 15
  %19 = icmp ugt i64 %18, 15
  %20 = icmp ult i64 %16, 64
  %21 = xor i64 %16, 64
  %22 = xor i64 %16, %17
  %23 = and i64 %21, %22
  %24 = icmp slt i64 %23, 0
  store i1 %19, i1* %az
  store i1 %20, i1* %cf
  store i1 %24, i1* %of
  %25 = icmp eq i64 %17, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i64 %17, 0
  store i1 %26, i1* %sf
  %27 = trunc i64 %17 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  %31 = ptrtoint i64* %stack_var_-72 to i64
  store i64 %31, i64* %rsp
  store volatile i64 41859, i64* @assembly_address
  %32 = load i64* %rdi
  store i64 %32, i64* %stack_var_-64
  store volatile i64 41863, i64* @assembly_address
  %33 = load i64* %stack_var_-64
  store i64 %33, i64* %rax
  store volatile i64 41867, i64* @assembly_address
  %34 = load i64* %rax
  %35 = inttoptr i64 %34 to i64*
  %36 = load i64* %35
  store i64 %36, i64* %rax
  store volatile i64 41870, i64* @assembly_address
  %37 = load i64* %rax
  %38 = inttoptr i64 %37 to i16*
  store i16* %38, i16** %stack_var_-24
  store volatile i64 41874, i64* @assembly_address
  %39 = load i64* %stack_var_-64
  store i64 %39, i64* %rax
  store volatile i64 41878, i64* @assembly_address
  %40 = load i64* %rax
  %41 = add i64 %40, 8
  %42 = inttoptr i64 %41 to i64*
  %43 = load i64* %42
  store i64 %43, i64* %rax
  store volatile i64 41882, i64* @assembly_address
  %44 = load i64* %rax
  store i64 %44, i64* %stack_var_-16
  store volatile i64 41886, i64* @assembly_address
  %45 = load i64* %stack_var_-64
  store i64 %45, i64* %rax
  store volatile i64 41890, i64* @assembly_address
  %46 = load i64* %rax
  %47 = add i64 %46, 28
  %48 = inttoptr i64 %47 to i32*
  %49 = load i32* %48
  %50 = zext i32 %49 to i64
  store i64 %50, i64* %rax
  store volatile i64 41893, i64* @assembly_address
  %51 = load i64* %rax
  %52 = trunc i64 %51 to i32
  store i32 %52, i32* %stack_var_-36
  store volatile i64 41896, i64* @assembly_address
  store i32 -1, i32* %stack_var_-44
  store volatile i64 41903, i64* @assembly_address
  %53 = load i32* %stack_var_-36
  %54 = zext i32 %53 to i64
  store i64 %54, i64* %rax
  store volatile i64 41906, i64* @assembly_address
  %55 = load i64* %rax
  %56 = trunc i64 %55 to i32
  store i32 %56, i32* %stack_var_-40
  store volatile i64 41909, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_218874 to i32*)
  store volatile i64 41919, i64* @assembly_address
  store i32 ptrtoint ([23 x i8]* @global_var_23d to i32), i32* bitcast (i64* @global_var_218878 to i32*)
  %57 = load i32* bitcast (i64* @global_var_218878 to i32*)
  call void @__ppdasm_undefined_function__store_1(i32 %57)
  store volatile i64 41929, i64* @assembly_address
  store i32 0, i32* %stack_var_-48
  store volatile i64 41936, i64* @assembly_address
  br label %block_a451

block_a3d2:                                       ; preds = %block_a451
  store volatile i64 41938, i64* @assembly_address
  %58 = load i32* %stack_var_-48
  %59 = zext i32 %58 to i64
  store i64 %59, i64* %rax
  store volatile i64 41941, i64* @assembly_address
  %60 = load i64* %rax
  %61 = trunc i64 %60 to i32
  %62 = sext i32 %61 to i64
  store i64 %62, i64* %rax
  store volatile i64 41943, i64* @assembly_address
  %63 = load i64* %rax
  %64 = mul i64 %63, 4
  store i64 %64, i64* %rdx
  store volatile i64 41951, i64* @assembly_address
  %65 = load i16** %stack_var_-24
  %66 = ptrtoint i16* %65 to i64
  store i64 %66, i64* %rax
  store volatile i64 41955, i64* @assembly_address
  %67 = load i64* %rax
  %68 = load i64* %rdx
  %69 = add i64 %67, %68
  %70 = and i64 %67, 15
  %71 = and i64 %68, 15
  %72 = add i64 %70, %71
  %73 = icmp ugt i64 %72, 15
  %74 = icmp ult i64 %69, %67
  %75 = xor i64 %67, %69
  %76 = xor i64 %68, %69
  %77 = and i64 %75, %76
  %78 = icmp slt i64 %77, 0
  store i1 %73, i1* %az
  store i1 %74, i1* %cf
  store i1 %78, i1* %of
  %79 = icmp eq i64 %69, 0
  store i1 %79, i1* %zf
  %80 = icmp slt i64 %69, 0
  store i1 %80, i1* %sf
  %81 = trunc i64 %69 to i8
  %82 = call i8 @llvm.ctpop.i8(i8 %81)
  %83 = and i8 %82, 1
  %84 = icmp eq i8 %83, 0
  store i1 %84, i1* %pf
  store i64 %69, i64* %rax
  store volatile i64 41958, i64* @assembly_address
  %85 = load i64* %rax
  %86 = inttoptr i64 %85 to i16*
  %87 = load i16* %86
  %88 = zext i16 %87 to i64
  store i64 %88, i64* %rax
  store volatile i64 41961, i64* @assembly_address
  %89 = load i64* %rax
  %90 = trunc i64 %89 to i16
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i16
  %93 = and i16 %90, %92
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %94 = icmp eq i16 %93, 0
  store i1 %94, i1* %zf
  %95 = icmp slt i16 %93, 0
  store i1 %95, i1* %sf
  %96 = trunc i16 %93 to i8
  %97 = call i8 @llvm.ctpop.i8(i8 %96)
  %98 = and i8 %97, 1
  %99 = icmp eq i8 %98, 0
  store i1 %99, i1* %pf
  store volatile i64 41964, i64* @assembly_address
  %100 = load i1* %zf
  br i1 %100, label %block_a433, label %block_a3ee

block_a3ee:                                       ; preds = %block_a3d2
  store volatile i64 41966, i64* @assembly_address
  %101 = load i32* %stack_var_-48
  %102 = zext i32 %101 to i64
  store i64 %102, i64* %rax
  store volatile i64 41969, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  store i32 %104, i32* %stack_var_-44
  store volatile i64 41972, i64* @assembly_address
  %105 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %106 = zext i32 %105 to i64
  store i64 %106, i64* %rax
  store volatile i64 41978, i64* @assembly_address
  %107 = load i64* %rax
  %108 = trunc i64 %107 to i32
  %109 = add i32 %108, 1
  %110 = and i32 %108, 15
  %111 = add i32 %110, 1
  %112 = icmp ugt i32 %111, 15
  %113 = icmp ult i32 %109, %108
  %114 = xor i32 %108, %109
  %115 = xor i32 1, %109
  %116 = and i32 %114, %115
  %117 = icmp slt i32 %116, 0
  store i1 %112, i1* %az
  store i1 %113, i1* %cf
  store i1 %117, i1* %of
  %118 = icmp eq i32 %109, 0
  store i1 %118, i1* %zf
  %119 = icmp slt i32 %109, 0
  store i1 %119, i1* %sf
  %120 = trunc i32 %109 to i8
  %121 = call i8 @llvm.ctpop.i8(i8 %120)
  %122 = and i8 %121, 1
  %123 = icmp eq i8 %122, 0
  store i1 %123, i1* %pf
  store i64 ptrtoint (i64* @global_var_218875 to i64), i64* %rax
  store volatile i64 41981, i64* @assembly_address
  %124 = load i64* %rax
  %125 = trunc i64 %124 to i32
  store i32 %125, i32* bitcast (i64* @global_var_218874 to i32*)
  store volatile i64 41987, i64* @assembly_address
  %126 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %127 = zext i32 %126 to i64
  store i64 %127, i64* %rax
  store volatile i64 41993, i64* @assembly_address
  %128 = load i64* %rax
  %129 = trunc i64 %128 to i32
  %130 = sext i32 %129 to i64
  store i64 %130, i64* %rax
  store volatile i64 41995, i64* @assembly_address
  %131 = load i64* %rax
  %132 = mul i64 %131, 4
  store i64 %132, i64* %rcx
  store volatile i64 42003, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 42010, i64* @assembly_address
  %133 = load i32* %stack_var_-44
  %134 = zext i32 %133 to i64
  store i64 %134, i64* %rdx
  store volatile i64 42013, i64* @assembly_address
  %135 = load i64* %rdx
  %136 = trunc i64 %135 to i32
  %137 = load i64* %rcx
  %138 = load i64* %rax
  %139 = mul i64 %138, 1
  %140 = add i64 %137, %139
  %141 = inttoptr i64 %140 to i32*
  store i32 %136, i32* %141
  store volatile i64 42016, i64* @assembly_address
  %142 = load i32* %stack_var_-48
  %143 = zext i32 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 42019, i64* @assembly_address
  %144 = load i64* %rax
  %145 = trunc i64 %144 to i32
  %146 = sext i32 %145 to i64
  store i64 %146, i64* %rdx
  store volatile i64 42022, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 42029, i64* @assembly_address
  %147 = load i64* %rdx
  %148 = load i64* %rax
  %149 = mul i64 %148, 1
  %150 = add i64 %147, %149
  %151 = inttoptr i64 %150 to i8*
  store i8 0, i8* %151
  store volatile i64 42033, i64* @assembly_address
  br label %block_a44d

block_a433:                                       ; preds = %block_a3d2
  store volatile i64 42035, i64* @assembly_address
  %152 = load i32* %stack_var_-48
  %153 = zext i32 %152 to i64
  store i64 %153, i64* %rax
  store volatile i64 42038, i64* @assembly_address
  %154 = load i64* %rax
  %155 = trunc i64 %154 to i32
  %156 = sext i32 %155 to i64
  store i64 %156, i64* %rax
  store volatile i64 42040, i64* @assembly_address
  %157 = load i64* %rax
  %158 = mul i64 %157, 4
  store i64 %158, i64* %rdx
  store volatile i64 42048, i64* @assembly_address
  %159 = load i16** %stack_var_-24
  %160 = ptrtoint i16* %159 to i64
  store i64 %160, i64* %rax
  store volatile i64 42052, i64* @assembly_address
  %161 = load i64* %rax
  %162 = load i64* %rdx
  %163 = add i64 %161, %162
  %164 = and i64 %161, 15
  %165 = and i64 %162, 15
  %166 = add i64 %164, %165
  %167 = icmp ugt i64 %166, 15
  %168 = icmp ult i64 %163, %161
  %169 = xor i64 %161, %163
  %170 = xor i64 %162, %163
  %171 = and i64 %169, %170
  %172 = icmp slt i64 %171, 0
  store i1 %167, i1* %az
  store i1 %168, i1* %cf
  store i1 %172, i1* %of
  %173 = icmp eq i64 %163, 0
  store i1 %173, i1* %zf
  %174 = icmp slt i64 %163, 0
  store i1 %174, i1* %sf
  %175 = trunc i64 %163 to i8
  %176 = call i8 @llvm.ctpop.i8(i8 %175)
  %177 = and i8 %176, 1
  %178 = icmp eq i8 %177, 0
  store i1 %178, i1* %pf
  store i64 %163, i64* %rax
  store volatile i64 42055, i64* @assembly_address
  %179 = load i64* %rax
  %180 = add i64 %179, 2
  %181 = inttoptr i64 %180 to i16*
  store i16 0, i16* %181
  br label %block_a44d

block_a44d:                                       ; preds = %block_a433, %block_a3ee
  store volatile i64 42061, i64* @assembly_address
  %182 = load i32* %stack_var_-48
  %183 = add i32 %182, 1
  %184 = and i32 %182, 15
  %185 = add i32 %184, 1
  %186 = icmp ugt i32 %185, 15
  %187 = icmp ult i32 %183, %182
  %188 = xor i32 %182, %183
  %189 = xor i32 1, %183
  %190 = and i32 %188, %189
  %191 = icmp slt i32 %190, 0
  store i1 %186, i1* %az
  store i1 %187, i1* %cf
  store i1 %191, i1* %of
  %192 = icmp eq i32 %183, 0
  store i1 %192, i1* %zf
  %193 = icmp slt i32 %183, 0
  store i1 %193, i1* %sf
  %194 = trunc i32 %183 to i8
  %195 = call i8 @llvm.ctpop.i8(i8 %194)
  %196 = and i8 %195, 1
  %197 = icmp eq i8 %196, 0
  store i1 %197, i1* %pf
  store i32 %183, i32* %stack_var_-48
  br label %block_a451

block_a451:                                       ; preds = %block_a44d, %block_a37b
  store volatile i64 42065, i64* @assembly_address
  %198 = load i32* %stack_var_-48
  %199 = zext i32 %198 to i64
  store i64 %199, i64* %rax
  store volatile i64 42068, i64* @assembly_address
  %200 = load i64* %rax
  %201 = trunc i64 %200 to i32
  %202 = load i32* %stack_var_-36
  %203 = trunc i64 %200 to i32
  store i32 %203, i32* %11
  store i32 %202, i32* %10
  %204 = sub i32 %201, %202
  %205 = and i32 %201, 15
  %206 = and i32 %202, 15
  %207 = sub i32 %205, %206
  %208 = icmp ugt i32 %207, 15
  %209 = icmp ult i32 %201, %202
  %210 = xor i32 %201, %202
  %211 = xor i32 %201, %204
  %212 = and i32 %210, %211
  %213 = icmp slt i32 %212, 0
  store i1 %208, i1* %az
  store i1 %209, i1* %cf
  store i1 %213, i1* %of
  %214 = icmp eq i32 %204, 0
  store i1 %214, i1* %zf
  %215 = icmp slt i32 %204, 0
  store i1 %215, i1* %sf
  %216 = trunc i32 %204 to i8
  %217 = call i8 @llvm.ctpop.i8(i8 %216)
  %218 = and i8 %217, 1
  %219 = icmp eq i8 %218, 0
  store i1 %219, i1* %pf
  store volatile i64 42071, i64* @assembly_address
  %220 = load i32* %11
  %221 = sext i32 %220 to i64
  %222 = load i32* %10
  %223 = trunc i64 %221 to i32
  %224 = icmp slt i32 %223, %222
  br i1 %224, label %block_a3d2, label %block_a45d

block_a45d:                                       ; preds = %block_a451
  store volatile i64 42077, i64* @assembly_address
  br label %block_a529

block_a462:                                       ; preds = %block_a529
  store volatile i64 42082, i64* @assembly_address
  %225 = load i32* %stack_var_-44
  store i32 %225, i32* %9
  store i32 1, i32* %8
  %226 = sub i32 %225, 1
  %227 = and i32 %225, 15
  %228 = sub i32 %227, 1
  %229 = icmp ugt i32 %228, 15
  %230 = icmp ult i32 %225, 1
  %231 = xor i32 %225, 1
  %232 = xor i32 %225, %226
  %233 = and i32 %231, %232
  %234 = icmp slt i32 %233, 0
  store i1 %229, i1* %az
  store i1 %230, i1* %cf
  store i1 %234, i1* %of
  %235 = icmp eq i32 %226, 0
  store i1 %235, i1* %zf
  %236 = icmp slt i32 %226, 0
  store i1 %236, i1* %sf
  %237 = trunc i32 %226 to i8
  %238 = call i8 @llvm.ctpop.i8(i8 %237)
  %239 = and i8 %238, 1
  %240 = icmp eq i8 %239, 0
  store i1 %240, i1* %pf
  store volatile i64 42086, i64* @assembly_address
  %241 = load i32* %9
  %242 = load i32* %8
  %243 = icmp sgt i32 %241, %242
  br i1 %243, label %block_a471, label %block_a468

block_a468:                                       ; preds = %block_a462
  store volatile i64 42088, i64* @assembly_address
  %244 = load i32* %stack_var_-44
  %245 = add i32 %244, 1
  %246 = and i32 %244, 15
  %247 = add i32 %246, 1
  %248 = icmp ugt i32 %247, 15
  %249 = icmp ult i32 %245, %244
  %250 = xor i32 %244, %245
  %251 = xor i32 1, %245
  %252 = and i32 %250, %251
  %253 = icmp slt i32 %252, 0
  store i1 %248, i1* %az
  store i1 %249, i1* %cf
  store i1 %253, i1* %of
  %254 = icmp eq i32 %245, 0
  store i1 %254, i1* %zf
  %255 = icmp slt i32 %245, 0
  store i1 %255, i1* %sf
  %256 = trunc i32 %245 to i8
  %257 = call i8 @llvm.ctpop.i8(i8 %256)
  %258 = and i8 %257, 1
  %259 = icmp eq i8 %258, 0
  store i1 %259, i1* %pf
  store i32 %245, i32* %stack_var_-44
  store volatile i64 42092, i64* @assembly_address
  %260 = load i32* %stack_var_-44
  %261 = zext i32 %260 to i64
  store i64 %261, i64* %rdx
  store volatile i64 42095, i64* @assembly_address
  br label %block_a476

block_a471:                                       ; preds = %block_a462
  store volatile i64 42097, i64* @assembly_address
  store i64 0, i64* %rdx
  br label %block_a476

block_a476:                                       ; preds = %block_a471, %block_a468
  store volatile i64 42102, i64* @assembly_address
  %262 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %263 = zext i32 %262 to i64
  store i64 %263, i64* %rax
  store volatile i64 42108, i64* @assembly_address
  %264 = load i64* %rax
  %265 = trunc i64 %264 to i32
  %266 = add i32 %265, 1
  %267 = and i32 %265, 15
  %268 = add i32 %267, 1
  %269 = icmp ugt i32 %268, 15
  %270 = icmp ult i32 %266, %265
  %271 = xor i32 %265, %266
  %272 = xor i32 1, %266
  %273 = and i32 %271, %272
  %274 = icmp slt i32 %273, 0
  store i1 %269, i1* %az
  store i1 %270, i1* %cf
  store i1 %274, i1* %of
  %275 = icmp eq i32 %266, 0
  store i1 %275, i1* %zf
  %276 = icmp slt i32 %266, 0
  store i1 %276, i1* %sf
  %277 = trunc i32 %266 to i8
  %278 = call i8 @llvm.ctpop.i8(i8 %277)
  %279 = and i8 %278, 1
  %280 = icmp eq i8 %279, 0
  store i1 %280, i1* %pf
  store i64 ptrtoint (i64* @global_var_218875 to i64), i64* %rax
  store volatile i64 42111, i64* @assembly_address
  %281 = load i64* %rax
  %282 = trunc i64 %281 to i32
  store i32 %282, i32* bitcast (i64* @global_var_218874 to i32*)
  store volatile i64 42117, i64* @assembly_address
  %283 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %284 = zext i32 %283 to i64
  store i64 %284, i64* %rax
  store volatile i64 42123, i64* @assembly_address
  %285 = load i64* %rax
  %286 = trunc i64 %285 to i32
  %287 = sext i32 %286 to i64
  store i64 %287, i64* %rcx
  store volatile i64 42126, i64* @assembly_address
  %288 = load i64* %rcx
  %289 = mul i64 %288, 4
  store i64 %289, i64* %rsi
  store volatile i64 42134, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rcx
  store volatile i64 42141, i64* @assembly_address
  %290 = load i64* %rdx
  %291 = trunc i64 %290 to i32
  %292 = load i64* %rsi
  %293 = load i64* %rcx
  %294 = mul i64 %293, 1
  %295 = add i64 %292, %294
  %296 = inttoptr i64 %295 to i32*
  store i32 %291, i32* %296
  store volatile i64 42144, i64* @assembly_address
  %297 = load i64* %rax
  %298 = trunc i64 %297 to i32
  %299 = sext i32 %298 to i64
  store i64 %299, i64* %rax
  store volatile i64 42146, i64* @assembly_address
  %300 = load i64* %rax
  %301 = mul i64 %300, 4
  store i64 %301, i64* %rdx
  store volatile i64 42154, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 42161, i64* @assembly_address
  %302 = load i64* %rdx
  %303 = load i64* %rax
  %304 = mul i64 %303, 1
  %305 = add i64 %302, %304
  %306 = inttoptr i64 %305 to i32*
  %307 = load i32* %306
  %308 = zext i32 %307 to i64
  store i64 %308, i64* %rax
  store volatile i64 42164, i64* @assembly_address
  %309 = load i64* %rax
  %310 = trunc i64 %309 to i32
  store i32 %310, i32* %stack_var_-28
  store volatile i64 42167, i64* @assembly_address
  %311 = load i32* %stack_var_-28
  %312 = zext i32 %311 to i64
  store i64 %312, i64* %rax
  store volatile i64 42170, i64* @assembly_address
  %313 = load i64* %rax
  %314 = trunc i64 %313 to i32
  %315 = sext i32 %314 to i64
  store i64 %315, i64* %rax
  store volatile i64 42172, i64* @assembly_address
  %316 = load i64* %rax
  %317 = mul i64 %316, 4
  store i64 %317, i64* %rdx
  store volatile i64 42180, i64* @assembly_address
  %318 = load i16** %stack_var_-24
  %319 = ptrtoint i16* %318 to i64
  store i64 %319, i64* %rax
  store volatile i64 42184, i64* @assembly_address
  %320 = load i64* %rax
  %321 = load i64* %rdx
  %322 = add i64 %320, %321
  %323 = and i64 %320, 15
  %324 = and i64 %321, 15
  %325 = add i64 %323, %324
  %326 = icmp ugt i64 %325, 15
  %327 = icmp ult i64 %322, %320
  %328 = xor i64 %320, %322
  %329 = xor i64 %321, %322
  %330 = and i64 %328, %329
  %331 = icmp slt i64 %330, 0
  store i1 %326, i1* %az
  store i1 %327, i1* %cf
  store i1 %331, i1* %of
  %332 = icmp eq i64 %322, 0
  store i1 %332, i1* %zf
  %333 = icmp slt i64 %322, 0
  store i1 %333, i1* %sf
  %334 = trunc i64 %322 to i8
  %335 = call i8 @llvm.ctpop.i8(i8 %334)
  %336 = and i8 %335, 1
  %337 = icmp eq i8 %336, 0
  store i1 %337, i1* %pf
  store i64 %322, i64* %rax
  store volatile i64 42187, i64* @assembly_address
  %338 = load i64* %rax
  %339 = inttoptr i64 %338 to i16*
  store i16 1, i16* %339
  store volatile i64 42192, i64* @assembly_address
  %340 = load i32* %stack_var_-28
  %341 = zext i32 %340 to i64
  store i64 %341, i64* %rax
  store volatile i64 42195, i64* @assembly_address
  %342 = load i64* %rax
  %343 = trunc i64 %342 to i32
  %344 = sext i32 %343 to i64
  store i64 %344, i64* %rdx
  store volatile i64 42198, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 42205, i64* @assembly_address
  %345 = load i64* %rdx
  %346 = load i64* %rax
  %347 = mul i64 %346, 1
  %348 = add i64 %345, %347
  %349 = inttoptr i64 %348 to i8*
  store i8 0, i8* %349
  store volatile i64 42209, i64* @assembly_address
  %350 = load i64* @global_var_219ed0
  store i64 %350, i64* %rax
  store volatile i64 42216, i64* @assembly_address
  %351 = load i64* %rax
  %352 = sub i64 %351, 1
  %353 = and i64 %351, 15
  %354 = sub i64 %353, 1
  %355 = icmp ugt i64 %354, 15
  %356 = icmp ult i64 %351, 1
  %357 = xor i64 %351, 1
  %358 = xor i64 %351, %352
  %359 = and i64 %357, %358
  %360 = icmp slt i64 %359, 0
  store i1 %355, i1* %az
  store i1 %356, i1* %cf
  store i1 %360, i1* %of
  %361 = icmp eq i64 %352, 0
  store i1 %361, i1* %zf
  %362 = icmp slt i64 %352, 0
  store i1 %362, i1* %sf
  %363 = trunc i64 %352 to i8
  %364 = call i8 @llvm.ctpop.i8(i8 %363)
  %365 = and i8 %364, 1
  %366 = icmp eq i8 %365, 0
  store i1 %366, i1* %pf
  store i64 %352, i64* %rax
  store volatile i64 42220, i64* @assembly_address
  %367 = load i64* %rax
  store i64 %367, i64* @global_var_219ed0
  store volatile i64 42227, i64* @assembly_address
  %368 = load i64* %stack_var_-16
  %369 = and i64 %368, 15
  %370 = icmp ugt i64 %369, 15
  %371 = icmp ult i64 %368, 0
  %372 = xor i64 %368, 0
  %373 = and i64 %372, 0
  %374 = icmp slt i64 %373, 0
  store i1 %370, i1* %az
  store i1 %371, i1* %cf
  store i1 %374, i1* %of
  %375 = icmp eq i64 %368, 0
  store i1 %375, i1* %zf
  %376 = icmp slt i64 %368, 0
  store i1 %376, i1* %sf
  %377 = trunc i64 %368 to i8
  %378 = call i8 @llvm.ctpop.i8(i8 %377)
  %379 = and i8 %378, 1
  %380 = icmp eq i8 %379, 0
  store i1 %380, i1* %pf
  store volatile i64 42232, i64* @assembly_address
  %381 = load i1* %zf
  br i1 %381, label %block_a529, label %block_a4fa

block_a4fa:                                       ; preds = %block_a476
  store volatile i64 42234, i64* @assembly_address
  %382 = load i64* @global_var_219ed8
  store i64 %382, i64* %rdx
  store volatile i64 42241, i64* @assembly_address
  %383 = load i32* %stack_var_-28
  %384 = zext i32 %383 to i64
  store i64 %384, i64* %rax
  store volatile i64 42244, i64* @assembly_address
  %385 = load i64* %rax
  %386 = trunc i64 %385 to i32
  %387 = sext i32 %386 to i64
  store i64 %387, i64* %rax
  store volatile i64 42246, i64* @assembly_address
  %388 = load i64* %rax
  %389 = mul i64 %388, 4
  store i64 %389, i64* %rcx
  store volatile i64 42254, i64* @assembly_address
  %390 = load i64* %stack_var_-16
  store i64 %390, i64* %rax
  store volatile i64 42258, i64* @assembly_address
  %391 = load i64* %rax
  %392 = load i64* %rcx
  %393 = add i64 %391, %392
  %394 = and i64 %391, 15
  %395 = and i64 %392, 15
  %396 = add i64 %394, %395
  %397 = icmp ugt i64 %396, 15
  %398 = icmp ult i64 %393, %391
  %399 = xor i64 %391, %393
  %400 = xor i64 %392, %393
  %401 = and i64 %399, %400
  %402 = icmp slt i64 %401, 0
  store i1 %397, i1* %az
  store i1 %398, i1* %cf
  store i1 %402, i1* %of
  %403 = icmp eq i64 %393, 0
  store i1 %403, i1* %zf
  %404 = icmp slt i64 %393, 0
  store i1 %404, i1* %sf
  %405 = trunc i64 %393 to i8
  %406 = call i8 @llvm.ctpop.i8(i8 %405)
  %407 = and i8 %406, 1
  %408 = icmp eq i8 %407, 0
  store i1 %408, i1* %pf
  store i64 %393, i64* %rax
  store volatile i64 42261, i64* @assembly_address
  %409 = load i64* %rax
  %410 = add i64 %409, 2
  %411 = inttoptr i64 %410 to i16*
  %412 = load i16* %411
  %413 = zext i16 %412 to i64
  store i64 %413, i64* %rax
  store volatile i64 42265, i64* @assembly_address
  %414 = load i64* %rax
  %415 = trunc i64 %414 to i16
  %416 = zext i16 %415 to i64
  store i64 %416, i64* %rax
  store volatile i64 42268, i64* @assembly_address
  %417 = load i64* %rdx
  %418 = load i64* %rax
  %419 = sub i64 %417, %418
  %420 = and i64 %417, 15
  %421 = and i64 %418, 15
  %422 = sub i64 %420, %421
  %423 = icmp ugt i64 %422, 15
  %424 = icmp ult i64 %417, %418
  %425 = xor i64 %417, %418
  %426 = xor i64 %417, %419
  %427 = and i64 %425, %426
  %428 = icmp slt i64 %427, 0
  store i1 %423, i1* %az
  store i1 %424, i1* %cf
  store i1 %428, i1* %of
  %429 = icmp eq i64 %419, 0
  store i1 %429, i1* %zf
  %430 = icmp slt i64 %419, 0
  store i1 %430, i1* %sf
  %431 = trunc i64 %419 to i8
  %432 = call i8 @llvm.ctpop.i8(i8 %431)
  %433 = and i8 %432, 1
  %434 = icmp eq i8 %433, 0
  store i1 %434, i1* %pf
  store i64 %419, i64* %rdx
  store volatile i64 42271, i64* @assembly_address
  %435 = load i64* %rdx
  store i64 %435, i64* %rax
  store volatile i64 42274, i64* @assembly_address
  %436 = load i64* %rax
  store i64 %436, i64* @global_var_219ed8
  br label %block_a529

block_a529:                                       ; preds = %block_a4fa, %block_a476, %block_a45d
  store volatile i64 42281, i64* @assembly_address
  %437 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %438 = zext i32 %437 to i64
  store i64 %438, i64* %rax
  store volatile i64 42287, i64* @assembly_address
  %439 = load i64* %rax
  %440 = trunc i64 %439 to i32
  %441 = trunc i64 %439 to i32
  store i32 %441, i32* %6
  store i32 1, i32* %5
  %442 = sub i32 %440, 1
  %443 = and i32 %440, 15
  %444 = sub i32 %443, 1
  %445 = icmp ugt i32 %444, 15
  %446 = icmp ult i32 %440, 1
  %447 = xor i32 %440, 1
  %448 = xor i32 %440, %442
  %449 = and i32 %447, %448
  %450 = icmp slt i32 %449, 0
  store i1 %445, i1* %az
  store i1 %446, i1* %cf
  store i1 %450, i1* %of
  %451 = icmp eq i32 %442, 0
  store i1 %451, i1* %zf
  %452 = icmp slt i32 %442, 0
  store i1 %452, i1* %sf
  %453 = trunc i32 %442 to i8
  %454 = call i8 @llvm.ctpop.i8(i8 %453)
  %455 = and i8 %454, 1
  %456 = icmp eq i8 %455, 0
  store i1 %456, i1* %pf
  store volatile i64 42290, i64* @assembly_address
  %457 = load i32* %6
  %458 = sext i32 %457 to i64
  %459 = load i32* %5
  %460 = trunc i64 %458 to i32
  %461 = icmp sle i32 %460, %459
  br i1 %461, label %block_a462, label %block_a538

block_a538:                                       ; preds = %block_a529
  store volatile i64 42296, i64* @assembly_address
  %462 = load i64* %stack_var_-64
  store i64 %462, i64* %rax
  store volatile i64 42300, i64* @assembly_address
  %463 = load i32* %stack_var_-44
  %464 = zext i32 %463 to i64
  store i64 %464, i64* %rdx
  store volatile i64 42303, i64* @assembly_address
  %465 = load i64* %rdx
  %466 = trunc i64 %465 to i32
  %467 = load i64* %rax
  %468 = add i64 %467, 36
  %469 = inttoptr i64 %468 to i32*
  store i32 %466, i32* %469
  store volatile i64 42306, i64* @assembly_address
  %470 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %471 = zext i32 %470 to i64
  store i64 %471, i64* %rax
  store volatile i64 42312, i64* @assembly_address
  %472 = load i64* %rax
  %473 = trunc i64 %472 to i32
  %474 = zext i32 %473 to i64
  store i64 %474, i64* %rdx
  store volatile i64 42314, i64* @assembly_address
  %475 = load i64* %rdx
  %476 = trunc i64 %475 to i32
  %477 = load i1* %of
  %478 = lshr i32 %476, 31
  %479 = icmp eq i32 %478, 0
  store i1 %479, i1* %zf
  %480 = icmp slt i32 %478, 0
  store i1 %480, i1* %sf
  %481 = trunc i32 %478 to i8
  %482 = call i8 @llvm.ctpop.i8(i8 %481)
  %483 = and i8 %482, 1
  %484 = icmp eq i8 %483, 0
  store i1 %484, i1* %pf
  %485 = zext i32 %478 to i64
  store i64 %485, i64* %rdx
  %486 = and i32 1073741824, %476
  %487 = icmp ne i32 %486, 0
  store i1 %487, i1* %cf
  %488 = icmp slt i32 %476, 0
  %489 = select i1 false, i1 %488, i1 %477
  store i1 %489, i1* %of
  store volatile i64 42317, i64* @assembly_address
  %490 = load i64* %rax
  %491 = trunc i64 %490 to i32
  %492 = load i64* %rdx
  %493 = trunc i64 %492 to i32
  %494 = add i32 %491, %493
  %495 = and i32 %491, 15
  %496 = and i32 %493, 15
  %497 = add i32 %495, %496
  %498 = icmp ugt i32 %497, 15
  %499 = icmp ult i32 %494, %491
  %500 = xor i32 %491, %494
  %501 = xor i32 %493, %494
  %502 = and i32 %500, %501
  %503 = icmp slt i32 %502, 0
  store i1 %498, i1* %az
  store i1 %499, i1* %cf
  store i1 %503, i1* %of
  %504 = icmp eq i32 %494, 0
  store i1 %504, i1* %zf
  %505 = icmp slt i32 %494, 0
  store i1 %505, i1* %sf
  %506 = trunc i32 %494 to i8
  %507 = call i8 @llvm.ctpop.i8(i8 %506)
  %508 = and i8 %507, 1
  %509 = icmp eq i8 %508, 0
  store i1 %509, i1* %pf
  %510 = zext i32 %494 to i64
  store i64 %510, i64* %rax
  store volatile i64 42319, i64* @assembly_address
  %511 = load i64* %rax
  %512 = trunc i64 %511 to i32
  %513 = load i1* %of
  %514 = ashr i32 %512, 1
  %515 = icmp eq i32 %514, 0
  store i1 %515, i1* %zf
  %516 = icmp slt i32 %514, 0
  store i1 %516, i1* %sf
  %517 = trunc i32 %514 to i8
  %518 = call i8 @llvm.ctpop.i8(i8 %517)
  %519 = and i8 %518, 1
  %520 = icmp eq i8 %519, 0
  store i1 %520, i1* %pf
  %521 = zext i32 %514 to i64
  store i64 %521, i64* %rax
  %522 = and i32 1, %512
  %523 = icmp ne i32 %522, 0
  store i1 %523, i1* %cf
  %524 = select i1 true, i1 false, i1 %513
  store i1 %524, i1* %of
  store volatile i64 42321, i64* @assembly_address
  %525 = load i64* %rax
  %526 = trunc i64 %525 to i32
  store i32 %526, i32* %stack_var_-48
  store volatile i64 42324, i64* @assembly_address
  br label %block_a56b

block_a556:                                       ; preds = %block_a56b
  store volatile i64 42326, i64* @assembly_address
  %527 = load i32* %stack_var_-48
  %528 = zext i32 %527 to i64
  store i64 %528, i64* %rdx
  store volatile i64 42329, i64* @assembly_address
  %529 = load i16** %stack_var_-24
  %530 = ptrtoint i16* %529 to i64
  store i64 %530, i64* %rax
  store volatile i64 42333, i64* @assembly_address
  %531 = load i64* %rdx
  %532 = trunc i64 %531 to i32
  %533 = zext i32 %532 to i64
  store i64 %533, i64* %rsi
  store volatile i64 42335, i64* @assembly_address
  %534 = load i64* %rax
  store i64 %534, i64* %rdi
  store volatile i64 42338, i64* @assembly_address
  %535 = load i64* %rdi
  %536 = load i64* %rsi
  %537 = inttoptr i64 %535 to i16*
  %538 = call i64 @pqdownheap(i16* %537, i64 %536)
  store i64 %538, i64* %rax
  store i64 %538, i64* %rax
  store volatile i64 42343, i64* @assembly_address
  %539 = load i32* %stack_var_-48
  %540 = sub i32 %539, 1
  %541 = and i32 %539, 15
  %542 = sub i32 %541, 1
  %543 = icmp ugt i32 %542, 15
  %544 = icmp ult i32 %539, 1
  %545 = xor i32 %539, 1
  %546 = xor i32 %539, %540
  %547 = and i32 %545, %546
  %548 = icmp slt i32 %547, 0
  store i1 %543, i1* %az
  store i1 %544, i1* %cf
  store i1 %548, i1* %of
  %549 = icmp eq i32 %540, 0
  store i1 %549, i1* %zf
  %550 = icmp slt i32 %540, 0
  store i1 %550, i1* %sf
  %551 = trunc i32 %540 to i8
  %552 = call i8 @llvm.ctpop.i8(i8 %551)
  %553 = and i8 %552, 1
  %554 = icmp eq i8 %553, 0
  store i1 %554, i1* %pf
  store i32 %540, i32* %stack_var_-48
  br label %block_a56b

block_a56b:                                       ; preds = %block_a556, %block_a538
  store volatile i64 42347, i64* @assembly_address
  %555 = load i32* %stack_var_-48
  %556 = and i32 %555, 15
  %557 = icmp ugt i32 %556, 15
  %558 = icmp ult i32 %555, 0
  %559 = xor i32 %555, 0
  %560 = and i32 %559, 0
  %561 = icmp slt i32 %560, 0
  store i1 %557, i1* %az
  store i1 %558, i1* %cf
  store i1 %561, i1* %of
  %562 = icmp eq i32 %555, 0
  store i1 %562, i1* %zf
  %563 = icmp slt i32 %555, 0
  store i1 %563, i1* %sf
  %564 = trunc i32 %555 to i8
  %565 = call i8 @llvm.ctpop.i8(i8 %564)
  %566 = and i8 %565, 1
  %567 = icmp eq i8 %566, 0
  store i1 %567, i1* %pf
  store volatile i64 42351, i64* @assembly_address
  %568 = load i1* %zf
  %569 = load i1* %sf
  %570 = load i1* %of
  %571 = icmp eq i1 %569, %570
  %572 = icmp eq i1 %568, false
  %573 = icmp eq i1 %571, %572
  br i1 %573, label %block_a556, label %block_a571

block_a571:                                       ; preds = %block_a6ac, %block_a56b
  store volatile i64 42353, i64* @assembly_address
  %574 = load i32* bitcast (i64* @global_var_217f84 to i32*)
  %575 = zext i32 %574 to i64
  store i64 %575, i64* %rax
  store volatile i64 42359, i64* @assembly_address
  %576 = load i64* %rax
  %577 = trunc i64 %576 to i32
  store i32 %577, i32* %stack_var_-48
  store volatile i64 42362, i64* @assembly_address
  %578 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %579 = zext i32 %578 to i64
  store i64 %579, i64* %rax
  store volatile i64 42368, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218873 to i64), i64* %rdx
  store volatile i64 42371, i64* @assembly_address
  %580 = load i64* %rdx
  %581 = trunc i64 %580 to i32
  store i32 %581, i32* bitcast (i64* @global_var_218874 to i32*)
  store volatile i64 42377, i64* @assembly_address
  %582 = load i64* %rax
  %583 = trunc i64 %582 to i32
  %584 = sext i32 %583 to i64
  store i64 %584, i64* %rax
  store volatile i64 42379, i64* @assembly_address
  %585 = load i64* %rax
  %586 = mul i64 %585, 4
  store i64 %586, i64* %rdx
  store volatile i64 42387, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 42394, i64* @assembly_address
  %587 = load i64* %rdx
  %588 = load i64* %rax
  %589 = mul i64 %588, 1
  %590 = add i64 %587, %589
  %591 = inttoptr i64 %590 to i32*
  %592 = load i32* %591
  %593 = zext i32 %592 to i64
  store i64 %593, i64* %rax
  store volatile i64 42397, i64* @assembly_address
  %594 = load i64* %rax
  %595 = trunc i64 %594 to i32
  store i32 %595, i32* bitcast (i64* @global_var_217f84 to i32*)
  store volatile i64 42403, i64* @assembly_address
  %596 = load i16** %stack_var_-24
  %597 = ptrtoint i16* %596 to i64
  store i64 %597, i64* %rax
  store volatile i64 42407, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 42412, i64* @assembly_address
  %598 = load i64* %rax
  store i64 %598, i64* %rdi
  store volatile i64 42415, i64* @assembly_address
  %599 = load i64* %rdi
  %600 = load i64* %rsi
  %601 = inttoptr i64 %599 to i16*
  %602 = call i64 @pqdownheap(i16* %601, i64 %600)
  store i64 %602, i64* %rax
  store i64 %602, i64* %rax
  store volatile i64 42420, i64* @assembly_address
  %603 = load i32* bitcast (i64* @global_var_217f84 to i32*)
  %604 = zext i32 %603 to i64
  store i64 %604, i64* %rax
  store volatile i64 42426, i64* @assembly_address
  %605 = load i64* %rax
  %606 = trunc i64 %605 to i32
  store i32 %606, i32* %stack_var_-32
  store volatile i64 42429, i64* @assembly_address
  %607 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %608 = zext i32 %607 to i64
  store i64 %608, i64* %rax
  store volatile i64 42435, i64* @assembly_address
  %609 = load i64* %rax
  %610 = trunc i64 %609 to i32
  %611 = sub i32 %610, 1
  %612 = and i32 %610, 15
  %613 = sub i32 %612, 1
  %614 = icmp ugt i32 %613, 15
  %615 = icmp ult i32 %610, 1
  %616 = xor i32 %610, 1
  %617 = xor i32 %610, %611
  %618 = and i32 %616, %617
  %619 = icmp slt i32 %618, 0
  store i1 %614, i1* %az
  store i1 %615, i1* %cf
  store i1 %619, i1* %of
  %620 = icmp eq i32 %611, 0
  store i1 %620, i1* %zf
  %621 = icmp slt i32 %611, 0
  store i1 %621, i1* %sf
  %622 = trunc i32 %611 to i8
  %623 = call i8 @llvm.ctpop.i8(i8 %622)
  %624 = and i8 %623, 1
  %625 = icmp eq i8 %624, 0
  store i1 %625, i1* %pf
  %626 = zext i32 %611 to i64
  store i64 %626, i64* %rax
  store volatile i64 42438, i64* @assembly_address
  %627 = load i64* %rax
  %628 = trunc i64 %627 to i32
  store i32 %628, i32* bitcast (i64* @global_var_218878 to i32*)
  store volatile i64 42444, i64* @assembly_address
  %629 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %630 = zext i32 %629 to i64
  store i64 %630, i64* %rax
  store volatile i64 42450, i64* @assembly_address
  %631 = load i64* %rax
  %632 = trunc i64 %631 to i32
  %633 = sext i32 %632 to i64
  store i64 %633, i64* %rax
  store volatile i64 42452, i64* @assembly_address
  %634 = load i64* %rax
  %635 = mul i64 %634, 4
  store i64 %635, i64* %rcx
  store volatile i64 42460, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 42467, i64* @assembly_address
  %636 = load i32* %stack_var_-48
  %637 = zext i32 %636 to i64
  store i64 %637, i64* %rdx
  store volatile i64 42470, i64* @assembly_address
  %638 = load i64* %rdx
  %639 = trunc i64 %638 to i32
  %640 = load i64* %rcx
  %641 = load i64* %rax
  %642 = mul i64 %641, 1
  %643 = add i64 %640, %642
  %644 = inttoptr i64 %643 to i32*
  store i32 %639, i32* %644
  store volatile i64 42473, i64* @assembly_address
  %645 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %646 = zext i32 %645 to i64
  store i64 %646, i64* %rax
  store volatile i64 42479, i64* @assembly_address
  %647 = load i64* %rax
  %648 = trunc i64 %647 to i32
  %649 = sub i32 %648, 1
  %650 = and i32 %648, 15
  %651 = sub i32 %650, 1
  %652 = icmp ugt i32 %651, 15
  %653 = icmp ult i32 %648, 1
  %654 = xor i32 %648, 1
  %655 = xor i32 %648, %649
  %656 = and i32 %654, %655
  %657 = icmp slt i32 %656, 0
  store i1 %652, i1* %az
  store i1 %653, i1* %cf
  store i1 %657, i1* %of
  %658 = icmp eq i32 %649, 0
  store i1 %658, i1* %zf
  %659 = icmp slt i32 %649, 0
  store i1 %659, i1* %sf
  %660 = trunc i32 %649 to i8
  %661 = call i8 @llvm.ctpop.i8(i8 %660)
  %662 = and i8 %661, 1
  %663 = icmp eq i8 %662, 0
  store i1 %663, i1* %pf
  %664 = zext i32 %649 to i64
  store i64 %664, i64* %rax
  store volatile i64 42482, i64* @assembly_address
  %665 = load i64* %rax
  %666 = trunc i64 %665 to i32
  store i32 %666, i32* bitcast (i64* @global_var_218878 to i32*)
  store volatile i64 42488, i64* @assembly_address
  %667 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %668 = zext i32 %667 to i64
  store i64 %668, i64* %rax
  store volatile i64 42494, i64* @assembly_address
  %669 = load i64* %rax
  %670 = trunc i64 %669 to i32
  %671 = sext i32 %670 to i64
  store i64 %671, i64* %rax
  store volatile i64 42496, i64* @assembly_address
  %672 = load i64* %rax
  %673 = mul i64 %672, 4
  store i64 %673, i64* %rcx
  store volatile i64 42504, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 42511, i64* @assembly_address
  %674 = load i32* %stack_var_-32
  %675 = zext i32 %674 to i64
  store i64 %675, i64* %rdx
  store volatile i64 42514, i64* @assembly_address
  %676 = load i64* %rdx
  %677 = trunc i64 %676 to i32
  %678 = load i64* %rcx
  %679 = load i64* %rax
  %680 = mul i64 %679, 1
  %681 = add i64 %678, %680
  %682 = inttoptr i64 %681 to i32*
  store i32 %677, i32* %682
  store volatile i64 42517, i64* @assembly_address
  %683 = load i32* %stack_var_-48
  %684 = zext i32 %683 to i64
  store i64 %684, i64* %rax
  store volatile i64 42520, i64* @assembly_address
  %685 = load i64* %rax
  %686 = trunc i64 %685 to i32
  %687 = sext i32 %686 to i64
  store i64 %687, i64* %rax
  store volatile i64 42522, i64* @assembly_address
  %688 = load i64* %rax
  %689 = mul i64 %688, 4
  store i64 %689, i64* %rdx
  store volatile i64 42530, i64* @assembly_address
  %690 = load i16** %stack_var_-24
  %691 = ptrtoint i16* %690 to i64
  store i64 %691, i64* %rax
  store volatile i64 42534, i64* @assembly_address
  %692 = load i64* %rax
  %693 = load i64* %rdx
  %694 = add i64 %692, %693
  %695 = and i64 %692, 15
  %696 = and i64 %693, 15
  %697 = add i64 %695, %696
  %698 = icmp ugt i64 %697, 15
  %699 = icmp ult i64 %694, %692
  %700 = xor i64 %692, %694
  %701 = xor i64 %693, %694
  %702 = and i64 %700, %701
  %703 = icmp slt i64 %702, 0
  store i1 %698, i1* %az
  store i1 %699, i1* %cf
  store i1 %703, i1* %of
  %704 = icmp eq i64 %694, 0
  store i1 %704, i1* %zf
  %705 = icmp slt i64 %694, 0
  store i1 %705, i1* %sf
  %706 = trunc i64 %694 to i8
  %707 = call i8 @llvm.ctpop.i8(i8 %706)
  %708 = and i8 %707, 1
  %709 = icmp eq i8 %708, 0
  store i1 %709, i1* %pf
  store i64 %694, i64* %rax
  store volatile i64 42537, i64* @assembly_address
  %710 = load i64* %rax
  %711 = inttoptr i64 %710 to i16*
  %712 = load i16* %711
  %713 = zext i16 %712 to i64
  store i64 %713, i64* %rcx
  store volatile i64 42540, i64* @assembly_address
  %714 = load i32* %stack_var_-32
  %715 = zext i32 %714 to i64
  store i64 %715, i64* %rax
  store volatile i64 42543, i64* @assembly_address
  %716 = load i64* %rax
  %717 = trunc i64 %716 to i32
  %718 = sext i32 %717 to i64
  store i64 %718, i64* %rax
  store volatile i64 42545, i64* @assembly_address
  %719 = load i64* %rax
  %720 = mul i64 %719, 4
  store i64 %720, i64* %rdx
  store volatile i64 42553, i64* @assembly_address
  %721 = load i16** %stack_var_-24
  %722 = ptrtoint i16* %721 to i64
  store i64 %722, i64* %rax
  store volatile i64 42557, i64* @assembly_address
  %723 = load i64* %rax
  %724 = load i64* %rdx
  %725 = add i64 %723, %724
  %726 = and i64 %723, 15
  %727 = and i64 %724, 15
  %728 = add i64 %726, %727
  %729 = icmp ugt i64 %728, 15
  %730 = icmp ult i64 %725, %723
  %731 = xor i64 %723, %725
  %732 = xor i64 %724, %725
  %733 = and i64 %731, %732
  %734 = icmp slt i64 %733, 0
  store i1 %729, i1* %az
  store i1 %730, i1* %cf
  store i1 %734, i1* %of
  %735 = icmp eq i64 %725, 0
  store i1 %735, i1* %zf
  %736 = icmp slt i64 %725, 0
  store i1 %736, i1* %sf
  %737 = trunc i64 %725 to i8
  %738 = call i8 @llvm.ctpop.i8(i8 %737)
  %739 = and i8 %738, 1
  %740 = icmp eq i8 %739, 0
  store i1 %740, i1* %pf
  store i64 %725, i64* %rax
  store volatile i64 42560, i64* @assembly_address
  %741 = load i64* %rax
  %742 = inttoptr i64 %741 to i16*
  %743 = load i16* %742
  %744 = zext i16 %743 to i64
  store i64 %744, i64* %rdx
  store volatile i64 42563, i64* @assembly_address
  %745 = load i32* %stack_var_-40
  %746 = zext i32 %745 to i64
  store i64 %746, i64* %rax
  store volatile i64 42566, i64* @assembly_address
  %747 = load i64* %rax
  %748 = trunc i64 %747 to i32
  %749 = sext i32 %748 to i64
  store i64 %749, i64* %rax
  store volatile i64 42568, i64* @assembly_address
  %750 = load i64* %rax
  %751 = mul i64 %750, 4
  store i64 %751, i64* %rsi
  store volatile i64 42576, i64* @assembly_address
  %752 = load i16** %stack_var_-24
  %753 = ptrtoint i16* %752 to i64
  store i64 %753, i64* %rax
  store volatile i64 42580, i64* @assembly_address
  %754 = load i64* %rax
  %755 = load i64* %rsi
  %756 = add i64 %754, %755
  %757 = and i64 %754, 15
  %758 = and i64 %755, 15
  %759 = add i64 %757, %758
  %760 = icmp ugt i64 %759, 15
  %761 = icmp ult i64 %756, %754
  %762 = xor i64 %754, %756
  %763 = xor i64 %755, %756
  %764 = and i64 %762, %763
  %765 = icmp slt i64 %764, 0
  store i1 %760, i1* %az
  store i1 %761, i1* %cf
  store i1 %765, i1* %of
  %766 = icmp eq i64 %756, 0
  store i1 %766, i1* %zf
  %767 = icmp slt i64 %756, 0
  store i1 %767, i1* %sf
  %768 = trunc i64 %756 to i8
  %769 = call i8 @llvm.ctpop.i8(i8 %768)
  %770 = and i8 %769, 1
  %771 = icmp eq i8 %770, 0
  store i1 %771, i1* %pf
  store i64 %756, i64* %rax
  store volatile i64 42583, i64* @assembly_address
  %772 = load i64* %rdx
  %773 = trunc i64 %772 to i32
  %774 = load i64* %rcx
  %775 = trunc i64 %774 to i32
  %776 = add i32 %773, %775
  %777 = and i32 %773, 15
  %778 = and i32 %775, 15
  %779 = add i32 %777, %778
  %780 = icmp ugt i32 %779, 15
  %781 = icmp ult i32 %776, %773
  %782 = xor i32 %773, %776
  %783 = xor i32 %775, %776
  %784 = and i32 %782, %783
  %785 = icmp slt i32 %784, 0
  store i1 %780, i1* %az
  store i1 %781, i1* %cf
  store i1 %785, i1* %of
  %786 = icmp eq i32 %776, 0
  store i1 %786, i1* %zf
  %787 = icmp slt i32 %776, 0
  store i1 %787, i1* %sf
  %788 = trunc i32 %776 to i8
  %789 = call i8 @llvm.ctpop.i8(i8 %788)
  %790 = and i8 %789, 1
  %791 = icmp eq i8 %790, 0
  store i1 %791, i1* %pf
  %792 = zext i32 %776 to i64
  store i64 %792, i64* %rdx
  store volatile i64 42585, i64* @assembly_address
  %793 = load i64* %rdx
  %794 = trunc i64 %793 to i16
  %795 = load i64* %rax
  %796 = inttoptr i64 %795 to i16*
  store i16 %794, i16* %796
  store volatile i64 42588, i64* @assembly_address
  %797 = load i32* %stack_var_-48
  %798 = zext i32 %797 to i64
  store i64 %798, i64* %rax
  store volatile i64 42591, i64* @assembly_address
  %799 = load i64* %rax
  %800 = trunc i64 %799 to i32
  %801 = sext i32 %800 to i64
  store i64 %801, i64* %rdx
  store volatile i64 42594, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 42601, i64* @assembly_address
  %802 = load i64* %rdx
  %803 = load i64* %rax
  %804 = mul i64 %803, 1
  %805 = add i64 %802, %804
  %806 = inttoptr i64 %805 to i8*
  %807 = load i8* %806
  %808 = zext i8 %807 to i64
  store i64 %808, i64* %rcx
  store volatile i64 42605, i64* @assembly_address
  %809 = load i32* %stack_var_-32
  %810 = zext i32 %809 to i64
  store i64 %810, i64* %rax
  store volatile i64 42608, i64* @assembly_address
  %811 = load i64* %rax
  %812 = trunc i64 %811 to i32
  %813 = sext i32 %812 to i64
  store i64 %813, i64* %rdx
  store volatile i64 42611, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 42618, i64* @assembly_address
  %814 = load i64* %rdx
  %815 = load i64* %rax
  %816 = mul i64 %815, 1
  %817 = add i64 %814, %816
  %818 = inttoptr i64 %817 to i8*
  %819 = load i8* %818
  %820 = zext i8 %819 to i64
  store i64 %820, i64* %rax
  store volatile i64 42622, i64* @assembly_address
  %821 = load i64* %rcx
  %822 = trunc i64 %821 to i8
  %823 = load i64* %rax
  %824 = trunc i64 %823 to i8
  %825 = sub i8 %822, %824
  %826 = and i8 %822, 15
  %827 = and i8 %824, 15
  %828 = sub i8 %826, %827
  %829 = icmp ugt i8 %828, 15
  %830 = icmp ult i8 %822, %824
  %831 = xor i8 %822, %824
  %832 = xor i8 %822, %825
  %833 = and i8 %831, %832
  %834 = icmp slt i8 %833, 0
  store i1 %829, i1* %az
  store i1 %830, i1* %cf
  store i1 %834, i1* %of
  %835 = icmp eq i8 %825, 0
  store i1 %835, i1* %zf
  %836 = icmp slt i8 %825, 0
  store i1 %836, i1* %sf
  %837 = call i8 @llvm.ctpop.i8(i8 %825)
  %838 = and i8 %837, 1
  %839 = icmp eq i8 %838, 0
  store i1 %839, i1* %pf
  store volatile i64 42624, i64* @assembly_address
  %840 = load i1* %cf
  br i1 %840, label %block_a698, label %block_a682

block_a682:                                       ; preds = %block_a571
  store volatile i64 42626, i64* @assembly_address
  %841 = load i32* %stack_var_-48
  %842 = zext i32 %841 to i64
  store i64 %842, i64* %rax
  store volatile i64 42629, i64* @assembly_address
  %843 = load i64* %rax
  %844 = trunc i64 %843 to i32
  %845 = sext i32 %844 to i64
  store i64 %845, i64* %rdx
  store volatile i64 42632, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 42639, i64* @assembly_address
  %846 = load i64* %rdx
  %847 = load i64* %rax
  %848 = mul i64 %847, 1
  %849 = add i64 %846, %848
  %850 = inttoptr i64 %849 to i8*
  %851 = load i8* %850
  %852 = zext i8 %851 to i64
  store i64 %852, i64* %rax
  store volatile i64 42643, i64* @assembly_address
  %853 = load i64* %rax
  %854 = trunc i64 %853 to i32
  %855 = add i32 %854, 1
  %856 = and i32 %854, 15
  %857 = add i32 %856, 1
  %858 = icmp ugt i32 %857, 15
  %859 = icmp ult i32 %855, %854
  %860 = xor i32 %854, %855
  %861 = xor i32 1, %855
  %862 = and i32 %860, %861
  %863 = icmp slt i32 %862, 0
  store i1 %858, i1* %az
  store i1 %859, i1* %cf
  store i1 %863, i1* %of
  %864 = icmp eq i32 %855, 0
  store i1 %864, i1* %zf
  %865 = icmp slt i32 %855, 0
  store i1 %865, i1* %sf
  %866 = trunc i32 %855 to i8
  %867 = call i8 @llvm.ctpop.i8(i8 %866)
  %868 = and i8 %867, 1
  %869 = icmp eq i8 %868, 0
  store i1 %869, i1* %pf
  %870 = zext i32 %855 to i64
  store i64 %870, i64* %rax
  store volatile i64 42646, i64* @assembly_address
  br label %block_a6ac

block_a698:                                       ; preds = %block_a571
  store volatile i64 42648, i64* @assembly_address
  %871 = load i32* %stack_var_-32
  %872 = zext i32 %871 to i64
  store i64 %872, i64* %rax
  store volatile i64 42651, i64* @assembly_address
  %873 = load i64* %rax
  %874 = trunc i64 %873 to i32
  %875 = sext i32 %874 to i64
  store i64 %875, i64* %rdx
  store volatile i64 42654, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rax
  store volatile i64 42661, i64* @assembly_address
  %876 = load i64* %rdx
  %877 = load i64* %rax
  %878 = mul i64 %877, 1
  %879 = add i64 %876, %878
  %880 = inttoptr i64 %879 to i8*
  %881 = load i8* %880
  %882 = zext i8 %881 to i64
  store i64 %882, i64* %rax
  store volatile i64 42665, i64* @assembly_address
  %883 = load i64* %rax
  %884 = trunc i64 %883 to i32
  %885 = add i32 %884, 1
  %886 = and i32 %884, 15
  %887 = add i32 %886, 1
  %888 = icmp ugt i32 %887, 15
  %889 = icmp ult i32 %885, %884
  %890 = xor i32 %884, %885
  %891 = xor i32 1, %885
  %892 = and i32 %890, %891
  %893 = icmp slt i32 %892, 0
  store i1 %888, i1* %az
  store i1 %889, i1* %cf
  store i1 %893, i1* %of
  %894 = icmp eq i32 %885, 0
  store i1 %894, i1* %zf
  %895 = icmp slt i32 %885, 0
  store i1 %895, i1* %sf
  %896 = trunc i32 %885 to i8
  %897 = call i8 @llvm.ctpop.i8(i8 %896)
  %898 = and i8 %897, 1
  %899 = icmp eq i8 %898, 0
  store i1 %899, i1* %pf
  %900 = zext i32 %885 to i64
  store i64 %900, i64* %rax
  br label %block_a6ac

block_a6ac:                                       ; preds = %block_a698, %block_a682
  store volatile i64 42668, i64* @assembly_address
  %901 = load i32* %stack_var_-40
  %902 = zext i32 %901 to i64
  store i64 %902, i64* %rdx
  store volatile i64 42671, i64* @assembly_address
  %903 = load i64* %rdx
  %904 = trunc i64 %903 to i32
  %905 = sext i32 %904 to i64
  store i64 %905, i64* %rcx
  store volatile i64 42674, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218880 to i64), i64* %rdx
  store volatile i64 42681, i64* @assembly_address
  %906 = load i64* %rax
  %907 = trunc i64 %906 to i8
  %908 = load i64* %rcx
  %909 = load i64* %rdx
  %910 = mul i64 %909, 1
  %911 = add i64 %908, %910
  %912 = inttoptr i64 %911 to i8*
  store i8 %907, i8* %912
  store volatile i64 42684, i64* @assembly_address
  %913 = load i32* %stack_var_-32
  %914 = zext i32 %913 to i64
  store i64 %914, i64* %rax
  store volatile i64 42687, i64* @assembly_address
  %915 = load i64* %rax
  %916 = trunc i64 %915 to i32
  %917 = sext i32 %916 to i64
  store i64 %917, i64* %rax
  store volatile i64 42689, i64* @assembly_address
  %918 = load i64* %rax
  %919 = mul i64 %918, 4
  store i64 %919, i64* %rdx
  store volatile i64 42697, i64* @assembly_address
  %920 = load i16** %stack_var_-24
  %921 = ptrtoint i16* %920 to i64
  store i64 %921, i64* %rax
  store volatile i64 42701, i64* @assembly_address
  %922 = load i64* %rax
  %923 = load i64* %rdx
  %924 = add i64 %922, %923
  %925 = and i64 %922, 15
  %926 = and i64 %923, 15
  %927 = add i64 %925, %926
  %928 = icmp ugt i64 %927, 15
  %929 = icmp ult i64 %924, %922
  %930 = xor i64 %922, %924
  %931 = xor i64 %923, %924
  %932 = and i64 %930, %931
  %933 = icmp slt i64 %932, 0
  store i1 %928, i1* %az
  store i1 %929, i1* %cf
  store i1 %933, i1* %of
  %934 = icmp eq i64 %924, 0
  store i1 %934, i1* %zf
  %935 = icmp slt i64 %924, 0
  store i1 %935, i1* %sf
  %936 = trunc i64 %924 to i8
  %937 = call i8 @llvm.ctpop.i8(i8 %936)
  %938 = and i8 %937, 1
  %939 = icmp eq i8 %938, 0
  store i1 %939, i1* %pf
  store i64 %924, i64* %rax
  store volatile i64 42704, i64* @assembly_address
  %940 = load i32* %stack_var_-40
  %941 = zext i32 %940 to i64
  store i64 %941, i64* %rdx
  store volatile i64 42707, i64* @assembly_address
  %942 = load i64* %rdx
  %943 = trunc i64 %942 to i16
  %944 = load i64* %rax
  %945 = add i64 %944, 2
  %946 = inttoptr i64 %945 to i16*
  store i16 %943, i16* %946
  store volatile i64 42711, i64* @assembly_address
  %947 = load i32* %stack_var_-48
  %948 = zext i32 %947 to i64
  store i64 %948, i64* %rdx
  store volatile i64 42714, i64* @assembly_address
  %949 = load i64* %rdx
  %950 = trunc i64 %949 to i32
  %951 = sext i32 %950 to i64
  store i64 %951, i64* %rdx
  store volatile i64 42717, i64* @assembly_address
  %952 = load i64* %rdx
  %953 = mul i64 %952, 4
  store i64 %953, i64* %rcx
  store volatile i64 42725, i64* @assembly_address
  %954 = load i16** %stack_var_-24
  %955 = ptrtoint i16* %954 to i64
  store i64 %955, i64* %rdx
  store volatile i64 42729, i64* @assembly_address
  %956 = load i64* %rdx
  %957 = load i64* %rcx
  %958 = add i64 %956, %957
  %959 = and i64 %956, 15
  %960 = and i64 %957, 15
  %961 = add i64 %959, %960
  %962 = icmp ugt i64 %961, 15
  %963 = icmp ult i64 %958, %956
  %964 = xor i64 %956, %958
  %965 = xor i64 %957, %958
  %966 = and i64 %964, %965
  %967 = icmp slt i64 %966, 0
  store i1 %962, i1* %az
  store i1 %963, i1* %cf
  store i1 %967, i1* %of
  %968 = icmp eq i64 %958, 0
  store i1 %968, i1* %zf
  %969 = icmp slt i64 %958, 0
  store i1 %969, i1* %sf
  %970 = trunc i64 %958 to i8
  %971 = call i8 @llvm.ctpop.i8(i8 %970)
  %972 = and i8 %971, 1
  %973 = icmp eq i8 %972, 0
  store i1 %973, i1* %pf
  store i64 %958, i64* %rdx
  store volatile i64 42732, i64* @assembly_address
  %974 = load i64* %rax
  %975 = add i64 %974, 2
  %976 = inttoptr i64 %975 to i16*
  %977 = load i16* %976
  %978 = zext i16 %977 to i64
  store i64 %978, i64* %rax
  store volatile i64 42736, i64* @assembly_address
  %979 = load i64* %rax
  %980 = trunc i64 %979 to i16
  %981 = load i64* %rdx
  %982 = add i64 %981, 2
  %983 = inttoptr i64 %982 to i16*
  store i16 %980, i16* %983
  store volatile i64 42740, i64* @assembly_address
  %984 = load i32* %stack_var_-40
  %985 = zext i32 %984 to i64
  store i64 %985, i64* %rax
  store volatile i64 42743, i64* @assembly_address
  %986 = load i64* %rax
  %987 = add i64 %986, 1
  %988 = trunc i64 %987 to i32
  %989 = zext i32 %988 to i64
  store i64 %989, i64* %rdx
  store volatile i64 42746, i64* @assembly_address
  %990 = load i64* %rdx
  %991 = trunc i64 %990 to i32
  store i32 %991, i32* %stack_var_-40
  store volatile i64 42749, i64* @assembly_address
  %992 = load i64* %rax
  %993 = trunc i64 %992 to i32
  store i32 %993, i32* bitcast (i64* @global_var_217f84 to i32*)
  store volatile i64 42755, i64* @assembly_address
  %994 = load i16** %stack_var_-24
  %995 = ptrtoint i16* %994 to i64
  store i64 %995, i64* %rax
  store volatile i64 42759, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 42764, i64* @assembly_address
  %996 = load i64* %rax
  store i64 %996, i64* %rdi
  store volatile i64 42767, i64* @assembly_address
  %997 = load i64* %rdi
  %998 = load i64* %rsi
  %999 = inttoptr i64 %997 to i16*
  %1000 = call i64 @pqdownheap(i16* %999, i64 %998)
  store i64 %1000, i64* %rax
  store i64 %1000, i64* %rax
  store volatile i64 42772, i64* @assembly_address
  %1001 = load i32* bitcast (i64* @global_var_218874 to i32*)
  %1002 = zext i32 %1001 to i64
  store i64 %1002, i64* %rax
  store volatile i64 42778, i64* @assembly_address
  %1003 = load i64* %rax
  %1004 = trunc i64 %1003 to i32
  %1005 = trunc i64 %1003 to i32
  store i32 %1005, i32* %3
  store i32 1, i32* %2
  %1006 = sub i32 %1004, 1
  %1007 = and i32 %1004, 15
  %1008 = sub i32 %1007, 1
  %1009 = icmp ugt i32 %1008, 15
  %1010 = icmp ult i32 %1004, 1
  %1011 = xor i32 %1004, 1
  %1012 = xor i32 %1004, %1006
  %1013 = and i32 %1011, %1012
  %1014 = icmp slt i32 %1013, 0
  store i1 %1009, i1* %az
  store i1 %1010, i1* %cf
  store i1 %1014, i1* %of
  %1015 = icmp eq i32 %1006, 0
  store i1 %1015, i1* %zf
  %1016 = icmp slt i32 %1006, 0
  store i1 %1016, i1* %sf
  %1017 = trunc i32 %1006 to i8
  %1018 = call i8 @llvm.ctpop.i8(i8 %1017)
  %1019 = and i8 %1018, 1
  %1020 = icmp eq i8 %1019, 0
  store i1 %1020, i1* %pf
  store volatile i64 42781, i64* @assembly_address
  %1021 = load i32* %3
  %1022 = sext i32 %1021 to i64
  %1023 = load i32* %2
  %1024 = trunc i64 %1022 to i32
  %1025 = icmp sgt i32 %1024, %1023
  br i1 %1025, label %block_a571, label %block_a723

block_a723:                                       ; preds = %block_a6ac
  store volatile i64 42787, i64* @assembly_address
  %1026 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %1027 = zext i32 %1026 to i64
  store i64 %1027, i64* %rax
  store volatile i64 42793, i64* @assembly_address
  %1028 = load i64* %rax
  %1029 = trunc i64 %1028 to i32
  %1030 = sub i32 %1029, 1
  %1031 = and i32 %1029, 15
  %1032 = sub i32 %1031, 1
  %1033 = icmp ugt i32 %1032, 15
  %1034 = icmp ult i32 %1029, 1
  %1035 = xor i32 %1029, 1
  %1036 = xor i32 %1029, %1030
  %1037 = and i32 %1035, %1036
  %1038 = icmp slt i32 %1037, 0
  store i1 %1033, i1* %az
  store i1 %1034, i1* %cf
  store i1 %1038, i1* %of
  %1039 = icmp eq i32 %1030, 0
  store i1 %1039, i1* %zf
  %1040 = icmp slt i32 %1030, 0
  store i1 %1040, i1* %sf
  %1041 = trunc i32 %1030 to i8
  %1042 = call i8 @llvm.ctpop.i8(i8 %1041)
  %1043 = and i8 %1042, 1
  %1044 = icmp eq i8 %1043, 0
  store i1 %1044, i1* %pf
  %1045 = zext i32 %1030 to i64
  store i64 %1045, i64* %rax
  store volatile i64 42796, i64* @assembly_address
  %1046 = load i64* %rax
  %1047 = trunc i64 %1046 to i32
  store i32 %1047, i32* bitcast (i64* @global_var_218878 to i32*)
  store volatile i64 42802, i64* @assembly_address
  %1048 = load i32* bitcast (i64* @global_var_218878 to i32*)
  %1049 = zext i32 %1048 to i64
  store i64 %1049, i64* %rax
  store volatile i64 42808, i64* @assembly_address
  %1050 = load i32* bitcast (i64* @global_var_217f84 to i32*)
  %1051 = zext i32 %1050 to i64
  store i64 %1051, i64* %rdx
  store volatile i64 42814, i64* @assembly_address
  %1052 = load i64* %rax
  %1053 = trunc i64 %1052 to i32
  %1054 = sext i32 %1053 to i64
  store i64 %1054, i64* %rax
  store volatile i64 42816, i64* @assembly_address
  %1055 = load i64* %rax
  %1056 = mul i64 %1055, 4
  store i64 %1056, i64* %rcx
  store volatile i64 42824, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217f80 to i64), i64* %rax
  store volatile i64 42831, i64* @assembly_address
  %1057 = load i64* %rdx
  %1058 = trunc i64 %1057 to i32
  %1059 = load i64* %rcx
  %1060 = load i64* %rax
  %1061 = mul i64 %1060, 1
  %1062 = add i64 %1059, %1061
  %1063 = inttoptr i64 %1062 to i32*
  store i32 %1058, i32* %1063
  store volatile i64 42834, i64* @assembly_address
  %1064 = load i64* %stack_var_-64
  store i64 %1064, i64* %rax
  store volatile i64 42838, i64* @assembly_address
  %1065 = load i64* %rax
  store i64 %1065, i64* %rdi
  store volatile i64 42841, i64* @assembly_address
  %1066 = load i64* %rdi
  %1067 = inttoptr i64 %1066 to i64*
  %1068 = call i64 @gen_bitlen(i64* %1067)
  store i64 %1068, i64* %rax
  store i64 %1068, i64* %rax
  store volatile i64 42846, i64* @assembly_address
  %1069 = load i32* %stack_var_-44
  %1070 = zext i32 %1069 to i64
  store i64 %1070, i64* %rdx
  store volatile i64 42849, i64* @assembly_address
  %1071 = load i16** %stack_var_-24
  %1072 = ptrtoint i16* %1071 to i64
  store i64 %1072, i64* %rax
  store volatile i64 42853, i64* @assembly_address
  %1073 = load i64* %rdx
  %1074 = trunc i64 %1073 to i32
  %1075 = zext i32 %1074 to i64
  store i64 %1075, i64* %rsi
  store volatile i64 42855, i64* @assembly_address
  %1076 = load i64* %rax
  store i64 %1076, i64* %rdi
  store volatile i64 42858, i64* @assembly_address
  %1077 = load i64* %rdi
  %1078 = inttoptr i64 %1077 to i64*
  %1079 = load i64* %rsi
  %1080 = bitcast i64* %1078 to i16*
  %1081 = call i64 @gen_codes(i16* %1080, i64 %1079)
  store i64 %1081, i64* %rax
  store i64 %1081, i64* %rax
  store volatile i64 42863, i64* @assembly_address
  store volatile i64 42864, i64* @assembly_address
  %1082 = load i64* %stack_var_-8
  store i64 %1082, i64* %rbp
  %1083 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1083, i64* %rsp
  store volatile i64 42865, i64* @assembly_address
  %1084 = load i64* %rax
  ret i64 %1084
}

define i64 @scan_tree(i64* %arg1, i32 %arg2) {
block_a772:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i64* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-24 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-52 = alloca i32
  %stack_var_-48 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i64
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i32
  %9 = alloca i64
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i64
  store volatile i64 42866, i64* @assembly_address
  %13 = load i64* %rbp
  store i64 %13, i64* %stack_var_-8
  %14 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %14, i64* %rsp
  store volatile i64 42867, i64* @assembly_address
  %15 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %15, i64* %rbp
  store volatile i64 42870, i64* @assembly_address
  %16 = load i64* %rdi
  store i64 %16, i64* %stack_var_-48
  store volatile i64 42874, i64* @assembly_address
  %17 = load i64* %rsi
  %18 = trunc i64 %17 to i32
  store i32 %18, i32* %stack_var_-52
  store volatile i64 42877, i64* @assembly_address
  store i32 -1, i32* %stack_var_-32
  store volatile i64 42884, i64* @assembly_address
  %19 = load i64* %stack_var_-48
  store i64 %19, i64* %rax
  store volatile i64 42888, i64* @assembly_address
  %20 = load i64* %rax
  %21 = add i64 %20, 2
  %22 = inttoptr i64 %21 to i16*
  %23 = load i16* %22
  %24 = zext i16 %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 42892, i64* @assembly_address
  %25 = load i64* %rax
  %26 = trunc i64 %25 to i16
  %27 = zext i16 %26 to i64
  store i64 %27, i64* %rax
  store volatile i64 42895, i64* @assembly_address
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  store i32 %29, i32* %stack_var_-28
  store volatile i64 42898, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 42905, i64* @assembly_address
  store i32 7, i32* %stack_var_-20
  store volatile i64 42912, i64* @assembly_address
  store i32 4, i32* %stack_var_-16
  store volatile i64 42919, i64* @assembly_address
  %30 = load i32* %stack_var_-28
  %31 = and i32 %30, 15
  %32 = icmp ugt i32 %31, 15
  %33 = icmp ult i32 %30, 0
  %34 = xor i32 %30, 0
  %35 = and i32 %34, 0
  %36 = icmp slt i32 %35, 0
  store i1 %32, i1* %az
  store i1 %33, i1* %cf
  store i1 %36, i1* %of
  %37 = icmp eq i32 %30, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i32 %30, 0
  store i1 %38, i1* %sf
  %39 = trunc i32 %30 to i8
  %40 = call i8 @llvm.ctpop.i8(i8 %39)
  %41 = and i8 %40, 1
  %42 = icmp eq i8 %41, 0
  store i1 %42, i1* %pf
  store volatile i64 42923, i64* @assembly_address
  %43 = load i1* %zf
  %44 = icmp eq i1 %43, false
  br i1 %44, label %block_a7bb, label %block_a7ad

block_a7ad:                                       ; preds = %block_a772
  store volatile i64 42925, i64* @assembly_address
  store i32 138, i32* %stack_var_-20
  store volatile i64 42932, i64* @assembly_address
  store i32 3, i32* %stack_var_-16
  br label %block_a7bb

block_a7bb:                                       ; preds = %block_a7ad, %block_a772
  store volatile i64 42939, i64* @assembly_address
  %45 = load i32* %stack_var_-52
  %46 = zext i32 %45 to i64
  store i64 %46, i64* %rax
  store volatile i64 42942, i64* @assembly_address
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i32
  %49 = sext i32 %48 to i64
  store i64 %49, i64* %rax
  store volatile i64 42944, i64* @assembly_address
  %50 = load i64* %rax
  %51 = add i64 %50, 1
  %52 = and i64 %50, 15
  %53 = add i64 %52, 1
  %54 = icmp ugt i64 %53, 15
  %55 = icmp ult i64 %51, %50
  %56 = xor i64 %50, %51
  %57 = xor i64 1, %51
  %58 = and i64 %56, %57
  %59 = icmp slt i64 %58, 0
  store i1 %54, i1* %az
  store i1 %55, i1* %cf
  store i1 %59, i1* %of
  %60 = icmp eq i64 %51, 0
  store i1 %60, i1* %zf
  %61 = icmp slt i64 %51, 0
  store i1 %61, i1* %sf
  %62 = trunc i64 %51 to i8
  %63 = call i8 @llvm.ctpop.i8(i8 %62)
  %64 = and i8 %63, 1
  %65 = icmp eq i8 %64, 0
  store i1 %65, i1* %pf
  store i64 %51, i64* %rax
  store volatile i64 42948, i64* @assembly_address
  %66 = load i64* %rax
  %67 = mul i64 %66, 4
  store i64 %67, i64* %rdx
  store volatile i64 42956, i64* @assembly_address
  %68 = load i64* %stack_var_-48
  store i64 %68, i64* %rax
  store volatile i64 42960, i64* @assembly_address
  %69 = load i64* %rax
  %70 = load i64* %rdx
  %71 = add i64 %69, %70
  %72 = and i64 %69, 15
  %73 = and i64 %70, 15
  %74 = add i64 %72, %73
  %75 = icmp ugt i64 %74, 15
  %76 = icmp ult i64 %71, %69
  %77 = xor i64 %69, %71
  %78 = xor i64 %70, %71
  %79 = and i64 %77, %78
  %80 = icmp slt i64 %79, 0
  store i1 %75, i1* %az
  store i1 %76, i1* %cf
  store i1 %80, i1* %of
  %81 = icmp eq i64 %71, 0
  store i1 %81, i1* %zf
  %82 = icmp slt i64 %71, 0
  store i1 %82, i1* %sf
  %83 = trunc i64 %71 to i8
  %84 = call i8 @llvm.ctpop.i8(i8 %83)
  %85 = and i8 %84, 1
  %86 = icmp eq i8 %85, 0
  store i1 %86, i1* %pf
  store i64 %71, i64* %rax
  store volatile i64 42963, i64* @assembly_address
  %87 = load i64* %rax
  %88 = add i64 %87, 2
  %89 = inttoptr i64 %88 to i16*
  store i16 -1, i16* %89
  store volatile i64 42969, i64* @assembly_address
  store i32 0, i32* %stack_var_-36
  store volatile i64 42976, i64* @assembly_address
  br label %block_a933

block_a7e5:                                       ; preds = %block_a933
  store volatile i64 42981, i64* @assembly_address
  %90 = load i32* %stack_var_-28
  %91 = zext i32 %90 to i64
  store i64 %91, i64* %rax
  store volatile i64 42984, i64* @assembly_address
  %92 = load i64* %rax
  %93 = trunc i64 %92 to i32
  store i32 %93, i32* %stack_var_-12
  store volatile i64 42987, i64* @assembly_address
  %94 = load i32* %stack_var_-36
  %95 = zext i32 %94 to i64
  store i64 %95, i64* %rax
  store volatile i64 42990, i64* @assembly_address
  %96 = load i64* %rax
  %97 = trunc i64 %96 to i32
  %98 = sext i32 %97 to i64
  store i64 %98, i64* %rax
  store volatile i64 42992, i64* @assembly_address
  %99 = load i64* %rax
  %100 = add i64 %99, 1
  %101 = and i64 %99, 15
  %102 = add i64 %101, 1
  %103 = icmp ugt i64 %102, 15
  %104 = icmp ult i64 %100, %99
  %105 = xor i64 %99, %100
  %106 = xor i64 1, %100
  %107 = and i64 %105, %106
  %108 = icmp slt i64 %107, 0
  store i1 %103, i1* %az
  store i1 %104, i1* %cf
  store i1 %108, i1* %of
  %109 = icmp eq i64 %100, 0
  store i1 %109, i1* %zf
  %110 = icmp slt i64 %100, 0
  store i1 %110, i1* %sf
  %111 = trunc i64 %100 to i8
  %112 = call i8 @llvm.ctpop.i8(i8 %111)
  %113 = and i8 %112, 1
  %114 = icmp eq i8 %113, 0
  store i1 %114, i1* %pf
  store i64 %100, i64* %rax
  store volatile i64 42996, i64* @assembly_address
  %115 = load i64* %rax
  %116 = mul i64 %115, 4
  store i64 %116, i64* %rdx
  store volatile i64 43004, i64* @assembly_address
  %117 = load i64* %stack_var_-48
  store i64 %117, i64* %rax
  store volatile i64 43008, i64* @assembly_address
  %118 = load i64* %rax
  %119 = load i64* %rdx
  %120 = add i64 %118, %119
  %121 = and i64 %118, 15
  %122 = and i64 %119, 15
  %123 = add i64 %121, %122
  %124 = icmp ugt i64 %123, 15
  %125 = icmp ult i64 %120, %118
  %126 = xor i64 %118, %120
  %127 = xor i64 %119, %120
  %128 = and i64 %126, %127
  %129 = icmp slt i64 %128, 0
  store i1 %124, i1* %az
  store i1 %125, i1* %cf
  store i1 %129, i1* %of
  %130 = icmp eq i64 %120, 0
  store i1 %130, i1* %zf
  %131 = icmp slt i64 %120, 0
  store i1 %131, i1* %sf
  %132 = trunc i64 %120 to i8
  %133 = call i8 @llvm.ctpop.i8(i8 %132)
  %134 = and i8 %133, 1
  %135 = icmp eq i8 %134, 0
  store i1 %135, i1* %pf
  store i64 %120, i64* %rax
  store volatile i64 43011, i64* @assembly_address
  %136 = load i64* %rax
  %137 = add i64 %136, 2
  %138 = inttoptr i64 %137 to i16*
  %139 = load i16* %138
  %140 = zext i16 %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 43015, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i16
  %143 = zext i16 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 43018, i64* @assembly_address
  %144 = load i64* %rax
  %145 = trunc i64 %144 to i32
  store i32 %145, i32* %stack_var_-28
  store volatile i64 43021, i64* @assembly_address
  %146 = load i32* %stack_var_-24
  %147 = add i32 %146, 1
  %148 = and i32 %146, 15
  %149 = add i32 %148, 1
  %150 = icmp ugt i32 %149, 15
  %151 = icmp ult i32 %147, %146
  %152 = xor i32 %146, %147
  %153 = xor i32 1, %147
  %154 = and i32 %152, %153
  %155 = icmp slt i32 %154, 0
  store i1 %150, i1* %az
  store i1 %151, i1* %cf
  store i1 %155, i1* %of
  %156 = icmp eq i32 %147, 0
  store i1 %156, i1* %zf
  %157 = icmp slt i32 %147, 0
  store i1 %157, i1* %sf
  %158 = trunc i32 %147 to i8
  %159 = call i8 @llvm.ctpop.i8(i8 %158)
  %160 = and i8 %159, 1
  %161 = icmp eq i8 %160, 0
  store i1 %161, i1* %pf
  store i32 %147, i32* %stack_var_-24
  store volatile i64 43025, i64* @assembly_address
  %162 = load i32* %stack_var_-24
  %163 = zext i32 %162 to i64
  store i64 %163, i64* %rax
  store volatile i64 43028, i64* @assembly_address
  %164 = load i64* %rax
  %165 = trunc i64 %164 to i32
  %166 = load i32* %stack_var_-20
  %167 = trunc i64 %164 to i32
  store i32 %167, i32* %11
  store i32 %166, i32* %10
  %168 = sub i32 %165, %166
  %169 = and i32 %165, 15
  %170 = and i32 %166, 15
  %171 = sub i32 %169, %170
  %172 = icmp ugt i32 %171, 15
  %173 = icmp ult i32 %165, %166
  %174 = xor i32 %165, %166
  %175 = xor i32 %165, %168
  %176 = and i32 %174, %175
  %177 = icmp slt i32 %176, 0
  store i1 %172, i1* %az
  store i1 %173, i1* %cf
  store i1 %177, i1* %of
  %178 = icmp eq i32 %168, 0
  store i1 %178, i1* %zf
  %179 = icmp slt i32 %168, 0
  store i1 %179, i1* %sf
  %180 = trunc i32 %168 to i8
  %181 = call i8 @llvm.ctpop.i8(i8 %180)
  %182 = and i8 %181, 1
  %183 = icmp eq i8 %182, 0
  store i1 %183, i1* %pf
  store volatile i64 43031, i64* @assembly_address
  %184 = load i32* %11
  %185 = sext i32 %184 to i64
  %186 = load i32* %10
  %187 = trunc i64 %185 to i32
  %188 = icmp sge i32 %187, %186
  br i1 %188, label %block_a825, label %block_a819

block_a819:                                       ; preds = %block_a7e5
  store volatile i64 43033, i64* @assembly_address
  %189 = load i32* %stack_var_-12
  %190 = zext i32 %189 to i64
  store i64 %190, i64* %rax
  store volatile i64 43036, i64* @assembly_address
  %191 = load i64* %rax
  %192 = trunc i64 %191 to i32
  %193 = load i32* %stack_var_-28
  %194 = sub i32 %192, %193
  %195 = and i32 %192, 15
  %196 = and i32 %193, 15
  %197 = sub i32 %195, %196
  %198 = icmp ugt i32 %197, 15
  %199 = icmp ult i32 %192, %193
  %200 = xor i32 %192, %193
  %201 = xor i32 %192, %194
  %202 = and i32 %200, %201
  %203 = icmp slt i32 %202, 0
  store i1 %198, i1* %az
  store i1 %199, i1* %cf
  store i1 %203, i1* %of
  %204 = icmp eq i32 %194, 0
  store i1 %204, i1* %zf
  %205 = icmp slt i32 %194, 0
  store i1 %205, i1* %sf
  %206 = trunc i32 %194 to i8
  %207 = call i8 @llvm.ctpop.i8(i8 %206)
  %208 = and i8 %207, 1
  %209 = icmp eq i8 %208, 0
  store i1 %209, i1* %pf
  store volatile i64 43039, i64* @assembly_address
  %210 = load i1* %zf
  br i1 %210, label %block_a92e, label %block_a825

block_a825:                                       ; preds = %block_a819, %block_a7e5
  store volatile i64 43045, i64* @assembly_address
  %211 = load i32* %stack_var_-24
  %212 = zext i32 %211 to i64
  store i64 %212, i64* %rax
  store volatile i64 43048, i64* @assembly_address
  %213 = load i64* %rax
  %214 = trunc i64 %213 to i32
  %215 = load i32* %stack_var_-16
  %216 = trunc i64 %213 to i32
  store i32 %216, i32* %8
  store i32 %215, i32* %7
  %217 = sub i32 %214, %215
  %218 = and i32 %214, 15
  %219 = and i32 %215, 15
  %220 = sub i32 %218, %219
  %221 = icmp ugt i32 %220, 15
  %222 = icmp ult i32 %214, %215
  %223 = xor i32 %214, %215
  %224 = xor i32 %214, %217
  %225 = and i32 %223, %224
  %226 = icmp slt i32 %225, 0
  store i1 %221, i1* %az
  store i1 %222, i1* %cf
  store i1 %226, i1* %of
  %227 = icmp eq i32 %217, 0
  store i1 %227, i1* %zf
  %228 = icmp slt i32 %217, 0
  store i1 %228, i1* %sf
  %229 = trunc i32 %217 to i8
  %230 = call i8 @llvm.ctpop.i8(i8 %229)
  %231 = and i8 %230, 1
  %232 = icmp eq i8 %231, 0
  store i1 %232, i1* %pf
  store volatile i64 43051, i64* @assembly_address
  %233 = load i32* %8
  %234 = sext i32 %233 to i64
  %235 = load i32* %7
  %236 = trunc i64 %234 to i32
  %237 = icmp sge i32 %236, %235
  br i1 %237, label %block_a865, label %block_a82d

block_a82d:                                       ; preds = %block_a825
  store volatile i64 43053, i64* @assembly_address
  %238 = load i32* %stack_var_-12
  %239 = zext i32 %238 to i64
  store i64 %239, i64* %rax
  store volatile i64 43056, i64* @assembly_address
  %240 = load i64* %rax
  %241 = trunc i64 %240 to i32
  %242 = sext i32 %241 to i64
  store i64 %242, i64* %rax
  store volatile i64 43058, i64* @assembly_address
  %243 = load i64* %rax
  %244 = mul i64 %243, 4
  store i64 %244, i64* %rdx
  store volatile i64 43066, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec0 to i64), i64* %rax
  store volatile i64 43073, i64* @assembly_address
  %245 = load i64* %rdx
  %246 = load i64* %rax
  %247 = mul i64 %246, 1
  %248 = add i64 %245, %247
  %249 = inttoptr i64 %248 to i16*
  %250 = load i16* %249
  %251 = zext i16 %250 to i64
  store i64 %251, i64* %rax
  store volatile i64 43077, i64* @assembly_address
  %252 = load i32* %stack_var_-24
  %253 = zext i32 %252 to i64
  store i64 %253, i64* %rdx
  store volatile i64 43080, i64* @assembly_address
  %254 = load i64* %rax
  %255 = load i64* %rdx
  %256 = mul i64 %255, 1
  %257 = add i64 %254, %256
  %258 = trunc i64 %257 to i32
  %259 = zext i32 %258 to i64
  store i64 %259, i64* %rcx
  store volatile i64 43083, i64* @assembly_address
  %260 = load i32* %stack_var_-12
  %261 = zext i32 %260 to i64
  store i64 %261, i64* %rax
  store volatile i64 43086, i64* @assembly_address
  %262 = load i64* %rax
  %263 = trunc i64 %262 to i32
  %264 = sext i32 %263 to i64
  store i64 %264, i64* %rax
  store volatile i64 43088, i64* @assembly_address
  %265 = load i64* %rax
  %266 = mul i64 %265, 4
  store i64 %266, i64* %rdx
  store volatile i64 43096, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec0 to i64), i64* %rax
  store volatile i64 43103, i64* @assembly_address
  %267 = load i64* %rcx
  %268 = trunc i64 %267 to i16
  %269 = load i64* %rdx
  %270 = load i64* %rax
  %271 = mul i64 %270, 1
  %272 = add i64 %269, %271
  %273 = inttoptr i64 %272 to i16*
  store i16 %268, i16* %273
  store volatile i64 43107, i64* @assembly_address
  br label %block_a8e3

block_a865:                                       ; preds = %block_a825
  store volatile i64 43109, i64* @assembly_address
  %274 = load i32* %stack_var_-12
  %275 = and i32 %274, 15
  %276 = icmp ugt i32 %275, 15
  %277 = icmp ult i32 %274, 0
  %278 = xor i32 %274, 0
  %279 = and i32 %278, 0
  %280 = icmp slt i32 %279, 0
  store i1 %276, i1* %az
  store i1 %277, i1* %cf
  store i1 %280, i1* %of
  %281 = icmp eq i32 %274, 0
  store i1 %281, i1* %zf
  %282 = icmp slt i32 %274, 0
  store i1 %282, i1* %sf
  %283 = trunc i32 %274 to i8
  %284 = call i8 @llvm.ctpop.i8(i8 %283)
  %285 = and i8 %284, 1
  %286 = icmp eq i8 %285, 0
  store i1 %286, i1* %pf
  store volatile i64 43113, i64* @assembly_address
  %287 = load i1* %zf
  br i1 %287, label %block_a8b9, label %block_a86b

block_a86b:                                       ; preds = %block_a865
  store volatile i64 43115, i64* @assembly_address
  %288 = load i32* %stack_var_-12
  %289 = zext i32 %288 to i64
  store i64 %289, i64* %rax
  store volatile i64 43118, i64* @assembly_address
  %290 = load i64* %rax
  %291 = trunc i64 %290 to i32
  %292 = load i32* %stack_var_-32
  %293 = sub i32 %291, %292
  %294 = and i32 %291, 15
  %295 = and i32 %292, 15
  %296 = sub i32 %294, %295
  %297 = icmp ugt i32 %296, 15
  %298 = icmp ult i32 %291, %292
  %299 = xor i32 %291, %292
  %300 = xor i32 %291, %293
  %301 = and i32 %299, %300
  %302 = icmp slt i32 %301, 0
  store i1 %297, i1* %az
  store i1 %298, i1* %cf
  store i1 %302, i1* %of
  %303 = icmp eq i32 %293, 0
  store i1 %303, i1* %zf
  %304 = icmp slt i32 %293, 0
  store i1 %304, i1* %sf
  %305 = trunc i32 %293 to i8
  %306 = call i8 @llvm.ctpop.i8(i8 %305)
  %307 = and i8 %306, 1
  %308 = icmp eq i8 %307, 0
  store i1 %308, i1* %pf
  store volatile i64 43121, i64* @assembly_address
  %309 = load i1* %zf
  br i1 %309, label %block_a8a6, label %block_a873

block_a873:                                       ; preds = %block_a86b
  store volatile i64 43123, i64* @assembly_address
  %310 = load i32* %stack_var_-12
  %311 = zext i32 %310 to i64
  store i64 %311, i64* %rax
  store volatile i64 43126, i64* @assembly_address
  %312 = load i64* %rax
  %313 = trunc i64 %312 to i32
  %314 = sext i32 %313 to i64
  store i64 %314, i64* %rax
  store volatile i64 43128, i64* @assembly_address
  %315 = load i64* %rax
  %316 = mul i64 %315, 4
  store i64 %316, i64* %rdx
  store volatile i64 43136, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec0 to i64), i64* %rax
  store volatile i64 43143, i64* @assembly_address
  %317 = load i64* %rdx
  %318 = load i64* %rax
  %319 = mul i64 %318, 1
  %320 = add i64 %317, %319
  %321 = inttoptr i64 %320 to i16*
  %322 = load i16* %321
  %323 = zext i16 %322 to i64
  store i64 %323, i64* %rax
  store volatile i64 43147, i64* @assembly_address
  %324 = load i64* %rax
  %325 = add i64 %324, 1
  %326 = trunc i64 %325 to i32
  %327 = zext i32 %326 to i64
  store i64 %327, i64* %rcx
  store volatile i64 43150, i64* @assembly_address
  %328 = load i32* %stack_var_-12
  %329 = zext i32 %328 to i64
  store i64 %329, i64* %rax
  store volatile i64 43153, i64* @assembly_address
  %330 = load i64* %rax
  %331 = trunc i64 %330 to i32
  %332 = sext i32 %331 to i64
  store i64 %332, i64* %rax
  store volatile i64 43155, i64* @assembly_address
  %333 = load i64* %rax
  %334 = mul i64 %333, 4
  store i64 %334, i64* %rdx
  store volatile i64 43163, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec0 to i64), i64* %rax
  store volatile i64 43170, i64* @assembly_address
  %335 = load i64* %rcx
  %336 = trunc i64 %335 to i16
  %337 = load i64* %rdx
  %338 = load i64* %rax
  %339 = mul i64 %338, 1
  %340 = add i64 %337, %339
  %341 = inttoptr i64 %340 to i16*
  store i16 %336, i16* %341
  br label %block_a8a6

block_a8a6:                                       ; preds = %block_a873, %block_a86b
  store volatile i64 43174, i64* @assembly_address
  %342 = load i16* bitcast (i64* @global_var_217f00 to i16*)
  %343 = zext i16 %342 to i64
  store i64 %343, i64* %rax
  store volatile i64 43181, i64* @assembly_address
  %344 = load i64* %rax
  %345 = trunc i64 %344 to i32
  %346 = add i32 %345, 1
  %347 = and i32 %345, 15
  %348 = add i32 %347, 1
  %349 = icmp ugt i32 %348, 15
  %350 = icmp ult i32 %346, %345
  %351 = xor i32 %345, %346
  %352 = xor i32 1, %346
  %353 = and i32 %351, %352
  %354 = icmp slt i32 %353, 0
  store i1 %349, i1* %az
  store i1 %350, i1* %cf
  store i1 %354, i1* %of
  %355 = icmp eq i32 %346, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i32 %346, 0
  store i1 %356, i1* %sf
  %357 = trunc i32 %346 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  store i64 ptrtoint (i64* @global_var_217f01 to i64), i64* %rax
  store volatile i64 43184, i64* @assembly_address
  %361 = load i64* %rax
  %362 = trunc i64 %361 to i16
  store i16 %362, i16* bitcast (i64* @global_var_217f00 to i16*)
  store volatile i64 43191, i64* @assembly_address
  br label %block_a8e3

block_a8b9:                                       ; preds = %block_a865
  store volatile i64 43193, i64* @assembly_address
  %363 = load i32* %stack_var_-24
  store i32 %363, i32* %6
  store i32 10, i32* %5
  %364 = sub i32 %363, 10
  %365 = and i32 %363, 15
  %366 = sub i32 %365, 10
  %367 = icmp ugt i32 %366, 15
  %368 = icmp ult i32 %363, 10
  %369 = xor i32 %363, 10
  %370 = xor i32 %363, %364
  %371 = and i32 %369, %370
  %372 = icmp slt i32 %371, 0
  store i1 %367, i1* %az
  store i1 %368, i1* %cf
  store i1 %372, i1* %of
  %373 = icmp eq i32 %364, 0
  store i1 %373, i1* %zf
  %374 = icmp slt i32 %364, 0
  store i1 %374, i1* %sf
  %375 = trunc i32 %364 to i8
  %376 = call i8 @llvm.ctpop.i8(i8 %375)
  %377 = and i8 %376, 1
  %378 = icmp eq i8 %377, 0
  store i1 %378, i1* %pf
  store volatile i64 43197, i64* @assembly_address
  %379 = load i32* %6
  %380 = load i32* %5
  %381 = icmp sgt i32 %379, %380
  br i1 %381, label %block_a8d2, label %block_a8bf

block_a8bf:                                       ; preds = %block_a8b9
  store volatile i64 43199, i64* @assembly_address
  %382 = load i16* bitcast (i64* @global_var_217f04 to i16*)
  %383 = zext i16 %382 to i64
  store i64 %383, i64* %rax
  store volatile i64 43206, i64* @assembly_address
  %384 = load i64* %rax
  %385 = trunc i64 %384 to i32
  %386 = add i32 %385, 1
  %387 = and i32 %385, 15
  %388 = add i32 %387, 1
  %389 = icmp ugt i32 %388, 15
  %390 = icmp ult i32 %386, %385
  %391 = xor i32 %385, %386
  %392 = xor i32 1, %386
  %393 = and i32 %391, %392
  %394 = icmp slt i32 %393, 0
  store i1 %389, i1* %az
  store i1 %390, i1* %cf
  store i1 %394, i1* %of
  %395 = icmp eq i32 %386, 0
  store i1 %395, i1* %zf
  %396 = icmp slt i32 %386, 0
  store i1 %396, i1* %sf
  %397 = trunc i32 %386 to i8
  %398 = call i8 @llvm.ctpop.i8(i8 %397)
  %399 = and i8 %398, 1
  %400 = icmp eq i8 %399, 0
  store i1 %400, i1* %pf
  store i64 ptrtoint (i64* @global_var_217f05 to i64), i64* %rax
  store volatile i64 43209, i64* @assembly_address
  %401 = load i64* %rax
  %402 = trunc i64 %401 to i16
  store i16 %402, i16* bitcast (i64* @global_var_217f04 to i16*)
  store volatile i64 43216, i64* @assembly_address
  br label %block_a8e3

block_a8d2:                                       ; preds = %block_a8b9
  store volatile i64 43218, i64* @assembly_address
  %403 = load i16* bitcast (i64* @global_var_217f08 to i16*)
  %404 = zext i16 %403 to i64
  store i64 %404, i64* %rax
  store volatile i64 43225, i64* @assembly_address
  %405 = load i64* %rax
  %406 = trunc i64 %405 to i32
  %407 = add i32 %406, 1
  %408 = and i32 %406, 15
  %409 = add i32 %408, 1
  %410 = icmp ugt i32 %409, 15
  %411 = icmp ult i32 %407, %406
  %412 = xor i32 %406, %407
  %413 = xor i32 1, %407
  %414 = and i32 %412, %413
  %415 = icmp slt i32 %414, 0
  store i1 %410, i1* %az
  store i1 %411, i1* %cf
  store i1 %415, i1* %of
  %416 = icmp eq i32 %407, 0
  store i1 %416, i1* %zf
  %417 = icmp slt i32 %407, 0
  store i1 %417, i1* %sf
  %418 = trunc i32 %407 to i8
  %419 = call i8 @llvm.ctpop.i8(i8 %418)
  %420 = and i8 %419, 1
  %421 = icmp eq i8 %420, 0
  store i1 %421, i1* %pf
  store i64 ptrtoint (i64* @global_var_217f09 to i64), i64* %rax
  store volatile i64 43228, i64* @assembly_address
  %422 = load i64* %rax
  %423 = trunc i64 %422 to i16
  store i16 %423, i16* bitcast (i64* @global_var_217f08 to i16*)
  br label %block_a8e3

block_a8e3:                                       ; preds = %block_a8d2, %block_a8bf, %block_a8a6, %block_a82d
  store volatile i64 43235, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 43242, i64* @assembly_address
  %424 = load i32* %stack_var_-12
  %425 = zext i32 %424 to i64
  store i64 %425, i64* %rax
  store volatile i64 43245, i64* @assembly_address
  %426 = load i64* %rax
  %427 = trunc i64 %426 to i32
  store i32 %427, i32* %stack_var_-32
  store volatile i64 43248, i64* @assembly_address
  %428 = load i32* %stack_var_-28
  %429 = and i32 %428, 15
  %430 = icmp ugt i32 %429, 15
  %431 = icmp ult i32 %428, 0
  %432 = xor i32 %428, 0
  %433 = and i32 %432, 0
  %434 = icmp slt i32 %433, 0
  store i1 %430, i1* %az
  store i1 %431, i1* %cf
  store i1 %434, i1* %of
  %435 = icmp eq i32 %428, 0
  store i1 %435, i1* %zf
  %436 = icmp slt i32 %428, 0
  store i1 %436, i1* %sf
  %437 = trunc i32 %428 to i8
  %438 = call i8 @llvm.ctpop.i8(i8 %437)
  %439 = and i8 %438, 1
  %440 = icmp eq i8 %439, 0
  store i1 %440, i1* %pf
  store volatile i64 43252, i64* @assembly_address
  %441 = load i1* %zf
  %442 = icmp eq i1 %441, false
  br i1 %442, label %block_a906, label %block_a8f6

block_a8f6:                                       ; preds = %block_a8e3
  store volatile i64 43254, i64* @assembly_address
  store i32 138, i32* %stack_var_-20
  store volatile i64 43261, i64* @assembly_address
  store i32 3, i32* %stack_var_-16
  store volatile i64 43268, i64* @assembly_address
  br label %block_a92f

block_a906:                                       ; preds = %block_a8e3
  store volatile i64 43270, i64* @assembly_address
  %443 = load i32* %stack_var_-12
  %444 = zext i32 %443 to i64
  store i64 %444, i64* %rax
  store volatile i64 43273, i64* @assembly_address
  %445 = load i64* %rax
  %446 = trunc i64 %445 to i32
  %447 = load i32* %stack_var_-28
  %448 = sub i32 %446, %447
  %449 = and i32 %446, 15
  %450 = and i32 %447, 15
  %451 = sub i32 %449, %450
  %452 = icmp ugt i32 %451, 15
  %453 = icmp ult i32 %446, %447
  %454 = xor i32 %446, %447
  %455 = xor i32 %446, %448
  %456 = and i32 %454, %455
  %457 = icmp slt i32 %456, 0
  store i1 %452, i1* %az
  store i1 %453, i1* %cf
  store i1 %457, i1* %of
  %458 = icmp eq i32 %448, 0
  store i1 %458, i1* %zf
  %459 = icmp slt i32 %448, 0
  store i1 %459, i1* %sf
  %460 = trunc i32 %448 to i8
  %461 = call i8 @llvm.ctpop.i8(i8 %460)
  %462 = and i8 %461, 1
  %463 = icmp eq i8 %462, 0
  store i1 %463, i1* %pf
  store volatile i64 43276, i64* @assembly_address
  %464 = load i1* %zf
  %465 = icmp eq i1 %464, false
  br i1 %465, label %block_a91e, label %block_a90e

block_a90e:                                       ; preds = %block_a906
  store volatile i64 43278, i64* @assembly_address
  store i32 6, i32* %stack_var_-20
  store volatile i64 43285, i64* @assembly_address
  store i32 3, i32* %stack_var_-16
  store volatile i64 43292, i64* @assembly_address
  br label %block_a92f

block_a91e:                                       ; preds = %block_a906
  store volatile i64 43294, i64* @assembly_address
  store i32 7, i32* %stack_var_-20
  store volatile i64 43301, i64* @assembly_address
  store i32 4, i32* %stack_var_-16
  store volatile i64 43308, i64* @assembly_address
  br label %block_a92f

block_a92e:                                       ; preds = %block_a819
  store volatile i64 43310, i64* @assembly_address
  br label %block_a92f

block_a92f:                                       ; preds = %block_a92e, %block_a91e, %block_a90e, %block_a8f6
  store volatile i64 43311, i64* @assembly_address
  %466 = load i32* %stack_var_-36
  %467 = add i32 %466, 1
  %468 = and i32 %466, 15
  %469 = add i32 %468, 1
  %470 = icmp ugt i32 %469, 15
  %471 = icmp ult i32 %467, %466
  %472 = xor i32 %466, %467
  %473 = xor i32 1, %467
  %474 = and i32 %472, %473
  %475 = icmp slt i32 %474, 0
  store i1 %470, i1* %az
  store i1 %471, i1* %cf
  store i1 %475, i1* %of
  %476 = icmp eq i32 %467, 0
  store i1 %476, i1* %zf
  %477 = icmp slt i32 %467, 0
  store i1 %477, i1* %sf
  %478 = trunc i32 %467 to i8
  %479 = call i8 @llvm.ctpop.i8(i8 %478)
  %480 = and i8 %479, 1
  %481 = icmp eq i8 %480, 0
  store i1 %481, i1* %pf
  store i32 %467, i32* %stack_var_-36
  br label %block_a933

block_a933:                                       ; preds = %block_a92f, %block_a7bb
  store volatile i64 43315, i64* @assembly_address
  %482 = load i32* %stack_var_-36
  %483 = zext i32 %482 to i64
  store i64 %483, i64* %rax
  store volatile i64 43318, i64* @assembly_address
  %484 = load i64* %rax
  %485 = trunc i64 %484 to i32
  %486 = load i32* %stack_var_-52
  %487 = trunc i64 %484 to i32
  store i32 %487, i32* %3
  store i32 %486, i32* %2
  %488 = sub i32 %485, %486
  %489 = and i32 %485, 15
  %490 = and i32 %486, 15
  %491 = sub i32 %489, %490
  %492 = icmp ugt i32 %491, 15
  %493 = icmp ult i32 %485, %486
  %494 = xor i32 %485, %486
  %495 = xor i32 %485, %488
  %496 = and i32 %494, %495
  %497 = icmp slt i32 %496, 0
  store i1 %492, i1* %az
  store i1 %493, i1* %cf
  store i1 %497, i1* %of
  %498 = icmp eq i32 %488, 0
  store i1 %498, i1* %zf
  %499 = icmp slt i32 %488, 0
  store i1 %499, i1* %sf
  %500 = trunc i32 %488 to i8
  %501 = call i8 @llvm.ctpop.i8(i8 %500)
  %502 = and i8 %501, 1
  %503 = icmp eq i8 %502, 0
  store i1 %503, i1* %pf
  store volatile i64 43321, i64* @assembly_address
  %504 = load i32* %3
  %505 = sext i32 %504 to i64
  %506 = load i32* %2
  %507 = trunc i64 %505 to i32
  %508 = icmp sle i32 %507, %506
  br i1 %508, label %block_a7e5, label %block_a93f

block_a93f:                                       ; preds = %block_a933
  store volatile i64 43327, i64* @assembly_address
  store volatile i64 43328, i64* @assembly_address
  %509 = load i64* %stack_var_-8
  store i64 %509, i64* %rbp
  %510 = ptrtoint i64* %stack_var_0 to i64
  store i64 %510, i64* %rsp
  store volatile i64 43329, i64* @assembly_address
  %511 = load i64* %rax
  ret i64 %511
}

declare i64 @208(i64*, i64)

define i64 @send_tree(i64* %arg1, i32 %arg2) {
block_a942:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i64* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-24 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-52 = alloca i32
  %stack_var_-48 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i64
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i32
  %9 = alloca i64
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i64
  store volatile i64 43330, i64* @assembly_address
  %13 = load i64* %rbp
  store i64 %13, i64* %stack_var_-8
  %14 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %14, i64* %rsp
  store volatile i64 43331, i64* @assembly_address
  %15 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %15, i64* %rbp
  store volatile i64 43334, i64* @assembly_address
  %16 = load i64* %rsp
  %17 = sub i64 %16, 48
  %18 = and i64 %16, 15
  %19 = icmp ugt i64 %18, 15
  %20 = icmp ult i64 %16, 48
  %21 = xor i64 %16, 48
  %22 = xor i64 %16, %17
  %23 = and i64 %21, %22
  %24 = icmp slt i64 %23, 0
  store i1 %19, i1* %az
  store i1 %20, i1* %cf
  store i1 %24, i1* %of
  %25 = icmp eq i64 %17, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i64 %17, 0
  store i1 %26, i1* %sf
  %27 = trunc i64 %17 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  %31 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %31, i64* %rsp
  store volatile i64 43338, i64* @assembly_address
  %32 = load i64* %rdi
  store i64 %32, i64* %stack_var_-48
  store volatile i64 43342, i64* @assembly_address
  %33 = load i64* %rsi
  %34 = trunc i64 %33 to i32
  store i32 %34, i32* %stack_var_-52
  store volatile i64 43345, i64* @assembly_address
  store i32 -1, i32* %stack_var_-32
  store volatile i64 43352, i64* @assembly_address
  %35 = load i64* %stack_var_-48
  store i64 %35, i64* %rax
  store volatile i64 43356, i64* @assembly_address
  %36 = load i64* %rax
  %37 = add i64 %36, 2
  %38 = inttoptr i64 %37 to i16*
  %39 = load i16* %38
  %40 = zext i16 %39 to i64
  store i64 %40, i64* %rax
  store volatile i64 43360, i64* @assembly_address
  %41 = load i64* %rax
  %42 = trunc i64 %41 to i16
  %43 = zext i16 %42 to i64
  store i64 %43, i64* %rax
  store volatile i64 43363, i64* @assembly_address
  %44 = load i64* %rax
  %45 = trunc i64 %44 to i32
  store i32 %45, i32* %stack_var_-28
  store volatile i64 43366, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 43373, i64* @assembly_address
  store i32 7, i32* %stack_var_-20
  store volatile i64 43380, i64* @assembly_address
  store i32 4, i32* %stack_var_-16
  store volatile i64 43387, i64* @assembly_address
  %46 = load i32* %stack_var_-28
  %47 = and i32 %46, 15
  %48 = icmp ugt i32 %47, 15
  %49 = icmp ult i32 %46, 0
  %50 = xor i32 %46, 0
  %51 = and i32 %50, 0
  %52 = icmp slt i32 %51, 0
  store i1 %48, i1* %az
  store i1 %49, i1* %cf
  store i1 %52, i1* %of
  %53 = icmp eq i32 %46, 0
  store i1 %53, i1* %zf
  %54 = icmp slt i32 %46, 0
  store i1 %54, i1* %sf
  %55 = trunc i32 %46 to i8
  %56 = call i8 @llvm.ctpop.i8(i8 %55)
  %57 = and i8 %56, 1
  %58 = icmp eq i8 %57, 0
  store i1 %58, i1* %pf
  store volatile i64 43391, i64* @assembly_address
  %59 = load i1* %zf
  %60 = icmp eq i1 %59, false
  br i1 %60, label %block_a98f, label %block_a981

block_a981:                                       ; preds = %block_a942
  store volatile i64 43393, i64* @assembly_address
  store i32 138, i32* %stack_var_-20
  store volatile i64 43400, i64* @assembly_address
  store i32 3, i32* %stack_var_-16
  br label %block_a98f

block_a98f:                                       ; preds = %block_a981, %block_a942
  store volatile i64 43407, i64* @assembly_address
  store i32 0, i32* %stack_var_-36
  store volatile i64 43414, i64* @assembly_address
  br label %block_ab69

block_a99b:                                       ; preds = %block_ab69
  store volatile i64 43419, i64* @assembly_address
  %61 = load i32* %stack_var_-28
  %62 = zext i32 %61 to i64
  store i64 %62, i64* %rax
  store volatile i64 43422, i64* @assembly_address
  %63 = load i64* %rax
  %64 = trunc i64 %63 to i32
  store i32 %64, i32* %stack_var_-12
  store volatile i64 43425, i64* @assembly_address
  %65 = load i32* %stack_var_-36
  %66 = zext i32 %65 to i64
  store i64 %66, i64* %rax
  store volatile i64 43428, i64* @assembly_address
  %67 = load i64* %rax
  %68 = trunc i64 %67 to i32
  %69 = sext i32 %68 to i64
  store i64 %69, i64* %rax
  store volatile i64 43430, i64* @assembly_address
  %70 = load i64* %rax
  %71 = add i64 %70, 1
  %72 = and i64 %70, 15
  %73 = add i64 %72, 1
  %74 = icmp ugt i64 %73, 15
  %75 = icmp ult i64 %71, %70
  %76 = xor i64 %70, %71
  %77 = xor i64 1, %71
  %78 = and i64 %76, %77
  %79 = icmp slt i64 %78, 0
  store i1 %74, i1* %az
  store i1 %75, i1* %cf
  store i1 %79, i1* %of
  %80 = icmp eq i64 %71, 0
  store i1 %80, i1* %zf
  %81 = icmp slt i64 %71, 0
  store i1 %81, i1* %sf
  %82 = trunc i64 %71 to i8
  %83 = call i8 @llvm.ctpop.i8(i8 %82)
  %84 = and i8 %83, 1
  %85 = icmp eq i8 %84, 0
  store i1 %85, i1* %pf
  store i64 %71, i64* %rax
  store volatile i64 43434, i64* @assembly_address
  %86 = load i64* %rax
  %87 = mul i64 %86, 4
  store i64 %87, i64* %rdx
  store volatile i64 43442, i64* @assembly_address
  %88 = load i64* %stack_var_-48
  store i64 %88, i64* %rax
  store volatile i64 43446, i64* @assembly_address
  %89 = load i64* %rax
  %90 = load i64* %rdx
  %91 = add i64 %89, %90
  %92 = and i64 %89, 15
  %93 = and i64 %90, 15
  %94 = add i64 %92, %93
  %95 = icmp ugt i64 %94, 15
  %96 = icmp ult i64 %91, %89
  %97 = xor i64 %89, %91
  %98 = xor i64 %90, %91
  %99 = and i64 %97, %98
  %100 = icmp slt i64 %99, 0
  store i1 %95, i1* %az
  store i1 %96, i1* %cf
  store i1 %100, i1* %of
  %101 = icmp eq i64 %91, 0
  store i1 %101, i1* %zf
  %102 = icmp slt i64 %91, 0
  store i1 %102, i1* %sf
  %103 = trunc i64 %91 to i8
  %104 = call i8 @llvm.ctpop.i8(i8 %103)
  %105 = and i8 %104, 1
  %106 = icmp eq i8 %105, 0
  store i1 %106, i1* %pf
  store i64 %91, i64* %rax
  store volatile i64 43449, i64* @assembly_address
  %107 = load i64* %rax
  %108 = add i64 %107, 2
  %109 = inttoptr i64 %108 to i16*
  %110 = load i16* %109
  %111 = zext i16 %110 to i64
  store i64 %111, i64* %rax
  store volatile i64 43453, i64* @assembly_address
  %112 = load i64* %rax
  %113 = trunc i64 %112 to i16
  %114 = zext i16 %113 to i64
  store i64 %114, i64* %rax
  store volatile i64 43456, i64* @assembly_address
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i32
  store i32 %116, i32* %stack_var_-28
  store volatile i64 43459, i64* @assembly_address
  %117 = load i32* %stack_var_-24
  %118 = add i32 %117, 1
  %119 = and i32 %117, 15
  %120 = add i32 %119, 1
  %121 = icmp ugt i32 %120, 15
  %122 = icmp ult i32 %118, %117
  %123 = xor i32 %117, %118
  %124 = xor i32 1, %118
  %125 = and i32 %123, %124
  %126 = icmp slt i32 %125, 0
  store i1 %121, i1* %az
  store i1 %122, i1* %cf
  store i1 %126, i1* %of
  %127 = icmp eq i32 %118, 0
  store i1 %127, i1* %zf
  %128 = icmp slt i32 %118, 0
  store i1 %128, i1* %sf
  %129 = trunc i32 %118 to i8
  %130 = call i8 @llvm.ctpop.i8(i8 %129)
  %131 = and i8 %130, 1
  %132 = icmp eq i8 %131, 0
  store i1 %132, i1* %pf
  store i32 %118, i32* %stack_var_-24
  store volatile i64 43463, i64* @assembly_address
  %133 = load i32* %stack_var_-24
  %134 = zext i32 %133 to i64
  store i64 %134, i64* %rax
  store volatile i64 43466, i64* @assembly_address
  %135 = load i64* %rax
  %136 = trunc i64 %135 to i32
  %137 = load i32* %stack_var_-20
  %138 = trunc i64 %135 to i32
  store i32 %138, i32* %11
  store i32 %137, i32* %10
  %139 = sub i32 %136, %137
  %140 = and i32 %136, 15
  %141 = and i32 %137, 15
  %142 = sub i32 %140, %141
  %143 = icmp ugt i32 %142, 15
  %144 = icmp ult i32 %136, %137
  %145 = xor i32 %136, %137
  %146 = xor i32 %136, %139
  %147 = and i32 %145, %146
  %148 = icmp slt i32 %147, 0
  store i1 %143, i1* %az
  store i1 %144, i1* %cf
  store i1 %148, i1* %of
  %149 = icmp eq i32 %139, 0
  store i1 %149, i1* %zf
  %150 = icmp slt i32 %139, 0
  store i1 %150, i1* %sf
  %151 = trunc i32 %139 to i8
  %152 = call i8 @llvm.ctpop.i8(i8 %151)
  %153 = and i8 %152, 1
  %154 = icmp eq i8 %153, 0
  store i1 %154, i1* %pf
  store volatile i64 43469, i64* @assembly_address
  %155 = load i32* %11
  %156 = sext i32 %155 to i64
  %157 = load i32* %10
  %158 = trunc i64 %156 to i32
  %159 = icmp sge i32 %158, %157
  br i1 %159, label %block_a9db, label %block_a9cf

block_a9cf:                                       ; preds = %block_a99b
  store volatile i64 43471, i64* @assembly_address
  %160 = load i32* %stack_var_-12
  %161 = zext i32 %160 to i64
  store i64 %161, i64* %rax
  store volatile i64 43474, i64* @assembly_address
  %162 = load i64* %rax
  %163 = trunc i64 %162 to i32
  %164 = load i32* %stack_var_-28
  %165 = sub i32 %163, %164
  %166 = and i32 %163, 15
  %167 = and i32 %164, 15
  %168 = sub i32 %166, %167
  %169 = icmp ugt i32 %168, 15
  %170 = icmp ult i32 %163, %164
  %171 = xor i32 %163, %164
  %172 = xor i32 %163, %165
  %173 = and i32 %171, %172
  %174 = icmp slt i32 %173, 0
  store i1 %169, i1* %az
  store i1 %170, i1* %cf
  store i1 %174, i1* %of
  %175 = icmp eq i32 %165, 0
  store i1 %175, i1* %zf
  %176 = icmp slt i32 %165, 0
  store i1 %176, i1* %sf
  %177 = trunc i32 %165 to i8
  %178 = call i8 @llvm.ctpop.i8(i8 %177)
  %179 = and i8 %178, 1
  %180 = icmp eq i8 %179, 0
  store i1 %180, i1* %pf
  store volatile i64 43477, i64* @assembly_address
  %181 = load i1* %zf
  br i1 %181, label %block_ab64, label %block_a9db

block_a9db:                                       ; preds = %block_a9cf, %block_a99b
  store volatile i64 43483, i64* @assembly_address
  %182 = load i32* %stack_var_-24
  %183 = zext i32 %182 to i64
  store i64 %183, i64* %rax
  store volatile i64 43486, i64* @assembly_address
  %184 = load i64* %rax
  %185 = trunc i64 %184 to i32
  %186 = load i32* %stack_var_-16
  %187 = trunc i64 %184 to i32
  store i32 %187, i32* %8
  store i32 %186, i32* %7
  %188 = sub i32 %185, %186
  %189 = and i32 %185, 15
  %190 = and i32 %186, 15
  %191 = sub i32 %189, %190
  %192 = icmp ugt i32 %191, 15
  %193 = icmp ult i32 %185, %186
  %194 = xor i32 %185, %186
  %195 = xor i32 %185, %188
  %196 = and i32 %194, %195
  %197 = icmp slt i32 %196, 0
  store i1 %192, i1* %az
  store i1 %193, i1* %cf
  store i1 %197, i1* %of
  %198 = icmp eq i32 %188, 0
  store i1 %198, i1* %zf
  %199 = icmp slt i32 %188, 0
  store i1 %199, i1* %sf
  %200 = trunc i32 %188 to i8
  %201 = call i8 @llvm.ctpop.i8(i8 %200)
  %202 = and i8 %201, 1
  %203 = icmp eq i8 %202, 0
  store i1 %203, i1* %pf
  store volatile i64 43489, i64* @assembly_address
  %204 = load i32* %8
  %205 = sext i32 %204 to i64
  %206 = load i32* %7
  %207 = trunc i64 %205 to i32
  %208 = icmp sge i32 %207, %206
  br i1 %208, label %block_aa31, label %block_a9e3

block_a9e3:                                       ; preds = %block_a9e3, %block_a9db
  store volatile i64 43491, i64* @assembly_address
  %209 = load i32* %stack_var_-12
  %210 = zext i32 %209 to i64
  store i64 %210, i64* %rax
  store volatile i64 43494, i64* @assembly_address
  %211 = load i64* %rax
  %212 = trunc i64 %211 to i32
  %213 = sext i32 %212 to i64
  store i64 %213, i64* %rax
  store volatile i64 43496, i64* @assembly_address
  %214 = load i64* %rax
  %215 = mul i64 %214, 4
  store i64 %215, i64* %rdx
  store volatile i64 43504, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec2 to i64), i64* %rax
  store volatile i64 43511, i64* @assembly_address
  %216 = load i64* %rdx
  %217 = load i64* %rax
  %218 = mul i64 %217, 1
  %219 = add i64 %216, %218
  %220 = inttoptr i64 %219 to i16*
  %221 = load i16* %220
  %222 = zext i16 %221 to i64
  store i64 %222, i64* %rax
  store volatile i64 43515, i64* @assembly_address
  %223 = load i64* %rax
  %224 = trunc i64 %223 to i16
  %225 = zext i16 %224 to i64
  store i64 %225, i64* %rdx
  store volatile i64 43518, i64* @assembly_address
  %226 = load i32* %stack_var_-12
  %227 = zext i32 %226 to i64
  store i64 %227, i64* %rax
  store volatile i64 43521, i64* @assembly_address
  %228 = load i64* %rax
  %229 = trunc i64 %228 to i32
  %230 = sext i32 %229 to i64
  store i64 %230, i64* %rax
  store volatile i64 43523, i64* @assembly_address
  %231 = load i64* %rax
  %232 = mul i64 %231, 4
  store i64 %232, i64* %rcx
  store volatile i64 43531, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec0 to i64), i64* %rax
  store volatile i64 43538, i64* @assembly_address
  %233 = load i64* %rcx
  %234 = load i64* %rax
  %235 = mul i64 %234, 1
  %236 = add i64 %233, %235
  %237 = inttoptr i64 %236 to i16*
  %238 = load i16* %237
  %239 = zext i16 %238 to i64
  store i64 %239, i64* %rax
  store volatile i64 43542, i64* @assembly_address
  %240 = load i64* %rax
  %241 = trunc i64 %240 to i16
  %242 = zext i16 %241 to i64
  store i64 %242, i64* %rax
  store volatile i64 43545, i64* @assembly_address
  %243 = load i64* %rdx
  %244 = trunc i64 %243 to i32
  %245 = zext i32 %244 to i64
  store i64 %245, i64* %rsi
  store volatile i64 43547, i64* @assembly_address
  %246 = load i64* %rax
  %247 = trunc i64 %246 to i32
  %248 = zext i32 %247 to i64
  store i64 %248, i64* %rdi
  store volatile i64 43549, i64* @assembly_address
  %249 = load i64* %rdi
  %250 = load i64* %rsi
  %251 = trunc i64 %249 to i32
  %252 = call i64 @send_bits(i32 %251, i64 %250)
  store i64 %252, i64* %rax
  store i64 %252, i64* %rax
  store volatile i64 43554, i64* @assembly_address
  %253 = load i32* %stack_var_-24
  %254 = sub i32 %253, 1
  %255 = and i32 %253, 15
  %256 = sub i32 %255, 1
  %257 = icmp ugt i32 %256, 15
  %258 = icmp ult i32 %253, 1
  %259 = xor i32 %253, 1
  %260 = xor i32 %253, %254
  %261 = and i32 %259, %260
  %262 = icmp slt i32 %261, 0
  store i1 %257, i1* %az
  store i1 %258, i1* %cf
  store i1 %262, i1* %of
  %263 = icmp eq i32 %254, 0
  store i1 %263, i1* %zf
  %264 = icmp slt i32 %254, 0
  store i1 %264, i1* %sf
  %265 = trunc i32 %254 to i8
  %266 = call i8 @llvm.ctpop.i8(i8 %265)
  %267 = and i8 %266, 1
  %268 = icmp eq i8 %267, 0
  store i1 %268, i1* %pf
  store i32 %254, i32* %stack_var_-24
  store volatile i64 43558, i64* @assembly_address
  %269 = load i32* %stack_var_-24
  %270 = and i32 %269, 15
  %271 = icmp ugt i32 %270, 15
  %272 = icmp ult i32 %269, 0
  %273 = xor i32 %269, 0
  %274 = and i32 %273, 0
  %275 = icmp slt i32 %274, 0
  store i1 %271, i1* %az
  store i1 %272, i1* %cf
  store i1 %275, i1* %of
  %276 = icmp eq i32 %269, 0
  store i1 %276, i1* %zf
  %277 = icmp slt i32 %269, 0
  store i1 %277, i1* %sf
  %278 = trunc i32 %269 to i8
  %279 = call i8 @llvm.ctpop.i8(i8 %278)
  %280 = and i8 %279, 1
  %281 = icmp eq i8 %280, 0
  store i1 %281, i1* %pf
  store volatile i64 43562, i64* @assembly_address
  %282 = load i1* %zf
  %283 = icmp eq i1 %282, false
  br i1 %283, label %block_a9e3, label %block_aa2c

block_aa2c:                                       ; preds = %block_a9e3
  store volatile i64 43564, i64* @assembly_address
  br label %block_ab19

block_aa31:                                       ; preds = %block_a9db
  store volatile i64 43569, i64* @assembly_address
  %284 = load i32* %stack_var_-12
  %285 = and i32 %284, 15
  %286 = icmp ugt i32 %285, 15
  %287 = icmp ult i32 %284, 0
  %288 = xor i32 %284, 0
  %289 = and i32 %288, 0
  %290 = icmp slt i32 %289, 0
  store i1 %286, i1* %az
  store i1 %287, i1* %cf
  store i1 %290, i1* %of
  %291 = icmp eq i32 %284, 0
  store i1 %291, i1* %zf
  %292 = icmp slt i32 %284, 0
  store i1 %292, i1* %sf
  %293 = trunc i32 %284 to i8
  %294 = call i8 @llvm.ctpop.i8(i8 %293)
  %295 = and i8 %294, 1
  %296 = icmp eq i8 %295, 0
  store i1 %296, i1* %pf
  store volatile i64 43573, i64* @assembly_address
  %297 = load i1* %zf
  br i1 %297, label %block_aab3, label %block_aa37

block_aa37:                                       ; preds = %block_aa31
  store volatile i64 43575, i64* @assembly_address
  %298 = load i32* %stack_var_-12
  %299 = zext i32 %298 to i64
  store i64 %299, i64* %rax
  store volatile i64 43578, i64* @assembly_address
  %300 = load i64* %rax
  %301 = trunc i64 %300 to i32
  %302 = load i32* %stack_var_-32
  %303 = sub i32 %301, %302
  %304 = and i32 %301, 15
  %305 = and i32 %302, 15
  %306 = sub i32 %304, %305
  %307 = icmp ugt i32 %306, 15
  %308 = icmp ult i32 %301, %302
  %309 = xor i32 %301, %302
  %310 = xor i32 %301, %303
  %311 = and i32 %309, %310
  %312 = icmp slt i32 %311, 0
  store i1 %307, i1* %az
  store i1 %308, i1* %cf
  store i1 %312, i1* %of
  %313 = icmp eq i32 %303, 0
  store i1 %313, i1* %zf
  %314 = icmp slt i32 %303, 0
  store i1 %314, i1* %sf
  %315 = trunc i32 %303 to i8
  %316 = call i8 @llvm.ctpop.i8(i8 %315)
  %317 = and i8 %316, 1
  %318 = icmp eq i8 %317, 0
  store i1 %318, i1* %pf
  store volatile i64 43581, i64* @assembly_address
  %319 = load i1* %zf
  br i1 %319, label %block_aa82, label %block_aa3f

block_aa3f:                                       ; preds = %block_aa37
  store volatile i64 43583, i64* @assembly_address
  %320 = load i32* %stack_var_-12
  %321 = zext i32 %320 to i64
  store i64 %321, i64* %rax
  store volatile i64 43586, i64* @assembly_address
  %322 = load i64* %rax
  %323 = trunc i64 %322 to i32
  %324 = sext i32 %323 to i64
  store i64 %324, i64* %rax
  store volatile i64 43588, i64* @assembly_address
  %325 = load i64* %rax
  %326 = mul i64 %325, 4
  store i64 %326, i64* %rdx
  store volatile i64 43596, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec2 to i64), i64* %rax
  store volatile i64 43603, i64* @assembly_address
  %327 = load i64* %rdx
  %328 = load i64* %rax
  %329 = mul i64 %328, 1
  %330 = add i64 %327, %329
  %331 = inttoptr i64 %330 to i16*
  %332 = load i16* %331
  %333 = zext i16 %332 to i64
  store i64 %333, i64* %rax
  store volatile i64 43607, i64* @assembly_address
  %334 = load i64* %rax
  %335 = trunc i64 %334 to i16
  %336 = zext i16 %335 to i64
  store i64 %336, i64* %rdx
  store volatile i64 43610, i64* @assembly_address
  %337 = load i32* %stack_var_-12
  %338 = zext i32 %337 to i64
  store i64 %338, i64* %rax
  store volatile i64 43613, i64* @assembly_address
  %339 = load i64* %rax
  %340 = trunc i64 %339 to i32
  %341 = sext i32 %340 to i64
  store i64 %341, i64* %rax
  store volatile i64 43615, i64* @assembly_address
  %342 = load i64* %rax
  %343 = mul i64 %342, 4
  store i64 %343, i64* %rcx
  store volatile i64 43623, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec0 to i64), i64* %rax
  store volatile i64 43630, i64* @assembly_address
  %344 = load i64* %rcx
  %345 = load i64* %rax
  %346 = mul i64 %345, 1
  %347 = add i64 %344, %346
  %348 = inttoptr i64 %347 to i16*
  %349 = load i16* %348
  %350 = zext i16 %349 to i64
  store i64 %350, i64* %rax
  store volatile i64 43634, i64* @assembly_address
  %351 = load i64* %rax
  %352 = trunc i64 %351 to i16
  %353 = zext i16 %352 to i64
  store i64 %353, i64* %rax
  store volatile i64 43637, i64* @assembly_address
  %354 = load i64* %rdx
  %355 = trunc i64 %354 to i32
  %356 = zext i32 %355 to i64
  store i64 %356, i64* %rsi
  store volatile i64 43639, i64* @assembly_address
  %357 = load i64* %rax
  %358 = trunc i64 %357 to i32
  %359 = zext i32 %358 to i64
  store i64 %359, i64* %rdi
  store volatile i64 43641, i64* @assembly_address
  %360 = load i64* %rdi
  %361 = load i64* %rsi
  %362 = trunc i64 %360 to i32
  %363 = call i64 @send_bits(i32 %362, i64 %361)
  store i64 %363, i64* %rax
  store i64 %363, i64* %rax
  store volatile i64 43646, i64* @assembly_address
  %364 = load i32* %stack_var_-24
  %365 = sub i32 %364, 1
  %366 = and i32 %364, 15
  %367 = sub i32 %366, 1
  %368 = icmp ugt i32 %367, 15
  %369 = icmp ult i32 %364, 1
  %370 = xor i32 %364, 1
  %371 = xor i32 %364, %365
  %372 = and i32 %370, %371
  %373 = icmp slt i32 %372, 0
  store i1 %368, i1* %az
  store i1 %369, i1* %cf
  store i1 %373, i1* %of
  %374 = icmp eq i32 %365, 0
  store i1 %374, i1* %zf
  %375 = icmp slt i32 %365, 0
  store i1 %375, i1* %sf
  %376 = trunc i32 %365 to i8
  %377 = call i8 @llvm.ctpop.i8(i8 %376)
  %378 = and i8 %377, 1
  %379 = icmp eq i8 %378, 0
  store i1 %379, i1* %pf
  store i32 %365, i32* %stack_var_-24
  br label %block_aa82

block_aa82:                                       ; preds = %block_aa3f, %block_aa37
  store volatile i64 43650, i64* @assembly_address
  %380 = load i16* bitcast (i64* @global_var_217f02 to i16*)
  %381 = zext i16 %380 to i64
  store i64 %381, i64* %rax
  store volatile i64 43657, i64* @assembly_address
  %382 = load i64* %rax
  %383 = trunc i64 %382 to i16
  %384 = zext i16 %383 to i64
  store i64 %384, i64* %rdx
  store volatile i64 43660, i64* @assembly_address
  %385 = load i16* bitcast (i64* @global_var_217f00 to i16*)
  %386 = zext i16 %385 to i64
  store i64 %386, i64* %rax
  store volatile i64 43667, i64* @assembly_address
  %387 = load i64* %rax
  %388 = trunc i64 %387 to i16
  %389 = zext i16 %388 to i64
  store i64 %389, i64* %rax
  store volatile i64 43670, i64* @assembly_address
  %390 = load i64* %rdx
  %391 = trunc i64 %390 to i32
  %392 = zext i32 %391 to i64
  store i64 %392, i64* %rsi
  store volatile i64 43672, i64* @assembly_address
  %393 = load i64* %rax
  %394 = trunc i64 %393 to i32
  %395 = zext i32 %394 to i64
  store i64 %395, i64* %rdi
  store volatile i64 43674, i64* @assembly_address
  %396 = load i64* %rdi
  %397 = load i64* %rsi
  %398 = trunc i64 %396 to i32
  %399 = call i64 @send_bits(i32 %398, i64 %397)
  store i64 %399, i64* %rax
  store i64 %399, i64* %rax
  store volatile i64 43679, i64* @assembly_address
  %400 = load i32* %stack_var_-24
  %401 = zext i32 %400 to i64
  store i64 %401, i64* %rax
  store volatile i64 43682, i64* @assembly_address
  %402 = load i64* %rax
  %403 = trunc i64 %402 to i32
  %404 = sub i32 %403, 3
  %405 = and i32 %403, 15
  %406 = sub i32 %405, 3
  %407 = icmp ugt i32 %406, 15
  %408 = icmp ult i32 %403, 3
  %409 = xor i32 %403, 3
  %410 = xor i32 %403, %404
  %411 = and i32 %409, %410
  %412 = icmp slt i32 %411, 0
  store i1 %407, i1* %az
  store i1 %408, i1* %cf
  store i1 %412, i1* %of
  %413 = icmp eq i32 %404, 0
  store i1 %413, i1* %zf
  %414 = icmp slt i32 %404, 0
  store i1 %414, i1* %sf
  %415 = trunc i32 %404 to i8
  %416 = call i8 @llvm.ctpop.i8(i8 %415)
  %417 = and i8 %416, 1
  %418 = icmp eq i8 %417, 0
  store i1 %418, i1* %pf
  %419 = zext i32 %404 to i64
  store i64 %419, i64* %rax
  store volatile i64 43685, i64* @assembly_address
  store i64 2, i64* %rsi
  store volatile i64 43690, i64* @assembly_address
  %420 = load i64* %rax
  %421 = trunc i64 %420 to i32
  %422 = zext i32 %421 to i64
  store i64 %422, i64* %rdi
  store volatile i64 43692, i64* @assembly_address
  %423 = load i64* %rdi
  %424 = load i64* %rsi
  %425 = trunc i64 %423 to i32
  %426 = call i64 @send_bits(i32 %425, i64 %424)
  store i64 %426, i64* %rax
  store i64 %426, i64* %rax
  store volatile i64 43697, i64* @assembly_address
  br label %block_ab19

block_aab3:                                       ; preds = %block_aa31
  store volatile i64 43699, i64* @assembly_address
  %427 = load i32* %stack_var_-24
  store i32 %427, i32* %6
  store i32 10, i32* %5
  %428 = sub i32 %427, 10
  %429 = and i32 %427, 15
  %430 = sub i32 %429, 10
  %431 = icmp ugt i32 %430, 15
  %432 = icmp ult i32 %427, 10
  %433 = xor i32 %427, 10
  %434 = xor i32 %427, %428
  %435 = and i32 %433, %434
  %436 = icmp slt i32 %435, 0
  store i1 %431, i1* %az
  store i1 %432, i1* %cf
  store i1 %436, i1* %of
  %437 = icmp eq i32 %428, 0
  store i1 %437, i1* %zf
  %438 = icmp slt i32 %428, 0
  store i1 %438, i1* %sf
  %439 = trunc i32 %428 to i8
  %440 = call i8 @llvm.ctpop.i8(i8 %439)
  %441 = and i8 %440, 1
  %442 = icmp eq i8 %441, 0
  store i1 %442, i1* %pf
  store volatile i64 43703, i64* @assembly_address
  %443 = load i32* %6
  %444 = load i32* %5
  %445 = icmp sgt i32 %443, %444
  br i1 %445, label %block_aaea, label %block_aab9

block_aab9:                                       ; preds = %block_aab3
  store volatile i64 43705, i64* @assembly_address
  %446 = load i16* bitcast (i64* @global_var_217f06 to i16*)
  %447 = zext i16 %446 to i64
  store i64 %447, i64* %rax
  store volatile i64 43712, i64* @assembly_address
  %448 = load i64* %rax
  %449 = trunc i64 %448 to i16
  %450 = zext i16 %449 to i64
  store i64 %450, i64* %rdx
  store volatile i64 43715, i64* @assembly_address
  %451 = load i16* bitcast (i64* @global_var_217f04 to i16*)
  %452 = zext i16 %451 to i64
  store i64 %452, i64* %rax
  store volatile i64 43722, i64* @assembly_address
  %453 = load i64* %rax
  %454 = trunc i64 %453 to i16
  %455 = zext i16 %454 to i64
  store i64 %455, i64* %rax
  store volatile i64 43725, i64* @assembly_address
  %456 = load i64* %rdx
  %457 = trunc i64 %456 to i32
  %458 = zext i32 %457 to i64
  store i64 %458, i64* %rsi
  store volatile i64 43727, i64* @assembly_address
  %459 = load i64* %rax
  %460 = trunc i64 %459 to i32
  %461 = zext i32 %460 to i64
  store i64 %461, i64* %rdi
  store volatile i64 43729, i64* @assembly_address
  %462 = load i64* %rdi
  %463 = load i64* %rsi
  %464 = trunc i64 %462 to i32
  %465 = call i64 @send_bits(i32 %464, i64 %463)
  store i64 %465, i64* %rax
  store i64 %465, i64* %rax
  store volatile i64 43734, i64* @assembly_address
  %466 = load i32* %stack_var_-24
  %467 = zext i32 %466 to i64
  store i64 %467, i64* %rax
  store volatile i64 43737, i64* @assembly_address
  %468 = load i64* %rax
  %469 = trunc i64 %468 to i32
  %470 = sub i32 %469, 3
  %471 = and i32 %469, 15
  %472 = sub i32 %471, 3
  %473 = icmp ugt i32 %472, 15
  %474 = icmp ult i32 %469, 3
  %475 = xor i32 %469, 3
  %476 = xor i32 %469, %470
  %477 = and i32 %475, %476
  %478 = icmp slt i32 %477, 0
  store i1 %473, i1* %az
  store i1 %474, i1* %cf
  store i1 %478, i1* %of
  %479 = icmp eq i32 %470, 0
  store i1 %479, i1* %zf
  %480 = icmp slt i32 %470, 0
  store i1 %480, i1* %sf
  %481 = trunc i32 %470 to i8
  %482 = call i8 @llvm.ctpop.i8(i8 %481)
  %483 = and i8 %482, 1
  %484 = icmp eq i8 %483, 0
  store i1 %484, i1* %pf
  %485 = zext i32 %470 to i64
  store i64 %485, i64* %rax
  store volatile i64 43740, i64* @assembly_address
  store i64 3, i64* %rsi
  store volatile i64 43745, i64* @assembly_address
  %486 = load i64* %rax
  %487 = trunc i64 %486 to i32
  %488 = zext i32 %487 to i64
  store i64 %488, i64* %rdi
  store volatile i64 43747, i64* @assembly_address
  %489 = load i64* %rdi
  %490 = load i64* %rsi
  %491 = trunc i64 %489 to i32
  %492 = call i64 @send_bits(i32 %491, i64 %490)
  store i64 %492, i64* %rax
  store i64 %492, i64* %rax
  store volatile i64 43752, i64* @assembly_address
  br label %block_ab19

block_aaea:                                       ; preds = %block_aab3
  store volatile i64 43754, i64* @assembly_address
  %493 = load i16* bitcast (i64* @global_var_217f0a to i16*)
  %494 = zext i16 %493 to i64
  store i64 %494, i64* %rax
  store volatile i64 43761, i64* @assembly_address
  %495 = load i64* %rax
  %496 = trunc i64 %495 to i16
  %497 = zext i16 %496 to i64
  store i64 %497, i64* %rdx
  store volatile i64 43764, i64* @assembly_address
  %498 = load i16* bitcast (i64* @global_var_217f08 to i16*)
  %499 = zext i16 %498 to i64
  store i64 %499, i64* %rax
  store volatile i64 43771, i64* @assembly_address
  %500 = load i64* %rax
  %501 = trunc i64 %500 to i16
  %502 = zext i16 %501 to i64
  store i64 %502, i64* %rax
  store volatile i64 43774, i64* @assembly_address
  %503 = load i64* %rdx
  %504 = trunc i64 %503 to i32
  %505 = zext i32 %504 to i64
  store i64 %505, i64* %rsi
  store volatile i64 43776, i64* @assembly_address
  %506 = load i64* %rax
  %507 = trunc i64 %506 to i32
  %508 = zext i32 %507 to i64
  store i64 %508, i64* %rdi
  store volatile i64 43778, i64* @assembly_address
  %509 = load i64* %rdi
  %510 = load i64* %rsi
  %511 = trunc i64 %509 to i32
  %512 = call i64 @send_bits(i32 %511, i64 %510)
  store i64 %512, i64* %rax
  store i64 %512, i64* %rax
  store volatile i64 43783, i64* @assembly_address
  %513 = load i32* %stack_var_-24
  %514 = zext i32 %513 to i64
  store i64 %514, i64* %rax
  store volatile i64 43786, i64* @assembly_address
  %515 = load i64* %rax
  %516 = trunc i64 %515 to i32
  %517 = sub i32 %516, 11
  %518 = and i32 %516, 15
  %519 = sub i32 %518, 11
  %520 = icmp ugt i32 %519, 15
  %521 = icmp ult i32 %516, 11
  %522 = xor i32 %516, 11
  %523 = xor i32 %516, %517
  %524 = and i32 %522, %523
  %525 = icmp slt i32 %524, 0
  store i1 %520, i1* %az
  store i1 %521, i1* %cf
  store i1 %525, i1* %of
  %526 = icmp eq i32 %517, 0
  store i1 %526, i1* %zf
  %527 = icmp slt i32 %517, 0
  store i1 %527, i1* %sf
  %528 = trunc i32 %517 to i8
  %529 = call i8 @llvm.ctpop.i8(i8 %528)
  %530 = and i8 %529, 1
  %531 = icmp eq i8 %530, 0
  store i1 %531, i1* %pf
  %532 = zext i32 %517 to i64
  store i64 %532, i64* %rax
  store volatile i64 43789, i64* @assembly_address
  store i64 7, i64* %rsi
  store volatile i64 43794, i64* @assembly_address
  %533 = load i64* %rax
  %534 = trunc i64 %533 to i32
  %535 = zext i32 %534 to i64
  store i64 %535, i64* %rdi
  store volatile i64 43796, i64* @assembly_address
  %536 = load i64* %rdi
  %537 = load i64* %rsi
  %538 = trunc i64 %536 to i32
  %539 = call i64 @send_bits(i32 %538, i64 %537)
  store i64 %539, i64* %rax
  store i64 %539, i64* %rax
  br label %block_ab19

block_ab19:                                       ; preds = %block_aaea, %block_aab9, %block_aa82, %block_aa2c
  store volatile i64 43801, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 43808, i64* @assembly_address
  %540 = load i32* %stack_var_-12
  %541 = zext i32 %540 to i64
  store i64 %541, i64* %rax
  store volatile i64 43811, i64* @assembly_address
  %542 = load i64* %rax
  %543 = trunc i64 %542 to i32
  store i32 %543, i32* %stack_var_-32
  store volatile i64 43814, i64* @assembly_address
  %544 = load i32* %stack_var_-28
  %545 = and i32 %544, 15
  %546 = icmp ugt i32 %545, 15
  %547 = icmp ult i32 %544, 0
  %548 = xor i32 %544, 0
  %549 = and i32 %548, 0
  %550 = icmp slt i32 %549, 0
  store i1 %546, i1* %az
  store i1 %547, i1* %cf
  store i1 %550, i1* %of
  %551 = icmp eq i32 %544, 0
  store i1 %551, i1* %zf
  %552 = icmp slt i32 %544, 0
  store i1 %552, i1* %sf
  %553 = trunc i32 %544 to i8
  %554 = call i8 @llvm.ctpop.i8(i8 %553)
  %555 = and i8 %554, 1
  %556 = icmp eq i8 %555, 0
  store i1 %556, i1* %pf
  store volatile i64 43818, i64* @assembly_address
  %557 = load i1* %zf
  %558 = icmp eq i1 %557, false
  br i1 %558, label %block_ab3c, label %block_ab2c

block_ab2c:                                       ; preds = %block_ab19
  store volatile i64 43820, i64* @assembly_address
  store i32 138, i32* %stack_var_-20
  store volatile i64 43827, i64* @assembly_address
  store i32 3, i32* %stack_var_-16
  store volatile i64 43834, i64* @assembly_address
  br label %block_ab65

block_ab3c:                                       ; preds = %block_ab19
  store volatile i64 43836, i64* @assembly_address
  %559 = load i32* %stack_var_-12
  %560 = zext i32 %559 to i64
  store i64 %560, i64* %rax
  store volatile i64 43839, i64* @assembly_address
  %561 = load i64* %rax
  %562 = trunc i64 %561 to i32
  %563 = load i32* %stack_var_-28
  %564 = sub i32 %562, %563
  %565 = and i32 %562, 15
  %566 = and i32 %563, 15
  %567 = sub i32 %565, %566
  %568 = icmp ugt i32 %567, 15
  %569 = icmp ult i32 %562, %563
  %570 = xor i32 %562, %563
  %571 = xor i32 %562, %564
  %572 = and i32 %570, %571
  %573 = icmp slt i32 %572, 0
  store i1 %568, i1* %az
  store i1 %569, i1* %cf
  store i1 %573, i1* %of
  %574 = icmp eq i32 %564, 0
  store i1 %574, i1* %zf
  %575 = icmp slt i32 %564, 0
  store i1 %575, i1* %sf
  %576 = trunc i32 %564 to i8
  %577 = call i8 @llvm.ctpop.i8(i8 %576)
  %578 = and i8 %577, 1
  %579 = icmp eq i8 %578, 0
  store i1 %579, i1* %pf
  store volatile i64 43842, i64* @assembly_address
  %580 = load i1* %zf
  %581 = icmp eq i1 %580, false
  br i1 %581, label %block_ab54, label %block_ab44

block_ab44:                                       ; preds = %block_ab3c
  store volatile i64 43844, i64* @assembly_address
  store i32 6, i32* %stack_var_-20
  store volatile i64 43851, i64* @assembly_address
  store i32 3, i32* %stack_var_-16
  store volatile i64 43858, i64* @assembly_address
  br label %block_ab65

block_ab54:                                       ; preds = %block_ab3c
  store volatile i64 43860, i64* @assembly_address
  store i32 7, i32* %stack_var_-20
  store volatile i64 43867, i64* @assembly_address
  store i32 4, i32* %stack_var_-16
  store volatile i64 43874, i64* @assembly_address
  br label %block_ab65

block_ab64:                                       ; preds = %block_a9cf
  store volatile i64 43876, i64* @assembly_address
  br label %block_ab65

block_ab65:                                       ; preds = %block_ab64, %block_ab54, %block_ab44, %block_ab2c
  store volatile i64 43877, i64* @assembly_address
  %582 = load i32* %stack_var_-36
  %583 = add i32 %582, 1
  %584 = and i32 %582, 15
  %585 = add i32 %584, 1
  %586 = icmp ugt i32 %585, 15
  %587 = icmp ult i32 %583, %582
  %588 = xor i32 %582, %583
  %589 = xor i32 1, %583
  %590 = and i32 %588, %589
  %591 = icmp slt i32 %590, 0
  store i1 %586, i1* %az
  store i1 %587, i1* %cf
  store i1 %591, i1* %of
  %592 = icmp eq i32 %583, 0
  store i1 %592, i1* %zf
  %593 = icmp slt i32 %583, 0
  store i1 %593, i1* %sf
  %594 = trunc i32 %583 to i8
  %595 = call i8 @llvm.ctpop.i8(i8 %594)
  %596 = and i8 %595, 1
  %597 = icmp eq i8 %596, 0
  store i1 %597, i1* %pf
  store i32 %583, i32* %stack_var_-36
  br label %block_ab69

block_ab69:                                       ; preds = %block_ab65, %block_a98f
  store volatile i64 43881, i64* @assembly_address
  %598 = load i32* %stack_var_-36
  %599 = zext i32 %598 to i64
  store i64 %599, i64* %rax
  store volatile i64 43884, i64* @assembly_address
  %600 = load i64* %rax
  %601 = trunc i64 %600 to i32
  %602 = load i32* %stack_var_-52
  %603 = trunc i64 %600 to i32
  store i32 %603, i32* %3
  store i32 %602, i32* %2
  %604 = sub i32 %601, %602
  %605 = and i32 %601, 15
  %606 = and i32 %602, 15
  %607 = sub i32 %605, %606
  %608 = icmp ugt i32 %607, 15
  %609 = icmp ult i32 %601, %602
  %610 = xor i32 %601, %602
  %611 = xor i32 %601, %604
  %612 = and i32 %610, %611
  %613 = icmp slt i32 %612, 0
  store i1 %608, i1* %az
  store i1 %609, i1* %cf
  store i1 %613, i1* %of
  %614 = icmp eq i32 %604, 0
  store i1 %614, i1* %zf
  %615 = icmp slt i32 %604, 0
  store i1 %615, i1* %sf
  %616 = trunc i32 %604 to i8
  %617 = call i8 @llvm.ctpop.i8(i8 %616)
  %618 = and i8 %617, 1
  %619 = icmp eq i8 %618, 0
  store i1 %619, i1* %pf
  store volatile i64 43887, i64* @assembly_address
  %620 = load i32* %3
  %621 = sext i32 %620 to i64
  %622 = load i32* %2
  %623 = trunc i64 %621 to i32
  %624 = icmp sle i32 %623, %622
  br i1 %624, label %block_a99b, label %block_ab75

block_ab75:                                       ; preds = %block_ab69
  store volatile i64 43893, i64* @assembly_address
  store volatile i64 43894, i64* @assembly_address
  %625 = load i64* %stack_var_-8
  store i64 %625, i64* %rbp
  %626 = ptrtoint i64* %stack_var_0 to i64
  store i64 %626, i64* %rsp
  store volatile i64 43895, i64* @assembly_address
  %627 = load i64* %rax
  ret i64 %627
}

declare i64 @209(i64*, i64)

define i64 @build_bl_tree() {
block_ab78:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  %0 = alloca i32
  %1 = alloca i32
  store volatile i64 43896, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 43897, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 43900, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 43904, i64* @assembly_address
  %21 = load i32* bitcast (i64* @global_var_2164c4 to i32*)
  %22 = zext i32 %21 to i64
  store i64 %22, i64* %rax
  store volatile i64 43910, i64* @assembly_address
  %23 = load i64* %rax
  %24 = trunc i64 %23 to i32
  %25 = zext i32 %24 to i64
  store i64 %25, i64* %rsi
  store volatile i64 43912, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rdi
  store volatile i64 43919, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to i64*
  %28 = load i64* %rsi
  %29 = trunc i64 %28 to i32
  %30 = call i64 @scan_tree(i64* %27, i32 %29)
  store i64 %30, i64* %rax
  store i64 %30, i64* %rax
  store volatile i64 43924, i64* @assembly_address
  %31 = load i32* bitcast (i64* @global_var_216504 to i32*)
  %32 = zext i32 %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 43930, i64* @assembly_address
  %33 = load i64* %rax
  %34 = trunc i64 %33 to i32
  %35 = zext i32 %34 to i64
  store i64 %35, i64* %rsi
  store volatile i64 43932, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2178c0 to i64), i64* %rdi
  store volatile i64 43939, i64* @assembly_address
  %36 = load i64* %rdi
  %37 = inttoptr i64 %36 to i64*
  %38 = load i64* %rsi
  %39 = trunc i64 %38 to i32
  %40 = call i64 @scan_tree(i64* %37, i32 %39)
  store i64 %40, i64* %rax
  store i64 %40, i64* %rax
  store volatile i64 43944, i64* @assembly_address
  store i64 ptrtoint (i64** @global_var_216520 to i64), i64* %rdi
  store volatile i64 43951, i64* @assembly_address
  %41 = load i64* %rdi
  %42 = inttoptr i64 %41 to i64**
  %43 = call i64 @build_tree(i64** %42)
  store i64 %43, i64* %rax
  store i64 %43, i64* %rax
  store volatile i64 43956, i64* @assembly_address
  store i32 18, i32* %stack_var_-12
  store volatile i64 43963, i64* @assembly_address
  br label %block_abef

block_abbd:                                       ; preds = %block_abef
  store volatile i64 43965, i64* @assembly_address
  %44 = load i32* %stack_var_-12
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 43968, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = sext i32 %47 to i64
  store i64 %48, i64* %rdx
  store volatile i64 43971, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216470 to i64), i64* %rax
  store volatile i64 43978, i64* @assembly_address
  %49 = load i64* %rdx
  %50 = load i64* %rax
  %51 = mul i64 %50, 1
  %52 = add i64 %49, %51
  %53 = inttoptr i64 %52 to i8*
  %54 = load i8* %53
  %55 = zext i8 %54 to i64
  store i64 %55, i64* %rax
  store volatile i64 43982, i64* @assembly_address
  %56 = load i64* %rax
  %57 = trunc i64 %56 to i8
  %58 = zext i8 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 43985, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  %61 = sext i32 %60 to i64
  store i64 %61, i64* %rax
  store volatile i64 43987, i64* @assembly_address
  %62 = load i64* %rax
  %63 = mul i64 %62, 4
  store i64 %63, i64* %rdx
  store volatile i64 43995, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec2 to i64), i64* %rax
  store volatile i64 44002, i64* @assembly_address
  %64 = load i64* %rdx
  %65 = load i64* %rax
  %66 = mul i64 %65, 1
  %67 = add i64 %64, %66
  %68 = inttoptr i64 %67 to i16*
  %69 = load i16* %68
  %70 = zext i16 %69 to i64
  store i64 %70, i64* %rax
  store volatile i64 44006, i64* @assembly_address
  %71 = load i64* %rax
  %72 = trunc i64 %71 to i16
  %73 = load i64* %rax
  %74 = trunc i64 %73 to i16
  %75 = and i16 %72, %74
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %76 = icmp eq i16 %75, 0
  store i1 %76, i1* %zf
  %77 = icmp slt i16 %75, 0
  store i1 %77, i1* %sf
  %78 = trunc i16 %75 to i8
  %79 = call i8 @llvm.ctpop.i8(i8 %78)
  %80 = and i8 %79, 1
  %81 = icmp eq i8 %80, 0
  store i1 %81, i1* %pf
  store volatile i64 44009, i64* @assembly_address
  %82 = load i1* %zf
  %83 = icmp eq i1 %82, false
  br i1 %83, label %block_abf7, label %block_abeb

block_abeb:                                       ; preds = %block_abbd
  store volatile i64 44011, i64* @assembly_address
  %84 = load i32* %stack_var_-12
  %85 = sub i32 %84, 1
  %86 = and i32 %84, 15
  %87 = sub i32 %86, 1
  %88 = icmp ugt i32 %87, 15
  %89 = icmp ult i32 %84, 1
  %90 = xor i32 %84, 1
  %91 = xor i32 %84, %85
  %92 = and i32 %90, %91
  %93 = icmp slt i32 %92, 0
  store i1 %88, i1* %az
  store i1 %89, i1* %cf
  store i1 %93, i1* %of
  %94 = icmp eq i32 %85, 0
  store i1 %94, i1* %zf
  %95 = icmp slt i32 %85, 0
  store i1 %95, i1* %sf
  %96 = trunc i32 %85 to i8
  %97 = call i8 @llvm.ctpop.i8(i8 %96)
  %98 = and i8 %97, 1
  %99 = icmp eq i8 %98, 0
  store i1 %99, i1* %pf
  store i32 %85, i32* %stack_var_-12
  br label %block_abef

block_abef:                                       ; preds = %block_abeb, %block_ab78
  store volatile i64 44015, i64* @assembly_address
  %100 = load i32* %stack_var_-12
  store i32 %100, i32* %1
  store i32 2, i32* %0
  %101 = sub i32 %100, 2
  %102 = and i32 %100, 15
  %103 = sub i32 %102, 2
  %104 = icmp ugt i32 %103, 15
  %105 = icmp ult i32 %100, 2
  %106 = xor i32 %100, 2
  %107 = xor i32 %100, %101
  %108 = and i32 %106, %107
  %109 = icmp slt i32 %108, 0
  store i1 %104, i1* %az
  store i1 %105, i1* %cf
  store i1 %109, i1* %of
  %110 = icmp eq i32 %101, 0
  store i1 %110, i1* %zf
  %111 = icmp slt i32 %101, 0
  store i1 %111, i1* %sf
  %112 = trunc i32 %101 to i8
  %113 = call i8 @llvm.ctpop.i8(i8 %112)
  %114 = and i8 %113, 1
  %115 = icmp eq i8 %114, 0
  store i1 %115, i1* %pf
  store volatile i64 44019, i64* @assembly_address
  %116 = load i32* %1
  %117 = load i32* %0
  %118 = icmp sgt i32 %116, %117
  br i1 %118, label %block_abbd, label %block_abf5

block_abf5:                                       ; preds = %block_abef
  store volatile i64 44021, i64* @assembly_address
  br label %block_abf8

block_abf7:                                       ; preds = %block_abbd
  store volatile i64 44023, i64* @assembly_address
  br label %block_abf8

block_abf8:                                       ; preds = %block_abf7, %block_abf5
  store volatile i64 44024, i64* @assembly_address
  %119 = load i32* %stack_var_-12
  %120 = zext i32 %119 to i64
  store i64 %120, i64* %rax
  store volatile i64 44027, i64* @assembly_address
  %121 = load i64* %rax
  %122 = add i64 %121, 1
  %123 = trunc i64 %122 to i32
  %124 = zext i32 %123 to i64
  store i64 %124, i64* %rdx
  store volatile i64 44030, i64* @assembly_address
  %125 = load i64* %rdx
  %126 = trunc i64 %125 to i32
  %127 = zext i32 %126 to i64
  store i64 %127, i64* %rax
  store volatile i64 44032, i64* @assembly_address
  %128 = load i64* %rax
  %129 = trunc i64 %128 to i32
  %130 = load i64* %rax
  %131 = trunc i64 %130 to i32
  %132 = add i32 %129, %131
  %133 = and i32 %129, 15
  %134 = and i32 %131, 15
  %135 = add i32 %133, %134
  %136 = icmp ugt i32 %135, 15
  %137 = icmp ult i32 %132, %129
  %138 = xor i32 %129, %132
  %139 = xor i32 %131, %132
  %140 = and i32 %138, %139
  %141 = icmp slt i32 %140, 0
  store i1 %136, i1* %az
  store i1 %137, i1* %cf
  store i1 %141, i1* %of
  %142 = icmp eq i32 %132, 0
  store i1 %142, i1* %zf
  %143 = icmp slt i32 %132, 0
  store i1 %143, i1* %sf
  %144 = trunc i32 %132 to i8
  %145 = call i8 @llvm.ctpop.i8(i8 %144)
  %146 = and i8 %145, 1
  %147 = icmp eq i8 %146, 0
  store i1 %147, i1* %pf
  %148 = zext i32 %132 to i64
  store i64 %148, i64* %rax
  store volatile i64 44034, i64* @assembly_address
  %149 = load i64* %rax
  %150 = trunc i64 %149 to i32
  %151 = load i64* %rdx
  %152 = trunc i64 %151 to i32
  %153 = add i32 %150, %152
  %154 = and i32 %150, 15
  %155 = and i32 %152, 15
  %156 = add i32 %154, %155
  %157 = icmp ugt i32 %156, 15
  %158 = icmp ult i32 %153, %150
  %159 = xor i32 %150, %153
  %160 = xor i32 %152, %153
  %161 = and i32 %159, %160
  %162 = icmp slt i32 %161, 0
  store i1 %157, i1* %az
  store i1 %158, i1* %cf
  store i1 %162, i1* %of
  %163 = icmp eq i32 %153, 0
  store i1 %163, i1* %zf
  %164 = icmp slt i32 %153, 0
  store i1 %164, i1* %sf
  %165 = trunc i32 %153 to i8
  %166 = call i8 @llvm.ctpop.i8(i8 %165)
  %167 = and i8 %166, 1
  %168 = icmp eq i8 %167, 0
  store i1 %168, i1* %pf
  %169 = zext i32 %153 to i64
  store i64 %169, i64* %rax
  store volatile i64 44036, i64* @assembly_address
  %170 = load i64* %rax
  %171 = trunc i64 %170 to i32
  %172 = add i32 %171, 14
  %173 = and i32 %171, 15
  %174 = add i32 %173, 14
  %175 = icmp ugt i32 %174, 15
  %176 = icmp ult i32 %172, %171
  %177 = xor i32 %171, %172
  %178 = xor i32 14, %172
  %179 = and i32 %177, %178
  %180 = icmp slt i32 %179, 0
  store i1 %175, i1* %az
  store i1 %176, i1* %cf
  store i1 %180, i1* %of
  %181 = icmp eq i32 %172, 0
  store i1 %181, i1* %zf
  %182 = icmp slt i32 %172, 0
  store i1 %182, i1* %sf
  %183 = trunc i32 %172 to i8
  %184 = call i8 @llvm.ctpop.i8(i8 %183)
  %185 = and i8 %184, 1
  %186 = icmp eq i8 %185, 0
  store i1 %186, i1* %pf
  %187 = zext i32 %172 to i64
  store i64 %187, i64* %rax
  store volatile i64 44039, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  %190 = sext i32 %189 to i64
  store i64 %190, i64* %rdx
  store volatile i64 44042, i64* @assembly_address
  %191 = load i64* @global_var_219ed0
  store i64 %191, i64* %rax
  store volatile i64 44049, i64* @assembly_address
  %192 = load i64* %rax
  %193 = load i64* %rdx
  %194 = add i64 %192, %193
  %195 = and i64 %192, 15
  %196 = and i64 %193, 15
  %197 = add i64 %195, %196
  %198 = icmp ugt i64 %197, 15
  %199 = icmp ult i64 %194, %192
  %200 = xor i64 %192, %194
  %201 = xor i64 %193, %194
  %202 = and i64 %200, %201
  %203 = icmp slt i64 %202, 0
  store i1 %198, i1* %az
  store i1 %199, i1* %cf
  store i1 %203, i1* %of
  %204 = icmp eq i64 %194, 0
  store i1 %204, i1* %zf
  %205 = icmp slt i64 %194, 0
  store i1 %205, i1* %sf
  %206 = trunc i64 %194 to i8
  %207 = call i8 @llvm.ctpop.i8(i8 %206)
  %208 = and i8 %207, 1
  %209 = icmp eq i8 %208, 0
  store i1 %209, i1* %pf
  store i64 %194, i64* %rax
  store volatile i64 44052, i64* @assembly_address
  %210 = load i64* %rax
  store i64 %210, i64* @global_var_219ed0
  store volatile i64 44059, i64* @assembly_address
  %211 = load i32* %stack_var_-12
  %212 = zext i32 %211 to i64
  store i64 %212, i64* %rax
  store volatile i64 44062, i64* @assembly_address
  %213 = load i64* %stack_var_-8
  store i64 %213, i64* %rbp
  %214 = ptrtoint i64* %stack_var_0 to i64
  store i64 %214, i64* %rsp
  store volatile i64 44063, i64* @assembly_address
  %215 = load i64* %rax
  %216 = load i64* %rax
  ret i64 %216
}

define i64 @send_all_trees(i32 %arg1, i64 %arg2, i64 %arg3) {
block_ac20:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i32
  %3 = alloca i64
  store volatile i64 44064, i64* @assembly_address
  %4 = load i64* %rbp
  store i64 %4, i64* %stack_var_-8
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rsp
  store volatile i64 44065, i64* @assembly_address
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rbp
  store volatile i64 44068, i64* @assembly_address
  %7 = load i64* %rsp
  %8 = sub i64 %7, 32
  %9 = and i64 %7, 15
  %10 = icmp ugt i64 %9, 15
  %11 = icmp ult i64 %7, 32
  %12 = xor i64 %7, 32
  %13 = xor i64 %7, %8
  %14 = and i64 %12, %13
  %15 = icmp slt i64 %14, 0
  store i1 %10, i1* %az
  store i1 %11, i1* %cf
  store i1 %15, i1* %of
  %16 = icmp eq i64 %8, 0
  store i1 %16, i1* %zf
  %17 = icmp slt i64 %8, 0
  store i1 %17, i1* %sf
  %18 = trunc i64 %8 to i8
  %19 = call i8 @llvm.ctpop.i8(i8 %18)
  %20 = and i8 %19, 1
  %21 = icmp eq i8 %20, 0
  store i1 %21, i1* %pf
  %22 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %22, i64* %rsp
  store volatile i64 44072, i64* @assembly_address
  %23 = load i64* %rdi
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-28
  store volatile i64 44075, i64* @assembly_address
  %25 = load i64* %rsi
  %26 = trunc i64 %25 to i32
  store i32 %26, i32* %stack_var_-32
  store volatile i64 44078, i64* @assembly_address
  %27 = load i64* %rdx
  %28 = trunc i64 %27 to i32
  store i32 %28, i32* %stack_var_-36
  store volatile i64 44081, i64* @assembly_address
  %29 = load i32* %stack_var_-28
  %30 = zext i32 %29 to i64
  store i64 %30, i64* %rax
  store volatile i64 44084, i64* @assembly_address
  %31 = load i64* %rax
  %32 = trunc i64 %31 to i32
  %33 = sub i32 %32, 257
  %34 = and i32 %32, 15
  %35 = sub i32 %34, 1
  %36 = icmp ugt i32 %35, 15
  %37 = icmp ult i32 %32, 257
  %38 = xor i32 %32, 257
  %39 = xor i32 %32, %33
  %40 = and i32 %38, %39
  %41 = icmp slt i32 %40, 0
  store i1 %36, i1* %az
  store i1 %37, i1* %cf
  store i1 %41, i1* %of
  %42 = icmp eq i32 %33, 0
  store i1 %42, i1* %zf
  %43 = icmp slt i32 %33, 0
  store i1 %43, i1* %sf
  %44 = trunc i32 %33 to i8
  %45 = call i8 @llvm.ctpop.i8(i8 %44)
  %46 = and i8 %45, 1
  %47 = icmp eq i8 %46, 0
  store i1 %47, i1* %pf
  %48 = zext i32 %33 to i64
  store i64 %48, i64* %rax
  store volatile i64 44089, i64* @assembly_address
  store i64 5, i64* %rsi
  store volatile i64 44094, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  %51 = zext i32 %50 to i64
  store i64 %51, i64* %rdi
  store volatile i64 44096, i64* @assembly_address
  %52 = load i64* %rdi
  %53 = load i64* %rsi
  %54 = trunc i64 %52 to i32
  %55 = call i64 @send_bits(i32 %54, i64 %53)
  store i64 %55, i64* %rax
  store i64 %55, i64* %rax
  store volatile i64 44101, i64* @assembly_address
  %56 = load i32* %stack_var_-32
  %57 = zext i32 %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 44104, i64* @assembly_address
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i32
  %60 = sub i32 %59, 1
  %61 = and i32 %59, 15
  %62 = sub i32 %61, 1
  %63 = icmp ugt i32 %62, 15
  %64 = icmp ult i32 %59, 1
  %65 = xor i32 %59, 1
  %66 = xor i32 %59, %60
  %67 = and i32 %65, %66
  %68 = icmp slt i32 %67, 0
  store i1 %63, i1* %az
  store i1 %64, i1* %cf
  store i1 %68, i1* %of
  %69 = icmp eq i32 %60, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 %60, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 %60 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  %75 = zext i32 %60 to i64
  store i64 %75, i64* %rax
  store volatile i64 44107, i64* @assembly_address
  store i64 5, i64* %rsi
  store volatile i64 44112, i64* @assembly_address
  %76 = load i64* %rax
  %77 = trunc i64 %76 to i32
  %78 = zext i32 %77 to i64
  store i64 %78, i64* %rdi
  store volatile i64 44114, i64* @assembly_address
  %79 = load i64* %rdi
  %80 = load i64* %rsi
  %81 = trunc i64 %79 to i32
  %82 = call i64 @send_bits(i32 %81, i64 %80)
  store i64 %82, i64* %rax
  store i64 %82, i64* %rax
  store volatile i64 44119, i64* @assembly_address
  %83 = load i32* %stack_var_-36
  %84 = zext i32 %83 to i64
  store i64 %84, i64* %rax
  store volatile i64 44122, i64* @assembly_address
  %85 = load i64* %rax
  %86 = trunc i64 %85 to i32
  %87 = sub i32 %86, 4
  %88 = and i32 %86, 15
  %89 = sub i32 %88, 4
  %90 = icmp ugt i32 %89, 15
  %91 = icmp ult i32 %86, 4
  %92 = xor i32 %86, 4
  %93 = xor i32 %86, %87
  %94 = and i32 %92, %93
  %95 = icmp slt i32 %94, 0
  store i1 %90, i1* %az
  store i1 %91, i1* %cf
  store i1 %95, i1* %of
  %96 = icmp eq i32 %87, 0
  store i1 %96, i1* %zf
  %97 = icmp slt i32 %87, 0
  store i1 %97, i1* %sf
  %98 = trunc i32 %87 to i8
  %99 = call i8 @llvm.ctpop.i8(i8 %98)
  %100 = and i8 %99, 1
  %101 = icmp eq i8 %100, 0
  store i1 %101, i1* %pf
  %102 = zext i32 %87 to i64
  store i64 %102, i64* %rax
  store volatile i64 44125, i64* @assembly_address
  store i64 4, i64* %rsi
  store volatile i64 44130, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  %105 = zext i32 %104 to i64
  store i64 %105, i64* %rdi
  store volatile i64 44132, i64* @assembly_address
  %106 = load i64* %rdi
  %107 = load i64* %rsi
  %108 = trunc i64 %106 to i32
  %109 = call i64 @send_bits(i32 %108, i64 %107)
  store i64 %109, i64* %rax
  store i64 %109, i64* %rax
  store volatile i64 44137, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 44144, i64* @assembly_address
  br label %block_acae

block_ac72:                                       ; preds = %block_acae
  store volatile i64 44146, i64* @assembly_address
  %110 = load i32* %stack_var_-12
  %111 = zext i32 %110 to i64
  store i64 %111, i64* %rax
  store volatile i64 44149, i64* @assembly_address
  %112 = load i64* %rax
  %113 = trunc i64 %112 to i32
  %114 = sext i32 %113 to i64
  store i64 %114, i64* %rdx
  store volatile i64 44152, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216470 to i64), i64* %rax
  store volatile i64 44159, i64* @assembly_address
  %115 = load i64* %rdx
  %116 = load i64* %rax
  %117 = mul i64 %116, 1
  %118 = add i64 %115, %117
  %119 = inttoptr i64 %118 to i8*
  %120 = load i8* %119
  %121 = zext i8 %120 to i64
  store i64 %121, i64* %rax
  store volatile i64 44163, i64* @assembly_address
  %122 = load i64* %rax
  %123 = trunc i64 %122 to i8
  %124 = zext i8 %123 to i64
  store i64 %124, i64* %rax
  store volatile i64 44166, i64* @assembly_address
  %125 = load i64* %rax
  %126 = trunc i64 %125 to i32
  %127 = sext i32 %126 to i64
  store i64 %127, i64* %rax
  store volatile i64 44168, i64* @assembly_address
  %128 = load i64* %rax
  %129 = mul i64 %128, 4
  store i64 %129, i64* %rdx
  store volatile i64 44176, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217ec2 to i64), i64* %rax
  store volatile i64 44183, i64* @assembly_address
  %130 = load i64* %rdx
  %131 = load i64* %rax
  %132 = mul i64 %131, 1
  %133 = add i64 %130, %132
  %134 = inttoptr i64 %133 to i16*
  %135 = load i16* %134
  %136 = zext i16 %135 to i64
  store i64 %136, i64* %rax
  store volatile i64 44187, i64* @assembly_address
  %137 = load i64* %rax
  %138 = trunc i64 %137 to i16
  %139 = zext i16 %138 to i64
  store i64 %139, i64* %rax
  store volatile i64 44190, i64* @assembly_address
  store i64 3, i64* %rsi
  store volatile i64 44195, i64* @assembly_address
  %140 = load i64* %rax
  %141 = trunc i64 %140 to i32
  %142 = zext i32 %141 to i64
  store i64 %142, i64* %rdi
  store volatile i64 44197, i64* @assembly_address
  %143 = load i64* %rdi
  %144 = load i64* %rsi
  %145 = trunc i64 %143 to i32
  %146 = call i64 @send_bits(i32 %145, i64 %144)
  store i64 %146, i64* %rax
  store i64 %146, i64* %rax
  store volatile i64 44202, i64* @assembly_address
  %147 = load i32* %stack_var_-12
  %148 = add i32 %147, 1
  %149 = and i32 %147, 15
  %150 = add i32 %149, 1
  %151 = icmp ugt i32 %150, 15
  %152 = icmp ult i32 %148, %147
  %153 = xor i32 %147, %148
  %154 = xor i32 1, %148
  %155 = and i32 %153, %154
  %156 = icmp slt i32 %155, 0
  store i1 %151, i1* %az
  store i1 %152, i1* %cf
  store i1 %156, i1* %of
  %157 = icmp eq i32 %148, 0
  store i1 %157, i1* %zf
  %158 = icmp slt i32 %148, 0
  store i1 %158, i1* %sf
  %159 = trunc i32 %148 to i8
  %160 = call i8 @llvm.ctpop.i8(i8 %159)
  %161 = and i8 %160, 1
  %162 = icmp eq i8 %161, 0
  store i1 %162, i1* %pf
  store i32 %148, i32* %stack_var_-12
  br label %block_acae

block_acae:                                       ; preds = %block_ac72, %block_ac20
  store volatile i64 44206, i64* @assembly_address
  %163 = load i32* %stack_var_-12
  %164 = zext i32 %163 to i64
  store i64 %164, i64* %rax
  store volatile i64 44209, i64* @assembly_address
  %165 = load i64* %rax
  %166 = trunc i64 %165 to i32
  %167 = load i32* %stack_var_-36
  %168 = trunc i64 %165 to i32
  store i32 %168, i32* %2
  store i32 %167, i32* %1
  %169 = sub i32 %166, %167
  %170 = and i32 %166, 15
  %171 = and i32 %167, 15
  %172 = sub i32 %170, %171
  %173 = icmp ugt i32 %172, 15
  %174 = icmp ult i32 %166, %167
  %175 = xor i32 %166, %167
  %176 = xor i32 %166, %169
  %177 = and i32 %175, %176
  %178 = icmp slt i32 %177, 0
  store i1 %173, i1* %az
  store i1 %174, i1* %cf
  store i1 %178, i1* %of
  %179 = icmp eq i32 %169, 0
  store i1 %179, i1* %zf
  %180 = icmp slt i32 %169, 0
  store i1 %180, i1* %sf
  %181 = trunc i32 %169 to i8
  %182 = call i8 @llvm.ctpop.i8(i8 %181)
  %183 = and i8 %182, 1
  %184 = icmp eq i8 %183, 0
  store i1 %184, i1* %pf
  store volatile i64 44212, i64* @assembly_address
  %185 = load i32* %2
  %186 = sext i32 %185 to i64
  %187 = load i32* %1
  %188 = trunc i64 %186 to i32
  %189 = icmp slt i32 %188, %187
  br i1 %189, label %block_ac72, label %block_acb6

block_acb6:                                       ; preds = %block_acae
  store volatile i64 44214, i64* @assembly_address
  %190 = load i32* %stack_var_-28
  %191 = zext i32 %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 44217, i64* @assembly_address
  %192 = load i64* %rax
  %193 = trunc i64 %192 to i32
  %194 = sub i32 %193, 1
  %195 = and i32 %193, 15
  %196 = sub i32 %195, 1
  %197 = icmp ugt i32 %196, 15
  %198 = icmp ult i32 %193, 1
  %199 = xor i32 %193, 1
  %200 = xor i32 %193, %194
  %201 = and i32 %199, %200
  %202 = icmp slt i32 %201, 0
  store i1 %197, i1* %az
  store i1 %198, i1* %cf
  store i1 %202, i1* %of
  %203 = icmp eq i32 %194, 0
  store i1 %203, i1* %zf
  %204 = icmp slt i32 %194, 0
  store i1 %204, i1* %sf
  %205 = trunc i32 %194 to i8
  %206 = call i8 @llvm.ctpop.i8(i8 %205)
  %207 = and i8 %206, 1
  %208 = icmp eq i8 %207, 0
  store i1 %208, i1* %pf
  %209 = zext i32 %194 to i64
  store i64 %209, i64* %rax
  store volatile i64 44220, i64* @assembly_address
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  %212 = zext i32 %211 to i64
  store i64 %212, i64* %rsi
  store volatile i64 44222, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rdi
  store volatile i64 44229, i64* @assembly_address
  %213 = load i64* %rdi
  %214 = inttoptr i64 %213 to i64*
  %215 = load i64* %rsi
  %216 = trunc i64 %215 to i32
  %217 = call i64 @send_tree(i64* %214, i32 %216)
  store i64 %217, i64* %rax
  store i64 %217, i64* %rax
  store volatile i64 44234, i64* @assembly_address
  %218 = load i32* %stack_var_-32
  %219 = zext i32 %218 to i64
  store i64 %219, i64* %rax
  store volatile i64 44237, i64* @assembly_address
  %220 = load i64* %rax
  %221 = trunc i64 %220 to i32
  %222 = sub i32 %221, 1
  %223 = and i32 %221, 15
  %224 = sub i32 %223, 1
  %225 = icmp ugt i32 %224, 15
  %226 = icmp ult i32 %221, 1
  %227 = xor i32 %221, 1
  %228 = xor i32 %221, %222
  %229 = and i32 %227, %228
  %230 = icmp slt i32 %229, 0
  store i1 %225, i1* %az
  store i1 %226, i1* %cf
  store i1 %230, i1* %of
  %231 = icmp eq i32 %222, 0
  store i1 %231, i1* %zf
  %232 = icmp slt i32 %222, 0
  store i1 %232, i1* %sf
  %233 = trunc i32 %222 to i8
  %234 = call i8 @llvm.ctpop.i8(i8 %233)
  %235 = and i8 %234, 1
  %236 = icmp eq i8 %235, 0
  store i1 %236, i1* %pf
  %237 = zext i32 %222 to i64
  store i64 %237, i64* %rax
  store volatile i64 44240, i64* @assembly_address
  %238 = load i64* %rax
  %239 = trunc i64 %238 to i32
  %240 = zext i32 %239 to i64
  store i64 %240, i64* %rsi
  store volatile i64 44242, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2178c0 to i64), i64* %rdi
  store volatile i64 44249, i64* @assembly_address
  %241 = load i64* %rdi
  %242 = inttoptr i64 %241 to i64*
  %243 = load i64* %rsi
  %244 = trunc i64 %243 to i32
  %245 = call i64 @send_tree(i64* %242, i32 %244)
  store i64 %245, i64* %rax
  store i64 %245, i64* %rax
  store volatile i64 44254, i64* @assembly_address
  store volatile i64 44255, i64* @assembly_address
  %246 = load i64* %stack_var_-8
  store i64 %246, i64* %rbp
  %247 = ptrtoint i64* %stack_var_0 to i64
  store i64 %247, i64* %rsp
  store volatile i64 44256, i64* @assembly_address
  %248 = load i64* %rax
  ret i64 %248
}

declare i64 @210(i64, i32, i64)

declare i64 @211(i64, i64, i32)

declare i64 @212(i64, i64, i64)

define i64 @flush_block(i64 %arg1, i32 %arg2, i64 %arg3, i64 %arg4) {
block_ace1:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg4, i64* %rcx
  store i64 %arg3, i64* %rdx
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-64 = alloca i32
  %stack_var_-60 = alloca i32
  %stack_var_-56 = alloca i32
  %1 = alloca i64
  %stack_var_-48 = alloca i64
  %stack_var_-72 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 44257, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 44258, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 44261, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 64
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 64
  %10 = xor i64 %5, 64
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-72 to i64
  store i64 %20, i64* %rsp
  store volatile i64 44265, i64* @assembly_address
  %21 = load i64* %rdi
  store i64 %21, i64* %stack_var_-48
  store volatile i64 44269, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-56
  store volatile i64 44273, i64* @assembly_address
  %24 = load i64* %rdx
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-60
  store volatile i64 44276, i64* @assembly_address
  %26 = load i64* %rcx
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-64
  store volatile i64 44279, i64* @assembly_address
  %28 = load i32* bitcast (i64* @global_var_219ec8 to i32*)
  %29 = zext i32 %28 to i64
  store i64 %29, i64* %rax
  store volatile i64 44285, i64* @assembly_address
  %30 = load i8* bitcast (i64* @global_var_219ecc to i8*)
  %31 = zext i8 %30 to i64
  store i64 %31, i64* %rdx
  store volatile i64 44292, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i32
  %34 = zext i32 %33 to i64
  store i64 %34, i64* %rcx
  store volatile i64 44294, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218ec0 to i64), i64* %rax
  store volatile i64 44301, i64* @assembly_address
  %35 = load i64* %rdx
  %36 = trunc i64 %35 to i8
  %37 = load i64* %rcx
  %38 = load i64* %rax
  %39 = mul i64 %38, 1
  %40 = add i64 %37, %39
  %41 = inttoptr i64 %40 to i8*
  store i8 %36, i8* %41
  store volatile i64 44304, i64* @assembly_address
  %42 = load i64* @global_var_219ef0
  store i64 %42, i64* %rax
  store volatile i64 44311, i64* @assembly_address
  %43 = load i64* %rax
  %44 = inttoptr i64 %43 to i16*
  %45 = load i16* %44
  %46 = zext i16 %45 to i64
  store i64 %46, i64* %rax
  store volatile i64 44314, i64* @assembly_address
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i16
  %49 = sub i16 %48, -1
  %50 = and i16 %48, 15
  %51 = sub i16 %50, 15
  %52 = icmp ugt i16 %51, 15
  %53 = icmp ult i16 %48, -1
  %54 = xor i16 %48, -1
  %55 = xor i16 %48, %49
  %56 = and i16 %54, %55
  %57 = icmp slt i16 %56, 0
  store i1 %52, i1* %az
  store i1 %53, i1* %cf
  store i1 %57, i1* %of
  %58 = icmp eq i16 %49, 0
  store i1 %58, i1* %zf
  %59 = icmp slt i16 %49, 0
  store i1 %59, i1* %sf
  %60 = trunc i16 %49 to i8
  %61 = call i8 @llvm.ctpop.i8(i8 %60)
  %62 = and i8 %61, 1
  %63 = icmp eq i8 %62, 0
  store i1 %63, i1* %pf
  store volatile i64 44318, i64* @assembly_address
  %64 = load i1* %zf
  %65 = icmp eq i1 %64, false
  br i1 %65, label %block_ad25, label %block_ad20

block_ad20:                                       ; preds = %block_ace1
  store volatile i64 44320, i64* @assembly_address
  %66 = call i64 @set_file_type()
  store i64 %66, i64* %rax
  store i64 %66, i64* %rax
  store i64 %66, i64* %rax
  br label %block_ad25

block_ad25:                                       ; preds = %block_ad20, %block_ace1
  store volatile i64 44325, i64* @assembly_address
  store i64 2188448, i64* %rdi
  store volatile i64 44332, i64* @assembly_address
  %67 = load i64* %rdi
  %68 = inttoptr i64 %67 to i64**
  %69 = call i64 @build_tree(i64** %68)
  store i64 %69, i64* %rax
  store i64 %69, i64* %rax
  store volatile i64 44337, i64* @assembly_address
  store i64 2188512, i64* %rdi
  store volatile i64 44344, i64* @assembly_address
  %70 = load i64* %rdi
  %71 = inttoptr i64 %70 to i64**
  %72 = call i64 @build_tree(i64** %71)
  store i64 %72, i64* %rax
  store i64 %72, i64* %rax
  store volatile i64 44349, i64* @assembly_address
  %73 = call i64 @build_bl_tree()
  store i64 %73, i64* %rax
  store i64 %73, i64* %rax
  store i64 %73, i64* %rax
  store volatile i64 44354, i64* @assembly_address
  %74 = load i64* %rax
  %75 = trunc i64 %74 to i32
  store i32 %75, i32* %stack_var_-28
  store volatile i64 44357, i64* @assembly_address
  %76 = load i64* @global_var_219ed0
  store i64 %76, i64* %rax
  store volatile i64 44364, i64* @assembly_address
  %77 = load i64* %rax
  %78 = add i64 %77, 10
  %79 = and i64 %77, 15
  %80 = add i64 %79, 10
  %81 = icmp ugt i64 %80, 15
  %82 = icmp ult i64 %78, %77
  %83 = xor i64 %77, %78
  %84 = xor i64 10, %78
  %85 = and i64 %83, %84
  %86 = icmp slt i64 %85, 0
  store i1 %81, i1* %az
  store i1 %82, i1* %cf
  store i1 %86, i1* %of
  %87 = icmp eq i64 %78, 0
  store i1 %87, i1* %zf
  %88 = icmp slt i64 %78, 0
  store i1 %88, i1* %sf
  %89 = trunc i64 %78 to i8
  %90 = call i8 @llvm.ctpop.i8(i8 %89)
  %91 = and i8 %90, 1
  %92 = icmp eq i8 %91, 0
  store i1 %92, i1* %pf
  store i64 %78, i64* %rax
  store volatile i64 44368, i64* @assembly_address
  %93 = load i64* %rax
  %94 = load i1* %of
  %95 = lshr i64 %93, 3
  %96 = icmp eq i64 %95, 0
  store i1 %96, i1* %zf
  %97 = icmp slt i64 %95, 0
  store i1 %97, i1* %sf
  %98 = trunc i64 %95 to i8
  %99 = call i8 @llvm.ctpop.i8(i8 %98)
  %100 = and i8 %99, 1
  %101 = icmp eq i8 %100, 0
  store i1 %101, i1* %pf
  store i64 %95, i64* %rax
  %102 = and i64 4, %93
  %103 = icmp ne i64 %102, 0
  store i1 %103, i1* %cf
  %104 = icmp slt i64 %93, 0
  %105 = select i1 false, i1 %104, i1 %94
  store i1 %105, i1* %of
  store volatile i64 44372, i64* @assembly_address
  %106 = load i64* %rax
  store i64 %106, i64* %stack_var_-24
  store volatile i64 44376, i64* @assembly_address
  %107 = load i64* @global_var_219ed8
  store i64 %107, i64* %rax
  store volatile i64 44383, i64* @assembly_address
  %108 = load i64* %rax
  %109 = add i64 %108, 10
  %110 = and i64 %108, 15
  %111 = add i64 %110, 10
  %112 = icmp ugt i64 %111, 15
  %113 = icmp ult i64 %109, %108
  %114 = xor i64 %108, %109
  %115 = xor i64 10, %109
  %116 = and i64 %114, %115
  %117 = icmp slt i64 %116, 0
  store i1 %112, i1* %az
  store i1 %113, i1* %cf
  store i1 %117, i1* %of
  %118 = icmp eq i64 %109, 0
  store i1 %118, i1* %zf
  %119 = icmp slt i64 %109, 0
  store i1 %119, i1* %sf
  %120 = trunc i64 %109 to i8
  %121 = call i8 @llvm.ctpop.i8(i8 %120)
  %122 = and i8 %121, 1
  %123 = icmp eq i8 %122, 0
  store i1 %123, i1* %pf
  store i64 %109, i64* %rax
  store volatile i64 44387, i64* @assembly_address
  %124 = load i64* %rax
  %125 = load i1* %of
  %126 = lshr i64 %124, 3
  %127 = icmp eq i64 %126, 0
  store i1 %127, i1* %zf
  %128 = icmp slt i64 %126, 0
  store i1 %128, i1* %sf
  %129 = trunc i64 %126 to i8
  %130 = call i8 @llvm.ctpop.i8(i8 %129)
  %131 = and i8 %130, 1
  %132 = icmp eq i8 %131, 0
  store i1 %132, i1* %pf
  store i64 %126, i64* %rax
  %133 = and i64 4, %124
  %134 = icmp ne i64 %133, 0
  store i1 %134, i1* %cf
  %135 = icmp slt i64 %124, 0
  %136 = select i1 false, i1 %135, i1 %125
  store i1 %136, i1* %of
  store volatile i64 44391, i64* @assembly_address
  %137 = load i64* %rax
  store i64 %137, i64* %stack_var_-16
  store volatile i64 44395, i64* @assembly_address
  %138 = load i64* @global_var_219ee8
  store i64 %138, i64* %rax
  store volatile i64 44402, i64* @assembly_address
  %139 = load i64* %rax
  store i64 %139, i64* %rdx
  store volatile i64 44405, i64* @assembly_address
  %140 = load i32* %stack_var_-56
  %141 = sext i32 %140 to i64
  store i64 %141, i64* %rax
  store volatile i64 44409, i64* @assembly_address
  %142 = load i64* %rax
  %143 = load i64* %rdx
  %144 = add i64 %142, %143
  %145 = and i64 %142, 15
  %146 = and i64 %143, 15
  %147 = add i64 %145, %146
  %148 = icmp ugt i64 %147, 15
  %149 = icmp ult i64 %144, %142
  %150 = xor i64 %142, %144
  %151 = xor i64 %143, %144
  %152 = and i64 %150, %151
  %153 = icmp slt i64 %152, 0
  store i1 %148, i1* %az
  store i1 %149, i1* %cf
  store i1 %153, i1* %of
  %154 = icmp eq i64 %144, 0
  store i1 %154, i1* %zf
  %155 = icmp slt i64 %144, 0
  store i1 %155, i1* %sf
  %156 = trunc i64 %144 to i8
  %157 = call i8 @llvm.ctpop.i8(i8 %156)
  %158 = and i8 %157, 1
  %159 = icmp eq i8 %158, 0
  store i1 %159, i1* %pf
  store i64 %144, i64* %rax
  store volatile i64 44412, i64* @assembly_address
  %160 = load i64* %rax
  store i64 %160, i64* @global_var_219ee8
  store volatile i64 44419, i64* @assembly_address
  %161 = load i64* %stack_var_-16
  store i64 %161, i64* %rax
  store volatile i64 44423, i64* @assembly_address
  %162 = load i64* %rax
  %163 = load i64* %stack_var_-24
  %164 = sub i64 %162, %163
  %165 = and i64 %162, 15
  %166 = and i64 %163, 15
  %167 = sub i64 %165, %166
  %168 = icmp ugt i64 %167, 15
  %169 = icmp ult i64 %162, %163
  %170 = xor i64 %162, %163
  %171 = xor i64 %162, %164
  %172 = and i64 %170, %171
  %173 = icmp slt i64 %172, 0
  store i1 %168, i1* %az
  store i1 %169, i1* %cf
  store i1 %173, i1* %of
  %174 = icmp eq i64 %164, 0
  store i1 %174, i1* %zf
  %175 = icmp slt i64 %164, 0
  store i1 %175, i1* %sf
  %176 = trunc i64 %164 to i8
  %177 = call i8 @llvm.ctpop.i8(i8 %176)
  %178 = and i8 %177, 1
  %179 = icmp eq i8 %178, 0
  store i1 %179, i1* %pf
  store volatile i64 44427, i64* @assembly_address
  %180 = load i1* %cf
  %181 = load i1* %zf
  %182 = or i1 %180, %181
  %183 = icmp ne i1 %182, true
  br i1 %183, label %block_ad95, label %block_ad8d

block_ad8d:                                       ; preds = %block_ad25
  store volatile i64 44429, i64* @assembly_address
  %184 = load i64* %stack_var_-16
  store i64 %184, i64* %rax
  store volatile i64 44433, i64* @assembly_address
  %185 = load i64* %rax
  store i64 %185, i64* %stack_var_-24
  br label %block_ad95

block_ad95:                                       ; preds = %block_ad8d, %block_ad25
  store volatile i64 44437, i64* @assembly_address
  %186 = load i32* %stack_var_-56
  %187 = sext i32 %186 to i64
  store i64 %187, i64* %rax
  store volatile i64 44441, i64* @assembly_address
  %188 = load i64* %rax
  %189 = add i64 %188, 4
  %190 = and i64 %188, 15
  %191 = add i64 %190, 4
  %192 = icmp ugt i64 %191, 15
  %193 = icmp ult i64 %189, %188
  %194 = xor i64 %188, %189
  %195 = xor i64 4, %189
  %196 = and i64 %194, %195
  %197 = icmp slt i64 %196, 0
  store i1 %192, i1* %az
  store i1 %193, i1* %cf
  store i1 %197, i1* %of
  %198 = icmp eq i64 %189, 0
  store i1 %198, i1* %zf
  %199 = icmp slt i64 %189, 0
  store i1 %199, i1* %sf
  %200 = trunc i64 %189 to i8
  %201 = call i8 @llvm.ctpop.i8(i8 %200)
  %202 = and i8 %201, 1
  %203 = icmp eq i8 %202, 0
  store i1 %203, i1* %pf
  store i64 %189, i64* %rax
  store volatile i64 44445, i64* @assembly_address
  %204 = load i64* %stack_var_-24
  %205 = load i64* %rax
  %206 = sub i64 %204, %205
  %207 = and i64 %204, 15
  %208 = and i64 %205, 15
  %209 = sub i64 %207, %208
  %210 = icmp ugt i64 %209, 15
  %211 = icmp ult i64 %204, %205
  %212 = xor i64 %204, %205
  %213 = xor i64 %204, %206
  %214 = and i64 %212, %213
  %215 = icmp slt i64 %214, 0
  store i1 %210, i1* %az
  store i1 %211, i1* %cf
  store i1 %215, i1* %of
  %216 = icmp eq i64 %206, 0
  store i1 %216, i1* %zf
  %217 = icmp slt i64 %206, 0
  store i1 %217, i1* %sf
  %218 = trunc i64 %206 to i8
  %219 = call i8 @llvm.ctpop.i8(i8 %218)
  %220 = and i8 %219, 1
  %221 = icmp eq i8 %220, 0
  store i1 %221, i1* %pf
  store volatile i64 44449, i64* @assembly_address
  %222 = load i1* %cf
  br i1 %222, label %block_ae0e, label %block_ada3

block_ada3:                                       ; preds = %block_ad95
  store volatile i64 44451, i64* @assembly_address
  %223 = load i64* %stack_var_-48
  %224 = and i64 %223, 15
  %225 = icmp ugt i64 %224, 15
  %226 = icmp ult i64 %223, 0
  %227 = xor i64 %223, 0
  %228 = and i64 %227, 0
  %229 = icmp slt i64 %228, 0
  store i1 %225, i1* %az
  store i1 %226, i1* %cf
  store i1 %229, i1* %of
  %230 = icmp eq i64 %223, 0
  store i1 %230, i1* %zf
  %231 = icmp slt i64 %223, 0
  store i1 %231, i1* %sf
  %232 = trunc i64 %223 to i8
  %233 = call i8 @llvm.ctpop.i8(i8 %232)
  %234 = and i8 %233, 1
  %235 = icmp eq i8 %234, 0
  store i1 %235, i1* %pf
  store volatile i64 44456, i64* @assembly_address
  %236 = load i1* %zf
  br i1 %236, label %block_ae0e, label %block_adaa

block_adaa:                                       ; preds = %block_ada3
  store volatile i64 44458, i64* @assembly_address
  %237 = load i32* %stack_var_-64
  %238 = zext i32 %237 to i64
  store i64 %238, i64* %rax
  store volatile i64 44461, i64* @assembly_address
  store i64 3, i64* %rsi
  store volatile i64 44466, i64* @assembly_address
  %239 = load i64* %rax
  %240 = trunc i64 %239 to i32
  %241 = zext i32 %240 to i64
  store i64 %241, i64* %rdi
  store volatile i64 44468, i64* @assembly_address
  %242 = load i64* %rdi
  %243 = load i64* %rsi
  %244 = trunc i64 %242 to i32
  %245 = call i64 @send_bits(i32 %244, i64 %243)
  store i64 %245, i64* %rax
  store i64 %245, i64* %rax
  store volatile i64 44473, i64* @assembly_address
  %246 = load i64* @global_var_219ee0
  store i64 %246, i64* %rax
  store volatile i64 44480, i64* @assembly_address
  %247 = load i64* %rax
  %248 = add i64 %247, 10
  %249 = and i64 %247, 15
  %250 = add i64 %249, 10
  %251 = icmp ugt i64 %250, 15
  %252 = icmp ult i64 %248, %247
  %253 = xor i64 %247, %248
  %254 = xor i64 10, %248
  %255 = and i64 %253, %254
  %256 = icmp slt i64 %255, 0
  store i1 %251, i1* %az
  store i1 %252, i1* %cf
  store i1 %256, i1* %of
  %257 = icmp eq i64 %248, 0
  store i1 %257, i1* %zf
  %258 = icmp slt i64 %248, 0
  store i1 %258, i1* %sf
  %259 = trunc i64 %248 to i8
  %260 = call i8 @llvm.ctpop.i8(i8 %259)
  %261 = and i8 %260, 1
  %262 = icmp eq i8 %261, 0
  store i1 %262, i1* %pf
  store i64 %248, i64* %rax
  store volatile i64 44484, i64* @assembly_address
  %263 = load i64* %rax
  %264 = and i64 %263, -8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %265 = icmp eq i64 %264, 0
  store i1 %265, i1* %zf
  %266 = icmp slt i64 %264, 0
  store i1 %266, i1* %sf
  %267 = trunc i64 %264 to i8
  %268 = call i8 @llvm.ctpop.i8(i8 %267)
  %269 = and i8 %268, 1
  %270 = icmp eq i8 %269, 0
  store i1 %270, i1* %pf
  store i64 %264, i64* %rax
  store volatile i64 44488, i64* @assembly_address
  %271 = load i64* %rax
  store i64 %271, i64* @global_var_219ee0
  store volatile i64 44495, i64* @assembly_address
  %272 = load i32* %stack_var_-56
  %273 = sext i32 %272 to i64
  store i64 %273, i64* %rax
  store volatile i64 44499, i64* @assembly_address
  %274 = load i64* %rax
  %275 = add i64 %274, 4
  %276 = and i64 %274, 15
  %277 = add i64 %276, 4
  %278 = icmp ugt i64 %277, 15
  %279 = icmp ult i64 %275, %274
  %280 = xor i64 %274, %275
  %281 = xor i64 4, %275
  %282 = and i64 %280, %281
  %283 = icmp slt i64 %282, 0
  store i1 %278, i1* %az
  store i1 %279, i1* %cf
  store i1 %283, i1* %of
  %284 = icmp eq i64 %275, 0
  store i1 %284, i1* %zf
  %285 = icmp slt i64 %275, 0
  store i1 %285, i1* %sf
  %286 = trunc i64 %275 to i8
  %287 = call i8 @llvm.ctpop.i8(i8 %286)
  %288 = and i8 %287, 1
  %289 = icmp eq i8 %288, 0
  store i1 %289, i1* %pf
  store i64 %275, i64* %rax
  store volatile i64 44503, i64* @assembly_address
  %290 = load i64* %rax
  %291 = mul i64 %290, 8
  store i64 %291, i64* %rdx
  store volatile i64 44511, i64* @assembly_address
  %292 = load i64* @global_var_219ee0
  store i64 %292, i64* %rax
  store volatile i64 44518, i64* @assembly_address
  %293 = load i64* %rax
  %294 = load i64* %rdx
  %295 = add i64 %293, %294
  %296 = and i64 %293, 15
  %297 = and i64 %294, 15
  %298 = add i64 %296, %297
  %299 = icmp ugt i64 %298, 15
  %300 = icmp ult i64 %295, %293
  %301 = xor i64 %293, %295
  %302 = xor i64 %294, %295
  %303 = and i64 %301, %302
  %304 = icmp slt i64 %303, 0
  store i1 %299, i1* %az
  store i1 %300, i1* %cf
  store i1 %304, i1* %of
  %305 = icmp eq i64 %295, 0
  store i1 %305, i1* %zf
  %306 = icmp slt i64 %295, 0
  store i1 %306, i1* %sf
  %307 = trunc i64 %295 to i8
  %308 = call i8 @llvm.ctpop.i8(i8 %307)
  %309 = and i8 %308, 1
  %310 = icmp eq i8 %309, 0
  store i1 %310, i1* %pf
  store i64 %295, i64* %rax
  store volatile i64 44521, i64* @assembly_address
  %311 = load i64* %rax
  store i64 %311, i64* @global_var_219ee0
  store volatile i64 44528, i64* @assembly_address
  %312 = load i32* %stack_var_-56
  %313 = sext i32 %312 to i64
  store i64 %313, i64* %rax
  store volatile i64 44532, i64* @assembly_address
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i32
  %316 = zext i32 %315 to i64
  store i64 %316, i64* %rcx
  store volatile i64 44534, i64* @assembly_address
  %317 = load i64* %stack_var_-48
  store i64 %317, i64* %rax
  store volatile i64 44538, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 44543, i64* @assembly_address
  %318 = load i64* %rcx
  %319 = trunc i64 %318 to i32
  %320 = zext i32 %319 to i64
  store i64 %320, i64* %rsi
  store volatile i64 44545, i64* @assembly_address
  %321 = load i64* %rax
  store i64 %321, i64* %rdi
  store volatile i64 44548, i64* @assembly_address
  %322 = load i64* %rdi
  %323 = load i64* %rsi
  %324 = load i64* %rdx
  %325 = inttoptr i64 %322 to i8*
  %326 = call i64 @copy_block(i8* %325, i64 %323, i64 %324)
  store i64 %326, i64* %rax
  store i64 %326, i64* %rax
  store volatile i64 44553, i64* @assembly_address
  br label %block_aebd

block_ae0e:                                       ; preds = %block_ada3, %block_ad95
  store volatile i64 44558, i64* @assembly_address
  %327 = load i64* %stack_var_-16
  store i64 %327, i64* %rax
  store volatile i64 44562, i64* @assembly_address
  %328 = load i64* %rax
  %329 = load i64* %stack_var_-24
  %330 = sub i64 %328, %329
  %331 = and i64 %328, 15
  %332 = and i64 %329, 15
  %333 = sub i64 %331, %332
  %334 = icmp ugt i64 %333, 15
  %335 = icmp ult i64 %328, %329
  %336 = xor i64 %328, %329
  %337 = xor i64 %328, %330
  %338 = and i64 %336, %337
  %339 = icmp slt i64 %338, 0
  store i1 %334, i1* %az
  store i1 %335, i1* %cf
  store i1 %339, i1* %of
  %340 = icmp eq i64 %330, 0
  store i1 %340, i1* %zf
  %341 = icmp slt i64 %330, 0
  store i1 %341, i1* %sf
  %342 = trunc i64 %330 to i8
  %343 = call i8 @llvm.ctpop.i8(i8 %342)
  %344 = and i8 %343, 1
  %345 = icmp eq i8 %344, 0
  store i1 %345, i1* %pf
  store volatile i64 44566, i64* @assembly_address
  %346 = load i1* %zf
  %347 = icmp eq i1 %346, false
  br i1 %347, label %block_ae5b, label %block_ae18

block_ae18:                                       ; preds = %block_ae0e
  store volatile i64 44568, i64* @assembly_address
  %348 = load i32* %stack_var_-64
  %349 = zext i32 %348 to i64
  store i64 %349, i64* %rax
  store volatile i64 44571, i64* @assembly_address
  %350 = load i64* %rax
  %351 = trunc i64 %350 to i32
  %352 = add i32 %351, 2
  %353 = and i32 %351, 15
  %354 = add i32 %353, 2
  %355 = icmp ugt i32 %354, 15
  %356 = icmp ult i32 %352, %351
  %357 = xor i32 %351, %352
  %358 = xor i32 2, %352
  %359 = and i32 %357, %358
  %360 = icmp slt i32 %359, 0
  store i1 %355, i1* %az
  store i1 %356, i1* %cf
  store i1 %360, i1* %of
  %361 = icmp eq i32 %352, 0
  store i1 %361, i1* %zf
  %362 = icmp slt i32 %352, 0
  store i1 %362, i1* %sf
  %363 = trunc i32 %352 to i8
  %364 = call i8 @llvm.ctpop.i8(i8 %363)
  %365 = and i8 %364, 1
  %366 = icmp eq i8 %365, 0
  store i1 %366, i1* %pf
  %367 = zext i32 %352 to i64
  store i64 %367, i64* %rax
  store volatile i64 44574, i64* @assembly_address
  store i64 3, i64* %rsi
  store volatile i64 44579, i64* @assembly_address
  %368 = load i64* %rax
  %369 = trunc i64 %368 to i32
  %370 = zext i32 %369 to i64
  store i64 %370, i64* %rdi
  store volatile i64 44581, i64* @assembly_address
  %371 = load i64* %rdi
  %372 = load i64* %rsi
  %373 = trunc i64 %371 to i32
  %374 = call i64 @send_bits(i32 %373, i64 %372)
  store i64 %374, i64* %rax
  store i64 %374, i64* %rax
  store volatile i64 44586, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_217e40 to i64), i64* %rsi
  store volatile i64 44593, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2179c0 to i64), i64* %rdi
  store volatile i64 44600, i64* @assembly_address
  %375 = load i64* %rdi
  %376 = inttoptr i64 %375 to i64*
  %377 = load i64* %rsi
  %378 = inttoptr i64 %377 to i64*
  %379 = bitcast i64* %376 to i16*
  %380 = call i64 @compress_block(i16* %379, i64* %378)
  store i64 %380, i64* %rax
  store i64 %380, i64* %rax
  store volatile i64 44605, i64* @assembly_address
  %381 = load i64* @global_var_219ed8
  store i64 %381, i64* %rax
  store volatile i64 44612, i64* @assembly_address
  %382 = load i64* @global_var_219ee0
  store i64 %382, i64* %rdx
  store volatile i64 44619, i64* @assembly_address
  %383 = load i64* %rax
  %384 = load i64* %rdx
  %385 = add i64 %383, %384
  %386 = and i64 %383, 15
  %387 = and i64 %384, 15
  %388 = add i64 %386, %387
  %389 = icmp ugt i64 %388, 15
  %390 = icmp ult i64 %385, %383
  %391 = xor i64 %383, %385
  %392 = xor i64 %384, %385
  %393 = and i64 %391, %392
  %394 = icmp slt i64 %393, 0
  store i1 %389, i1* %az
  store i1 %390, i1* %cf
  store i1 %394, i1* %of
  %395 = icmp eq i64 %385, 0
  store i1 %395, i1* %zf
  %396 = icmp slt i64 %385, 0
  store i1 %396, i1* %sf
  %397 = trunc i64 %385 to i8
  %398 = call i8 @llvm.ctpop.i8(i8 %397)
  %399 = and i8 %398, 1
  %400 = icmp eq i8 %399, 0
  store i1 %400, i1* %pf
  store i64 %385, i64* %rax
  store volatile i64 44622, i64* @assembly_address
  %401 = load i64* %rax
  %402 = add i64 %401, 3
  %403 = and i64 %401, 15
  %404 = add i64 %403, 3
  %405 = icmp ugt i64 %404, 15
  %406 = icmp ult i64 %402, %401
  %407 = xor i64 %401, %402
  %408 = xor i64 3, %402
  %409 = and i64 %407, %408
  %410 = icmp slt i64 %409, 0
  store i1 %405, i1* %az
  store i1 %406, i1* %cf
  store i1 %410, i1* %of
  %411 = icmp eq i64 %402, 0
  store i1 %411, i1* %zf
  %412 = icmp slt i64 %402, 0
  store i1 %412, i1* %sf
  %413 = trunc i64 %402 to i8
  %414 = call i8 @llvm.ctpop.i8(i8 %413)
  %415 = and i8 %414, 1
  %416 = icmp eq i8 %415, 0
  store i1 %416, i1* %pf
  store i64 %402, i64* %rax
  store volatile i64 44626, i64* @assembly_address
  %417 = load i64* %rax
  store i64 %417, i64* @global_var_219ee0
  store volatile i64 44633, i64* @assembly_address
  br label %block_aebd

block_ae5b:                                       ; preds = %block_ae0e
  store volatile i64 44635, i64* @assembly_address
  %418 = load i32* %stack_var_-64
  %419 = zext i32 %418 to i64
  store i64 %419, i64* %rax
  store volatile i64 44638, i64* @assembly_address
  %420 = load i64* %rax
  %421 = trunc i64 %420 to i32
  %422 = add i32 %421, 4
  %423 = and i32 %421, 15
  %424 = add i32 %423, 4
  %425 = icmp ugt i32 %424, 15
  %426 = icmp ult i32 %422, %421
  %427 = xor i32 %421, %422
  %428 = xor i32 4, %422
  %429 = and i32 %427, %428
  %430 = icmp slt i32 %429, 0
  store i1 %425, i1* %az
  store i1 %426, i1* %cf
  store i1 %430, i1* %of
  %431 = icmp eq i32 %422, 0
  store i1 %431, i1* %zf
  %432 = icmp slt i32 %422, 0
  store i1 %432, i1* %sf
  %433 = trunc i32 %422 to i8
  %434 = call i8 @llvm.ctpop.i8(i8 %433)
  %435 = and i8 %434, 1
  %436 = icmp eq i8 %435, 0
  store i1 %436, i1* %pf
  %437 = zext i32 %422 to i64
  store i64 %437, i64* %rax
  store volatile i64 44641, i64* @assembly_address
  store i64 3, i64* %rsi
  store volatile i64 44646, i64* @assembly_address
  %438 = load i64* %rax
  %439 = trunc i64 %438 to i32
  %440 = zext i32 %439 to i64
  store i64 %440, i64* %rdi
  store volatile i64 44648, i64* @assembly_address
  %441 = load i64* %rdi
  %442 = load i64* %rsi
  %443 = trunc i64 %441 to i32
  %444 = call i64 @send_bits(i32 %443, i64 %442)
  store i64 %444, i64* %rax
  store i64 %444, i64* %rax
  store volatile i64 44653, i64* @assembly_address
  %445 = load i32* %stack_var_-28
  %446 = zext i32 %445 to i64
  store i64 %446, i64* %rax
  store volatile i64 44656, i64* @assembly_address
  %447 = load i64* %rax
  %448 = add i64 %447, 1
  %449 = trunc i64 %448 to i32
  %450 = zext i32 %449 to i64
  store i64 %450, i64* %rdx
  store volatile i64 44659, i64* @assembly_address
  %451 = load i32* bitcast (i64* @global_var_216504 to i32*)
  %452 = zext i32 %451 to i64
  store i64 %452, i64* %rax
  store volatile i64 44665, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216505 to i64), i64* %rcx
  store volatile i64 44668, i64* @assembly_address
  %453 = load i32* bitcast (i64* @global_var_2164c4 to i32*)
  %454 = zext i32 %453 to i64
  store i64 %454, i64* %rax
  store volatile i64 44674, i64* @assembly_address
  %455 = load i64* %rax
  %456 = trunc i64 %455 to i32
  %457 = add i32 %456, 1
  %458 = and i32 %456, 15
  %459 = add i32 %458, 1
  %460 = icmp ugt i32 %459, 15
  %461 = icmp ult i32 %457, %456
  %462 = xor i32 %456, %457
  %463 = xor i32 1, %457
  %464 = and i32 %462, %463
  %465 = icmp slt i32 %464, 0
  store i1 %460, i1* %az
  store i1 %461, i1* %cf
  store i1 %465, i1* %of
  %466 = icmp eq i32 %457, 0
  store i1 %466, i1* %zf
  %467 = icmp slt i32 %457, 0
  store i1 %467, i1* %sf
  %468 = trunc i32 %457 to i8
  %469 = call i8 @llvm.ctpop.i8(i8 %468)
  %470 = and i8 %469, 1
  %471 = icmp eq i8 %470, 0
  store i1 %471, i1* %pf
  store i64 ptrtoint (i64* @global_var_2164c5 to i64), i64* %rax
  store volatile i64 44677, i64* @assembly_address
  %472 = load i64* %rcx
  %473 = trunc i64 %472 to i32
  %474 = zext i32 %473 to i64
  store i64 %474, i64* %rsi
  store volatile i64 44679, i64* @assembly_address
  %475 = load i64* %rax
  %476 = trunc i64 %475 to i32
  %477 = zext i32 %476 to i64
  store i64 %477, i64* %rdi
  store volatile i64 44681, i64* @assembly_address
  %478 = load i64* %rdi
  %479 = load i64* %rsi
  %480 = load i64* %rdx
  %481 = trunc i64 %478 to i32
  %482 = call i64 @send_all_trees(i32 %481, i64 %479, i64 %480)
  store i64 %482, i64* %rax
  store i64 %482, i64* %rax
  store volatile i64 44686, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2178c0 to i64), i64* %rsi
  store volatile i64 44693, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rdi
  store volatile i64 44700, i64* @assembly_address
  %483 = load i64* %rdi
  %484 = inttoptr i64 %483 to i64*
  %485 = load i64* %rsi
  %486 = inttoptr i64 %485 to i64*
  %487 = bitcast i64* %484 to i16*
  %488 = call i64 @compress_block(i16* %487, i64* %486)
  store i64 %488, i64* %rax
  store i64 %488, i64* %rax
  store volatile i64 44705, i64* @assembly_address
  %489 = load i64* @global_var_219ed0
  store i64 %489, i64* %rax
  store volatile i64 44712, i64* @assembly_address
  %490 = load i64* @global_var_219ee0
  store i64 %490, i64* %rdx
  store volatile i64 44719, i64* @assembly_address
  %491 = load i64* %rax
  %492 = load i64* %rdx
  %493 = add i64 %491, %492
  %494 = and i64 %491, 15
  %495 = and i64 %492, 15
  %496 = add i64 %494, %495
  %497 = icmp ugt i64 %496, 15
  %498 = icmp ult i64 %493, %491
  %499 = xor i64 %491, %493
  %500 = xor i64 %492, %493
  %501 = and i64 %499, %500
  %502 = icmp slt i64 %501, 0
  store i1 %497, i1* %az
  store i1 %498, i1* %cf
  store i1 %502, i1* %of
  %503 = icmp eq i64 %493, 0
  store i1 %503, i1* %zf
  %504 = icmp slt i64 %493, 0
  store i1 %504, i1* %sf
  %505 = trunc i64 %493 to i8
  %506 = call i8 @llvm.ctpop.i8(i8 %505)
  %507 = and i8 %506, 1
  %508 = icmp eq i8 %507, 0
  store i1 %508, i1* %pf
  store i64 %493, i64* %rax
  store volatile i64 44722, i64* @assembly_address
  %509 = load i64* %rax
  %510 = add i64 %509, 3
  %511 = and i64 %509, 15
  %512 = add i64 %511, 3
  %513 = icmp ugt i64 %512, 15
  %514 = icmp ult i64 %510, %509
  %515 = xor i64 %509, %510
  %516 = xor i64 3, %510
  %517 = and i64 %515, %516
  %518 = icmp slt i64 %517, 0
  store i1 %513, i1* %az
  store i1 %514, i1* %cf
  store i1 %518, i1* %of
  %519 = icmp eq i64 %510, 0
  store i1 %519, i1* %zf
  %520 = icmp slt i64 %510, 0
  store i1 %520, i1* %sf
  %521 = trunc i64 %510 to i8
  %522 = call i8 @llvm.ctpop.i8(i8 %521)
  %523 = and i8 %522, 1
  %524 = icmp eq i8 %523, 0
  store i1 %524, i1* %pf
  store i64 %510, i64* %rax
  store volatile i64 44726, i64* @assembly_address
  %525 = load i64* %rax
  store i64 %525, i64* @global_var_219ee0
  br label %block_aebd

block_aebd:                                       ; preds = %block_ae5b, %block_ae18, %block_adaa
  store volatile i64 44733, i64* @assembly_address
  %526 = call i64 @init_block()
  store i64 %526, i64* %rax
  store i64 %526, i64* %rax
  store i64 %526, i64* %rax
  store volatile i64 44738, i64* @assembly_address
  %527 = load i32* %stack_var_-64
  %528 = and i32 %527, 15
  %529 = icmp ugt i32 %528, 15
  %530 = icmp ult i32 %527, 0
  %531 = xor i32 %527, 0
  %532 = and i32 %531, 0
  %533 = icmp slt i32 %532, 0
  store i1 %529, i1* %az
  store i1 %530, i1* %cf
  store i1 %533, i1* %of
  %534 = icmp eq i32 %527, 0
  store i1 %534, i1* %zf
  %535 = icmp slt i32 %527, 0
  store i1 %535, i1* %sf
  %536 = trunc i32 %527 to i8
  %537 = call i8 @llvm.ctpop.i8(i8 %536)
  %538 = and i8 %537, 1
  %539 = icmp eq i8 %538, 0
  store i1 %539, i1* %pf
  store volatile i64 44742, i64* @assembly_address
  %540 = load i1* %zf
  br i1 %540, label %block_aee1, label %block_aec8

block_aec8:                                       ; preds = %block_aebd
  store volatile i64 44744, i64* @assembly_address
  %541 = call i64 @bi_windup()
  store i64 %541, i64* %rax
  store i64 %541, i64* %rax
  store i64 %541, i64* %rax
  store volatile i64 44749, i64* @assembly_address
  %542 = load i64* @global_var_219ee0
  store i64 %542, i64* %rax
  store volatile i64 44756, i64* @assembly_address
  %543 = load i64* %rax
  %544 = add i64 %543, 7
  %545 = and i64 %543, 15
  %546 = add i64 %545, 7
  %547 = icmp ugt i64 %546, 15
  %548 = icmp ult i64 %544, %543
  %549 = xor i64 %543, %544
  %550 = xor i64 7, %544
  %551 = and i64 %549, %550
  %552 = icmp slt i64 %551, 0
  store i1 %547, i1* %az
  store i1 %548, i1* %cf
  store i1 %552, i1* %of
  %553 = icmp eq i64 %544, 0
  store i1 %553, i1* %zf
  %554 = icmp slt i64 %544, 0
  store i1 %554, i1* %sf
  %555 = trunc i64 %544 to i8
  %556 = call i8 @llvm.ctpop.i8(i8 %555)
  %557 = and i8 %556, 1
  %558 = icmp eq i8 %557, 0
  store i1 %558, i1* %pf
  store i64 %544, i64* %rax
  store volatile i64 44760, i64* @assembly_address
  %559 = load i64* %rax
  store i64 %559, i64* @global_var_219ee0
  store volatile i64 44767, i64* @assembly_address
  br label %block_af31

block_aee1:                                       ; preds = %block_aebd
  store volatile i64 44769, i64* @assembly_address
  %560 = load i32* %stack_var_-60
  %561 = and i32 %560, 15
  %562 = icmp ugt i32 %561, 15
  %563 = icmp ult i32 %560, 0
  %564 = xor i32 %560, 0
  %565 = and i32 %564, 0
  %566 = icmp slt i32 %565, 0
  store i1 %562, i1* %az
  store i1 %563, i1* %cf
  store i1 %566, i1* %of
  %567 = icmp eq i32 %560, 0
  store i1 %567, i1* %zf
  %568 = icmp slt i32 %560, 0
  store i1 %568, i1* %sf
  %569 = trunc i32 %560 to i8
  %570 = call i8 @llvm.ctpop.i8(i8 %569)
  %571 = and i8 %570, 1
  %572 = icmp eq i8 %571, 0
  store i1 %572, i1* %pf
  store volatile i64 44773, i64* @assembly_address
  %573 = load i1* %zf
  br i1 %573, label %block_af31, label %block_aee7

block_aee7:                                       ; preds = %block_aee1
  store volatile i64 44775, i64* @assembly_address
  %574 = load i64* @global_var_219ee0
  store i64 %574, i64* %rax
  store volatile i64 44782, i64* @assembly_address
  %575 = load i64* %rax
  %576 = trunc i64 %575 to i32
  %577 = and i32 %576, 7
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %578 = icmp eq i32 %577, 0
  store i1 %578, i1* %zf
  %579 = icmp slt i32 %577, 0
  store i1 %579, i1* %sf
  %580 = trunc i32 %577 to i8
  %581 = call i8 @llvm.ctpop.i8(i8 %580)
  %582 = and i8 %581, 1
  %583 = icmp eq i8 %582, 0
  store i1 %583, i1* %pf
  %584 = zext i32 %577 to i64
  store i64 %584, i64* %rax
  store volatile i64 44785, i64* @assembly_address
  %585 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %586 = icmp eq i64 %585, 0
  store i1 %586, i1* %zf
  %587 = icmp slt i64 %585, 0
  store i1 %587, i1* %sf
  %588 = trunc i64 %585 to i8
  %589 = call i8 @llvm.ctpop.i8(i8 %588)
  %590 = and i8 %589, 1
  %591 = icmp eq i8 %590, 0
  store i1 %591, i1* %pf
  store volatile i64 44788, i64* @assembly_address
  %592 = load i1* %zf
  br i1 %592, label %block_af31, label %block_aef6

block_aef6:                                       ; preds = %block_aee7
  store volatile i64 44790, i64* @assembly_address
  %593 = load i32* %stack_var_-64
  %594 = zext i32 %593 to i64
  store i64 %594, i64* %rax
  store volatile i64 44793, i64* @assembly_address
  store i64 3, i64* %rsi
  store volatile i64 44798, i64* @assembly_address
  %595 = load i64* %rax
  %596 = trunc i64 %595 to i32
  %597 = zext i32 %596 to i64
  store i64 %597, i64* %rdi
  store volatile i64 44800, i64* @assembly_address
  %598 = load i64* %rdi
  %599 = load i64* %rsi
  %600 = trunc i64 %598 to i32
  %601 = call i64 @send_bits(i32 %600, i64 %599)
  store i64 %601, i64* %rax
  store i64 %601, i64* %rax
  store volatile i64 44805, i64* @assembly_address
  %602 = load i64* @global_var_219ee0
  store i64 %602, i64* %rax
  store volatile i64 44812, i64* @assembly_address
  %603 = load i64* %rax
  %604 = add i64 %603, 10
  %605 = and i64 %603, 15
  %606 = add i64 %605, 10
  %607 = icmp ugt i64 %606, 15
  %608 = icmp ult i64 %604, %603
  %609 = xor i64 %603, %604
  %610 = xor i64 10, %604
  %611 = and i64 %609, %610
  %612 = icmp slt i64 %611, 0
  store i1 %607, i1* %az
  store i1 %608, i1* %cf
  store i1 %612, i1* %of
  %613 = icmp eq i64 %604, 0
  store i1 %613, i1* %zf
  %614 = icmp slt i64 %604, 0
  store i1 %614, i1* %sf
  %615 = trunc i64 %604 to i8
  %616 = call i8 @llvm.ctpop.i8(i8 %615)
  %617 = and i8 %616, 1
  %618 = icmp eq i8 %617, 0
  store i1 %618, i1* %pf
  store i64 %604, i64* %rax
  store volatile i64 44816, i64* @assembly_address
  %619 = load i64* %rax
  %620 = and i64 %619, -8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %621 = icmp eq i64 %620, 0
  store i1 %621, i1* %zf
  %622 = icmp slt i64 %620, 0
  store i1 %622, i1* %sf
  %623 = trunc i64 %620 to i8
  %624 = call i8 @llvm.ctpop.i8(i8 %623)
  %625 = and i8 %624, 1
  %626 = icmp eq i8 %625, 0
  store i1 %626, i1* %pf
  store i64 %620, i64* %rax
  store volatile i64 44820, i64* @assembly_address
  %627 = load i64* %rax
  store i64 %627, i64* @global_var_219ee0
  store volatile i64 44827, i64* @assembly_address
  %628 = load i64* %stack_var_-48
  store i64 %628, i64* %rax
  store volatile i64 44831, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 44836, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 44841, i64* @assembly_address
  %629 = load i64* %rax
  store i64 %629, i64* %rdi
  store volatile i64 44844, i64* @assembly_address
  %630 = load i64* %rdi
  %631 = load i64* %rsi
  %632 = load i64* %rdx
  %633 = inttoptr i64 %630 to i8*
  %634 = call i64 @copy_block(i8* %633, i64 %631, i64 %632)
  store i64 %634, i64* %rax
  store i64 %634, i64* %rax
  br label %block_af31

block_af31:                                       ; preds = %block_aef6, %block_aee7, %block_aee1, %block_aec8
  store volatile i64 44849, i64* @assembly_address
  %635 = load i64* @global_var_219ee0
  store i64 %635, i64* %rax
  store volatile i64 44856, i64* @assembly_address
  %636 = load i64* %rax
  %637 = load i1* %of
  %638 = ashr i64 %636, 3
  %639 = icmp eq i64 %638, 0
  store i1 %639, i1* %zf
  %640 = icmp slt i64 %638, 0
  store i1 %640, i1* %sf
  %641 = trunc i64 %638 to i8
  %642 = call i8 @llvm.ctpop.i8(i8 %641)
  %643 = and i8 %642, 1
  %644 = icmp eq i8 %643, 0
  store i1 %644, i1* %pf
  store i64 %638, i64* %rax
  %645 = and i64 4, %636
  %646 = icmp ne i64 %645, 0
  store i1 %646, i1* %cf
  %647 = select i1 false, i1 false, i1 %637
  store i1 %647, i1* %of
  store volatile i64 44860, i64* @assembly_address
  %648 = load i64* %stack_var_-8
  store i64 %648, i64* %rbp
  %649 = ptrtoint i64* %stack_var_0 to i64
  store i64 %649, i64* %rsp
  store volatile i64 44861, i64* @assembly_address
  %650 = load i64* %rax
  ret i64 %650
}

declare i64 @213(i64, i64, i32, i64)

declare i64 @214(i64, i64, i64, i32)

declare i64 @215(i64, i64, i64, i64)

define i64 @ct_tally(i32 %arg1, i64 %arg2) {
block_af3e:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-48 = alloca i32
  %stack_var_-44 = alloca i32
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca %z_stream_s*
  %5 = alloca i64
  %6 = alloca i32
  %7 = alloca i32
  store volatile i64 44862, i64* @assembly_address
  %8 = load i64* %rbp
  store i64 %8, i64* %stack_var_-8
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rsp
  store volatile i64 44863, i64* @assembly_address
  %10 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %10, i64* %rbp
  store volatile i64 44866, i64* @assembly_address
  %11 = load i64* %rdi
  %12 = trunc i64 %11 to i32
  store i32 %12, i32* %stack_var_-44
  store volatile i64 44869, i64* @assembly_address
  %13 = load i64* %rsi
  %14 = trunc i64 %13 to i32
  store i32 %14, i32* %stack_var_-48
  store volatile i64 44872, i64* @assembly_address
  %15 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %16 = zext i32 %15 to i64
  store i64 %16, i64* %rax
  store volatile i64 44878, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219ec1 to i64), i64* %rdx
  store volatile i64 44881, i64* @assembly_address
  %17 = load i64* %rdx
  %18 = trunc i64 %17 to i32
  store i32 %18, i32* bitcast (i64* @global_var_219ec0 to i32*)
  store volatile i64 44887, i64* @assembly_address
  %19 = load i32* %stack_var_-48
  %20 = zext i32 %19 to i64
  store i64 %20, i64* %rdx
  store volatile i64 44890, i64* @assembly_address
  %21 = load i64* %rdx
  %22 = trunc i64 %21 to i32
  %23 = zext i32 %22 to i64
  store i64 %23, i64* %rcx
  store volatile i64 44892, i64* @assembly_address
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i32
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rdx
  store volatile i64 44894, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 44901, i64* @assembly_address
  %27 = load i64* %rcx
  %28 = trunc i64 %27 to i8
  %29 = load i64* %rdx
  %30 = load i64* %rax
  %31 = mul i64 %30, 1
  %32 = add i64 %29, %31
  %33 = inttoptr i64 %32 to i8*
  store i8 %28, i8* %33
  store volatile i64 44904, i64* @assembly_address
  %34 = load i32* %stack_var_-44
  %35 = and i32 %34, 15
  %36 = icmp ugt i32 %35, 15
  %37 = icmp ult i32 %34, 0
  %38 = xor i32 %34, 0
  %39 = and i32 %38, 0
  %40 = icmp slt i32 %39, 0
  store i1 %36, i1* %az
  store i1 %37, i1* %cf
  store i1 %40, i1* %of
  %41 = icmp eq i32 %34, 0
  store i1 %41, i1* %zf
  %42 = icmp slt i32 %34, 0
  store i1 %42, i1* %sf
  %43 = trunc i32 %34 to i8
  %44 = call i8 @llvm.ctpop.i8(i8 %43)
  %45 = and i8 %44, 1
  %46 = icmp eq i8 %45, 0
  store i1 %46, i1* %pf
  store volatile i64 44908, i64* @assembly_address
  %47 = load i1* %zf
  %48 = icmp eq i1 %47, false
  br i1 %48, label %block_afa6, label %block_af6e

block_af6e:                                       ; preds = %block_af3e
  store volatile i64 44910, i64* @assembly_address
  %49 = load i32* %stack_var_-48
  %50 = zext i32 %49 to i64
  store i64 %50, i64* %rax
  store volatile i64 44913, i64* @assembly_address
  %51 = load i64* %rax
  %52 = trunc i64 %51 to i32
  %53 = sext i32 %52 to i64
  store i64 %53, i64* %rax
  store volatile i64 44915, i64* @assembly_address
  %54 = load i64* %rax
  %55 = mul i64 %54, 4
  store i64 %55, i64* %rdx
  store volatile i64 44923, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rax
  store volatile i64 44930, i64* @assembly_address
  %56 = load i64* %rdx
  %57 = load i64* %rax
  %58 = mul i64 %57, 1
  %59 = add i64 %56, %58
  %60 = inttoptr i64 %59 to i16*
  %61 = load i16* %60
  %62 = zext i16 %61 to i64
  store i64 %62, i64* %rax
  store volatile i64 44934, i64* @assembly_address
  %63 = load i64* %rax
  %64 = add i64 %63, 1
  %65 = trunc i64 %64 to i32
  %66 = zext i32 %65 to i64
  store i64 %66, i64* %rcx
  store volatile i64 44937, i64* @assembly_address
  %67 = load i32* %stack_var_-48
  %68 = zext i32 %67 to i64
  store i64 %68, i64* %rax
  store volatile i64 44940, i64* @assembly_address
  %69 = load i64* %rax
  %70 = trunc i64 %69 to i32
  %71 = sext i32 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 44942, i64* @assembly_address
  %72 = load i64* %rax
  %73 = mul i64 %72, 4
  store i64 %73, i64* %rdx
  store volatile i64 44950, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rax
  store volatile i64 44957, i64* @assembly_address
  %74 = load i64* %rcx
  %75 = trunc i64 %74 to i16
  %76 = load i64* %rdx
  %77 = load i64* %rax
  %78 = mul i64 %77, 1
  %79 = add i64 %76, %78
  %80 = inttoptr i64 %79 to i16*
  store i16 %75, i16* %80
  store volatile i64 44961, i64* @assembly_address
  br label %block_b095

block_afa6:                                       ; preds = %block_af3e
  store volatile i64 44966, i64* @assembly_address
  %81 = load i32* %stack_var_-44
  %82 = sub i32 %81, 1
  %83 = and i32 %81, 15
  %84 = sub i32 %83, 1
  %85 = icmp ugt i32 %84, 15
  %86 = icmp ult i32 %81, 1
  %87 = xor i32 %81, 1
  %88 = xor i32 %81, %82
  %89 = and i32 %87, %88
  %90 = icmp slt i32 %89, 0
  store i1 %85, i1* %az
  store i1 %86, i1* %cf
  store i1 %90, i1* %of
  %91 = icmp eq i32 %82, 0
  store i1 %91, i1* %zf
  %92 = icmp slt i32 %82, 0
  store i1 %92, i1* %sf
  %93 = trunc i32 %82 to i8
  %94 = call i8 @llvm.ctpop.i8(i8 %93)
  %95 = and i8 %94, 1
  %96 = icmp eq i8 %95, 0
  store i1 %96, i1* %pf
  store i32 %82, i32* %stack_var_-44
  store volatile i64 44970, i64* @assembly_address
  %97 = load i32* %stack_var_-48
  %98 = zext i32 %97 to i64
  store i64 %98, i64* %rax
  store volatile i64 44973, i64* @assembly_address
  %99 = load i64* %rax
  %100 = trunc i64 %99 to i32
  %101 = sext i32 %100 to i64
  store i64 %101, i64* %rdx
  store volatile i64 44976, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218ac0 to i64), i64* %rax
  store volatile i64 44983, i64* @assembly_address
  %102 = load i64* %rdx
  %103 = load i64* %rax
  %104 = mul i64 %103, 1
  %105 = add i64 %102, %104
  %106 = inttoptr i64 %105 to i8*
  %107 = load i8* %106
  %108 = zext i8 %107 to i64
  store i64 %108, i64* %rax
  store volatile i64 44987, i64* @assembly_address
  %109 = load i64* %rax
  %110 = trunc i64 %109 to i8
  %111 = zext i8 %110 to i64
  store i64 %111, i64* %rax
  store volatile i64 44990, i64* @assembly_address
  %112 = load i64* %rax
  %113 = trunc i64 %112 to i32
  %114 = add i32 %113, 257
  %115 = and i32 %113, 15
  %116 = add i32 %115, 1
  %117 = icmp ugt i32 %116, 15
  %118 = icmp ult i32 %114, %113
  %119 = xor i32 %113, %114
  %120 = xor i32 257, %114
  %121 = and i32 %119, %120
  %122 = icmp slt i32 %121, 0
  store i1 %117, i1* %az
  store i1 %118, i1* %cf
  store i1 %122, i1* %of
  %123 = icmp eq i32 %114, 0
  store i1 %123, i1* %zf
  %124 = icmp slt i32 %114, 0
  store i1 %124, i1* %sf
  %125 = trunc i32 %114 to i8
  %126 = call i8 @llvm.ctpop.i8(i8 %125)
  %127 = and i8 %126, 1
  %128 = icmp eq i8 %127, 0
  store i1 %128, i1* %pf
  %129 = zext i32 %114 to i64
  store i64 %129, i64* %rax
  store volatile i64 44995, i64* @assembly_address
  %130 = load i64* %rax
  %131 = trunc i64 %130 to i32
  %132 = sext i32 %131 to i64
  store i64 %132, i64* %rdx
  store volatile i64 44998, i64* @assembly_address
  %133 = load i64* %rdx
  %134 = mul i64 %133, 4
  store i64 %134, i64* %rcx
  store volatile i64 45006, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rdx
  store volatile i64 45013, i64* @assembly_address
  %135 = load i64* %rcx
  %136 = load i64* %rdx
  %137 = mul i64 %136, 1
  %138 = add i64 %135, %137
  %139 = inttoptr i64 %138 to i16*
  %140 = load i16* %139
  %141 = zext i16 %140 to i64
  store i64 %141, i64* %rdx
  store volatile i64 45017, i64* @assembly_address
  %142 = load i64* %rdx
  %143 = add i64 %142, 1
  %144 = trunc i64 %143 to i32
  %145 = zext i32 %144 to i64
  store i64 %145, i64* %rcx
  store volatile i64 45020, i64* @assembly_address
  %146 = load i64* %rax
  %147 = trunc i64 %146 to i32
  %148 = sext i32 %147 to i64
  store i64 %148, i64* %rax
  store volatile i64 45022, i64* @assembly_address
  %149 = load i64* %rax
  %150 = mul i64 %149, 4
  store i64 %150, i64* %rdx
  store volatile i64 45030, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rax
  store volatile i64 45037, i64* @assembly_address
  %151 = load i64* %rcx
  %152 = trunc i64 %151 to i16
  %153 = load i64* %rdx
  %154 = load i64* %rax
  %155 = mul i64 %154, 1
  %156 = add i64 %153, %155
  %157 = inttoptr i64 %156 to i16*
  store i16 %152, i16* %157
  store volatile i64 45041, i64* @assembly_address
  %158 = load i32* %stack_var_-44
  store i32 %158, i32* %7
  store i32 255, i32* %6
  %159 = sub i32 %158, 255
  %160 = and i32 %158, 15
  %161 = sub i32 %160, 15
  %162 = icmp ugt i32 %161, 15
  %163 = icmp ult i32 %158, 255
  %164 = xor i32 %158, 255
  %165 = xor i32 %158, %159
  %166 = and i32 %164, %165
  %167 = icmp slt i32 %166, 0
  store i1 %162, i1* %az
  store i1 %163, i1* %cf
  store i1 %167, i1* %of
  %168 = icmp eq i32 %159, 0
  store i1 %168, i1* %zf
  %169 = icmp slt i32 %159, 0
  store i1 %169, i1* %sf
  %170 = trunc i32 %159 to i8
  %171 = call i8 @llvm.ctpop.i8(i8 %170)
  %172 = and i8 %171, 1
  %173 = icmp eq i8 %172, 0
  store i1 %173, i1* %pf
  store volatile i64 45048, i64* @assembly_address
  %174 = load i32* %7
  %175 = load i32* %6
  %176 = icmp sgt i32 %174, %175
  br i1 %176, label %block_b010, label %block_affa

block_affa:                                       ; preds = %block_afa6
  store volatile i64 45050, i64* @assembly_address
  %177 = load i32* %stack_var_-44
  %178 = zext i32 %177 to i64
  store i64 %178, i64* %rax
  store volatile i64 45053, i64* @assembly_address
  %179 = load i64* %rax
  %180 = trunc i64 %179 to i32
  %181 = sext i32 %180 to i64
  store i64 %181, i64* %rdx
  store volatile i64 45056, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218bc0 to i64), i64* %rax
  store volatile i64 45063, i64* @assembly_address
  %182 = load i64* %rdx
  %183 = load i64* %rax
  %184 = mul i64 %183, 1
  %185 = add i64 %182, %184
  %186 = inttoptr i64 %185 to i8*
  %187 = load i8* %186
  %188 = zext i8 %187 to i64
  store i64 %188, i64* %rax
  store volatile i64 45067, i64* @assembly_address
  %189 = load i64* %rax
  %190 = trunc i64 %189 to i8
  %191 = zext i8 %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 45070, i64* @assembly_address
  br label %block_b02c

block_b010:                                       ; preds = %block_afa6
  store volatile i64 45072, i64* @assembly_address
  %192 = load i32* %stack_var_-44
  %193 = zext i32 %192 to i64
  store i64 %193, i64* %rax
  store volatile i64 45075, i64* @assembly_address
  %194 = load i64* %rax
  %195 = trunc i64 %194 to i32
  %196 = load i1* %of
  %197 = ashr i32 %195, 7
  %198 = icmp eq i32 %197, 0
  store i1 %198, i1* %zf
  %199 = icmp slt i32 %197, 0
  store i1 %199, i1* %sf
  %200 = trunc i32 %197 to i8
  %201 = call i8 @llvm.ctpop.i8(i8 %200)
  %202 = and i8 %201, 1
  %203 = icmp eq i8 %202, 0
  store i1 %203, i1* %pf
  %204 = zext i32 %197 to i64
  store i64 %204, i64* %rax
  %205 = and i32 64, %195
  %206 = icmp ne i32 %205, 0
  store i1 %206, i1* %cf
  %207 = select i1 false, i1 false, i1 %196
  store i1 %207, i1* %of
  store volatile i64 45078, i64* @assembly_address
  %208 = load i64* %rax
  %209 = trunc i64 %208 to i32
  %210 = add i32 %209, 256
  %211 = and i32 %209, 15
  %212 = icmp ugt i32 %211, 15
  %213 = icmp ult i32 %210, %209
  %214 = xor i32 %209, %210
  %215 = xor i32 256, %210
  %216 = and i32 %214, %215
  %217 = icmp slt i32 %216, 0
  store i1 %212, i1* %az
  store i1 %213, i1* %cf
  store i1 %217, i1* %of
  %218 = icmp eq i32 %210, 0
  store i1 %218, i1* %zf
  %219 = icmp slt i32 %210, 0
  store i1 %219, i1* %sf
  %220 = trunc i32 %210 to i8
  %221 = call i8 @llvm.ctpop.i8(i8 %220)
  %222 = and i8 %221, 1
  %223 = icmp eq i8 %222, 0
  store i1 %223, i1* %pf
  %224 = zext i32 %210 to i64
  store i64 %224, i64* %rax
  store volatile i64 45083, i64* @assembly_address
  %225 = load i64* %rax
  %226 = trunc i64 %225 to i32
  %227 = sext i32 %226 to i64
  store i64 %227, i64* %rdx
  store volatile i64 45086, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218bc0 to i64), i64* %rax
  store volatile i64 45093, i64* @assembly_address
  %228 = load i64* %rdx
  %229 = load i64* %rax
  %230 = mul i64 %229, 1
  %231 = add i64 %228, %230
  %232 = inttoptr i64 %231 to i8*
  %233 = load i8* %232
  %234 = zext i8 %233 to i64
  store i64 %234, i64* %rax
  store volatile i64 45097, i64* @assembly_address
  %235 = load i64* %rax
  %236 = trunc i64 %235 to i8
  %237 = zext i8 %236 to i64
  store i64 %237, i64* %rax
  br label %block_b02c

block_b02c:                                       ; preds = %block_b010, %block_affa
  store volatile i64 45100, i64* @assembly_address
  %238 = load i64* %rax
  %239 = trunc i64 %238 to i32
  %240 = sext i32 %239 to i64
  store i64 %240, i64* %rdx
  store volatile i64 45103, i64* @assembly_address
  %241 = load i64* %rdx
  %242 = mul i64 %241, 4
  store i64 %242, i64* %rcx
  store volatile i64 45111, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2178c0 to i64), i64* %rdx
  store volatile i64 45118, i64* @assembly_address
  %243 = load i64* %rcx
  %244 = load i64* %rdx
  %245 = mul i64 %244, 1
  %246 = add i64 %243, %245
  %247 = inttoptr i64 %246 to i16*
  %248 = load i16* %247
  %249 = zext i16 %248 to i64
  store i64 %249, i64* %rdx
  store volatile i64 45122, i64* @assembly_address
  %250 = load i64* %rdx
  %251 = add i64 %250, 1
  %252 = trunc i64 %251 to i32
  %253 = zext i32 %252 to i64
  store i64 %253, i64* %rcx
  store volatile i64 45125, i64* @assembly_address
  %254 = load i64* %rax
  %255 = trunc i64 %254 to i32
  %256 = sext i32 %255 to i64
  store i64 %256, i64* %rax
  store volatile i64 45127, i64* @assembly_address
  %257 = load i64* %rax
  %258 = mul i64 %257, 4
  store i64 %258, i64* %rdx
  store volatile i64 45135, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2178c0 to i64), i64* %rax
  store volatile i64 45142, i64* @assembly_address
  %259 = load i64* %rcx
  %260 = trunc i64 %259 to i16
  %261 = load i64* %rdx
  %262 = load i64* %rax
  %263 = mul i64 %262, 1
  %264 = add i64 %261, %263
  %265 = inttoptr i64 %264 to i16*
  store i16 %260, i16* %265
  store volatile i64 45146, i64* @assembly_address
  %266 = load i32* bitcast (i64* @global_var_219ec4 to i32*)
  %267 = zext i32 %266 to i64
  store i64 %267, i64* %rax
  store volatile i64 45152, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219ec5 to i64), i64* %rdx
  store volatile i64 45155, i64* @assembly_address
  %268 = load i64* %rdx
  %269 = trunc i64 %268 to i32
  store i32 %269, i32* bitcast (i64* @global_var_219ec4 to i32*)
  store volatile i64 45161, i64* @assembly_address
  %270 = load i32* %stack_var_-44
  %271 = zext i32 %270 to i64
  store i64 %271, i64* %rdx
  store volatile i64 45164, i64* @assembly_address
  %272 = load i64* %rdx
  %273 = trunc i64 %272 to i32
  %274 = zext i32 %273 to i64
  store i64 %274, i64* %rcx
  store volatile i64 45166, i64* @assembly_address
  %275 = load i64* %rax
  %276 = trunc i64 %275 to i32
  %277 = zext i32 %276 to i64
  store i64 %277, i64* %rax
  store volatile i64 45168, i64* @assembly_address
  %278 = load i64* %rax
  %279 = load i64* %rax
  %280 = mul i64 %279, 1
  %281 = add i64 %278, %280
  store i64 %281, i64* %rdx
  store volatile i64 45172, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_23a880 to i64), i64* %rax
  store volatile i64 45179, i64* @assembly_address
  %282 = load i64* %rcx
  %283 = trunc i64 %282 to i16
  %284 = load i64* %rdx
  %285 = load i64* %rax
  %286 = mul i64 %285, 1
  %287 = add i64 %284, %286
  %288 = inttoptr i64 %287 to i16*
  store i16 %283, i16* %288
  store volatile i64 45183, i64* @assembly_address
  %289 = load i8* bitcast (i64* @global_var_219ecc to i8*)
  %290 = zext i8 %289 to i64
  store i64 %290, i64* %rdx
  store volatile i64 45190, i64* @assembly_address
  %291 = load i8* bitcast (i64* @global_var_219ecd to i8*)
  %292 = zext i8 %291 to i64
  store i64 %292, i64* %rax
  store volatile i64 45197, i64* @assembly_address
  %293 = load i64* %rax
  %294 = trunc i64 %293 to i32
  %295 = load i64* %rdx
  %296 = trunc i64 %295 to i32
  %297 = or i32 %294, %296
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %298 = icmp eq i32 %297, 0
  store i1 %298, i1* %zf
  %299 = icmp slt i32 %297, 0
  store i1 %299, i1* %sf
  %300 = trunc i32 %297 to i8
  %301 = call i8 @llvm.ctpop.i8(i8 %300)
  %302 = and i8 %301, 1
  %303 = icmp eq i8 %302, 0
  store i1 %303, i1* %pf
  %304 = zext i32 %297 to i64
  store i64 %304, i64* %rax
  store volatile i64 45199, i64* @assembly_address
  %305 = load i64* %rax
  %306 = trunc i64 %305 to i8
  store i8 %306, i8* bitcast (i64* @global_var_219ecc to i8*)
  br label %block_b095

block_b095:                                       ; preds = %block_b02c, %block_af6e
  store volatile i64 45205, i64* @assembly_address
  %307 = load i8* bitcast (i64* @global_var_219ecd to i8*)
  %308 = zext i8 %307 to i64
  store i64 %308, i64* %rax
  store volatile i64 45212, i64* @assembly_address
  %309 = load i64* %rax
  %310 = trunc i64 %309 to i32
  %311 = load i64* %rax
  %312 = trunc i64 %311 to i32
  %313 = add i32 %310, %312
  %314 = and i32 %310, 15
  %315 = and i32 %312, 15
  %316 = add i32 %314, %315
  %317 = icmp ugt i32 %316, 15
  %318 = icmp ult i32 %313, %310
  %319 = xor i32 %310, %313
  %320 = xor i32 %312, %313
  %321 = and i32 %319, %320
  %322 = icmp slt i32 %321, 0
  store i1 %317, i1* %az
  store i1 %318, i1* %cf
  store i1 %322, i1* %of
  %323 = icmp eq i32 %313, 0
  store i1 %323, i1* %zf
  %324 = icmp slt i32 %313, 0
  store i1 %324, i1* %sf
  %325 = trunc i32 %313 to i8
  %326 = call i8 @llvm.ctpop.i8(i8 %325)
  %327 = and i8 %326, 1
  %328 = icmp eq i8 %327, 0
  store i1 %328, i1* %pf
  %329 = zext i32 %313 to i64
  store i64 %329, i64* %rax
  store volatile i64 45214, i64* @assembly_address
  %330 = load i64* %rax
  %331 = trunc i64 %330 to i8
  store i8 %331, i8* bitcast (i64* @global_var_219ecd to i8*)
  store volatile i64 45220, i64* @assembly_address
  %332 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %333 = zext i32 %332 to i64
  store i64 %333, i64* %rax
  store volatile i64 45226, i64* @assembly_address
  %334 = load i64* %rax
  %335 = trunc i64 %334 to i32
  %336 = and i32 %335, 7
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %337 = icmp eq i32 %336, 0
  store i1 %337, i1* %zf
  %338 = icmp slt i32 %336, 0
  store i1 %338, i1* %sf
  %339 = trunc i32 %336 to i8
  %340 = call i8 @llvm.ctpop.i8(i8 %339)
  %341 = and i8 %340, 1
  %342 = icmp eq i8 %341, 0
  store i1 %342, i1* %pf
  %343 = zext i32 %336 to i64
  store i64 %343, i64* %rax
  store volatile i64 45229, i64* @assembly_address
  %344 = load i64* %rax
  %345 = trunc i64 %344 to i32
  %346 = load i64* %rax
  %347 = trunc i64 %346 to i32
  %348 = and i32 %345, %347
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %349 = icmp eq i32 %348, 0
  store i1 %349, i1* %zf
  %350 = icmp slt i32 %348, 0
  store i1 %350, i1* %sf
  %351 = trunc i32 %348 to i8
  %352 = call i8 @llvm.ctpop.i8(i8 %351)
  %353 = and i8 %352, 1
  %354 = icmp eq i8 %353, 0
  store i1 %354, i1* %pf
  store volatile i64 45231, i64* @assembly_address
  %355 = load i1* %zf
  %356 = icmp eq i1 %355, false
  br i1 %356, label %block_b0e1, label %block_b0b1

block_b0b1:                                       ; preds = %block_b095
  store volatile i64 45233, i64* @assembly_address
  %357 = load i32* bitcast (i64* @global_var_219ec8 to i32*)
  %358 = zext i32 %357 to i64
  store i64 %358, i64* %rax
  store volatile i64 45239, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219ec9 to i64), i64* %rdx
  store volatile i64 45242, i64* @assembly_address
  %359 = load i64* %rdx
  %360 = trunc i64 %359 to i32
  store i32 %360, i32* bitcast (i64* @global_var_219ec8 to i32*)
  store volatile i64 45248, i64* @assembly_address
  %361 = load i8* bitcast (i64* @global_var_219ecc to i8*)
  %362 = zext i8 %361 to i64
  store i64 %362, i64* %rdx
  store volatile i64 45255, i64* @assembly_address
  %363 = load i64* %rax
  %364 = trunc i64 %363 to i32
  %365 = zext i32 %364 to i64
  store i64 %365, i64* %rcx
  store volatile i64 45257, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218ec0 to i64), i64* %rax
  store volatile i64 45264, i64* @assembly_address
  %366 = load i64* %rdx
  %367 = trunc i64 %366 to i8
  %368 = load i64* %rcx
  %369 = load i64* %rax
  %370 = mul i64 %369, 1
  %371 = add i64 %368, %370
  %372 = inttoptr i64 %371 to i8*
  store i8 %367, i8* %372
  store volatile i64 45267, i64* @assembly_address
  store i8 0, i8* bitcast (i64* @global_var_219ecc to i8*)
  store volatile i64 45274, i64* @assembly_address
  store i8 1, i8* bitcast (i64* @global_var_219ecd to i8*)
  br label %block_b0e1

block_b0e1:                                       ; preds = %block_b0b1, %block_b095
  store volatile i64 45281, i64* @assembly_address
  %373 = load i32* bitcast (i64* @global_var_2160a0 to i32*)
  %374 = zext i32 %373 to i64
  store i64 %374, i64* %rax
  store volatile i64 45287, i64* @assembly_address
  %375 = load i64* %rax
  %376 = trunc i64 %375 to i32
  %377 = inttoptr i64 %375 to %z_stream_s*
  store %z_stream_s* %377, %z_stream_s** %4
  store i32 2, i32* %3
  %378 = sub i32 %376, 2
  %379 = and i32 %376, 15
  %380 = sub i32 %379, 2
  %381 = icmp ugt i32 %380, 15
  %382 = icmp ult i32 %376, 2
  %383 = xor i32 %376, 2
  %384 = xor i32 %376, %378
  %385 = and i32 %383, %384
  %386 = icmp slt i32 %385, 0
  store i1 %381, i1* %az
  store i1 %382, i1* %cf
  store i1 %386, i1* %of
  %387 = icmp eq i32 %378, 0
  store i1 %387, i1* %zf
  %388 = icmp slt i32 %378, 0
  store i1 %388, i1* %sf
  %389 = trunc i32 %378 to i8
  %390 = call i8 @llvm.ctpop.i8(i8 %389)
  %391 = and i8 %390, 1
  %392 = icmp eq i8 %391, 0
  store i1 %392, i1* %pf
  store volatile i64 45290, i64* @assembly_address
  %393 = load %z_stream_s** %4
  %394 = ptrtoint %z_stream_s* %393 to i64
  %395 = load i32* %3
  %396 = trunc i64 %394 to i32
  %397 = icmp sle i32 %396, %395
  br i1 %397, label %block_b1ac, label %block_b0f0

block_b0f0:                                       ; preds = %block_b0e1
  store volatile i64 45296, i64* @assembly_address
  %398 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %399 = zext i32 %398 to i64
  store i64 %399, i64* %rax
  store volatile i64 45302, i64* @assembly_address
  %400 = load i64* %rax
  %401 = trunc i64 %400 to i32
  %402 = and i32 %401, ptrtoint (i64** @global_var_fff to i32)
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %403 = icmp eq i32 %402, 0
  store i1 %403, i1* %zf
  %404 = icmp slt i32 %402, 0
  store i1 %404, i1* %sf
  %405 = trunc i32 %402 to i8
  %406 = call i8 @llvm.ctpop.i8(i8 %405)
  %407 = and i8 %406, 1
  %408 = icmp eq i8 %407, 0
  store i1 %408, i1* %pf
  %409 = zext i32 %402 to i64
  store i64 %409, i64* %rax
  store volatile i64 45307, i64* @assembly_address
  %410 = load i64* %rax
  %411 = trunc i64 %410 to i32
  %412 = load i64* %rax
  %413 = trunc i64 %412 to i32
  %414 = and i32 %411, %413
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %415 = icmp eq i32 %414, 0
  store i1 %415, i1* %zf
  %416 = icmp slt i32 %414, 0
  store i1 %416, i1* %sf
  %417 = trunc i32 %414 to i8
  %418 = call i8 @llvm.ctpop.i8(i8 %417)
  %419 = and i8 %418, 1
  %420 = icmp eq i8 %419, 0
  store i1 %420, i1* %pf
  store volatile i64 45309, i64* @assembly_address
  %421 = load i1* %zf
  %422 = icmp eq i1 %421, false
  br i1 %422, label %block_b1ac, label %block_b103

block_b103:                                       ; preds = %block_b0f0
  store volatile i64 45315, i64* @assembly_address
  %423 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %424 = zext i32 %423 to i64
  store i64 %424, i64* %rax
  store volatile i64 45321, i64* @assembly_address
  %425 = load i64* %rax
  %426 = trunc i64 %425 to i32
  %427 = zext i32 %426 to i64
  store i64 %427, i64* %rax
  store volatile i64 45323, i64* @assembly_address
  %428 = load i64* %rax
  %429 = load i1* %of
  %430 = shl i64 %428, 3
  %431 = icmp eq i64 %430, 0
  store i1 %431, i1* %zf
  %432 = icmp slt i64 %430, 0
  store i1 %432, i1* %sf
  %433 = trunc i64 %430 to i8
  %434 = call i8 @llvm.ctpop.i8(i8 %433)
  %435 = and i8 %434, 1
  %436 = icmp eq i8 %435, 0
  store i1 %436, i1* %pf
  store i64 %430, i64* %rax
  %437 = shl i64 %428, 2
  %438 = lshr i64 %437, 63
  %439 = trunc i64 %438 to i1
  store i1 %439, i1* %cf
  %440 = lshr i64 %430, 63
  %441 = icmp ne i64 %440, %438
  %442 = select i1 false, i1 %441, i1 %429
  store i1 %442, i1* %of
  store volatile i64 45327, i64* @assembly_address
  %443 = load i64* %rax
  store i64 %443, i64* %stack_var_-24
  store volatile i64 45331, i64* @assembly_address
  %444 = load i32* bitcast (i64* @global_var_21a428 to i32*)
  %445 = zext i32 %444 to i64
  store i64 %445, i64* %rax
  store volatile i64 45337, i64* @assembly_address
  %446 = load i64* %rax
  %447 = trunc i64 %446 to i32
  %448 = zext i32 %447 to i64
  store i64 %448, i64* %rdx
  store volatile i64 45339, i64* @assembly_address
  %449 = load i64* @global_var_21a430
  store i64 %449, i64* %rax
  store volatile i64 45346, i64* @assembly_address
  %450 = load i64* %rdx
  %451 = load i64* %rax
  %452 = sub i64 %450, %451
  %453 = and i64 %450, 15
  %454 = and i64 %451, 15
  %455 = sub i64 %453, %454
  %456 = icmp ugt i64 %455, 15
  %457 = icmp ult i64 %450, %451
  %458 = xor i64 %450, %451
  %459 = xor i64 %450, %452
  %460 = and i64 %458, %459
  %461 = icmp slt i64 %460, 0
  store i1 %456, i1* %az
  store i1 %457, i1* %cf
  store i1 %461, i1* %of
  %462 = icmp eq i64 %452, 0
  store i1 %462, i1* %zf
  %463 = icmp slt i64 %452, 0
  store i1 %463, i1* %sf
  %464 = trunc i64 %452 to i8
  %465 = call i8 @llvm.ctpop.i8(i8 %464)
  %466 = and i8 %465, 1
  %467 = icmp eq i8 %466, 0
  store i1 %467, i1* %pf
  store i64 %452, i64* %rdx
  store volatile i64 45349, i64* @assembly_address
  %468 = load i64* %rdx
  store i64 %468, i64* %rax
  store volatile i64 45352, i64* @assembly_address
  %469 = load i64* %rax
  store i64 %469, i64* %stack_var_-16
  store volatile i64 45356, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 45363, i64* @assembly_address
  br label %block_b179

block_b135:                                       ; preds = %block_b179
  store volatile i64 45365, i64* @assembly_address
  %470 = load i32* %stack_var_-28
  %471 = zext i32 %470 to i64
  store i64 %471, i64* %rax
  store volatile i64 45368, i64* @assembly_address
  %472 = load i64* %rax
  %473 = trunc i64 %472 to i32
  %474 = sext i32 %473 to i64
  store i64 %474, i64* %rax
  store volatile i64 45370, i64* @assembly_address
  %475 = load i64* %rax
  %476 = mul i64 %475, 4
  store i64 %476, i64* %rdx
  store volatile i64 45378, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2178c0 to i64), i64* %rax
  store volatile i64 45385, i64* @assembly_address
  %477 = load i64* %rdx
  %478 = load i64* %rax
  %479 = mul i64 %478, 1
  %480 = add i64 %477, %479
  %481 = inttoptr i64 %480 to i16*
  %482 = load i16* %481
  %483 = zext i16 %482 to i64
  store i64 %483, i64* %rax
  store volatile i64 45389, i64* @assembly_address
  %484 = load i64* %rax
  %485 = trunc i64 %484 to i16
  %486 = zext i16 %485 to i64
  store i64 %486, i64* %rdx
  store volatile i64 45392, i64* @assembly_address
  %487 = load i32* %stack_var_-28
  %488 = zext i32 %487 to i64
  store i64 %488, i64* %rax
  store volatile i64 45395, i64* @assembly_address
  %489 = load i64* %rax
  %490 = trunc i64 %489 to i32
  %491 = sext i32 %490 to i64
  store i64 %491, i64* %rax
  store volatile i64 45397, i64* @assembly_address
  %492 = load i64* %rax
  %493 = mul i64 %492, 4
  store i64 %493, i64* %rcx
  store volatile i64 45405, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2163a0 to i64), i64* %rax
  store volatile i64 45412, i64* @assembly_address
  %494 = load i64* %rcx
  %495 = load i64* %rax
  %496 = mul i64 %495, 1
  %497 = add i64 %494, %496
  %498 = inttoptr i64 %497 to i32*
  %499 = load i32* %498
  %500 = zext i32 %499 to i64
  store i64 %500, i64* %rax
  store volatile i64 45415, i64* @assembly_address
  %501 = load i64* %rax
  %502 = trunc i64 %501 to i32
  %503 = sext i32 %502 to i64
  store i64 %503, i64* %rax
  store volatile i64 45417, i64* @assembly_address
  %504 = load i64* %rax
  %505 = add i64 %504, 5
  %506 = and i64 %504, 15
  %507 = add i64 %506, 5
  %508 = icmp ugt i64 %507, 15
  %509 = icmp ult i64 %505, %504
  %510 = xor i64 %504, %505
  %511 = xor i64 5, %505
  %512 = and i64 %510, %511
  %513 = icmp slt i64 %512, 0
  store i1 %508, i1* %az
  store i1 %509, i1* %cf
  store i1 %513, i1* %of
  %514 = icmp eq i64 %505, 0
  store i1 %514, i1* %zf
  %515 = icmp slt i64 %505, 0
  store i1 %515, i1* %sf
  %516 = trunc i64 %505 to i8
  %517 = call i8 @llvm.ctpop.i8(i8 %516)
  %518 = and i8 %517, 1
  %519 = icmp eq i8 %518, 0
  store i1 %519, i1* %pf
  store i64 %505, i64* %rax
  store volatile i64 45421, i64* @assembly_address
  %520 = load i64* %rax
  %521 = load i64* %rdx
  %522 = sext i64 %520 to i128
  %523 = sext i64 %521 to i128
  %524 = mul i128 %522, %523
  %525 = trunc i128 %524 to i64
  store i64 %525, i64* %rax
  %526 = trunc i128 %524 to i64
  %527 = sext i64 %526 to i128
  %528 = icmp ne i128 %524, %527
  store i1 %528, i1* %of
  store i1 %528, i1* %cf
  store volatile i64 45425, i64* @assembly_address
  %529 = load i64* %stack_var_-24
  %530 = load i64* %rax
  %531 = add i64 %529, %530
  %532 = and i64 %529, 15
  %533 = and i64 %530, 15
  %534 = add i64 %532, %533
  %535 = icmp ugt i64 %534, 15
  %536 = icmp ult i64 %531, %529
  %537 = xor i64 %529, %531
  %538 = xor i64 %530, %531
  %539 = and i64 %537, %538
  %540 = icmp slt i64 %539, 0
  store i1 %535, i1* %az
  store i1 %536, i1* %cf
  store i1 %540, i1* %of
  %541 = icmp eq i64 %531, 0
  store i1 %541, i1* %zf
  %542 = icmp slt i64 %531, 0
  store i1 %542, i1* %sf
  %543 = trunc i64 %531 to i8
  %544 = call i8 @llvm.ctpop.i8(i8 %543)
  %545 = and i8 %544, 1
  %546 = icmp eq i8 %545, 0
  store i1 %546, i1* %pf
  store i64 %531, i64* %stack_var_-24
  store volatile i64 45429, i64* @assembly_address
  %547 = load i32* %stack_var_-28
  %548 = add i32 %547, 1
  %549 = and i32 %547, 15
  %550 = add i32 %549, 1
  %551 = icmp ugt i32 %550, 15
  %552 = icmp ult i32 %548, %547
  %553 = xor i32 %547, %548
  %554 = xor i32 1, %548
  %555 = and i32 %553, %554
  %556 = icmp slt i32 %555, 0
  store i1 %551, i1* %az
  store i1 %552, i1* %cf
  store i1 %556, i1* %of
  %557 = icmp eq i32 %548, 0
  store i1 %557, i1* %zf
  %558 = icmp slt i32 %548, 0
  store i1 %558, i1* %sf
  %559 = trunc i32 %548 to i8
  %560 = call i8 @llvm.ctpop.i8(i8 %559)
  %561 = and i8 %560, 1
  %562 = icmp eq i8 %561, 0
  store i1 %562, i1* %pf
  store i32 %548, i32* %stack_var_-28
  br label %block_b179

block_b179:                                       ; preds = %block_b135, %block_b103
  store volatile i64 45433, i64* @assembly_address
  %563 = load i32* %stack_var_-28
  store i32 %563, i32* %2
  store i32 29, i32* %1
  %564 = sub i32 %563, 29
  %565 = and i32 %563, 15
  %566 = sub i32 %565, 13
  %567 = icmp ugt i32 %566, 15
  %568 = icmp ult i32 %563, 29
  %569 = xor i32 %563, 29
  %570 = xor i32 %563, %564
  %571 = and i32 %569, %570
  %572 = icmp slt i32 %571, 0
  store i1 %567, i1* %az
  store i1 %568, i1* %cf
  store i1 %572, i1* %of
  %573 = icmp eq i32 %564, 0
  store i1 %573, i1* %zf
  %574 = icmp slt i32 %564, 0
  store i1 %574, i1* %sf
  %575 = trunc i32 %564 to i8
  %576 = call i8 @llvm.ctpop.i8(i8 %575)
  %577 = and i8 %576, 1
  %578 = icmp eq i8 %577, 0
  store i1 %578, i1* %pf
  store volatile i64 45437, i64* @assembly_address
  %579 = load i32* %2
  %580 = load i32* %1
  %581 = icmp sle i32 %579, %580
  br i1 %581, label %block_b135, label %block_b17f

block_b17f:                                       ; preds = %block_b179
  store volatile i64 45439, i64* @assembly_address
  %582 = load i64* %stack_var_-24
  %583 = load i1* %of
  %584 = lshr i64 %582, 3
  %585 = icmp eq i64 %584, 0
  store i1 %585, i1* %zf
  %586 = icmp slt i64 %584, 0
  store i1 %586, i1* %sf
  %587 = trunc i64 %584 to i8
  %588 = call i8 @llvm.ctpop.i8(i8 %587)
  %589 = and i8 %588, 1
  %590 = icmp eq i8 %589, 0
  store i1 %590, i1* %pf
  store i64 %584, i64* %stack_var_-24
  %591 = and i64 4, %582
  %592 = icmp ne i64 %591, 0
  store i1 %592, i1* %cf
  %593 = icmp slt i64 %582, 0
  %594 = select i1 false, i1 %593, i1 %583
  store i1 %594, i1* %of
  store volatile i64 45444, i64* @assembly_address
  %595 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %596 = zext i32 %595 to i64
  store i64 %596, i64* %rax
  store volatile i64 45450, i64* @assembly_address
  %597 = load i64* %rax
  %598 = trunc i64 %597 to i32
  %599 = load i1* %of
  %600 = lshr i32 %598, 1
  %601 = icmp eq i32 %600, 0
  store i1 %601, i1* %zf
  %602 = icmp slt i32 %600, 0
  store i1 %602, i1* %sf
  %603 = trunc i32 %600 to i8
  %604 = call i8 @llvm.ctpop.i8(i8 %603)
  %605 = and i8 %604, 1
  %606 = icmp eq i8 %605, 0
  store i1 %606, i1* %pf
  %607 = zext i32 %600 to i64
  store i64 %607, i64* %rax
  %608 = and i32 1, %598
  %609 = icmp ne i32 %608, 0
  store i1 %609, i1* %cf
  %610 = icmp slt i32 %598, 0
  %611 = select i1 true, i1 %610, i1 %599
  store i1 %611, i1* %of
  store volatile i64 45452, i64* @assembly_address
  %612 = load i64* %rax
  %613 = trunc i64 %612 to i32
  %614 = zext i32 %613 to i64
  store i64 %614, i64* %rdx
  store volatile i64 45454, i64* @assembly_address
  %615 = load i32* bitcast (i64* @global_var_219ec4 to i32*)
  %616 = zext i32 %615 to i64
  store i64 %616, i64* %rax
  store volatile i64 45460, i64* @assembly_address
  %617 = load i64* %rdx
  %618 = trunc i64 %617 to i32
  %619 = load i64* %rax
  %620 = trunc i64 %619 to i32
  %621 = sub i32 %618, %620
  %622 = and i32 %618, 15
  %623 = and i32 %620, 15
  %624 = sub i32 %622, %623
  %625 = icmp ugt i32 %624, 15
  %626 = icmp ult i32 %618, %620
  %627 = xor i32 %618, %620
  %628 = xor i32 %618, %621
  %629 = and i32 %627, %628
  %630 = icmp slt i32 %629, 0
  store i1 %625, i1* %az
  store i1 %626, i1* %cf
  store i1 %630, i1* %of
  %631 = icmp eq i32 %621, 0
  store i1 %631, i1* %zf
  %632 = icmp slt i32 %621, 0
  store i1 %632, i1* %sf
  %633 = trunc i32 %621 to i8
  %634 = call i8 @llvm.ctpop.i8(i8 %633)
  %635 = and i8 %634, 1
  %636 = icmp eq i8 %635, 0
  store i1 %636, i1* %pf
  store volatile i64 45462, i64* @assembly_address
  %637 = load i1* %cf
  %638 = load i1* %zf
  %639 = or i1 %637, %638
  br i1 %639, label %block_b1ac, label %block_b198

block_b198:                                       ; preds = %block_b17f
  store volatile i64 45464, i64* @assembly_address
  %640 = load i64* %stack_var_-16
  store i64 %640, i64* %rax
  store volatile i64 45468, i64* @assembly_address
  %641 = load i64* %rax
  %642 = load i1* %of
  %643 = lshr i64 %641, 1
  %644 = icmp eq i64 %643, 0
  store i1 %644, i1* %zf
  %645 = icmp slt i64 %643, 0
  store i1 %645, i1* %sf
  %646 = trunc i64 %643 to i8
  %647 = call i8 @llvm.ctpop.i8(i8 %646)
  %648 = and i8 %647, 1
  %649 = icmp eq i8 %648, 0
  store i1 %649, i1* %pf
  store i64 %643, i64* %rax
  %650 = and i64 1, %641
  %651 = icmp ne i64 %650, 0
  store i1 %651, i1* %cf
  %652 = icmp slt i64 %641, 0
  %653 = select i1 true, i1 %652, i1 %642
  store i1 %653, i1* %of
  store volatile i64 45471, i64* @assembly_address
  %654 = load i64* %stack_var_-24
  %655 = load i64* %rax
  %656 = sub i64 %654, %655
  %657 = and i64 %654, 15
  %658 = and i64 %655, 15
  %659 = sub i64 %657, %658
  %660 = icmp ugt i64 %659, 15
  %661 = icmp ult i64 %654, %655
  %662 = xor i64 %654, %655
  %663 = xor i64 %654, %656
  %664 = and i64 %662, %663
  %665 = icmp slt i64 %664, 0
  store i1 %660, i1* %az
  store i1 %661, i1* %cf
  store i1 %665, i1* %of
  %666 = icmp eq i64 %656, 0
  store i1 %666, i1* %zf
  %667 = icmp slt i64 %656, 0
  store i1 %667, i1* %sf
  %668 = trunc i64 %656 to i8
  %669 = call i8 @llvm.ctpop.i8(i8 %668)
  %670 = and i8 %669, 1
  %671 = icmp eq i8 %670, 0
  store i1 %671, i1* %pf
  store volatile i64 45475, i64* @assembly_address
  %672 = load i1* %cf
  %673 = icmp eq i1 %672, false
  br i1 %673, label %block_b1ac, label %block_b1a5

block_b1a5:                                       ; preds = %block_b198
  store volatile i64 45477, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 45482, i64* @assembly_address
  br label %block_b1d2

block_b1ac:                                       ; preds = %block_b198, %block_b17f, %block_b0f0, %block_b0e1
  store volatile i64 45484, i64* @assembly_address
  %674 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %675 = zext i32 %674 to i64
  store i64 %675, i64* %rax
  store volatile i64 45490, i64* @assembly_address
  %676 = load i64* %rax
  %677 = trunc i64 %676 to i32
  %678 = sub i32 %677, 32767
  %679 = and i32 %677, 15
  %680 = sub i32 %679, 15
  %681 = icmp ugt i32 %680, 15
  %682 = icmp ult i32 %677, 32767
  %683 = xor i32 %677, 32767
  %684 = xor i32 %677, %678
  %685 = and i32 %683, %684
  %686 = icmp slt i32 %685, 0
  store i1 %681, i1* %az
  store i1 %682, i1* %cf
  store i1 %686, i1* %of
  %687 = icmp eq i32 %678, 0
  store i1 %687, i1* %zf
  %688 = icmp slt i32 %678, 0
  store i1 %688, i1* %sf
  %689 = trunc i32 %678 to i8
  %690 = call i8 @llvm.ctpop.i8(i8 %689)
  %691 = and i8 %690, 1
  %692 = icmp eq i8 %691, 0
  store i1 %692, i1* %pf
  store volatile i64 45495, i64* @assembly_address
  %693 = load i1* %zf
  br i1 %693, label %block_b1c6, label %block_b1b9

block_b1b9:                                       ; preds = %block_b1ac
  store volatile i64 45497, i64* @assembly_address
  %694 = load i32* bitcast (i64* @global_var_219ec4 to i32*)
  %695 = zext i32 %694 to i64
  store i64 %695, i64* %rax
  store volatile i64 45503, i64* @assembly_address
  %696 = load i64* %rax
  %697 = trunc i64 %696 to i32
  %698 = sub i32 %697, 32768
  %699 = and i32 %697, 15
  %700 = icmp ugt i32 %699, 15
  %701 = icmp ult i32 %697, 32768
  %702 = xor i32 %697, 32768
  %703 = xor i32 %697, %698
  %704 = and i32 %702, %703
  %705 = icmp slt i32 %704, 0
  store i1 %700, i1* %az
  store i1 %701, i1* %cf
  store i1 %705, i1* %of
  %706 = icmp eq i32 %698, 0
  store i1 %706, i1* %zf
  %707 = icmp slt i32 %698, 0
  store i1 %707, i1* %sf
  %708 = trunc i32 %698 to i8
  %709 = call i8 @llvm.ctpop.i8(i8 %708)
  %710 = and i8 %709, 1
  %711 = icmp eq i8 %710, 0
  store i1 %711, i1* %pf
  store volatile i64 45508, i64* @assembly_address
  %712 = load i1* %zf
  %713 = icmp eq i1 %712, false
  br i1 %713, label %block_b1cd, label %block_b1c6

block_b1c6:                                       ; preds = %block_b1b9, %block_b1ac
  store volatile i64 45510, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 45515, i64* @assembly_address
  br label %block_b1d2

block_b1cd:                                       ; preds = %block_b1b9
  store volatile i64 45517, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_b1d2

block_b1d2:                                       ; preds = %block_b1cd, %block_b1c6, %block_b1a5
  store volatile i64 45522, i64* @assembly_address
  %714 = load i64* %stack_var_-8
  store i64 %714, i64* %rbp
  %715 = ptrtoint i64* %stack_var_0 to i64
  store i64 %715, i64* %rsp
  store volatile i64 45523, i64* @assembly_address
  %716 = load i64* %rax
  ret i64 %716
}

declare i64 @216(i64, i32)

declare i64 @217(i64, i64)

define i64 @compress_block(i16* %arg1, i64* %arg2) {
block_b1d4:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = bitcast i16* %arg1 to i64*
  %2 = ptrtoint i64* %1 to i64
  store i64 %2, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-24 = alloca i32
  %stack_var_-37 = alloca i32
  %3 = alloca i8
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-48 = alloca i16*
  %4 = alloca i64
  %stack_var_-56 = alloca i16*
  %5 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 45524, i64* @assembly_address
  %6 = load i64* %rbp
  store i64 %6, i64* %stack_var_-8
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rsp
  store volatile i64 45525, i64* @assembly_address
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rbp
  store volatile i64 45528, i64* @assembly_address
  %9 = load i64* %rsp
  %10 = sub i64 %9, 48
  %11 = and i64 %9, 15
  %12 = icmp ugt i64 %11, 15
  %13 = icmp ult i64 %9, 48
  %14 = xor i64 %9, 48
  %15 = xor i64 %9, %10
  %16 = and i64 %14, %15
  %17 = icmp slt i64 %16, 0
  store i1 %12, i1* %az
  store i1 %13, i1* %cf
  store i1 %17, i1* %of
  %18 = icmp eq i64 %10, 0
  store i1 %18, i1* %zf
  %19 = icmp slt i64 %10, 0
  store i1 %19, i1* %sf
  %20 = trunc i64 %10 to i8
  %21 = call i8 @llvm.ctpop.i8(i8 %20)
  %22 = and i8 %21, 1
  %23 = icmp eq i8 %22, 0
  store i1 %23, i1* %pf
  %24 = ptrtoint i16** %stack_var_-56 to i64
  store i64 %24, i64* %rsp
  store volatile i64 45532, i64* @assembly_address
  %25 = load i64* %rdi
  %26 = inttoptr i64 %25 to i16*
  store i16* %26, i16** %stack_var_-48
  store volatile i64 45536, i64* @assembly_address
  %27 = load i64* %rsi
  %28 = inttoptr i64 %27 to i16*
  store i16* %28, i16** %stack_var_-56
  store volatile i64 45540, i64* @assembly_address
  store i32 0, i32* %stack_var_-36
  store volatile i64 45547, i64* @assembly_address
  store i32 0, i32* %stack_var_-32
  store volatile i64 45554, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 45561, i64* @assembly_address
  %29 = sext i8 0 to i32
  store i32 %29, i32* %stack_var_-37
  store volatile i64 45565, i64* @assembly_address
  %30 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %31 = zext i32 %30 to i64
  store i64 %31, i64* %rax
  store volatile i64 45571, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i32
  %34 = load i64* %rax
  %35 = trunc i64 %34 to i32
  %36 = and i32 %33, %35
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %37 = icmp eq i32 %36, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i32 %36, 0
  store i1 %38, i1* %sf
  %39 = trunc i32 %36 to i8
  %40 = call i8 @llvm.ctpop.i8(i8 %39)
  %41 = and i8 %40, 1
  %42 = icmp eq i8 %41, 0
  store i1 %42, i1* %pf
  store volatile i64 45573, i64* @assembly_address
  %43 = load i1* %zf
  br i1 %43, label %block_b427, label %block_b20b

block_b20b:                                       ; preds = %block_b415, %block_b1d4
  store volatile i64 45579, i64* @assembly_address
  %44 = load i32* %stack_var_-36
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 45582, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = and i32 %47, 7
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %49 = icmp eq i32 %48, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i32 %48, 0
  store i1 %50, i1* %sf
  %51 = trunc i32 %48 to i8
  %52 = call i8 @llvm.ctpop.i8(i8 %51)
  %53 = and i8 %52, 1
  %54 = icmp eq i8 %53, 0
  store i1 %54, i1* %pf
  %55 = zext i32 %48 to i64
  store i64 %55, i64* %rax
  store volatile i64 45585, i64* @assembly_address
  %56 = load i64* %rax
  %57 = trunc i64 %56 to i32
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i32
  %60 = and i32 %57, %59
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %61 = icmp eq i32 %60, 0
  store i1 %61, i1* %zf
  %62 = icmp slt i32 %60, 0
  store i1 %62, i1* %sf
  %63 = trunc i32 %60 to i8
  %64 = call i8 @llvm.ctpop.i8(i8 %63)
  %65 = and i8 %64, 1
  %66 = icmp eq i8 %65, 0
  store i1 %66, i1* %pf
  store volatile i64 45587, i64* @assembly_address
  %67 = load i1* %zf
  %68 = icmp eq i1 %67, false
  br i1 %68, label %block_b22e, label %block_b215

block_b215:                                       ; preds = %block_b20b
  store volatile i64 45589, i64* @assembly_address
  %69 = load i32* %stack_var_-28
  %70 = zext i32 %69 to i64
  store i64 %70, i64* %rax
  store volatile i64 45592, i64* @assembly_address
  %71 = load i64* %rax
  %72 = add i64 %71, 1
  %73 = trunc i64 %72 to i32
  %74 = zext i32 %73 to i64
  store i64 %74, i64* %rdx
  store volatile i64 45595, i64* @assembly_address
  %75 = load i64* %rdx
  %76 = trunc i64 %75 to i32
  store i32 %76, i32* %stack_var_-28
  store volatile i64 45598, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i32
  %79 = zext i32 %78 to i64
  store i64 %79, i64* %rdx
  store volatile i64 45600, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218ec0 to i64), i64* %rax
  store volatile i64 45607, i64* @assembly_address
  %80 = load i64* %rdx
  %81 = load i64* %rax
  %82 = mul i64 %81, 1
  %83 = add i64 %80, %82
  %84 = inttoptr i64 %83 to i8*
  %85 = load i8* %84
  %86 = zext i8 %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 45611, i64* @assembly_address
  %87 = load i64* %rax
  %88 = trunc i64 %87 to i8
  %89 = sext i8 %88 to i32
  store i32 %89, i32* %stack_var_-37
  br label %block_b22e

block_b22e:                                       ; preds = %block_b215, %block_b20b
  store volatile i64 45614, i64* @assembly_address
  %90 = load i32* %stack_var_-36
  %91 = zext i32 %90 to i64
  store i64 %91, i64* %rax
  store volatile i64 45617, i64* @assembly_address
  %92 = load i64* %rax
  %93 = add i64 %92, 1
  %94 = trunc i64 %93 to i32
  %95 = zext i32 %94 to i64
  store i64 %95, i64* %rdx
  store volatile i64 45620, i64* @assembly_address
  %96 = load i64* %rdx
  %97 = trunc i64 %96 to i32
  store i32 %97, i32* %stack_var_-36
  store volatile i64 45623, i64* @assembly_address
  %98 = load i64* %rax
  %99 = trunc i64 %98 to i32
  %100 = zext i32 %99 to i64
  store i64 %100, i64* %rdx
  store volatile i64 45625, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 45632, i64* @assembly_address
  %101 = load i64* %rdx
  %102 = load i64* %rax
  %103 = mul i64 %102, 1
  %104 = add i64 %101, %103
  %105 = inttoptr i64 %104 to i8*
  %106 = load i8* %105
  %107 = zext i8 %106 to i64
  store i64 %107, i64* %rax
  store volatile i64 45636, i64* @assembly_address
  %108 = load i64* %rax
  %109 = trunc i64 %108 to i8
  %110 = zext i8 %109 to i64
  store i64 %110, i64* %rax
  store volatile i64 45639, i64* @assembly_address
  %111 = load i64* %rax
  %112 = trunc i64 %111 to i32
  store i32 %112, i32* %stack_var_-24
  store volatile i64 45642, i64* @assembly_address
  %113 = load i32* %stack_var_-37
  %114 = trunc i32 %113 to i8
  %115 = zext i8 %114 to i64
  store i64 %115, i64* %rax
  store volatile i64 45646, i64* @assembly_address
  %116 = load i64* %rax
  %117 = trunc i64 %116 to i32
  %118 = and i32 %117, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %119 = icmp eq i32 %118, 0
  store i1 %119, i1* %zf
  %120 = icmp slt i32 %118, 0
  store i1 %120, i1* %sf
  %121 = trunc i32 %118 to i8
  %122 = call i8 @llvm.ctpop.i8(i8 %121)
  %123 = and i8 %122, 1
  %124 = icmp eq i8 %123, 0
  store i1 %124, i1* %pf
  %125 = zext i32 %118 to i64
  store i64 %125, i64* %rax
  store volatile i64 45649, i64* @assembly_address
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i32
  %128 = load i64* %rax
  %129 = trunc i64 %128 to i32
  %130 = and i32 %127, %129
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %131 = icmp eq i32 %130, 0
  store i1 %131, i1* %zf
  %132 = icmp slt i32 %130, 0
  store i1 %132, i1* %sf
  %133 = trunc i32 %130 to i8
  %134 = call i8 @llvm.ctpop.i8(i8 %133)
  %135 = and i8 %134, 1
  %136 = icmp eq i8 %135, 0
  store i1 %136, i1* %pf
  store volatile i64 45651, i64* @assembly_address
  %137 = load i1* %zf
  %138 = icmp eq i1 %137, false
  br i1 %138, label %block_b298, label %block_b255

block_b255:                                       ; preds = %block_b22e
  store volatile i64 45653, i64* @assembly_address
  %139 = load i32* %stack_var_-24
  %140 = zext i32 %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 45656, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  %143 = sext i32 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 45658, i64* @assembly_address
  %144 = load i64* %rax
  %145 = mul i64 %144, 4
  store i64 %145, i64* %rdx
  store volatile i64 45666, i64* @assembly_address
  %146 = load i16** %stack_var_-48
  %147 = ptrtoint i16* %146 to i64
  store i64 %147, i64* %rax
  store volatile i64 45670, i64* @assembly_address
  %148 = load i64* %rax
  %149 = load i64* %rdx
  %150 = add i64 %148, %149
  %151 = and i64 %148, 15
  %152 = and i64 %149, 15
  %153 = add i64 %151, %152
  %154 = icmp ugt i64 %153, 15
  %155 = icmp ult i64 %150, %148
  %156 = xor i64 %148, %150
  %157 = xor i64 %149, %150
  %158 = and i64 %156, %157
  %159 = icmp slt i64 %158, 0
  store i1 %154, i1* %az
  store i1 %155, i1* %cf
  store i1 %159, i1* %of
  %160 = icmp eq i64 %150, 0
  store i1 %160, i1* %zf
  %161 = icmp slt i64 %150, 0
  store i1 %161, i1* %sf
  %162 = trunc i64 %150 to i8
  %163 = call i8 @llvm.ctpop.i8(i8 %162)
  %164 = and i8 %163, 1
  %165 = icmp eq i8 %164, 0
  store i1 %165, i1* %pf
  store i64 %150, i64* %rax
  store volatile i64 45673, i64* @assembly_address
  %166 = load i64* %rax
  %167 = add i64 %166, 2
  %168 = inttoptr i64 %167 to i16*
  %169 = load i16* %168
  %170 = zext i16 %169 to i64
  store i64 %170, i64* %rax
  store volatile i64 45677, i64* @assembly_address
  %171 = load i64* %rax
  %172 = trunc i64 %171 to i16
  %173 = zext i16 %172 to i64
  store i64 %173, i64* %rdx
  store volatile i64 45680, i64* @assembly_address
  %174 = load i32* %stack_var_-24
  %175 = zext i32 %174 to i64
  store i64 %175, i64* %rax
  store volatile i64 45683, i64* @assembly_address
  %176 = load i64* %rax
  %177 = trunc i64 %176 to i32
  %178 = sext i32 %177 to i64
  store i64 %178, i64* %rax
  store volatile i64 45685, i64* @assembly_address
  %179 = load i64* %rax
  %180 = mul i64 %179, 4
  store i64 %180, i64* %rcx
  store volatile i64 45693, i64* @assembly_address
  %181 = load i16** %stack_var_-48
  %182 = ptrtoint i16* %181 to i64
  store i64 %182, i64* %rax
  store volatile i64 45697, i64* @assembly_address
  %183 = load i64* %rax
  %184 = load i64* %rcx
  %185 = add i64 %183, %184
  %186 = and i64 %183, 15
  %187 = and i64 %184, 15
  %188 = add i64 %186, %187
  %189 = icmp ugt i64 %188, 15
  %190 = icmp ult i64 %185, %183
  %191 = xor i64 %183, %185
  %192 = xor i64 %184, %185
  %193 = and i64 %191, %192
  %194 = icmp slt i64 %193, 0
  store i1 %189, i1* %az
  store i1 %190, i1* %cf
  store i1 %194, i1* %of
  %195 = icmp eq i64 %185, 0
  store i1 %195, i1* %zf
  %196 = icmp slt i64 %185, 0
  store i1 %196, i1* %sf
  %197 = trunc i64 %185 to i8
  %198 = call i8 @llvm.ctpop.i8(i8 %197)
  %199 = and i8 %198, 1
  %200 = icmp eq i8 %199, 0
  store i1 %200, i1* %pf
  store i64 %185, i64* %rax
  store volatile i64 45700, i64* @assembly_address
  %201 = load i64* %rax
  %202 = inttoptr i64 %201 to i16*
  %203 = load i16* %202
  %204 = zext i16 %203 to i64
  store i64 %204, i64* %rax
  store volatile i64 45703, i64* @assembly_address
  %205 = load i64* %rax
  %206 = trunc i64 %205 to i16
  %207 = zext i16 %206 to i64
  store i64 %207, i64* %rax
  store volatile i64 45706, i64* @assembly_address
  %208 = load i64* %rdx
  %209 = trunc i64 %208 to i32
  %210 = zext i32 %209 to i64
  store i64 %210, i64* %rsi
  store volatile i64 45708, i64* @assembly_address
  %211 = load i64* %rax
  %212 = trunc i64 %211 to i32
  %213 = zext i32 %212 to i64
  store i64 %213, i64* %rdi
  store volatile i64 45710, i64* @assembly_address
  %214 = load i64* %rdi
  %215 = load i64* %rsi
  %216 = trunc i64 %214 to i32
  %217 = call i64 @send_bits(i32 %216, i64 %215)
  store i64 %217, i64* %rax
  store i64 %217, i64* %rax
  store volatile i64 45715, i64* @assembly_address
  br label %block_b415

block_b298:                                       ; preds = %block_b22e
  store volatile i64 45720, i64* @assembly_address
  %218 = load i32* %stack_var_-24
  %219 = zext i32 %218 to i64
  store i64 %219, i64* %rax
  store volatile i64 45723, i64* @assembly_address
  %220 = load i64* %rax
  %221 = trunc i64 %220 to i32
  %222 = sext i32 %221 to i64
  store i64 %222, i64* %rdx
  store volatile i64 45726, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218ac0 to i64), i64* %rax
  store volatile i64 45733, i64* @assembly_address
  %223 = load i64* %rdx
  %224 = load i64* %rax
  %225 = mul i64 %224, 1
  %226 = add i64 %223, %225
  %227 = inttoptr i64 %226 to i8*
  %228 = load i8* %227
  %229 = zext i8 %228 to i64
  store i64 %229, i64* %rax
  store volatile i64 45737, i64* @assembly_address
  %230 = load i64* %rax
  %231 = trunc i64 %230 to i8
  %232 = zext i8 %231 to i64
  store i64 %232, i64* %rax
  store volatile i64 45740, i64* @assembly_address
  %233 = load i64* %rax
  %234 = trunc i64 %233 to i32
  store i32 %234, i32* %stack_var_-20
  store volatile i64 45743, i64* @assembly_address
  %235 = load i32* %stack_var_-20
  %236 = zext i32 %235 to i64
  store i64 %236, i64* %rax
  store volatile i64 45746, i64* @assembly_address
  %237 = load i64* %rax
  %238 = trunc i64 %237 to i32
  %239 = add i32 %238, 257
  %240 = and i32 %238, 15
  %241 = add i32 %240, 1
  %242 = icmp ugt i32 %241, 15
  %243 = icmp ult i32 %239, %238
  %244 = xor i32 %238, %239
  %245 = xor i32 257, %239
  %246 = and i32 %244, %245
  %247 = icmp slt i32 %246, 0
  store i1 %242, i1* %az
  store i1 %243, i1* %cf
  store i1 %247, i1* %of
  %248 = icmp eq i32 %239, 0
  store i1 %248, i1* %zf
  %249 = icmp slt i32 %239, 0
  store i1 %249, i1* %sf
  %250 = trunc i32 %239 to i8
  %251 = call i8 @llvm.ctpop.i8(i8 %250)
  %252 = and i8 %251, 1
  %253 = icmp eq i8 %252, 0
  store i1 %253, i1* %pf
  %254 = zext i32 %239 to i64
  store i64 %254, i64* %rax
  store volatile i64 45751, i64* @assembly_address
  %255 = load i64* %rax
  %256 = trunc i64 %255 to i32
  %257 = zext i32 %256 to i64
  store i64 %257, i64* %rax
  store volatile i64 45753, i64* @assembly_address
  %258 = load i64* %rax
  %259 = mul i64 %258, 4
  store i64 %259, i64* %rdx
  store volatile i64 45761, i64* @assembly_address
  %260 = load i16** %stack_var_-48
  %261 = ptrtoint i16* %260 to i64
  store i64 %261, i64* %rax
  store volatile i64 45765, i64* @assembly_address
  %262 = load i64* %rax
  %263 = load i64* %rdx
  %264 = add i64 %262, %263
  %265 = and i64 %262, 15
  %266 = and i64 %263, 15
  %267 = add i64 %265, %266
  %268 = icmp ugt i64 %267, 15
  %269 = icmp ult i64 %264, %262
  %270 = xor i64 %262, %264
  %271 = xor i64 %263, %264
  %272 = and i64 %270, %271
  %273 = icmp slt i64 %272, 0
  store i1 %268, i1* %az
  store i1 %269, i1* %cf
  store i1 %273, i1* %of
  %274 = icmp eq i64 %264, 0
  store i1 %274, i1* %zf
  %275 = icmp slt i64 %264, 0
  store i1 %275, i1* %sf
  %276 = trunc i64 %264 to i8
  %277 = call i8 @llvm.ctpop.i8(i8 %276)
  %278 = and i8 %277, 1
  %279 = icmp eq i8 %278, 0
  store i1 %279, i1* %pf
  store i64 %264, i64* %rax
  store volatile i64 45768, i64* @assembly_address
  %280 = load i64* %rax
  %281 = add i64 %280, 2
  %282 = inttoptr i64 %281 to i16*
  %283 = load i16* %282
  %284 = zext i16 %283 to i64
  store i64 %284, i64* %rax
  store volatile i64 45772, i64* @assembly_address
  %285 = load i64* %rax
  %286 = trunc i64 %285 to i16
  %287 = zext i16 %286 to i64
  store i64 %287, i64* %rdx
  store volatile i64 45775, i64* @assembly_address
  %288 = load i32* %stack_var_-20
  %289 = zext i32 %288 to i64
  store i64 %289, i64* %rax
  store volatile i64 45778, i64* @assembly_address
  %290 = load i64* %rax
  %291 = trunc i64 %290 to i32
  %292 = add i32 %291, 257
  %293 = and i32 %291, 15
  %294 = add i32 %293, 1
  %295 = icmp ugt i32 %294, 15
  %296 = icmp ult i32 %292, %291
  %297 = xor i32 %291, %292
  %298 = xor i32 257, %292
  %299 = and i32 %297, %298
  %300 = icmp slt i32 %299, 0
  store i1 %295, i1* %az
  store i1 %296, i1* %cf
  store i1 %300, i1* %of
  %301 = icmp eq i32 %292, 0
  store i1 %301, i1* %zf
  %302 = icmp slt i32 %292, 0
  store i1 %302, i1* %sf
  %303 = trunc i32 %292 to i8
  %304 = call i8 @llvm.ctpop.i8(i8 %303)
  %305 = and i8 %304, 1
  %306 = icmp eq i8 %305, 0
  store i1 %306, i1* %pf
  %307 = zext i32 %292 to i64
  store i64 %307, i64* %rax
  store volatile i64 45783, i64* @assembly_address
  %308 = load i64* %rax
  %309 = trunc i64 %308 to i32
  %310 = zext i32 %309 to i64
  store i64 %310, i64* %rax
  store volatile i64 45785, i64* @assembly_address
  %311 = load i64* %rax
  %312 = mul i64 %311, 4
  store i64 %312, i64* %rcx
  store volatile i64 45793, i64* @assembly_address
  %313 = load i16** %stack_var_-48
  %314 = ptrtoint i16* %313 to i64
  store i64 %314, i64* %rax
  store volatile i64 45797, i64* @assembly_address
  %315 = load i64* %rax
  %316 = load i64* %rcx
  %317 = add i64 %315, %316
  %318 = and i64 %315, 15
  %319 = and i64 %316, 15
  %320 = add i64 %318, %319
  %321 = icmp ugt i64 %320, 15
  %322 = icmp ult i64 %317, %315
  %323 = xor i64 %315, %317
  %324 = xor i64 %316, %317
  %325 = and i64 %323, %324
  %326 = icmp slt i64 %325, 0
  store i1 %321, i1* %az
  store i1 %322, i1* %cf
  store i1 %326, i1* %of
  %327 = icmp eq i64 %317, 0
  store i1 %327, i1* %zf
  %328 = icmp slt i64 %317, 0
  store i1 %328, i1* %sf
  %329 = trunc i64 %317 to i8
  %330 = call i8 @llvm.ctpop.i8(i8 %329)
  %331 = and i8 %330, 1
  %332 = icmp eq i8 %331, 0
  store i1 %332, i1* %pf
  store i64 %317, i64* %rax
  store volatile i64 45800, i64* @assembly_address
  %333 = load i64* %rax
  %334 = inttoptr i64 %333 to i16*
  %335 = load i16* %334
  %336 = zext i16 %335 to i64
  store i64 %336, i64* %rax
  store volatile i64 45803, i64* @assembly_address
  %337 = load i64* %rax
  %338 = trunc i64 %337 to i16
  %339 = zext i16 %338 to i64
  store i64 %339, i64* %rax
  store volatile i64 45806, i64* @assembly_address
  %340 = load i64* %rdx
  %341 = trunc i64 %340 to i32
  %342 = zext i32 %341 to i64
  store i64 %342, i64* %rsi
  store volatile i64 45808, i64* @assembly_address
  %343 = load i64* %rax
  %344 = trunc i64 %343 to i32
  %345 = zext i32 %344 to i64
  store i64 %345, i64* %rdi
  store volatile i64 45810, i64* @assembly_address
  %346 = load i64* %rdi
  %347 = load i64* %rsi
  %348 = trunc i64 %346 to i32
  %349 = call i64 @send_bits(i32 %348, i64 %347)
  store i64 %349, i64* %rax
  store i64 %349, i64* %rax
  store volatile i64 45815, i64* @assembly_address
  %350 = load i32* %stack_var_-20
  %351 = zext i32 %350 to i64
  store i64 %351, i64* %rax
  store volatile i64 45818, i64* @assembly_address
  %352 = load i64* %rax
  %353 = mul i64 %352, 4
  store i64 %353, i64* %rdx
  store volatile i64 45826, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216320 to i64), i64* %rax
  store volatile i64 45833, i64* @assembly_address
  %354 = load i64* %rdx
  %355 = load i64* %rax
  %356 = mul i64 %355, 1
  %357 = add i64 %354, %356
  %358 = inttoptr i64 %357 to i32*
  %359 = load i32* %358
  %360 = zext i32 %359 to i64
  store i64 %360, i64* %rax
  store volatile i64 45836, i64* @assembly_address
  %361 = load i64* %rax
  %362 = trunc i64 %361 to i32
  store i32 %362, i32* %stack_var_-16
  store volatile i64 45839, i64* @assembly_address
  %363 = load i32* %stack_var_-16
  %364 = and i32 %363, 15
  %365 = icmp ugt i32 %364, 15
  %366 = icmp ult i32 %363, 0
  %367 = xor i32 %363, 0
  %368 = and i32 %367, 0
  %369 = icmp slt i32 %368, 0
  store i1 %365, i1* %az
  store i1 %366, i1* %cf
  store i1 %369, i1* %of
  %370 = icmp eq i32 %363, 0
  store i1 %370, i1* %zf
  %371 = icmp slt i32 %363, 0
  store i1 %371, i1* %sf
  %372 = trunc i32 %363 to i8
  %373 = call i8 @llvm.ctpop.i8(i8 %372)
  %374 = and i8 %373, 1
  %375 = icmp eq i8 %374, 0
  store i1 %375, i1* %pf
  store volatile i64 45843, i64* @assembly_address
  %376 = load i1* %zf
  br i1 %376, label %block_b33c, label %block_b315

block_b315:                                       ; preds = %block_b298
  store volatile i64 45845, i64* @assembly_address
  %377 = load i32* %stack_var_-20
  %378 = zext i32 %377 to i64
  store i64 %378, i64* %rax
  store volatile i64 45848, i64* @assembly_address
  %379 = load i64* %rax
  %380 = mul i64 %379, 4
  store i64 %380, i64* %rdx
  store volatile i64 45856, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218dc0 to i64), i64* %rax
  store volatile i64 45863, i64* @assembly_address
  %381 = load i64* %rdx
  %382 = load i64* %rax
  %383 = mul i64 %382, 1
  %384 = add i64 %381, %383
  %385 = inttoptr i64 %384 to i32*
  %386 = load i32* %385
  %387 = zext i32 %386 to i64
  store i64 %387, i64* %rax
  store volatile i64 45866, i64* @assembly_address
  %388 = load i32* %stack_var_-24
  %389 = load i64* %rax
  %390 = trunc i64 %389 to i32
  %391 = sub i32 %388, %390
  %392 = and i32 %388, 15
  %393 = and i32 %390, 15
  %394 = sub i32 %392, %393
  %395 = icmp ugt i32 %394, 15
  %396 = icmp ult i32 %388, %390
  %397 = xor i32 %388, %390
  %398 = xor i32 %388, %391
  %399 = and i32 %397, %398
  %400 = icmp slt i32 %399, 0
  store i1 %395, i1* %az
  store i1 %396, i1* %cf
  store i1 %400, i1* %of
  %401 = icmp eq i32 %391, 0
  store i1 %401, i1* %zf
  %402 = icmp slt i32 %391, 0
  store i1 %402, i1* %sf
  %403 = trunc i32 %391 to i8
  %404 = call i8 @llvm.ctpop.i8(i8 %403)
  %405 = and i8 %404, 1
  %406 = icmp eq i8 %405, 0
  store i1 %406, i1* %pf
  store i32 %391, i32* %stack_var_-24
  store volatile i64 45869, i64* @assembly_address
  %407 = load i32* %stack_var_-16
  %408 = zext i32 %407 to i64
  store i64 %408, i64* %rdx
  store volatile i64 45872, i64* @assembly_address
  %409 = load i32* %stack_var_-24
  %410 = zext i32 %409 to i64
  store i64 %410, i64* %rax
  store volatile i64 45875, i64* @assembly_address
  %411 = load i64* %rdx
  %412 = trunc i64 %411 to i32
  %413 = zext i32 %412 to i64
  store i64 %413, i64* %rsi
  store volatile i64 45877, i64* @assembly_address
  %414 = load i64* %rax
  %415 = trunc i64 %414 to i32
  %416 = zext i32 %415 to i64
  store i64 %416, i64* %rdi
  store volatile i64 45879, i64* @assembly_address
  %417 = load i64* %rdi
  %418 = load i64* %rsi
  %419 = trunc i64 %417 to i32
  %420 = call i64 @send_bits(i32 %419, i64 %418)
  store i64 %420, i64* %rax
  store i64 %420, i64* %rax
  br label %block_b33c

block_b33c:                                       ; preds = %block_b315, %block_b298
  store volatile i64 45884, i64* @assembly_address
  %421 = load i32* %stack_var_-32
  %422 = zext i32 %421 to i64
  store i64 %422, i64* %rax
  store volatile i64 45887, i64* @assembly_address
  %423 = load i64* %rax
  %424 = add i64 %423, 1
  %425 = trunc i64 %424 to i32
  %426 = zext i32 %425 to i64
  store i64 %426, i64* %rdx
  store volatile i64 45890, i64* @assembly_address
  %427 = load i64* %rdx
  %428 = trunc i64 %427 to i32
  store i32 %428, i32* %stack_var_-32
  store volatile i64 45893, i64* @assembly_address
  %429 = load i64* %rax
  %430 = trunc i64 %429 to i32
  %431 = zext i32 %430 to i64
  store i64 %431, i64* %rax
  store volatile i64 45895, i64* @assembly_address
  %432 = load i64* %rax
  %433 = load i64* %rax
  %434 = mul i64 %433, 1
  %435 = add i64 %432, %434
  store i64 %435, i64* %rdx
  store volatile i64 45899, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_23a880 to i64), i64* %rax
  store volatile i64 45906, i64* @assembly_address
  %436 = load i64* %rdx
  %437 = load i64* %rax
  %438 = mul i64 %437, 1
  %439 = add i64 %436, %438
  %440 = inttoptr i64 %439 to i16*
  %441 = load i16* %440
  %442 = zext i16 %441 to i64
  store i64 %442, i64* %rax
  store volatile i64 45910, i64* @assembly_address
  %443 = load i64* %rax
  %444 = trunc i64 %443 to i16
  %445 = zext i16 %444 to i64
  store i64 %445, i64* %rax
  store volatile i64 45913, i64* @assembly_address
  %446 = load i64* %rax
  %447 = trunc i64 %446 to i32
  store i32 %447, i32* %stack_var_-12
  store volatile i64 45916, i64* @assembly_address
  %448 = load i32* %stack_var_-12
  %449 = sub i32 %448, 255
  %450 = and i32 %448, 15
  %451 = sub i32 %450, 15
  %452 = icmp ugt i32 %451, 15
  %453 = icmp ult i32 %448, 255
  %454 = xor i32 %448, 255
  %455 = xor i32 %448, %449
  %456 = and i32 %454, %455
  %457 = icmp slt i32 %456, 0
  store i1 %452, i1* %az
  store i1 %453, i1* %cf
  store i1 %457, i1* %of
  %458 = icmp eq i32 %449, 0
  store i1 %458, i1* %zf
  %459 = icmp slt i32 %449, 0
  store i1 %459, i1* %sf
  %460 = trunc i32 %449 to i8
  %461 = call i8 @llvm.ctpop.i8(i8 %460)
  %462 = and i8 %461, 1
  %463 = icmp eq i8 %462, 0
  store i1 %463, i1* %pf
  store volatile i64 45923, i64* @assembly_address
  %464 = load i1* %cf
  %465 = load i1* %zf
  %466 = or i1 %464, %465
  %467 = icmp ne i1 %466, true
  br i1 %467, label %block_b378, label %block_b365

block_b365:                                       ; preds = %block_b33c
  store volatile i64 45925, i64* @assembly_address
  %468 = load i32* %stack_var_-12
  %469 = zext i32 %468 to i64
  store i64 %469, i64* %rdx
  store volatile i64 45928, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218bc0 to i64), i64* %rax
  store volatile i64 45935, i64* @assembly_address
  %470 = load i64* %rdx
  %471 = load i64* %rax
  %472 = mul i64 %471, 1
  %473 = add i64 %470, %472
  %474 = inttoptr i64 %473 to i8*
  %475 = load i8* %474
  %476 = zext i8 %475 to i64
  store i64 %476, i64* %rax
  store volatile i64 45939, i64* @assembly_address
  %477 = load i64* %rax
  %478 = trunc i64 %477 to i8
  %479 = zext i8 %478 to i64
  store i64 %479, i64* %rax
  store volatile i64 45942, i64* @assembly_address
  br label %block_b393

block_b378:                                       ; preds = %block_b33c
  store volatile i64 45944, i64* @assembly_address
  %480 = load i32* %stack_var_-12
  %481 = zext i32 %480 to i64
  store i64 %481, i64* %rax
  store volatile i64 45947, i64* @assembly_address
  %482 = load i64* %rax
  %483 = trunc i64 %482 to i32
  %484 = load i1* %of
  %485 = lshr i32 %483, 7
  %486 = icmp eq i32 %485, 0
  store i1 %486, i1* %zf
  %487 = icmp slt i32 %485, 0
  store i1 %487, i1* %sf
  %488 = trunc i32 %485 to i8
  %489 = call i8 @llvm.ctpop.i8(i8 %488)
  %490 = and i8 %489, 1
  %491 = icmp eq i8 %490, 0
  store i1 %491, i1* %pf
  %492 = zext i32 %485 to i64
  store i64 %492, i64* %rax
  %493 = and i32 64, %483
  %494 = icmp ne i32 %493, 0
  store i1 %494, i1* %cf
  %495 = icmp slt i32 %483, 0
  %496 = select i1 false, i1 %495, i1 %484
  store i1 %496, i1* %of
  store volatile i64 45950, i64* @assembly_address
  %497 = load i64* %rax
  %498 = trunc i64 %497 to i32
  %499 = add i32 %498, 256
  %500 = and i32 %498, 15
  %501 = icmp ugt i32 %500, 15
  %502 = icmp ult i32 %499, %498
  %503 = xor i32 %498, %499
  %504 = xor i32 256, %499
  %505 = and i32 %503, %504
  %506 = icmp slt i32 %505, 0
  store i1 %501, i1* %az
  store i1 %502, i1* %cf
  store i1 %506, i1* %of
  %507 = icmp eq i32 %499, 0
  store i1 %507, i1* %zf
  %508 = icmp slt i32 %499, 0
  store i1 %508, i1* %sf
  %509 = trunc i32 %499 to i8
  %510 = call i8 @llvm.ctpop.i8(i8 %509)
  %511 = and i8 %510, 1
  %512 = icmp eq i8 %511, 0
  store i1 %512, i1* %pf
  %513 = zext i32 %499 to i64
  store i64 %513, i64* %rax
  store volatile i64 45955, i64* @assembly_address
  %514 = load i64* %rax
  %515 = trunc i64 %514 to i32
  %516 = zext i32 %515 to i64
  store i64 %516, i64* %rdx
  store volatile i64 45957, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218bc0 to i64), i64* %rax
  store volatile i64 45964, i64* @assembly_address
  %517 = load i64* %rdx
  %518 = load i64* %rax
  %519 = mul i64 %518, 1
  %520 = add i64 %517, %519
  %521 = inttoptr i64 %520 to i8*
  %522 = load i8* %521
  %523 = zext i8 %522 to i64
  store i64 %523, i64* %rax
  store volatile i64 45968, i64* @assembly_address
  %524 = load i64* %rax
  %525 = trunc i64 %524 to i8
  %526 = zext i8 %525 to i64
  store i64 %526, i64* %rax
  br label %block_b393

block_b393:                                       ; preds = %block_b378, %block_b365
  store volatile i64 45971, i64* @assembly_address
  %527 = load i64* %rax
  %528 = trunc i64 %527 to i32
  store i32 %528, i32* %stack_var_-20
  store volatile i64 45974, i64* @assembly_address
  %529 = load i32* %stack_var_-20
  %530 = zext i32 %529 to i64
  store i64 %530, i64* %rax
  store volatile i64 45977, i64* @assembly_address
  %531 = load i64* %rax
  %532 = mul i64 %531, 4
  store i64 %532, i64* %rdx
  store volatile i64 45985, i64* @assembly_address
  %533 = load i16** %stack_var_-56
  %534 = ptrtoint i16* %533 to i64
  store i64 %534, i64* %rax
  store volatile i64 45989, i64* @assembly_address
  %535 = load i64* %rax
  %536 = load i64* %rdx
  %537 = add i64 %535, %536
  %538 = and i64 %535, 15
  %539 = and i64 %536, 15
  %540 = add i64 %538, %539
  %541 = icmp ugt i64 %540, 15
  %542 = icmp ult i64 %537, %535
  %543 = xor i64 %535, %537
  %544 = xor i64 %536, %537
  %545 = and i64 %543, %544
  %546 = icmp slt i64 %545, 0
  store i1 %541, i1* %az
  store i1 %542, i1* %cf
  store i1 %546, i1* %of
  %547 = icmp eq i64 %537, 0
  store i1 %547, i1* %zf
  %548 = icmp slt i64 %537, 0
  store i1 %548, i1* %sf
  %549 = trunc i64 %537 to i8
  %550 = call i8 @llvm.ctpop.i8(i8 %549)
  %551 = and i8 %550, 1
  %552 = icmp eq i8 %551, 0
  store i1 %552, i1* %pf
  store i64 %537, i64* %rax
  store volatile i64 45992, i64* @assembly_address
  %553 = load i64* %rax
  %554 = add i64 %553, 2
  %555 = inttoptr i64 %554 to i16*
  %556 = load i16* %555
  %557 = zext i16 %556 to i64
  store i64 %557, i64* %rax
  store volatile i64 45996, i64* @assembly_address
  %558 = load i64* %rax
  %559 = trunc i64 %558 to i16
  %560 = zext i16 %559 to i64
  store i64 %560, i64* %rdx
  store volatile i64 45999, i64* @assembly_address
  %561 = load i32* %stack_var_-20
  %562 = zext i32 %561 to i64
  store i64 %562, i64* %rax
  store volatile i64 46002, i64* @assembly_address
  %563 = load i64* %rax
  %564 = mul i64 %563, 4
  store i64 %564, i64* %rcx
  store volatile i64 46010, i64* @assembly_address
  %565 = load i16** %stack_var_-56
  %566 = ptrtoint i16* %565 to i64
  store i64 %566, i64* %rax
  store volatile i64 46014, i64* @assembly_address
  %567 = load i64* %rax
  %568 = load i64* %rcx
  %569 = add i64 %567, %568
  %570 = and i64 %567, 15
  %571 = and i64 %568, 15
  %572 = add i64 %570, %571
  %573 = icmp ugt i64 %572, 15
  %574 = icmp ult i64 %569, %567
  %575 = xor i64 %567, %569
  %576 = xor i64 %568, %569
  %577 = and i64 %575, %576
  %578 = icmp slt i64 %577, 0
  store i1 %573, i1* %az
  store i1 %574, i1* %cf
  store i1 %578, i1* %of
  %579 = icmp eq i64 %569, 0
  store i1 %579, i1* %zf
  %580 = icmp slt i64 %569, 0
  store i1 %580, i1* %sf
  %581 = trunc i64 %569 to i8
  %582 = call i8 @llvm.ctpop.i8(i8 %581)
  %583 = and i8 %582, 1
  %584 = icmp eq i8 %583, 0
  store i1 %584, i1* %pf
  store i64 %569, i64* %rax
  store volatile i64 46017, i64* @assembly_address
  %585 = load i64* %rax
  %586 = inttoptr i64 %585 to i16*
  %587 = load i16* %586
  %588 = zext i16 %587 to i64
  store i64 %588, i64* %rax
  store volatile i64 46020, i64* @assembly_address
  %589 = load i64* %rax
  %590 = trunc i64 %589 to i16
  %591 = zext i16 %590 to i64
  store i64 %591, i64* %rax
  store volatile i64 46023, i64* @assembly_address
  %592 = load i64* %rdx
  %593 = trunc i64 %592 to i32
  %594 = zext i32 %593 to i64
  store i64 %594, i64* %rsi
  store volatile i64 46025, i64* @assembly_address
  %595 = load i64* %rax
  %596 = trunc i64 %595 to i32
  %597 = zext i32 %596 to i64
  store i64 %597, i64* %rdi
  store volatile i64 46027, i64* @assembly_address
  %598 = load i64* %rdi
  %599 = load i64* %rsi
  %600 = trunc i64 %598 to i32
  %601 = call i64 @send_bits(i32 %600, i64 %599)
  store i64 %601, i64* %rax
  store i64 %601, i64* %rax
  store volatile i64 46032, i64* @assembly_address
  %602 = load i32* %stack_var_-20
  %603 = zext i32 %602 to i64
  store i64 %603, i64* %rax
  store volatile i64 46035, i64* @assembly_address
  %604 = load i64* %rax
  %605 = mul i64 %604, 4
  store i64 %605, i64* %rdx
  store volatile i64 46043, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_2163a0 to i64), i64* %rax
  store volatile i64 46050, i64* @assembly_address
  %606 = load i64* %rdx
  %607 = load i64* %rax
  %608 = mul i64 %607, 1
  %609 = add i64 %606, %608
  %610 = inttoptr i64 %609 to i32*
  %611 = load i32* %610
  %612 = zext i32 %611 to i64
  store i64 %612, i64* %rax
  store volatile i64 46053, i64* @assembly_address
  %613 = load i64* %rax
  %614 = trunc i64 %613 to i32
  store i32 %614, i32* %stack_var_-16
  store volatile i64 46056, i64* @assembly_address
  %615 = load i32* %stack_var_-16
  %616 = and i32 %615, 15
  %617 = icmp ugt i32 %616, 15
  %618 = icmp ult i32 %615, 0
  %619 = xor i32 %615, 0
  %620 = and i32 %619, 0
  %621 = icmp slt i32 %620, 0
  store i1 %617, i1* %az
  store i1 %618, i1* %cf
  store i1 %621, i1* %of
  %622 = icmp eq i32 %615, 0
  store i1 %622, i1* %zf
  %623 = icmp slt i32 %615, 0
  store i1 %623, i1* %sf
  %624 = trunc i32 %615 to i8
  %625 = call i8 @llvm.ctpop.i8(i8 %624)
  %626 = and i8 %625, 1
  %627 = icmp eq i8 %626, 0
  store i1 %627, i1* %pf
  store volatile i64 46060, i64* @assembly_address
  %628 = load i1* %zf
  br i1 %628, label %block_b415, label %block_b3ee

block_b3ee:                                       ; preds = %block_b393
  store volatile i64 46062, i64* @assembly_address
  %629 = load i32* %stack_var_-20
  %630 = zext i32 %629 to i64
  store i64 %630, i64* %rax
  store volatile i64 46065, i64* @assembly_address
  %631 = load i64* %rax
  %632 = mul i64 %631, 4
  store i64 %632, i64* %rdx
  store volatile i64 46073, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_218e40 to i64), i64* %rax
  store volatile i64 46080, i64* @assembly_address
  %633 = load i64* %rdx
  %634 = load i64* %rax
  %635 = mul i64 %634, 1
  %636 = add i64 %633, %635
  %637 = inttoptr i64 %636 to i32*
  %638 = load i32* %637
  %639 = zext i32 %638 to i64
  store i64 %639, i64* %rax
  store volatile i64 46083, i64* @assembly_address
  %640 = load i32* %stack_var_-12
  %641 = load i64* %rax
  %642 = trunc i64 %641 to i32
  %643 = sub i32 %640, %642
  %644 = and i32 %640, 15
  %645 = and i32 %642, 15
  %646 = sub i32 %644, %645
  %647 = icmp ugt i32 %646, 15
  %648 = icmp ult i32 %640, %642
  %649 = xor i32 %640, %642
  %650 = xor i32 %640, %643
  %651 = and i32 %649, %650
  %652 = icmp slt i32 %651, 0
  store i1 %647, i1* %az
  store i1 %648, i1* %cf
  store i1 %652, i1* %of
  %653 = icmp eq i32 %643, 0
  store i1 %653, i1* %zf
  %654 = icmp slt i32 %643, 0
  store i1 %654, i1* %sf
  %655 = trunc i32 %643 to i8
  %656 = call i8 @llvm.ctpop.i8(i8 %655)
  %657 = and i8 %656, 1
  %658 = icmp eq i8 %657, 0
  store i1 %658, i1* %pf
  store i32 %643, i32* %stack_var_-12
  store volatile i64 46086, i64* @assembly_address
  %659 = load i32* %stack_var_-12
  %660 = zext i32 %659 to i64
  store i64 %660, i64* %rax
  store volatile i64 46089, i64* @assembly_address
  %661 = load i32* %stack_var_-16
  %662 = zext i32 %661 to i64
  store i64 %662, i64* %rdx
  store volatile i64 46092, i64* @assembly_address
  %663 = load i64* %rdx
  %664 = trunc i64 %663 to i32
  %665 = zext i32 %664 to i64
  store i64 %665, i64* %rsi
  store volatile i64 46094, i64* @assembly_address
  %666 = load i64* %rax
  %667 = trunc i64 %666 to i32
  %668 = zext i32 %667 to i64
  store i64 %668, i64* %rdi
  store volatile i64 46096, i64* @assembly_address
  %669 = load i64* %rdi
  %670 = load i64* %rsi
  %671 = trunc i64 %669 to i32
  %672 = call i64 @send_bits(i32 %671, i64 %670)
  store i64 %672, i64* %rax
  store i64 %672, i64* %rax
  br label %block_b415

block_b415:                                       ; preds = %block_b3ee, %block_b393, %block_b255
  store volatile i64 46101, i64* @assembly_address
  %673 = load i32* %stack_var_-37
  %674 = trunc i32 %673 to i8
  %675 = load i1* %of
  %676 = lshr i8 %674, 1
  %677 = icmp eq i8 %676, 0
  store i1 %677, i1* %zf
  %678 = icmp slt i8 %676, 0
  store i1 %678, i1* %sf
  %679 = call i8 @llvm.ctpop.i8(i8 %676)
  %680 = and i8 %679, 1
  %681 = icmp eq i8 %680, 0
  store i1 %681, i1* %pf
  %682 = sext i8 %676 to i32
  store i32 %682, i32* %stack_var_-37
  %683 = and i8 1, %674
  %684 = icmp ne i8 %683, 0
  store i1 %684, i1* %cf
  %685 = icmp slt i8 %674, 0
  %686 = select i1 true, i1 %685, i1 %675
  store i1 %686, i1* %of
  store volatile i64 46104, i64* @assembly_address
  %687 = load i32* bitcast (i64* @global_var_219ec0 to i32*)
  %688 = zext i32 %687 to i64
  store i64 %688, i64* %rax
  store volatile i64 46110, i64* @assembly_address
  %689 = load i32* %stack_var_-36
  %690 = load i64* %rax
  %691 = trunc i64 %690 to i32
  %692 = sub i32 %689, %691
  %693 = and i32 %689, 15
  %694 = and i32 %691, 15
  %695 = sub i32 %693, %694
  %696 = icmp ugt i32 %695, 15
  %697 = icmp ult i32 %689, %691
  %698 = xor i32 %689, %691
  %699 = xor i32 %689, %692
  %700 = and i32 %698, %699
  %701 = icmp slt i32 %700, 0
  store i1 %696, i1* %az
  store i1 %697, i1* %cf
  store i1 %701, i1* %of
  %702 = icmp eq i32 %692, 0
  store i1 %702, i1* %zf
  %703 = icmp slt i32 %692, 0
  store i1 %703, i1* %sf
  %704 = trunc i32 %692 to i8
  %705 = call i8 @llvm.ctpop.i8(i8 %704)
  %706 = and i8 %705, 1
  %707 = icmp eq i8 %706, 0
  store i1 %707, i1* %pf
  store volatile i64 46113, i64* @assembly_address
  %708 = load i1* %cf
  br i1 %708, label %block_b20b, label %block_b427

block_b427:                                       ; preds = %block_b415, %block_b1d4
  store volatile i64 46119, i64* @assembly_address
  %709 = load i16** %stack_var_-48
  %710 = ptrtoint i16* %709 to i64
  store i64 %710, i64* %rax
  store volatile i64 46123, i64* @assembly_address
  %711 = load i64* %rax
  %712 = add i64 %711, ptrtoint (i64* @global_var_400 to i64)
  %713 = and i64 %711, 15
  %714 = icmp ugt i64 %713, 15
  %715 = icmp ult i64 %712, %711
  %716 = xor i64 %711, %712
  %717 = xor i64 1024, %712
  %718 = and i64 %716, %717
  %719 = icmp slt i64 %718, 0
  store i1 %714, i1* %az
  store i1 %715, i1* %cf
  store i1 %719, i1* %of
  %720 = icmp eq i64 %712, 0
  store i1 %720, i1* %zf
  %721 = icmp slt i64 %712, 0
  store i1 %721, i1* %sf
  %722 = trunc i64 %712 to i8
  %723 = call i8 @llvm.ctpop.i8(i8 %722)
  %724 = and i8 %723, 1
  %725 = icmp eq i8 %724, 0
  store i1 %725, i1* %pf
  store i64 %712, i64* %rax
  store volatile i64 46129, i64* @assembly_address
  %726 = load i64* %rax
  %727 = add i64 %726, 2
  %728 = inttoptr i64 %727 to i16*
  %729 = load i16* %728
  %730 = zext i16 %729 to i64
  store i64 %730, i64* %rax
  store volatile i64 46133, i64* @assembly_address
  %731 = load i64* %rax
  %732 = trunc i64 %731 to i16
  %733 = zext i16 %732 to i64
  store i64 %733, i64* %rdx
  store volatile i64 46136, i64* @assembly_address
  %734 = load i16** %stack_var_-48
  %735 = ptrtoint i16* %734 to i64
  store i64 %735, i64* %rax
  store volatile i64 46140, i64* @assembly_address
  %736 = load i64* %rax
  %737 = add i64 %736, ptrtoint (i64* @global_var_400 to i64)
  %738 = and i64 %736, 15
  %739 = icmp ugt i64 %738, 15
  %740 = icmp ult i64 %737, %736
  %741 = xor i64 %736, %737
  %742 = xor i64 1024, %737
  %743 = and i64 %741, %742
  %744 = icmp slt i64 %743, 0
  store i1 %739, i1* %az
  store i1 %740, i1* %cf
  store i1 %744, i1* %of
  %745 = icmp eq i64 %737, 0
  store i1 %745, i1* %zf
  %746 = icmp slt i64 %737, 0
  store i1 %746, i1* %sf
  %747 = trunc i64 %737 to i8
  %748 = call i8 @llvm.ctpop.i8(i8 %747)
  %749 = and i8 %748, 1
  %750 = icmp eq i8 %749, 0
  store i1 %750, i1* %pf
  store i64 %737, i64* %rax
  store volatile i64 46146, i64* @assembly_address
  %751 = load i64* %rax
  %752 = inttoptr i64 %751 to i16*
  %753 = load i16* %752
  %754 = zext i16 %753 to i64
  store i64 %754, i64* %rax
  store volatile i64 46149, i64* @assembly_address
  %755 = load i64* %rax
  %756 = trunc i64 %755 to i16
  %757 = zext i16 %756 to i64
  store i64 %757, i64* %rax
  store volatile i64 46152, i64* @assembly_address
  %758 = load i64* %rdx
  %759 = trunc i64 %758 to i32
  %760 = zext i32 %759 to i64
  store i64 %760, i64* %rsi
  store volatile i64 46154, i64* @assembly_address
  %761 = load i64* %rax
  %762 = trunc i64 %761 to i32
  %763 = zext i32 %762 to i64
  store i64 %763, i64* %rdi
  store volatile i64 46156, i64* @assembly_address
  %764 = load i64* %rdi
  %765 = load i64* %rsi
  %766 = trunc i64 %764 to i32
  %767 = call i64 @send_bits(i32 %766, i64 %765)
  store i64 %767, i64* %rax
  store i64 %767, i64* %rax
  store volatile i64 46161, i64* @assembly_address
  store volatile i64 46162, i64* @assembly_address
  %768 = load i64* %stack_var_-8
  store i64 %768, i64* %rbp
  %769 = ptrtoint i64* %stack_var_0 to i64
  store i64 %769, i64* %rsp
  store volatile i64 46163, i64* @assembly_address
  %770 = load i64* %rax
  ret i64 %770
}

declare i64 @218(i64*, i16*)

declare i64 @219(i64*, i64*)

define i64 @set_file_type() {
block_b454:
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-8 = alloca i64
  %0 = alloca i32
  %1 = alloca i32
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i32
  store volatile i64 46164, i64* @assembly_address
  %6 = load i64* %rbp
  store i64 %6, i64* %stack_var_-8
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rsp
  store volatile i64 46165, i64* @assembly_address
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rbp
  store volatile i64 46168, i64* @assembly_address
  store i32 0, i32* %stack_var_-20
  store volatile i64 46175, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 46182, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 46189, i64* @assembly_address
  br label %block_b493

block_b46f:                                       ; preds = %block_b493
  store volatile i64 46191, i64* @assembly_address
  %9 = load i32* %stack_var_-20
  %10 = zext i32 %9 to i64
  store i64 %10, i64* %rax
  store volatile i64 46194, i64* @assembly_address
  %11 = load i64* %rax
  %12 = add i64 %11, 1
  %13 = trunc i64 %12 to i32
  %14 = zext i32 %13 to i64
  store i64 %14, i64* %rdx
  store volatile i64 46197, i64* @assembly_address
  %15 = load i64* %rdx
  %16 = trunc i64 %15 to i32
  store i32 %16, i32* %stack_var_-20
  store volatile i64 46200, i64* @assembly_address
  %17 = load i64* %rax
  %18 = trunc i64 %17 to i32
  %19 = sext i32 %18 to i64
  store i64 %19, i64* %rax
  store volatile i64 46202, i64* @assembly_address
  %20 = load i64* %rax
  %21 = mul i64 %20, 4
  store i64 %21, i64* %rdx
  store volatile i64 46210, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rax
  store volatile i64 46217, i64* @assembly_address
  %22 = load i64* %rdx
  %23 = load i64* %rax
  %24 = mul i64 %23, 1
  %25 = add i64 %22, %24
  %26 = inttoptr i64 %25 to i16*
  %27 = load i16* %26
  %28 = zext i16 %27 to i64
  store i64 %28, i64* %rax
  store volatile i64 46221, i64* @assembly_address
  %29 = load i64* %rax
  %30 = trunc i64 %29 to i16
  %31 = zext i16 %30 to i64
  store i64 %31, i64* %rax
  store volatile i64 46224, i64* @assembly_address
  %32 = load i32* %stack_var_-12
  %33 = load i64* %rax
  %34 = trunc i64 %33 to i32
  %35 = add i32 %32, %34
  %36 = and i32 %32, 15
  %37 = and i32 %34, 15
  %38 = add i32 %36, %37
  %39 = icmp ugt i32 %38, 15
  %40 = icmp ult i32 %35, %32
  %41 = xor i32 %32, %35
  %42 = xor i32 %34, %35
  %43 = and i32 %41, %42
  %44 = icmp slt i32 %43, 0
  store i1 %39, i1* %az
  store i1 %40, i1* %cf
  store i1 %44, i1* %of
  %45 = icmp eq i32 %35, 0
  store i1 %45, i1* %zf
  %46 = icmp slt i32 %35, 0
  store i1 %46, i1* %sf
  %47 = trunc i32 %35 to i8
  %48 = call i8 @llvm.ctpop.i8(i8 %47)
  %49 = and i8 %48, 1
  %50 = icmp eq i8 %49, 0
  store i1 %50, i1* %pf
  store i32 %35, i32* %stack_var_-12
  br label %block_b493

block_b493:                                       ; preds = %block_b46f, %block_b454
  store volatile i64 46227, i64* @assembly_address
  %51 = load i32* %stack_var_-20
  store i32 %51, i32* %5
  store i32 6, i32* %4
  %52 = sub i32 %51, 6
  %53 = and i32 %51, 15
  %54 = sub i32 %53, 6
  %55 = icmp ugt i32 %54, 15
  %56 = icmp ult i32 %51, 6
  %57 = xor i32 %51, 6
  %58 = xor i32 %51, %52
  %59 = and i32 %57, %58
  %60 = icmp slt i32 %59, 0
  store i1 %55, i1* %az
  store i1 %56, i1* %cf
  store i1 %60, i1* %of
  %61 = icmp eq i32 %52, 0
  store i1 %61, i1* %zf
  %62 = icmp slt i32 %52, 0
  store i1 %62, i1* %sf
  %63 = trunc i32 %52 to i8
  %64 = call i8 @llvm.ctpop.i8(i8 %63)
  %65 = and i8 %64, 1
  %66 = icmp eq i8 %65, 0
  store i1 %66, i1* %pf
  store volatile i64 46231, i64* @assembly_address
  %67 = load i32* %5
  %68 = load i32* %4
  %69 = icmp sle i32 %67, %68
  br i1 %69, label %block_b46f, label %block_b499

block_b499:                                       ; preds = %block_b493
  store volatile i64 46233, i64* @assembly_address
  br label %block_b4bf

block_b49b:                                       ; preds = %block_b4bf
  store volatile i64 46235, i64* @assembly_address
  %70 = load i32* %stack_var_-20
  %71 = zext i32 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 46238, i64* @assembly_address
  %72 = load i64* %rax
  %73 = add i64 %72, 1
  %74 = trunc i64 %73 to i32
  %75 = zext i32 %74 to i64
  store i64 %75, i64* %rdx
  store volatile i64 46241, i64* @assembly_address
  %76 = load i64* %rdx
  %77 = trunc i64 %76 to i32
  store i32 %77, i32* %stack_var_-20
  store volatile i64 46244, i64* @assembly_address
  %78 = load i64* %rax
  %79 = trunc i64 %78 to i32
  %80 = sext i32 %79 to i64
  store i64 %80, i64* %rax
  store volatile i64 46246, i64* @assembly_address
  %81 = load i64* %rax
  %82 = mul i64 %81, 4
  store i64 %82, i64* %rdx
  store volatile i64 46254, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rax
  store volatile i64 46261, i64* @assembly_address
  %83 = load i64* %rdx
  %84 = load i64* %rax
  %85 = mul i64 %84, 1
  %86 = add i64 %83, %85
  %87 = inttoptr i64 %86 to i16*
  %88 = load i16* %87
  %89 = zext i16 %88 to i64
  store i64 %89, i64* %rax
  store volatile i64 46265, i64* @assembly_address
  %90 = load i64* %rax
  %91 = trunc i64 %90 to i16
  %92 = zext i16 %91 to i64
  store i64 %92, i64* %rax
  store volatile i64 46268, i64* @assembly_address
  %93 = load i32* %stack_var_-16
  %94 = load i64* %rax
  %95 = trunc i64 %94 to i32
  %96 = add i32 %93, %95
  %97 = and i32 %93, 15
  %98 = and i32 %95, 15
  %99 = add i32 %97, %98
  %100 = icmp ugt i32 %99, 15
  %101 = icmp ult i32 %96, %93
  %102 = xor i32 %93, %96
  %103 = xor i32 %95, %96
  %104 = and i32 %102, %103
  %105 = icmp slt i32 %104, 0
  store i1 %100, i1* %az
  store i1 %101, i1* %cf
  store i1 %105, i1* %of
  %106 = icmp eq i32 %96, 0
  store i1 %106, i1* %zf
  %107 = icmp slt i32 %96, 0
  store i1 %107, i1* %sf
  %108 = trunc i32 %96 to i8
  %109 = call i8 @llvm.ctpop.i8(i8 %108)
  %110 = and i8 %109, 1
  %111 = icmp eq i8 %110, 0
  store i1 %111, i1* %pf
  store i32 %96, i32* %stack_var_-16
  br label %block_b4bf

block_b4bf:                                       ; preds = %block_b49b, %block_b499
  store volatile i64 46271, i64* @assembly_address
  %112 = load i32* %stack_var_-20
  store i32 %112, i32* %3
  store i32 127, i32* %2
  %113 = sub i32 %112, 127
  %114 = and i32 %112, 15
  %115 = sub i32 %114, 15
  %116 = icmp ugt i32 %115, 15
  %117 = icmp ult i32 %112, 127
  %118 = xor i32 %112, 127
  %119 = xor i32 %112, %113
  %120 = and i32 %118, %119
  %121 = icmp slt i32 %120, 0
  store i1 %116, i1* %az
  store i1 %117, i1* %cf
  store i1 %121, i1* %of
  %122 = icmp eq i32 %113, 0
  store i1 %122, i1* %zf
  %123 = icmp slt i32 %113, 0
  store i1 %123, i1* %sf
  %124 = trunc i32 %113 to i8
  %125 = call i8 @llvm.ctpop.i8(i8 %124)
  %126 = and i8 %125, 1
  %127 = icmp eq i8 %126, 0
  store i1 %127, i1* %pf
  store volatile i64 46275, i64* @assembly_address
  %128 = load i32* %3
  %129 = load i32* %2
  %130 = icmp sle i32 %128, %129
  br i1 %130, label %block_b49b, label %block_b4c5

block_b4c5:                                       ; preds = %block_b4bf
  store volatile i64 46277, i64* @assembly_address
  br label %block_b4eb

block_b4c7:                                       ; preds = %block_b4eb
  store volatile i64 46279, i64* @assembly_address
  %131 = load i32* %stack_var_-20
  %132 = zext i32 %131 to i64
  store i64 %132, i64* %rax
  store volatile i64 46282, i64* @assembly_address
  %133 = load i64* %rax
  %134 = add i64 %133, 1
  %135 = trunc i64 %134 to i32
  %136 = zext i32 %135 to i64
  store i64 %136, i64* %rdx
  store volatile i64 46285, i64* @assembly_address
  %137 = load i64* %rdx
  %138 = trunc i64 %137 to i32
  store i32 %138, i32* %stack_var_-20
  store volatile i64 46288, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  %141 = sext i32 %140 to i64
  store i64 %141, i64* %rax
  store volatile i64 46290, i64* @assembly_address
  %142 = load i64* %rax
  %143 = mul i64 %142, 4
  store i64 %143, i64* %rdx
  store volatile i64 46298, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_216fc0 to i64), i64* %rax
  store volatile i64 46305, i64* @assembly_address
  %144 = load i64* %rdx
  %145 = load i64* %rax
  %146 = mul i64 %145, 1
  %147 = add i64 %144, %146
  %148 = inttoptr i64 %147 to i16*
  %149 = load i16* %148
  %150 = zext i16 %149 to i64
  store i64 %150, i64* %rax
  store volatile i64 46309, i64* @assembly_address
  %151 = load i64* %rax
  %152 = trunc i64 %151 to i16
  %153 = zext i16 %152 to i64
  store i64 %153, i64* %rax
  store volatile i64 46312, i64* @assembly_address
  %154 = load i32* %stack_var_-12
  %155 = load i64* %rax
  %156 = trunc i64 %155 to i32
  %157 = add i32 %154, %156
  %158 = and i32 %154, 15
  %159 = and i32 %156, 15
  %160 = add i32 %158, %159
  %161 = icmp ugt i32 %160, 15
  %162 = icmp ult i32 %157, %154
  %163 = xor i32 %154, %157
  %164 = xor i32 %156, %157
  %165 = and i32 %163, %164
  %166 = icmp slt i32 %165, 0
  store i1 %161, i1* %az
  store i1 %162, i1* %cf
  store i1 %166, i1* %of
  %167 = icmp eq i32 %157, 0
  store i1 %167, i1* %zf
  %168 = icmp slt i32 %157, 0
  store i1 %168, i1* %sf
  %169 = trunc i32 %157 to i8
  %170 = call i8 @llvm.ctpop.i8(i8 %169)
  %171 = and i8 %170, 1
  %172 = icmp eq i8 %171, 0
  store i1 %172, i1* %pf
  store i32 %157, i32* %stack_var_-12
  br label %block_b4eb

block_b4eb:                                       ; preds = %block_b4c7, %block_b4c5
  store volatile i64 46315, i64* @assembly_address
  %173 = load i32* %stack_var_-20
  store i32 %173, i32* %1
  store i32 255, i32* %0
  %174 = sub i32 %173, 255
  %175 = and i32 %173, 15
  %176 = sub i32 %175, 15
  %177 = icmp ugt i32 %176, 15
  %178 = icmp ult i32 %173, 255
  %179 = xor i32 %173, 255
  %180 = xor i32 %173, %174
  %181 = and i32 %179, %180
  %182 = icmp slt i32 %181, 0
  store i1 %177, i1* %az
  store i1 %178, i1* %cf
  store i1 %182, i1* %of
  %183 = icmp eq i32 %174, 0
  store i1 %183, i1* %zf
  %184 = icmp slt i32 %174, 0
  store i1 %184, i1* %sf
  %185 = trunc i32 %174 to i8
  %186 = call i8 @llvm.ctpop.i8(i8 %185)
  %187 = and i8 %186, 1
  %188 = icmp eq i8 %187, 0
  store i1 %188, i1* %pf
  store volatile i64 46322, i64* @assembly_address
  %189 = load i32* %1
  %190 = load i32* %0
  %191 = icmp sle i32 %189, %190
  br i1 %191, label %block_b4c7, label %block_b4f4

block_b4f4:                                       ; preds = %block_b4eb
  store volatile i64 46324, i64* @assembly_address
  %192 = load i32* %stack_var_-16
  %193 = zext i32 %192 to i64
  store i64 %193, i64* %rax
  store volatile i64 46327, i64* @assembly_address
  %194 = load i64* %rax
  %195 = trunc i64 %194 to i32
  %196 = load i1* %of
  %197 = lshr i32 %195, 2
  %198 = icmp eq i32 %197, 0
  store i1 %198, i1* %zf
  %199 = icmp slt i32 %197, 0
  store i1 %199, i1* %sf
  %200 = trunc i32 %197 to i8
  %201 = call i8 @llvm.ctpop.i8(i8 %200)
  %202 = and i8 %201, 1
  %203 = icmp eq i8 %202, 0
  store i1 %203, i1* %pf
  %204 = zext i32 %197 to i64
  store i64 %204, i64* %rax
  %205 = and i32 2, %195
  %206 = icmp ne i32 %205, 0
  store i1 %206, i1* %cf
  %207 = icmp slt i32 %195, 0
  %208 = select i1 false, i1 %207, i1 %196
  store i1 %208, i1* %of
  store volatile i64 46330, i64* @assembly_address
  %209 = load i32* %stack_var_-12
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  %212 = sub i32 %209, %211
  %213 = and i32 %209, 15
  %214 = and i32 %211, 15
  %215 = sub i32 %213, %214
  %216 = icmp ugt i32 %215, 15
  %217 = icmp ult i32 %209, %211
  %218 = xor i32 %209, %211
  %219 = xor i32 %209, %212
  %220 = and i32 %218, %219
  %221 = icmp slt i32 %220, 0
  store i1 %216, i1* %az
  store i1 %217, i1* %cf
  store i1 %221, i1* %of
  %222 = icmp eq i32 %212, 0
  store i1 %222, i1* %zf
  %223 = icmp slt i32 %212, 0
  store i1 %223, i1* %sf
  %224 = trunc i32 %212 to i8
  %225 = call i8 @llvm.ctpop.i8(i8 %224)
  %226 = and i8 %225, 1
  %227 = icmp eq i8 %226, 0
  store i1 %227, i1* %pf
  store volatile i64 46333, i64* @assembly_address
  %228 = load i1* %cf
  %229 = load i1* %zf
  %230 = or i1 %228, %229
  %231 = zext i1 %230 to i8
  %232 = zext i8 %231 to i64
  %233 = load i64* %rdx
  %234 = and i64 %233, -256
  %235 = or i64 %234, %232
  store i64 %235, i64* %rdx
  store volatile i64 46336, i64* @assembly_address
  %236 = load i64* @global_var_219ef0
  store i64 %236, i64* %rax
  store volatile i64 46343, i64* @assembly_address
  %237 = load i64* %rdx
  %238 = trunc i64 %237 to i8
  %239 = zext i8 %238 to i64
  store i64 %239, i64* %rdx
  store volatile i64 46346, i64* @assembly_address
  %240 = load i64* %rdx
  %241 = trunc i64 %240 to i16
  %242 = load i64* %rax
  %243 = inttoptr i64 %242 to i16*
  store i16 %241, i16* %243
  store volatile i64 46349, i64* @assembly_address
  store volatile i64 46350, i64* @assembly_address
  %244 = load i64* %stack_var_-8
  store i64 %244, i64* %rbp
  %245 = ptrtoint i64* %stack_var_0 to i64
  store i64 %245, i64* %rsp
  store volatile i64 46351, i64* @assembly_address
  %246 = load i64* %rax
  %247 = load i64* %rax
  ret i64 %247
}

define i64 @fillbuf(i32 %arg1) {
block_b510:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i64
  %3 = alloca i32
  store volatile i64 46352, i64* @assembly_address
  %4 = load i64* %rbp
  store i64 %4, i64* %stack_var_-8
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rsp
  store volatile i64 46353, i64* @assembly_address
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rbp
  store volatile i64 46356, i64* @assembly_address
  %7 = load i64* %rsp
  %8 = sub i64 %7, 16
  %9 = and i64 %7, 15
  %10 = icmp ugt i64 %9, 15
  %11 = icmp ult i64 %7, 16
  %12 = xor i64 %7, 16
  %13 = xor i64 %7, %8
  %14 = and i64 %12, %13
  %15 = icmp slt i64 %14, 0
  store i1 %10, i1* %az
  store i1 %11, i1* %cf
  store i1 %15, i1* %of
  %16 = icmp eq i64 %8, 0
  store i1 %16, i1* %zf
  %17 = icmp slt i64 %8, 0
  store i1 %17, i1* %sf
  %18 = trunc i64 %8 to i8
  %19 = call i8 @llvm.ctpop.i8(i8 %18)
  %20 = and i8 %19, 1
  %21 = icmp eq i8 %20, 0
  store i1 %21, i1* %pf
  %22 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %22, i64* %rsp
  store volatile i64 46360, i64* @assembly_address
  %23 = load i64* %rdi
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-12
  store volatile i64 46363, i64* @assembly_address
  %25 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %26 = zext i16 %25 to i64
  store i64 %26, i64* %rax
  store volatile i64 46370, i64* @assembly_address
  %27 = load i64* %rax
  %28 = trunc i64 %27 to i16
  %29 = zext i16 %28 to i64
  store i64 %29, i64* %rdx
  store volatile i64 46373, i64* @assembly_address
  %30 = load i32* %stack_var_-12
  %31 = zext i32 %30 to i64
  store i64 %31, i64* %rax
  store volatile i64 46376, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i32
  %34 = zext i32 %33 to i64
  store i64 %34, i64* %rcx
  store volatile i64 46378, i64* @assembly_address
  %35 = load i64* %rdx
  %36 = trunc i64 %35 to i32
  %37 = load i64* %rcx
  %38 = trunc i64 %37 to i8
  %39 = zext i8 %38 to i32
  %40 = and i32 %39, 31
  %41 = load i1* %of
  %42 = icmp eq i32 %40, 0
  br i1 %42, label %60, label %43

; <label>:43                                      ; preds = %block_b510
  %44 = shl i32 %36, %40
  %45 = icmp eq i32 %44, 0
  store i1 %45, i1* %zf
  %46 = icmp slt i32 %44, 0
  store i1 %46, i1* %sf
  %47 = trunc i32 %44 to i8
  %48 = call i8 @llvm.ctpop.i8(i8 %47)
  %49 = and i8 %48, 1
  %50 = icmp eq i8 %49, 0
  store i1 %50, i1* %pf
  %51 = zext i32 %44 to i64
  store i64 %51, i64* %rdx
  %52 = sub i32 %40, 1
  %53 = shl i32 %36, %52
  %54 = lshr i32 %53, 31
  %55 = trunc i32 %54 to i1
  store i1 %55, i1* %cf
  %56 = lshr i32 %44, 31
  %57 = icmp ne i32 %56, %54
  %58 = icmp eq i32 %40, 1
  %59 = select i1 %58, i1 %57, i1 %41
  store i1 %59, i1* %of
  br label %60

; <label>:60                                      ; preds = %block_b510, %43
  store volatile i64 46380, i64* @assembly_address
  %61 = load i64* %rdx
  %62 = trunc i64 %61 to i32
  %63 = zext i32 %62 to i64
  store i64 %63, i64* %rax
  store volatile i64 46382, i64* @assembly_address
  %64 = load i64* %rax
  %65 = trunc i64 %64 to i16
  store i16 %65, i16* bitcast (i64* @global_var_21a140 to i16*)
  store volatile i64 46389, i64* @assembly_address
  br label %block_b5c4

block_b53a:                                       ; preds = %block_b5c4
  store volatile i64 46394, i64* @assembly_address
  %66 = load i32* bitcast (i64* @global_var_21a144 to i32*)
  %67 = zext i32 %66 to i64
  store i64 %67, i64* %rdx
  store volatile i64 46400, i64* @assembly_address
  %68 = load i32* bitcast (i64* @global_var_21a148 to i32*)
  %69 = zext i32 %68 to i64
  store i64 %69, i64* %rax
  store volatile i64 46406, i64* @assembly_address
  %70 = load i32* %stack_var_-12
  %71 = load i64* %rax
  %72 = trunc i64 %71 to i32
  %73 = sub i32 %70, %72
  %74 = and i32 %70, 15
  %75 = and i32 %72, 15
  %76 = sub i32 %74, %75
  %77 = icmp ugt i32 %76, 15
  %78 = icmp ult i32 %70, %72
  %79 = xor i32 %70, %72
  %80 = xor i32 %70, %73
  %81 = and i32 %79, %80
  %82 = icmp slt i32 %81, 0
  store i1 %77, i1* %az
  store i1 %78, i1* %cf
  store i1 %82, i1* %of
  %83 = icmp eq i32 %73, 0
  store i1 %83, i1* %zf
  %84 = icmp slt i32 %73, 0
  store i1 %84, i1* %sf
  %85 = trunc i32 %73 to i8
  %86 = call i8 @llvm.ctpop.i8(i8 %85)
  %87 = and i8 %86, 1
  %88 = icmp eq i8 %87, 0
  store i1 %88, i1* %pf
  store i32 %73, i32* %stack_var_-12
  store volatile i64 46409, i64* @assembly_address
  %89 = load i32* %stack_var_-12
  %90 = zext i32 %89 to i64
  store i64 %90, i64* %rax
  store volatile i64 46412, i64* @assembly_address
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i32
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rcx
  store volatile i64 46414, i64* @assembly_address
  %94 = load i64* %rdx
  %95 = trunc i64 %94 to i32
  %96 = load i64* %rcx
  %97 = trunc i64 %96 to i8
  %98 = zext i8 %97 to i32
  %99 = and i32 %98, 31
  %100 = load i1* %of
  %101 = icmp eq i32 %99, 0
  br i1 %101, label %119, label %102

; <label>:102                                     ; preds = %block_b53a
  %103 = shl i32 %95, %99
  %104 = icmp eq i32 %103, 0
  store i1 %104, i1* %zf
  %105 = icmp slt i32 %103, 0
  store i1 %105, i1* %sf
  %106 = trunc i32 %103 to i8
  %107 = call i8 @llvm.ctpop.i8(i8 %106)
  %108 = and i8 %107, 1
  %109 = icmp eq i8 %108, 0
  store i1 %109, i1* %pf
  %110 = zext i32 %103 to i64
  store i64 %110, i64* %rdx
  %111 = sub i32 %99, 1
  %112 = shl i32 %95, %111
  %113 = lshr i32 %112, 31
  %114 = trunc i32 %113 to i1
  store i1 %114, i1* %cf
  %115 = lshr i32 %103, 31
  %116 = icmp ne i32 %115, %113
  %117 = icmp eq i32 %99, 1
  %118 = select i1 %117, i1 %116, i1 %100
  store i1 %118, i1* %of
  br label %119

; <label>:119                                     ; preds = %block_b53a, %102
  store volatile i64 46416, i64* @assembly_address
  %120 = load i64* %rdx
  %121 = trunc i64 %120 to i32
  %122 = zext i32 %121 to i64
  store i64 %122, i64* %rax
  store volatile i64 46418, i64* @assembly_address
  %123 = load i64* %rax
  %124 = trunc i64 %123 to i32
  %125 = zext i32 %124 to i64
  store i64 %125, i64* %rdx
  store volatile i64 46420, i64* @assembly_address
  %126 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %127 = zext i16 %126 to i64
  store i64 %127, i64* %rax
  store volatile i64 46427, i64* @assembly_address
  %128 = load i64* %rax
  %129 = trunc i64 %128 to i32
  %130 = load i64* %rdx
  %131 = trunc i64 %130 to i32
  %132 = or i32 %129, %131
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %133 = icmp eq i32 %132, 0
  store i1 %133, i1* %zf
  %134 = icmp slt i32 %132, 0
  store i1 %134, i1* %sf
  %135 = trunc i32 %132 to i8
  %136 = call i8 @llvm.ctpop.i8(i8 %135)
  %137 = and i8 %136, 1
  %138 = icmp eq i8 %137, 0
  store i1 %138, i1* %pf
  %139 = zext i32 %132 to i64
  store i64 %139, i64* %rax
  store volatile i64 46429, i64* @assembly_address
  %140 = load i64* %rax
  %141 = trunc i64 %140 to i16
  store i16 %141, i16* bitcast (i64* @global_var_21a140 to i16*)
  store volatile i64 46436, i64* @assembly_address
  %142 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %143 = zext i32 %142 to i64
  store i64 %143, i64* %rdx
  store volatile i64 46442, i64* @assembly_address
  %144 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %145 = zext i32 %144 to i64
  store i64 %145, i64* %rax
  store volatile i64 46448, i64* @assembly_address
  %146 = load i64* %rdx
  %147 = trunc i64 %146 to i32
  %148 = load i64* %rax
  %149 = trunc i64 %148 to i32
  %150 = sub i32 %147, %149
  %151 = and i32 %147, 15
  %152 = and i32 %149, 15
  %153 = sub i32 %151, %152
  %154 = icmp ugt i32 %153, 15
  %155 = icmp ult i32 %147, %149
  %156 = xor i32 %147, %149
  %157 = xor i32 %147, %150
  %158 = and i32 %156, %157
  %159 = icmp slt i32 %158, 0
  store i1 %154, i1* %az
  store i1 %155, i1* %cf
  store i1 %159, i1* %of
  %160 = icmp eq i32 %150, 0
  store i1 %160, i1* %zf
  %161 = icmp slt i32 %150, 0
  store i1 %161, i1* %sf
  %162 = trunc i32 %150 to i8
  %163 = call i8 @llvm.ctpop.i8(i8 %162)
  %164 = and i8 %163, 1
  %165 = icmp eq i8 %164, 0
  store i1 %165, i1* %pf
  store volatile i64 46450, i64* @assembly_address
  %166 = load i1* %cf
  %167 = icmp eq i1 %166, false
  br i1 %167, label %block_b595, label %block_b574

block_b574:                                       ; preds = %119
  store volatile i64 46452, i64* @assembly_address
  %168 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %169 = zext i32 %168 to i64
  store i64 %169, i64* %rax
  store volatile i64 46458, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 46461, i64* @assembly_address
  %170 = load i64* %rdx
  %171 = trunc i64 %170 to i32
  store i32 %171, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 46467, i64* @assembly_address
  %172 = load i64* %rax
  %173 = trunc i64 %172 to i32
  %174 = zext i32 %173 to i64
  store i64 %174, i64* %rdx
  store volatile i64 46469, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 46476, i64* @assembly_address
  %175 = load i64* %rdx
  %176 = load i64* %rax
  %177 = mul i64 %176, 1
  %178 = add i64 %175, %177
  %179 = inttoptr i64 %178 to i8*
  %180 = load i8* %179
  %181 = zext i8 %180 to i64
  store i64 %181, i64* %rax
  store volatile i64 46480, i64* @assembly_address
  %182 = load i64* %rax
  %183 = trunc i64 %182 to i8
  %184 = zext i8 %183 to i64
  store i64 %184, i64* %rax
  store volatile i64 46483, i64* @assembly_address
  br label %block_b59f

block_b595:                                       ; preds = %119
  store volatile i64 46485, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 46490, i64* @assembly_address
  %185 = load i64* %rdi
  %186 = trunc i64 %185 to i32
  %187 = call i64 @fill_inbuf(i32 %186)
  store i64 %187, i64* %rax
  store i64 %187, i64* %rax
  br label %block_b59f

block_b59f:                                       ; preds = %block_b595, %block_b574
  store volatile i64 46495, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  store i32 %189, i32* bitcast (i64* @global_var_21a144 to i32*)
  store volatile i64 46501, i64* @assembly_address
  %190 = load i32* bitcast (i64* @global_var_21a144 to i32*)
  %191 = zext i32 %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 46507, i64* @assembly_address
  %192 = load i64* %rax
  %193 = trunc i64 %192 to i32
  %194 = sub i32 %193, -1
  %195 = and i32 %193, 15
  %196 = sub i32 %195, 15
  %197 = icmp ugt i32 %196, 15
  %198 = icmp ult i32 %193, -1
  %199 = xor i32 %193, -1
  %200 = xor i32 %193, %194
  %201 = and i32 %199, %200
  %202 = icmp slt i32 %201, 0
  store i1 %197, i1* %az
  store i1 %198, i1* %cf
  store i1 %202, i1* %of
  %203 = icmp eq i32 %194, 0
  store i1 %203, i1* %zf
  %204 = icmp slt i32 %194, 0
  store i1 %204, i1* %sf
  %205 = trunc i32 %194 to i8
  %206 = call i8 @llvm.ctpop.i8(i8 %205)
  %207 = and i8 %206, 1
  %208 = icmp eq i8 %207, 0
  store i1 %208, i1* %pf
  store volatile i64 46510, i64* @assembly_address
  %209 = load i1* %zf
  %210 = icmp eq i1 %209, false
  br i1 %210, label %block_b5ba, label %block_b5b0

block_b5b0:                                       ; preds = %block_b59f
  store volatile i64 46512, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a144 to i32*)
  br label %block_b5ba

block_b5ba:                                       ; preds = %block_b5b0, %block_b59f
  store volatile i64 46522, i64* @assembly_address
  store i32 8, i32* bitcast (i64* @global_var_21a148 to i32*)
  br label %block_b5c4

block_b5c4:                                       ; preds = %block_b5ba, %60
  store volatile i64 46532, i64* @assembly_address
  %211 = load i32* bitcast (i64* @global_var_21a148 to i32*)
  %212 = zext i32 %211 to i64
  store i64 %212, i64* %rax
  store volatile i64 46538, i64* @assembly_address
  %213 = load i32* %stack_var_-12
  %214 = load i64* %rax
  %215 = trunc i64 %214 to i32
  store i32 %213, i32* %3
  %216 = trunc i64 %214 to i32
  store i32 %216, i32* %1
  %217 = sub i32 %213, %215
  %218 = and i32 %213, 15
  %219 = and i32 %215, 15
  %220 = sub i32 %218, %219
  %221 = icmp ugt i32 %220, 15
  %222 = icmp ult i32 %213, %215
  %223 = xor i32 %213, %215
  %224 = xor i32 %213, %217
  %225 = and i32 %223, %224
  %226 = icmp slt i32 %225, 0
  store i1 %221, i1* %az
  store i1 %222, i1* %cf
  store i1 %226, i1* %of
  %227 = icmp eq i32 %217, 0
  store i1 %227, i1* %zf
  %228 = icmp slt i32 %217, 0
  store i1 %228, i1* %sf
  %229 = trunc i32 %217 to i8
  %230 = call i8 @llvm.ctpop.i8(i8 %229)
  %231 = and i8 %230, 1
  %232 = icmp eq i8 %231, 0
  store i1 %232, i1* %pf
  store volatile i64 46541, i64* @assembly_address
  %233 = load i32* %3
  %234 = load i32* %1
  %235 = sext i32 %234 to i64
  %236 = sext i32 %233 to i64
  %237 = icmp sgt i64 %236, %235
  br i1 %237, label %block_b53a, label %block_b5d3

block_b5d3:                                       ; preds = %block_b5c4
  store volatile i64 46547, i64* @assembly_address
  %238 = load i32* bitcast (i64* @global_var_21a144 to i32*)
  %239 = zext i32 %238 to i64
  store i64 %239, i64* %rdx
  store volatile i64 46553, i64* @assembly_address
  %240 = load i32* bitcast (i64* @global_var_21a148 to i32*)
  %241 = zext i32 %240 to i64
  store i64 %241, i64* %rax
  store volatile i64 46559, i64* @assembly_address
  %242 = load i64* %rax
  %243 = trunc i64 %242 to i32
  %244 = load i32* %stack_var_-12
  %245 = sub i32 %243, %244
  %246 = and i32 %243, 15
  %247 = and i32 %244, 15
  %248 = sub i32 %246, %247
  %249 = icmp ugt i32 %248, 15
  %250 = icmp ult i32 %243, %244
  %251 = xor i32 %243, %244
  %252 = xor i32 %243, %245
  %253 = and i32 %251, %252
  %254 = icmp slt i32 %253, 0
  store i1 %249, i1* %az
  store i1 %250, i1* %cf
  store i1 %254, i1* %of
  %255 = icmp eq i32 %245, 0
  store i1 %255, i1* %zf
  %256 = icmp slt i32 %245, 0
  store i1 %256, i1* %sf
  %257 = trunc i32 %245 to i8
  %258 = call i8 @llvm.ctpop.i8(i8 %257)
  %259 = and i8 %258, 1
  %260 = icmp eq i8 %259, 0
  store i1 %260, i1* %pf
  %261 = zext i32 %245 to i64
  store i64 %261, i64* %rax
  store volatile i64 46562, i64* @assembly_address
  %262 = load i64* %rax
  %263 = trunc i64 %262 to i32
  store i32 %263, i32* bitcast (i64* @global_var_21a148 to i32*)
  store volatile i64 46568, i64* @assembly_address
  %264 = load i32* bitcast (i64* @global_var_21a148 to i32*)
  %265 = zext i32 %264 to i64
  store i64 %265, i64* %rax
  store volatile i64 46574, i64* @assembly_address
  %266 = load i64* %rax
  %267 = trunc i64 %266 to i32
  %268 = zext i32 %267 to i64
  store i64 %268, i64* %rcx
  store volatile i64 46576, i64* @assembly_address
  %269 = load i64* %rdx
  %270 = trunc i64 %269 to i32
  %271 = load i64* %rcx
  %272 = trunc i64 %271 to i8
  %273 = zext i8 %272 to i32
  %274 = and i32 %273, 31
  %275 = load i1* %of
  %276 = icmp eq i32 %274, 0
  br i1 %276, label %293, label %277

; <label>:277                                     ; preds = %block_b5d3
  %278 = lshr i32 %270, %274
  %279 = icmp eq i32 %278, 0
  store i1 %279, i1* %zf
  %280 = icmp slt i32 %278, 0
  store i1 %280, i1* %sf
  %281 = trunc i32 %278 to i8
  %282 = call i8 @llvm.ctpop.i8(i8 %281)
  %283 = and i8 %282, 1
  %284 = icmp eq i8 %283, 0
  store i1 %284, i1* %pf
  %285 = zext i32 %278 to i64
  store i64 %285, i64* %rdx
  %286 = sub i32 %274, 1
  %287 = shl i32 1, %286
  %288 = and i32 %287, %270
  %289 = icmp ne i32 %288, 0
  store i1 %289, i1* %cf
  %290 = icmp eq i32 %274, 1
  %291 = icmp slt i32 %270, 0
  %292 = select i1 %290, i1 %291, i1 %275
  store i1 %292, i1* %of
  br label %293

; <label>:293                                     ; preds = %block_b5d3, %277
  store volatile i64 46578, i64* @assembly_address
  %294 = load i64* %rdx
  %295 = trunc i64 %294 to i32
  %296 = zext i32 %295 to i64
  store i64 %296, i64* %rax
  store volatile i64 46580, i64* @assembly_address
  %297 = load i64* %rax
  %298 = trunc i64 %297 to i32
  %299 = zext i32 %298 to i64
  store i64 %299, i64* %rdx
  store volatile i64 46582, i64* @assembly_address
  %300 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %301 = zext i16 %300 to i64
  store i64 %301, i64* %rax
  store volatile i64 46589, i64* @assembly_address
  %302 = load i64* %rax
  %303 = trunc i64 %302 to i32
  %304 = load i64* %rdx
  %305 = trunc i64 %304 to i32
  %306 = or i32 %303, %305
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %307 = icmp eq i32 %306, 0
  store i1 %307, i1* %zf
  %308 = icmp slt i32 %306, 0
  store i1 %308, i1* %sf
  %309 = trunc i32 %306 to i8
  %310 = call i8 @llvm.ctpop.i8(i8 %309)
  %311 = and i8 %310, 1
  %312 = icmp eq i8 %311, 0
  store i1 %312, i1* %pf
  %313 = zext i32 %306 to i64
  store i64 %313, i64* %rax
  store volatile i64 46591, i64* @assembly_address
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i16
  store i16 %315, i16* bitcast (i64* @global_var_21a140 to i16*)
  store volatile i64 46598, i64* @assembly_address
  store volatile i64 46599, i64* @assembly_address
  %316 = load i64* %stack_var_-8
  store i64 %316, i64* %rbp
  %317 = ptrtoint i64* %stack_var_0 to i64
  store i64 %317, i64* %rsp
  store volatile i64 46600, i64* @assembly_address
  %318 = load i64* %rax
  ret i64 %318
}

declare i64 @220(i64)

define i64 @getbits(i32 %arg1) {
block_b609:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 46601, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 46602, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 46605, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 46609, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-28
  store volatile i64 46612, i64* @assembly_address
  %22 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %23 = zext i16 %22 to i64
  store i64 %23, i64* %rax
  store volatile i64 46619, i64* @assembly_address
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i16
  %26 = zext i16 %25 to i64
  store i64 %26, i64* %rdx
  store volatile i64 46622, i64* @assembly_address
  %27 = load i32* %stack_var_-28
  %28 = zext i32 %27 to i64
  store i64 %28, i64* %rax
  store volatile i64 46625, i64* @assembly_address
  store i64 16, i64* %rcx
  store volatile i64 46630, i64* @assembly_address
  %29 = load i64* %rcx
  %30 = trunc i64 %29 to i32
  %31 = load i64* %rax
  %32 = trunc i64 %31 to i32
  %33 = sub i32 %30, %32
  %34 = and i32 %30, 15
  %35 = and i32 %32, 15
  %36 = sub i32 %34, %35
  %37 = icmp ugt i32 %36, 15
  %38 = icmp ult i32 %30, %32
  %39 = xor i32 %30, %32
  %40 = xor i32 %30, %33
  %41 = and i32 %39, %40
  %42 = icmp slt i32 %41, 0
  store i1 %37, i1* %az
  store i1 %38, i1* %cf
  store i1 %42, i1* %of
  %43 = icmp eq i32 %33, 0
  store i1 %43, i1* %zf
  %44 = icmp slt i32 %33, 0
  store i1 %44, i1* %sf
  %45 = trunc i32 %33 to i8
  %46 = call i8 @llvm.ctpop.i8(i8 %45)
  %47 = and i8 %46, 1
  %48 = icmp eq i8 %47, 0
  store i1 %48, i1* %pf
  %49 = zext i32 %33 to i64
  store i64 %49, i64* %rcx
  store volatile i64 46632, i64* @assembly_address
  %50 = load i64* %rcx
  %51 = trunc i64 %50 to i32
  %52 = zext i32 %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 46634, i64* @assembly_address
  %53 = load i64* %rax
  %54 = trunc i64 %53 to i32
  %55 = zext i32 %54 to i64
  store i64 %55, i64* %rcx
  store volatile i64 46636, i64* @assembly_address
  %56 = load i64* %rdx
  %57 = trunc i64 %56 to i32
  %58 = load i64* %rcx
  %59 = trunc i64 %58 to i8
  %60 = zext i8 %59 to i32
  %61 = and i32 %60, 31
  %62 = load i1* %of
  %63 = icmp eq i32 %61, 0
  br i1 %63, label %79, label %64

; <label>:64                                      ; preds = %block_b609
  %65 = ashr i32 %57, %61
  %66 = icmp eq i32 %65, 0
  store i1 %66, i1* %zf
  %67 = icmp slt i32 %65, 0
  store i1 %67, i1* %sf
  %68 = trunc i32 %65 to i8
  %69 = call i8 @llvm.ctpop.i8(i8 %68)
  %70 = and i8 %69, 1
  %71 = icmp eq i8 %70, 0
  store i1 %71, i1* %pf
  %72 = zext i32 %65 to i64
  store i64 %72, i64* %rdx
  %73 = sub i32 %61, 1
  %74 = shl i32 1, %73
  %75 = and i32 %74, %57
  %76 = icmp ne i32 %75, 0
  store i1 %76, i1* %cf
  %77 = icmp eq i32 %61, 1
  %78 = select i1 %77, i1 false, i1 %62
  store i1 %78, i1* %of
  br label %79

; <label>:79                                      ; preds = %block_b609, %64
  store volatile i64 46638, i64* @assembly_address
  %80 = load i64* %rdx
  %81 = trunc i64 %80 to i32
  %82 = zext i32 %81 to i64
  store i64 %82, i64* %rax
  store volatile i64 46640, i64* @assembly_address
  %83 = load i64* %rax
  %84 = trunc i64 %83 to i32
  store i32 %84, i32* %stack_var_-12
  store volatile i64 46643, i64* @assembly_address
  %85 = load i32* %stack_var_-28
  %86 = zext i32 %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 46646, i64* @assembly_address
  %87 = load i64* %rax
  %88 = trunc i64 %87 to i32
  %89 = zext i32 %88 to i64
  store i64 %89, i64* %rdi
  store volatile i64 46648, i64* @assembly_address
  %90 = load i64* %rdi
  %91 = trunc i64 %90 to i32
  %92 = call i64 @fillbuf(i32 %91)
  store i64 %92, i64* %rax
  store i64 %92, i64* %rax
  store volatile i64 46653, i64* @assembly_address
  %93 = load i32* %stack_var_-12
  %94 = zext i32 %93 to i64
  store i64 %94, i64* %rax
  store volatile i64 46656, i64* @assembly_address
  %95 = load i64* %stack_var_-8
  store i64 %95, i64* %rbp
  %96 = ptrtoint i64* %stack_var_0 to i64
  store i64 %96, i64* %rsp
  store volatile i64 46657, i64* @assembly_address
  %97 = load i64* %rax
  ret i64 %97
}

declare i64 @221(i64)

define i64 @init_getbits() {
block_b642:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 46658, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 46659, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 46662, i64* @assembly_address
  store i16 0, i16* bitcast (i64* @global_var_21a140 to i16*)
  store volatile i64 46671, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a144 to i32*)
  store volatile i64 46681, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a148 to i32*)
  store volatile i64 46691, i64* @assembly_address
  store i64 16, i64* %rdi
  store volatile i64 46696, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = call i64 @fillbuf(i32 %4)
  store i64 %5, i64* %rax
  store i64 %5, i64* %rax
  store volatile i64 46701, i64* @assembly_address
  store volatile i64 46702, i64* @assembly_address
  %6 = load i64* %stack_var_-8
  store i64 %6, i64* %rbp
  %7 = ptrtoint i64* %stack_var_0 to i64
  store i64 %7, i64* %rsp
  store volatile i64 46703, i64* @assembly_address
  %8 = load i64* %rax
  %9 = load i64* %rax
  ret i64 %9
}

define i64 @make_table(i32 %arg1, i64* %arg2, i64 %arg3, i64* %arg4) {
block_b670:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg4 to i64
  store i64 %0, i64* %rcx
  store i64 %arg3, i64* %rdx
  %1 = ptrtoint i64* %arg2 to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %arg1 to i64
  store i64 %2, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-160 = alloca i64
  %stack_var_-164 = alloca i32
  %stack_var_-168 = alloca i32
  %stack_var_-184 = alloca i32
  %stack_var_-172 = alloca i32
  %stack_var_-180 = alloca i32
  %stack_var_-188 = alloca i32
  %stack_var_-176 = alloca i32
  %stack_var_-22 = alloca i16
  %stack_var_-54 = alloca i16
  %stack_var_-192 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-224 = alloca i16*
  %3 = alloca i64
  %stack_var_-208 = alloca i32
  %stack_var_-216 = alloca i8*
  %4 = alloca i64
  %stack_var_-204 = alloca i32
  %stack_var_-232 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 46704, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 46705, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 46708, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 224
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 224
  %13 = xor i64 %8, 224
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-232 to i64
  store i64 %23, i64* %rsp
  store volatile i64 46715, i64* @assembly_address
  %24 = load i64* %rdi
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-204
  store volatile i64 46721, i64* @assembly_address
  %26 = load i64* %rsi
  %27 = inttoptr i64 %26 to i8*
  store i8* %27, i8** %stack_var_-216
  store volatile i64 46728, i64* @assembly_address
  %28 = load i64* %rdx
  %29 = trunc i64 %28 to i32
  store i32 %29, i32* %stack_var_-208
  store volatile i64 46734, i64* @assembly_address
  %30 = load i64* %rcx
  %31 = inttoptr i64 %30 to i16*
  store i16* %31, i16** %stack_var_-224
  store volatile i64 46741, i64* @assembly_address
  %32 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %32, i64* %rax
  store volatile i64 46750, i64* @assembly_address
  %33 = load i64* %rax
  store i64 %33, i64* %stack_var_-16
  store volatile i64 46754, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %34 = icmp eq i32 0, 0
  store i1 %34, i1* %zf
  %35 = icmp slt i32 0, 0
  store i1 %35, i1* %sf
  %36 = trunc i32 0 to i8
  %37 = call i8 @llvm.ctpop.i8(i8 %36)
  %38 = and i8 %37, 1
  %39 = icmp eq i8 %38, 0
  store i1 %39, i1* %pf
  %40 = zext i32 0 to i64
  store i64 %40, i64* %rax
  store volatile i64 46756, i64* @assembly_address
  store i32 1, i32* %stack_var_-192
  store volatile i64 46766, i64* @assembly_address
  br label %block_b6c7

block_b6b0:                                       ; preds = %block_b6c7
  store volatile i64 46768, i64* @assembly_address
  %41 = load i32* %stack_var_-192
  %42 = zext i32 %41 to i64
  store i64 %42, i64* %rax
  store volatile i64 46774, i64* @assembly_address
  %43 = load i64* %rbp
  %44 = load i64* %rax
  %45 = mul i64 %44, 2
  %46 = add i64 %43, -144
  %47 = add i64 %46, %45
  %48 = inttoptr i64 %47 to i16*
  store i16 0, i16* %48
  store volatile i64 46784, i64* @assembly_address
  %49 = load i32* %stack_var_-192
  %50 = add i32 %49, 1
  %51 = and i32 %49, 15
  %52 = add i32 %51, 1
  %53 = icmp ugt i32 %52, 15
  %54 = icmp ult i32 %50, %49
  %55 = xor i32 %49, %50
  %56 = xor i32 1, %50
  %57 = and i32 %55, %56
  %58 = icmp slt i32 %57, 0
  store i1 %53, i1* %az
  store i1 %54, i1* %cf
  store i1 %58, i1* %of
  %59 = icmp eq i32 %50, 0
  store i1 %59, i1* %zf
  %60 = icmp slt i32 %50, 0
  store i1 %60, i1* %sf
  %61 = trunc i32 %50 to i8
  %62 = call i8 @llvm.ctpop.i8(i8 %61)
  %63 = and i8 %62, 1
  %64 = icmp eq i8 %63, 0
  store i1 %64, i1* %pf
  store i32 %50, i32* %stack_var_-192
  br label %block_b6c7

block_b6c7:                                       ; preds = %block_b6b0, %block_b670
  store volatile i64 46791, i64* @assembly_address
  %65 = load i32* %stack_var_-192
  %66 = sub i32 %65, 16
  %67 = and i32 %65, 15
  %68 = icmp ugt i32 %67, 15
  %69 = icmp ult i32 %65, 16
  %70 = xor i32 %65, 16
  %71 = xor i32 %65, %66
  %72 = and i32 %70, %71
  %73 = icmp slt i32 %72, 0
  store i1 %68, i1* %az
  store i1 %69, i1* %cf
  store i1 %73, i1* %of
  %74 = icmp eq i32 %66, 0
  store i1 %74, i1* %zf
  %75 = icmp slt i32 %66, 0
  store i1 %75, i1* %sf
  %76 = trunc i32 %66 to i8
  %77 = call i8 @llvm.ctpop.i8(i8 %76)
  %78 = and i8 %77, 1
  %79 = icmp eq i8 %78, 0
  store i1 %79, i1* %pf
  store volatile i64 46798, i64* @assembly_address
  %80 = load i1* %cf
  %81 = load i1* %zf
  %82 = or i1 %80, %81
  br i1 %82, label %block_b6b0, label %block_b6d0

block_b6d0:                                       ; preds = %block_b6c7
  store volatile i64 46800, i64* @assembly_address
  store i32 0, i32* %stack_var_-192
  store volatile i64 46810, i64* @assembly_address
  br label %block_b711

block_b6dc:                                       ; preds = %block_b711
  store volatile i64 46812, i64* @assembly_address
  %83 = load i32* %stack_var_-192
  %84 = zext i32 %83 to i64
  store i64 %84, i64* %rdx
  store volatile i64 46818, i64* @assembly_address
  %85 = load i8** %stack_var_-216
  %86 = ptrtoint i8* %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 46825, i64* @assembly_address
  %87 = load i64* %rax
  %88 = load i64* %rdx
  %89 = add i64 %87, %88
  %90 = and i64 %87, 15
  %91 = and i64 %88, 15
  %92 = add i64 %90, %91
  %93 = icmp ugt i64 %92, 15
  %94 = icmp ult i64 %89, %87
  %95 = xor i64 %87, %89
  %96 = xor i64 %88, %89
  %97 = and i64 %95, %96
  %98 = icmp slt i64 %97, 0
  store i1 %93, i1* %az
  store i1 %94, i1* %cf
  store i1 %98, i1* %of
  %99 = icmp eq i64 %89, 0
  store i1 %99, i1* %zf
  %100 = icmp slt i64 %89, 0
  store i1 %100, i1* %sf
  %101 = trunc i64 %89 to i8
  %102 = call i8 @llvm.ctpop.i8(i8 %101)
  %103 = and i8 %102, 1
  %104 = icmp eq i8 %103, 0
  store i1 %104, i1* %pf
  store i64 %89, i64* %rax
  store volatile i64 46828, i64* @assembly_address
  %105 = load i64* %rax
  %106 = inttoptr i64 %105 to i8*
  %107 = load i8* %106
  %108 = zext i8 %107 to i64
  store i64 %108, i64* %rax
  store volatile i64 46831, i64* @assembly_address
  %109 = load i64* %rax
  %110 = trunc i64 %109 to i8
  %111 = zext i8 %110 to i64
  store i64 %111, i64* %rax
  store volatile i64 46834, i64* @assembly_address
  %112 = load i64* %rax
  %113 = trunc i64 %112 to i32
  %114 = sext i32 %113 to i64
  store i64 %114, i64* %rdx
  store volatile i64 46837, i64* @assembly_address
  %115 = load i64* %rbp
  %116 = load i64* %rdx
  %117 = mul i64 %116, 2
  %118 = add i64 %115, -144
  %119 = add i64 %118, %117
  %120 = inttoptr i64 %119 to i16*
  %121 = load i16* %120
  %122 = zext i16 %121 to i64
  store i64 %122, i64* %rdx
  store volatile i64 46845, i64* @assembly_address
  %123 = load i64* %rdx
  %124 = trunc i64 %123 to i32
  %125 = add i32 %124, 1
  %126 = and i32 %124, 15
  %127 = add i32 %126, 1
  %128 = icmp ugt i32 %127, 15
  %129 = icmp ult i32 %125, %124
  %130 = xor i32 %124, %125
  %131 = xor i32 1, %125
  %132 = and i32 %130, %131
  %133 = icmp slt i32 %132, 0
  store i1 %128, i1* %az
  store i1 %129, i1* %cf
  store i1 %133, i1* %of
  %134 = icmp eq i32 %125, 0
  store i1 %134, i1* %zf
  %135 = icmp slt i32 %125, 0
  store i1 %135, i1* %sf
  %136 = trunc i32 %125 to i8
  %137 = call i8 @llvm.ctpop.i8(i8 %136)
  %138 = and i8 %137, 1
  %139 = icmp eq i8 %138, 0
  store i1 %139, i1* %pf
  %140 = zext i32 %125 to i64
  store i64 %140, i64* %rdx
  store volatile i64 46848, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  %143 = sext i32 %142 to i64
  store i64 %143, i64* %rax
  store volatile i64 46850, i64* @assembly_address
  %144 = load i64* %rdx
  %145 = trunc i64 %144 to i16
  %146 = load i64* %rbp
  %147 = load i64* %rax
  %148 = mul i64 %147, 2
  %149 = add i64 %146, -144
  %150 = add i64 %149, %148
  %151 = inttoptr i64 %150 to i16*
  store i16 %145, i16* %151
  store volatile i64 46858, i64* @assembly_address
  %152 = load i32* %stack_var_-192
  %153 = add i32 %152, 1
  %154 = and i32 %152, 15
  %155 = add i32 %154, 1
  %156 = icmp ugt i32 %155, 15
  %157 = icmp ult i32 %153, %152
  %158 = xor i32 %152, %153
  %159 = xor i32 1, %153
  %160 = and i32 %158, %159
  %161 = icmp slt i32 %160, 0
  store i1 %156, i1* %az
  store i1 %157, i1* %cf
  store i1 %161, i1* %of
  %162 = icmp eq i32 %153, 0
  store i1 %162, i1* %zf
  %163 = icmp slt i32 %153, 0
  store i1 %163, i1* %sf
  %164 = trunc i32 %153 to i8
  %165 = call i8 @llvm.ctpop.i8(i8 %164)
  %166 = and i8 %165, 1
  %167 = icmp eq i8 %166, 0
  store i1 %167, i1* %pf
  store i32 %153, i32* %stack_var_-192
  br label %block_b711

block_b711:                                       ; preds = %block_b6dc, %block_b6d0
  store volatile i64 46865, i64* @assembly_address
  %168 = load i32* %stack_var_-204
  %169 = zext i32 %168 to i64
  store i64 %169, i64* %rax
  store volatile i64 46871, i64* @assembly_address
  %170 = load i32* %stack_var_-192
  %171 = load i64* %rax
  %172 = trunc i64 %171 to i32
  %173 = sub i32 %170, %172
  %174 = and i32 %170, 15
  %175 = and i32 %172, 15
  %176 = sub i32 %174, %175
  %177 = icmp ugt i32 %176, 15
  %178 = icmp ult i32 %170, %172
  %179 = xor i32 %170, %172
  %180 = xor i32 %170, %173
  %181 = and i32 %179, %180
  %182 = icmp slt i32 %181, 0
  store i1 %177, i1* %az
  store i1 %178, i1* %cf
  store i1 %182, i1* %of
  %183 = icmp eq i32 %173, 0
  store i1 %183, i1* %zf
  %184 = icmp slt i32 %173, 0
  store i1 %184, i1* %sf
  %185 = trunc i32 %173 to i8
  %186 = call i8 @llvm.ctpop.i8(i8 %185)
  %187 = and i8 %186, 1
  %188 = icmp eq i8 %187, 0
  store i1 %188, i1* %pf
  store volatile i64 46877, i64* @assembly_address
  %189 = load i1* %cf
  br i1 %189, label %block_b6dc, label %block_b71f

block_b71f:                                       ; preds = %block_b711
  store volatile i64 46879, i64* @assembly_address
  store i16 0, i16* %stack_var_-54
  store volatile i64 46885, i64* @assembly_address
  store i32 1, i32* %stack_var_-192
  store volatile i64 46895, i64* @assembly_address
  br label %block_b77a

block_b731:                                       ; preds = %block_b77a
  store volatile i64 46897, i64* @assembly_address
  %190 = load i32* %stack_var_-192
  %191 = zext i32 %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 46903, i64* @assembly_address
  %192 = load i64* %rbp
  %193 = load i64* %rax
  %194 = mul i64 %193, 2
  %195 = add i64 %192, -48
  %196 = add i64 %195, %194
  %197 = inttoptr i64 %196 to i16*
  %198 = load i16* %197
  %199 = zext i16 %198 to i64
  store i64 %199, i64* %rdx
  store volatile i64 46908, i64* @assembly_address
  %200 = load i32* %stack_var_-192
  %201 = zext i32 %200 to i64
  store i64 %201, i64* %rax
  store volatile i64 46914, i64* @assembly_address
  %202 = load i64* %rbp
  %203 = load i64* %rax
  %204 = mul i64 %203, 2
  %205 = add i64 %202, -144
  %206 = add i64 %205, %204
  %207 = inttoptr i64 %206 to i16*
  %208 = load i16* %207
  %209 = zext i16 %208 to i64
  store i64 %209, i64* %rax
  store volatile i64 46922, i64* @assembly_address
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i16
  %212 = zext i16 %211 to i64
  store i64 %212, i64* %rsi
  store volatile i64 46925, i64* @assembly_address
  store i64 16, i64* %rax
  store volatile i64 46930, i64* @assembly_address
  %213 = load i64* %rax
  %214 = trunc i64 %213 to i32
  %215 = load i32* %stack_var_-192
  %216 = sub i32 %214, %215
  %217 = and i32 %214, 15
  %218 = and i32 %215, 15
  %219 = sub i32 %217, %218
  %220 = icmp ugt i32 %219, 15
  %221 = icmp ult i32 %214, %215
  %222 = xor i32 %214, %215
  %223 = xor i32 %214, %216
  %224 = and i32 %222, %223
  %225 = icmp slt i32 %224, 0
  store i1 %220, i1* %az
  store i1 %221, i1* %cf
  store i1 %225, i1* %of
  %226 = icmp eq i32 %216, 0
  store i1 %226, i1* %zf
  %227 = icmp slt i32 %216, 0
  store i1 %227, i1* %sf
  %228 = trunc i32 %216 to i8
  %229 = call i8 @llvm.ctpop.i8(i8 %228)
  %230 = and i8 %229, 1
  %231 = icmp eq i8 %230, 0
  store i1 %231, i1* %pf
  %232 = zext i32 %216 to i64
  store i64 %232, i64* %rax
  store volatile i64 46936, i64* @assembly_address
  %233 = load i64* %rax
  %234 = trunc i64 %233 to i32
  %235 = zext i32 %234 to i64
  store i64 %235, i64* %rcx
  store volatile i64 46938, i64* @assembly_address
  %236 = load i64* %rsi
  %237 = trunc i64 %236 to i32
  %238 = load i64* %rcx
  %239 = trunc i64 %238 to i8
  %240 = zext i8 %239 to i32
  %241 = and i32 %240, 31
  %242 = load i1* %of
  %243 = icmp eq i32 %241, 0
  br i1 %243, label %261, label %244

; <label>:244                                     ; preds = %block_b731
  %245 = shl i32 %237, %241
  %246 = icmp eq i32 %245, 0
  store i1 %246, i1* %zf
  %247 = icmp slt i32 %245, 0
  store i1 %247, i1* %sf
  %248 = trunc i32 %245 to i8
  %249 = call i8 @llvm.ctpop.i8(i8 %248)
  %250 = and i8 %249, 1
  %251 = icmp eq i8 %250, 0
  store i1 %251, i1* %pf
  %252 = zext i32 %245 to i64
  store i64 %252, i64* %rsi
  %253 = sub i32 %241, 1
  %254 = shl i32 %237, %253
  %255 = lshr i32 %254, 31
  %256 = trunc i32 %255 to i1
  store i1 %256, i1* %cf
  %257 = lshr i32 %245, 31
  %258 = icmp ne i32 %257, %255
  %259 = icmp eq i32 %241, 1
  %260 = select i1 %259, i1 %258, i1 %242
  store i1 %260, i1* %of
  br label %261

; <label>:261                                     ; preds = %block_b731, %244
  store volatile i64 46940, i64* @assembly_address
  %262 = load i64* %rsi
  %263 = trunc i64 %262 to i32
  %264 = zext i32 %263 to i64
  store i64 %264, i64* %rax
  store volatile i64 46942, i64* @assembly_address
  %265 = load i64* %rax
  %266 = trunc i64 %265 to i32
  %267 = zext i32 %266 to i64
  store i64 %267, i64* %rsi
  store volatile i64 46944, i64* @assembly_address
  %268 = load i32* %stack_var_-192
  %269 = zext i32 %268 to i64
  store i64 %269, i64* %rax
  store volatile i64 46950, i64* @assembly_address
  %270 = load i64* %rax
  %271 = add i64 %270, 1
  %272 = trunc i64 %271 to i32
  %273 = zext i32 %272 to i64
  store i64 %273, i64* %rcx
  store volatile i64 46953, i64* @assembly_address
  %274 = load i64* %rdx
  %275 = load i64* %rsi
  %276 = mul i64 %275, 1
  %277 = add i64 %274, %276
  %278 = trunc i64 %277 to i32
  %279 = zext i32 %278 to i64
  store i64 %279, i64* %rax
  store volatile i64 46956, i64* @assembly_address
  %280 = load i64* %rcx
  %281 = trunc i64 %280 to i32
  %282 = zext i32 %281 to i64
  store i64 %282, i64* %rdx
  store volatile i64 46958, i64* @assembly_address
  %283 = load i64* %rax
  %284 = trunc i64 %283 to i16
  %285 = load i64* %rbp
  %286 = load i64* %rdx
  %287 = mul i64 %286, 2
  %288 = add i64 %285, -48
  %289 = add i64 %288, %287
  %290 = inttoptr i64 %289 to i16*
  store i16 %284, i16* %290
  store volatile i64 46963, i64* @assembly_address
  %291 = load i32* %stack_var_-192
  %292 = add i32 %291, 1
  %293 = and i32 %291, 15
  %294 = add i32 %293, 1
  %295 = icmp ugt i32 %294, 15
  %296 = icmp ult i32 %292, %291
  %297 = xor i32 %291, %292
  %298 = xor i32 1, %292
  %299 = and i32 %297, %298
  %300 = icmp slt i32 %299, 0
  store i1 %295, i1* %az
  store i1 %296, i1* %cf
  store i1 %300, i1* %of
  %301 = icmp eq i32 %292, 0
  store i1 %301, i1* %zf
  %302 = icmp slt i32 %292, 0
  store i1 %302, i1* %sf
  %303 = trunc i32 %292 to i8
  %304 = call i8 @llvm.ctpop.i8(i8 %303)
  %305 = and i8 %304, 1
  %306 = icmp eq i8 %305, 0
  store i1 %306, i1* %pf
  store i32 %292, i32* %stack_var_-192
  br label %block_b77a

block_b77a:                                       ; preds = %261, %block_b71f
  store volatile i64 46970, i64* @assembly_address
  %307 = load i32* %stack_var_-192
  %308 = sub i32 %307, 16
  %309 = and i32 %307, 15
  %310 = icmp ugt i32 %309, 15
  %311 = icmp ult i32 %307, 16
  %312 = xor i32 %307, 16
  %313 = xor i32 %307, %308
  %314 = and i32 %312, %313
  %315 = icmp slt i32 %314, 0
  store i1 %310, i1* %az
  store i1 %311, i1* %cf
  store i1 %315, i1* %of
  %316 = icmp eq i32 %308, 0
  store i1 %316, i1* %zf
  %317 = icmp slt i32 %308, 0
  store i1 %317, i1* %sf
  %318 = trunc i32 %308 to i8
  %319 = call i8 @llvm.ctpop.i8(i8 %318)
  %320 = and i8 %319, 1
  %321 = icmp eq i8 %320, 0
  store i1 %321, i1* %pf
  store volatile i64 46977, i64* @assembly_address
  %322 = load i1* %cf
  %323 = load i1* %zf
  %324 = or i1 %322, %323
  br i1 %324, label %block_b731, label %block_b783

block_b783:                                       ; preds = %block_b77a
  store volatile i64 46979, i64* @assembly_address
  %325 = load i16* %stack_var_-22
  %326 = zext i16 %325 to i64
  store i64 %326, i64* %rax
  store volatile i64 46983, i64* @assembly_address
  %327 = load i64* %rax
  %328 = trunc i64 %327 to i16
  %329 = load i64* %rax
  %330 = trunc i64 %329 to i16
  %331 = and i16 %328, %330
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %332 = icmp eq i16 %331, 0
  store i1 %332, i1* %zf
  %333 = icmp slt i16 %331, 0
  store i1 %333, i1* %sf
  %334 = trunc i16 %331 to i8
  %335 = call i8 @llvm.ctpop.i8(i8 %334)
  %336 = and i8 %335, 1
  %337 = icmp eq i8 %336, 0
  store i1 %337, i1* %pf
  store volatile i64 46986, i64* @assembly_address
  %338 = load i1* %zf
  br i1 %338, label %block_b798, label %block_b78c

block_b78c:                                       ; preds = %block_b783
  store volatile i64 46988, i64* @assembly_address
  store i64 ptrtoint ([11 x i8]* @global_var_120bc to i64), i64* %rdi
  store volatile i64 46995, i64* @assembly_address
  %339 = load i64* %rdi
  %340 = inttoptr i64 %339 to i8*
  %341 = call i64 @gzip_error(i8* %340)
  store i64 %341, i64* %rax
  store i64 %341, i64* %rax
  unreachable

block_b798:                                       ; preds = %block_b783
  store volatile i64 47000, i64* @assembly_address
  store i64 16, i64* %rax
  store volatile i64 47005, i64* @assembly_address
  %342 = load i64* %rax
  %343 = trunc i64 %342 to i32
  %344 = load i32* %stack_var_-208
  %345 = sub i32 %343, %344
  %346 = and i32 %343, 15
  %347 = and i32 %344, 15
  %348 = sub i32 %346, %347
  %349 = icmp ugt i32 %348, 15
  %350 = icmp ult i32 %343, %344
  %351 = xor i32 %343, %344
  %352 = xor i32 %343, %345
  %353 = and i32 %351, %352
  %354 = icmp slt i32 %353, 0
  store i1 %349, i1* %az
  store i1 %350, i1* %cf
  store i1 %354, i1* %of
  %355 = icmp eq i32 %345, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i32 %345, 0
  store i1 %356, i1* %sf
  %357 = trunc i32 %345 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  %361 = zext i32 %345 to i64
  store i64 %361, i64* %rax
  store volatile i64 47011, i64* @assembly_address
  %362 = load i64* %rax
  %363 = trunc i64 %362 to i32
  store i32 %363, i32* %stack_var_-176
  store volatile i64 47017, i64* @assembly_address
  store i32 1, i32* %stack_var_-192
  store volatile i64 47027, i64* @assembly_address
  br label %block_b807

block_b7b5:                                       ; preds = %block_b807
  store volatile i64 47029, i64* @assembly_address
  %364 = load i32* %stack_var_-192
  %365 = zext i32 %364 to i64
  store i64 %365, i64* %rax
  store volatile i64 47035, i64* @assembly_address
  %366 = load i64* %rbp
  %367 = load i64* %rax
  %368 = mul i64 %367, 2
  %369 = add i64 %366, -48
  %370 = add i64 %369, %368
  %371 = inttoptr i64 %370 to i16*
  %372 = load i16* %371
  %373 = zext i16 %372 to i64
  store i64 %373, i64* %rax
  store volatile i64 47040, i64* @assembly_address
  %374 = load i64* %rax
  %375 = trunc i64 %374 to i16
  %376 = zext i16 %375 to i64
  store i64 %376, i64* %rdx
  store volatile i64 47043, i64* @assembly_address
  %377 = load i32* %stack_var_-176
  %378 = zext i32 %377 to i64
  store i64 %378, i64* %rax
  store volatile i64 47049, i64* @assembly_address
  %379 = load i64* %rax
  %380 = trunc i64 %379 to i32
  %381 = zext i32 %380 to i64
  store i64 %381, i64* %rcx
  store volatile i64 47051, i64* @assembly_address
  %382 = load i64* %rdx
  %383 = trunc i64 %382 to i32
  %384 = load i64* %rcx
  %385 = trunc i64 %384 to i8
  %386 = zext i8 %385 to i32
  %387 = and i32 %386, 31
  %388 = load i1* %of
  %389 = icmp eq i32 %387, 0
  br i1 %389, label %405, label %390

; <label>:390                                     ; preds = %block_b7b5
  %391 = ashr i32 %383, %387
  %392 = icmp eq i32 %391, 0
  store i1 %392, i1* %zf
  %393 = icmp slt i32 %391, 0
  store i1 %393, i1* %sf
  %394 = trunc i32 %391 to i8
  %395 = call i8 @llvm.ctpop.i8(i8 %394)
  %396 = and i8 %395, 1
  %397 = icmp eq i8 %396, 0
  store i1 %397, i1* %pf
  %398 = zext i32 %391 to i64
  store i64 %398, i64* %rdx
  %399 = sub i32 %387, 1
  %400 = shl i32 1, %399
  %401 = and i32 %400, %383
  %402 = icmp ne i32 %401, 0
  store i1 %402, i1* %cf
  %403 = icmp eq i32 %387, 1
  %404 = select i1 %403, i1 false, i1 %388
  store i1 %404, i1* %of
  br label %405

; <label>:405                                     ; preds = %block_b7b5, %390
  store volatile i64 47053, i64* @assembly_address
  %406 = load i64* %rdx
  %407 = trunc i64 %406 to i32
  %408 = zext i32 %407 to i64
  store i64 %408, i64* %rax
  store volatile i64 47055, i64* @assembly_address
  %409 = load i64* %rax
  %410 = trunc i64 %409 to i32
  %411 = zext i32 %410 to i64
  store i64 %411, i64* %rdx
  store volatile i64 47057, i64* @assembly_address
  %412 = load i32* %stack_var_-192
  %413 = zext i32 %412 to i64
  store i64 %413, i64* %rax
  store volatile i64 47063, i64* @assembly_address
  %414 = load i64* %rdx
  %415 = trunc i64 %414 to i16
  %416 = load i64* %rbp
  %417 = load i64* %rax
  %418 = mul i64 %417, 2
  %419 = add i64 %416, -48
  %420 = add i64 %419, %418
  %421 = inttoptr i64 %420 to i16*
  store i16 %415, i16* %421
  store volatile i64 47068, i64* @assembly_address
  %422 = load i32* %stack_var_-208
  %423 = zext i32 %422 to i64
  store i64 %423, i64* %rax
  store volatile i64 47074, i64* @assembly_address
  %424 = load i64* %rax
  %425 = trunc i64 %424 to i32
  %426 = load i32* %stack_var_-192
  %427 = sub i32 %425, %426
  %428 = and i32 %425, 15
  %429 = and i32 %426, 15
  %430 = sub i32 %428, %429
  %431 = icmp ugt i32 %430, 15
  %432 = icmp ult i32 %425, %426
  %433 = xor i32 %425, %426
  %434 = xor i32 %425, %427
  %435 = and i32 %433, %434
  %436 = icmp slt i32 %435, 0
  store i1 %431, i1* %az
  store i1 %432, i1* %cf
  store i1 %436, i1* %of
  %437 = icmp eq i32 %427, 0
  store i1 %437, i1* %zf
  %438 = icmp slt i32 %427, 0
  store i1 %438, i1* %sf
  %439 = trunc i32 %427 to i8
  %440 = call i8 @llvm.ctpop.i8(i8 %439)
  %441 = and i8 %440, 1
  %442 = icmp eq i8 %441, 0
  store i1 %442, i1* %pf
  %443 = zext i32 %427 to i64
  store i64 %443, i64* %rax
  store volatile i64 47080, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 47085, i64* @assembly_address
  %444 = load i64* %rax
  %445 = trunc i64 %444 to i32
  %446 = zext i32 %445 to i64
  store i64 %446, i64* %rcx
  store volatile i64 47087, i64* @assembly_address
  %447 = load i64* %rdx
  %448 = trunc i64 %447 to i32
  %449 = load i64* %rcx
  %450 = trunc i64 %449 to i8
  %451 = zext i8 %450 to i32
  %452 = and i32 %451, 31
  %453 = load i1* %of
  %454 = icmp eq i32 %452, 0
  br i1 %454, label %472, label %455

; <label>:455                                     ; preds = %405
  %456 = shl i32 %448, %452
  %457 = icmp eq i32 %456, 0
  store i1 %457, i1* %zf
  %458 = icmp slt i32 %456, 0
  store i1 %458, i1* %sf
  %459 = trunc i32 %456 to i8
  %460 = call i8 @llvm.ctpop.i8(i8 %459)
  %461 = and i8 %460, 1
  %462 = icmp eq i8 %461, 0
  store i1 %462, i1* %pf
  %463 = zext i32 %456 to i64
  store i64 %463, i64* %rdx
  %464 = sub i32 %452, 1
  %465 = shl i32 %448, %464
  %466 = lshr i32 %465, 31
  %467 = trunc i32 %466 to i1
  store i1 %467, i1* %cf
  %468 = lshr i32 %456, 31
  %469 = icmp ne i32 %468, %466
  %470 = icmp eq i32 %452, 1
  %471 = select i1 %470, i1 %469, i1 %453
  store i1 %471, i1* %of
  br label %472

; <label>:472                                     ; preds = %405, %455
  store volatile i64 47089, i64* @assembly_address
  %473 = load i64* %rdx
  %474 = trunc i64 %473 to i32
  %475 = zext i32 %474 to i64
  store i64 %475, i64* %rax
  store volatile i64 47091, i64* @assembly_address
  %476 = load i64* %rax
  %477 = trunc i64 %476 to i32
  %478 = zext i32 %477 to i64
  store i64 %478, i64* %rdx
  store volatile i64 47093, i64* @assembly_address
  %479 = load i32* %stack_var_-192
  %480 = zext i32 %479 to i64
  store i64 %480, i64* %rax
  store volatile i64 47099, i64* @assembly_address
  %481 = load i64* %rdx
  %482 = trunc i64 %481 to i16
  %483 = load i64* %rbp
  %484 = load i64* %rax
  %485 = mul i64 %484, 2
  %486 = add i64 %483, -96
  %487 = add i64 %486, %485
  %488 = inttoptr i64 %487 to i16*
  store i16 %482, i16* %488
  store volatile i64 47104, i64* @assembly_address
  %489 = load i32* %stack_var_-192
  %490 = add i32 %489, 1
  %491 = and i32 %489, 15
  %492 = add i32 %491, 1
  %493 = icmp ugt i32 %492, 15
  %494 = icmp ult i32 %490, %489
  %495 = xor i32 %489, %490
  %496 = xor i32 1, %490
  %497 = and i32 %495, %496
  %498 = icmp slt i32 %497, 0
  store i1 %493, i1* %az
  store i1 %494, i1* %cf
  store i1 %498, i1* %of
  %499 = icmp eq i32 %490, 0
  store i1 %499, i1* %zf
  %500 = icmp slt i32 %490, 0
  store i1 %500, i1* %sf
  %501 = trunc i32 %490 to i8
  %502 = call i8 @llvm.ctpop.i8(i8 %501)
  %503 = and i8 %502, 1
  %504 = icmp eq i8 %503, 0
  store i1 %504, i1* %pf
  store i32 %490, i32* %stack_var_-192
  br label %block_b807

block_b807:                                       ; preds = %472, %block_b798
  store volatile i64 47111, i64* @assembly_address
  %505 = load i32* %stack_var_-208
  %506 = zext i32 %505 to i64
  store i64 %506, i64* %rax
  store volatile i64 47117, i64* @assembly_address
  %507 = load i32* %stack_var_-192
  %508 = load i64* %rax
  %509 = trunc i64 %508 to i32
  %510 = sub i32 %507, %509
  %511 = and i32 %507, 15
  %512 = and i32 %509, 15
  %513 = sub i32 %511, %512
  %514 = icmp ugt i32 %513, 15
  %515 = icmp ult i32 %507, %509
  %516 = xor i32 %507, %509
  %517 = xor i32 %507, %510
  %518 = and i32 %516, %517
  %519 = icmp slt i32 %518, 0
  store i1 %514, i1* %az
  store i1 %515, i1* %cf
  store i1 %519, i1* %of
  %520 = icmp eq i32 %510, 0
  store i1 %520, i1* %zf
  %521 = icmp slt i32 %510, 0
  store i1 %521, i1* %sf
  %522 = trunc i32 %510 to i8
  %523 = call i8 @llvm.ctpop.i8(i8 %522)
  %524 = and i8 %523, 1
  %525 = icmp eq i8 %524, 0
  store i1 %525, i1* %pf
  store volatile i64 47123, i64* @assembly_address
  %526 = load i1* %cf
  %527 = load i1* %zf
  %528 = or i1 %526, %527
  br i1 %528, label %block_b7b5, label %block_b815

block_b815:                                       ; preds = %block_b807
  store volatile i64 47125, i64* @assembly_address
  br label %block_b841

block_b817:                                       ; preds = %block_b841
  store volatile i64 47127, i64* @assembly_address
  store i64 16, i64* %rax
  store volatile i64 47132, i64* @assembly_address
  %529 = load i64* %rax
  %530 = trunc i64 %529 to i32
  %531 = load i32* %stack_var_-192
  %532 = sub i32 %530, %531
  %533 = and i32 %530, 15
  %534 = and i32 %531, 15
  %535 = sub i32 %533, %534
  %536 = icmp ugt i32 %535, 15
  %537 = icmp ult i32 %530, %531
  %538 = xor i32 %530, %531
  %539 = xor i32 %530, %532
  %540 = and i32 %538, %539
  %541 = icmp slt i32 %540, 0
  store i1 %536, i1* %az
  store i1 %537, i1* %cf
  store i1 %541, i1* %of
  %542 = icmp eq i32 %532, 0
  store i1 %542, i1* %zf
  %543 = icmp slt i32 %532, 0
  store i1 %543, i1* %sf
  %544 = trunc i32 %532 to i8
  %545 = call i8 @llvm.ctpop.i8(i8 %544)
  %546 = and i8 %545, 1
  %547 = icmp eq i8 %546, 0
  store i1 %547, i1* %pf
  %548 = zext i32 %532 to i64
  store i64 %548, i64* %rax
  store volatile i64 47138, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 47143, i64* @assembly_address
  %549 = load i64* %rax
  %550 = trunc i64 %549 to i32
  %551 = zext i32 %550 to i64
  store i64 %551, i64* %rcx
  store volatile i64 47145, i64* @assembly_address
  %552 = load i64* %rdx
  %553 = trunc i64 %552 to i32
  %554 = load i64* %rcx
  %555 = trunc i64 %554 to i8
  %556 = zext i8 %555 to i32
  %557 = and i32 %556, 31
  %558 = load i1* %of
  %559 = icmp eq i32 %557, 0
  br i1 %559, label %577, label %560

; <label>:560                                     ; preds = %block_b817
  %561 = shl i32 %553, %557
  %562 = icmp eq i32 %561, 0
  store i1 %562, i1* %zf
  %563 = icmp slt i32 %561, 0
  store i1 %563, i1* %sf
  %564 = trunc i32 %561 to i8
  %565 = call i8 @llvm.ctpop.i8(i8 %564)
  %566 = and i8 %565, 1
  %567 = icmp eq i8 %566, 0
  store i1 %567, i1* %pf
  %568 = zext i32 %561 to i64
  store i64 %568, i64* %rdx
  %569 = sub i32 %557, 1
  %570 = shl i32 %553, %569
  %571 = lshr i32 %570, 31
  %572 = trunc i32 %571 to i1
  store i1 %572, i1* %cf
  %573 = lshr i32 %561, 31
  %574 = icmp ne i32 %573, %571
  %575 = icmp eq i32 %557, 1
  %576 = select i1 %575, i1 %574, i1 %558
  store i1 %576, i1* %of
  br label %577

; <label>:577                                     ; preds = %block_b817, %560
  store volatile i64 47147, i64* @assembly_address
  %578 = load i64* %rdx
  %579 = trunc i64 %578 to i32
  %580 = zext i32 %579 to i64
  store i64 %580, i64* %rax
  store volatile i64 47149, i64* @assembly_address
  %581 = load i64* %rax
  %582 = trunc i64 %581 to i32
  %583 = zext i32 %582 to i64
  store i64 %583, i64* %rdx
  store volatile i64 47151, i64* @assembly_address
  %584 = load i32* %stack_var_-192
  %585 = zext i32 %584 to i64
  store i64 %585, i64* %rax
  store volatile i64 47157, i64* @assembly_address
  %586 = load i64* %rdx
  %587 = trunc i64 %586 to i16
  %588 = load i64* %rbp
  %589 = load i64* %rax
  %590 = mul i64 %589, 2
  %591 = add i64 %588, -96
  %592 = add i64 %591, %590
  %593 = inttoptr i64 %592 to i16*
  store i16 %587, i16* %593
  store volatile i64 47162, i64* @assembly_address
  %594 = load i32* %stack_var_-192
  %595 = add i32 %594, 1
  %596 = and i32 %594, 15
  %597 = add i32 %596, 1
  %598 = icmp ugt i32 %597, 15
  %599 = icmp ult i32 %595, %594
  %600 = xor i32 %594, %595
  %601 = xor i32 1, %595
  %602 = and i32 %600, %601
  %603 = icmp slt i32 %602, 0
  store i1 %598, i1* %az
  store i1 %599, i1* %cf
  store i1 %603, i1* %of
  %604 = icmp eq i32 %595, 0
  store i1 %604, i1* %zf
  %605 = icmp slt i32 %595, 0
  store i1 %605, i1* %sf
  %606 = trunc i32 %595 to i8
  %607 = call i8 @llvm.ctpop.i8(i8 %606)
  %608 = and i8 %607, 1
  %609 = icmp eq i8 %608, 0
  store i1 %609, i1* %pf
  store i32 %595, i32* %stack_var_-192
  br label %block_b841

block_b841:                                       ; preds = %577, %block_b815
  store volatile i64 47169, i64* @assembly_address
  %610 = load i32* %stack_var_-192
  %611 = sub i32 %610, 16
  %612 = and i32 %610, 15
  %613 = icmp ugt i32 %612, 15
  %614 = icmp ult i32 %610, 16
  %615 = xor i32 %610, 16
  %616 = xor i32 %610, %611
  %617 = and i32 %615, %616
  %618 = icmp slt i32 %617, 0
  store i1 %613, i1* %az
  store i1 %614, i1* %cf
  store i1 %618, i1* %of
  %619 = icmp eq i32 %611, 0
  store i1 %619, i1* %zf
  %620 = icmp slt i32 %611, 0
  store i1 %620, i1* %sf
  %621 = trunc i32 %611 to i8
  %622 = call i8 @llvm.ctpop.i8(i8 %621)
  %623 = and i8 %622, 1
  %624 = icmp eq i8 %623, 0
  store i1 %624, i1* %pf
  store volatile i64 47176, i64* @assembly_address
  %625 = load i1* %cf
  %626 = load i1* %zf
  %627 = or i1 %625, %626
  br i1 %627, label %block_b817, label %block_b84a

block_b84a:                                       ; preds = %block_b841
  store volatile i64 47178, i64* @assembly_address
  %628 = load i32* %stack_var_-208
  %629 = zext i32 %628 to i64
  store i64 %629, i64* %rax
  store volatile i64 47184, i64* @assembly_address
  %630 = load i64* %rax
  %631 = trunc i64 %630 to i32
  %632 = add i32 %631, 1
  %633 = and i32 %631, 15
  %634 = add i32 %633, 1
  %635 = icmp ugt i32 %634, 15
  %636 = icmp ult i32 %632, %631
  %637 = xor i32 %631, %632
  %638 = xor i32 1, %632
  %639 = and i32 %637, %638
  %640 = icmp slt i32 %639, 0
  store i1 %635, i1* %az
  store i1 %636, i1* %cf
  store i1 %640, i1* %of
  %641 = icmp eq i32 %632, 0
  store i1 %641, i1* %zf
  %642 = icmp slt i32 %632, 0
  store i1 %642, i1* %sf
  %643 = trunc i32 %632 to i8
  %644 = call i8 @llvm.ctpop.i8(i8 %643)
  %645 = and i8 %644, 1
  %646 = icmp eq i8 %645, 0
  store i1 %646, i1* %pf
  %647 = zext i32 %632 to i64
  store i64 %647, i64* %rax
  store volatile i64 47187, i64* @assembly_address
  %648 = load i64* %rax
  %649 = trunc i64 %648 to i32
  %650 = sext i32 %649 to i64
  store i64 %650, i64* %rax
  store volatile i64 47189, i64* @assembly_address
  %651 = load i64* %rbp
  %652 = load i64* %rax
  %653 = mul i64 %652, 2
  %654 = add i64 %651, -48
  %655 = add i64 %654, %653
  %656 = inttoptr i64 %655 to i16*
  %657 = load i16* %656
  %658 = zext i16 %657 to i64
  store i64 %658, i64* %rax
  store volatile i64 47194, i64* @assembly_address
  %659 = load i64* %rax
  %660 = trunc i64 %659 to i16
  %661 = zext i16 %660 to i64
  store i64 %661, i64* %rdx
  store volatile i64 47197, i64* @assembly_address
  %662 = load i32* %stack_var_-176
  %663 = zext i32 %662 to i64
  store i64 %663, i64* %rax
  store volatile i64 47203, i64* @assembly_address
  %664 = load i64* %rax
  %665 = trunc i64 %664 to i32
  %666 = zext i32 %665 to i64
  store i64 %666, i64* %rcx
  store volatile i64 47205, i64* @assembly_address
  %667 = load i64* %rdx
  %668 = trunc i64 %667 to i32
  %669 = load i64* %rcx
  %670 = trunc i64 %669 to i8
  %671 = zext i8 %670 to i32
  %672 = and i32 %671, 31
  %673 = load i1* %of
  %674 = icmp eq i32 %672, 0
  br i1 %674, label %690, label %675

; <label>:675                                     ; preds = %block_b84a
  %676 = ashr i32 %668, %672
  %677 = icmp eq i32 %676, 0
  store i1 %677, i1* %zf
  %678 = icmp slt i32 %676, 0
  store i1 %678, i1* %sf
  %679 = trunc i32 %676 to i8
  %680 = call i8 @llvm.ctpop.i8(i8 %679)
  %681 = and i8 %680, 1
  %682 = icmp eq i8 %681, 0
  store i1 %682, i1* %pf
  %683 = zext i32 %676 to i64
  store i64 %683, i64* %rdx
  %684 = sub i32 %672, 1
  %685 = shl i32 1, %684
  %686 = and i32 %685, %668
  %687 = icmp ne i32 %686, 0
  store i1 %687, i1* %cf
  %688 = icmp eq i32 %672, 1
  %689 = select i1 %688, i1 false, i1 %673
  store i1 %689, i1* %of
  br label %690

; <label>:690                                     ; preds = %block_b84a, %675
  store volatile i64 47207, i64* @assembly_address
  %691 = load i64* %rdx
  %692 = trunc i64 %691 to i32
  %693 = zext i32 %692 to i64
  store i64 %693, i64* %rax
  store volatile i64 47209, i64* @assembly_address
  %694 = load i64* %rax
  %695 = trunc i64 %694 to i32
  store i32 %695, i32* %stack_var_-192
  store volatile i64 47215, i64* @assembly_address
  %696 = load i32* %stack_var_-192
  %697 = and i32 %696, 15
  %698 = icmp ugt i32 %697, 15
  %699 = icmp ult i32 %696, 0
  %700 = xor i32 %696, 0
  %701 = and i32 %700, 0
  %702 = icmp slt i32 %701, 0
  store i1 %698, i1* %az
  store i1 %699, i1* %cf
  store i1 %702, i1* %of
  %703 = icmp eq i32 %696, 0
  store i1 %703, i1* %zf
  %704 = icmp slt i32 %696, 0
  store i1 %704, i1* %sf
  %705 = trunc i32 %696 to i8
  %706 = call i8 @llvm.ctpop.i8(i8 %705)
  %707 = and i8 %706, 1
  %708 = icmp eq i8 %707, 0
  store i1 %708, i1* %pf
  store volatile i64 47222, i64* @assembly_address
  %709 = load i1* %zf
  br i1 %709, label %block_b8c3, label %block_b878

block_b878:                                       ; preds = %690
  store volatile i64 47224, i64* @assembly_address
  %710 = load i32* %stack_var_-208
  %711 = zext i32 %710 to i64
  store i64 %711, i64* %rax
  store volatile i64 47230, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 47235, i64* @assembly_address
  %712 = load i64* %rax
  %713 = trunc i64 %712 to i32
  %714 = zext i32 %713 to i64
  store i64 %714, i64* %rcx
  store volatile i64 47237, i64* @assembly_address
  %715 = load i64* %rdx
  %716 = trunc i64 %715 to i32
  %717 = load i64* %rcx
  %718 = trunc i64 %717 to i8
  %719 = zext i8 %718 to i32
  %720 = and i32 %719, 31
  %721 = load i1* %of
  %722 = icmp eq i32 %720, 0
  br i1 %722, label %740, label %723

; <label>:723                                     ; preds = %block_b878
  %724 = shl i32 %716, %720
  %725 = icmp eq i32 %724, 0
  store i1 %725, i1* %zf
  %726 = icmp slt i32 %724, 0
  store i1 %726, i1* %sf
  %727 = trunc i32 %724 to i8
  %728 = call i8 @llvm.ctpop.i8(i8 %727)
  %729 = and i8 %728, 1
  %730 = icmp eq i8 %729, 0
  store i1 %730, i1* %pf
  %731 = zext i32 %724 to i64
  store i64 %731, i64* %rdx
  %732 = sub i32 %720, 1
  %733 = shl i32 %716, %732
  %734 = lshr i32 %733, 31
  %735 = trunc i32 %734 to i1
  store i1 %735, i1* %cf
  %736 = lshr i32 %724, 31
  %737 = icmp ne i32 %736, %734
  %738 = icmp eq i32 %720, 1
  %739 = select i1 %738, i1 %737, i1 %721
  store i1 %739, i1* %of
  br label %740

; <label>:740                                     ; preds = %block_b878, %723
  store volatile i64 47239, i64* @assembly_address
  %741 = load i64* %rdx
  %742 = trunc i64 %741 to i32
  %743 = zext i32 %742 to i64
  store i64 %743, i64* %rax
  store volatile i64 47241, i64* @assembly_address
  %744 = load i64* %rax
  %745 = trunc i64 %744 to i32
  store i32 %745, i32* %stack_var_-188
  store volatile i64 47247, i64* @assembly_address
  br label %block_b8b5

block_b891:                                       ; preds = %block_b8b5
  store volatile i64 47249, i64* @assembly_address
  %746 = load i32* %stack_var_-192
  %747 = zext i32 %746 to i64
  store i64 %747, i64* %rax
  store volatile i64 47255, i64* @assembly_address
  %748 = load i64* %rax
  %749 = add i64 %748, 1
  %750 = trunc i64 %749 to i32
  %751 = zext i32 %750 to i64
  store i64 %751, i64* %rdx
  store volatile i64 47258, i64* @assembly_address
  %752 = load i64* %rdx
  %753 = trunc i64 %752 to i32
  store i32 %753, i32* %stack_var_-192
  store volatile i64 47264, i64* @assembly_address
  %754 = load i64* %rax
  %755 = trunc i64 %754 to i32
  %756 = zext i32 %755 to i64
  store i64 %756, i64* %rax
  store volatile i64 47266, i64* @assembly_address
  %757 = load i64* %rax
  %758 = load i64* %rax
  %759 = mul i64 %758, 1
  %760 = add i64 %757, %759
  store i64 %760, i64* %rdx
  store volatile i64 47270, i64* @assembly_address
  %761 = load i16** %stack_var_-224
  %762 = ptrtoint i16* %761 to i64
  store i64 %762, i64* %rax
  store volatile i64 47277, i64* @assembly_address
  %763 = load i64* %rax
  %764 = load i64* %rdx
  %765 = add i64 %763, %764
  %766 = and i64 %763, 15
  %767 = and i64 %764, 15
  %768 = add i64 %766, %767
  %769 = icmp ugt i64 %768, 15
  %770 = icmp ult i64 %765, %763
  %771 = xor i64 %763, %765
  %772 = xor i64 %764, %765
  %773 = and i64 %771, %772
  %774 = icmp slt i64 %773, 0
  store i1 %769, i1* %az
  store i1 %770, i1* %cf
  store i1 %774, i1* %of
  %775 = icmp eq i64 %765, 0
  store i1 %775, i1* %zf
  %776 = icmp slt i64 %765, 0
  store i1 %776, i1* %sf
  %777 = trunc i64 %765 to i8
  %778 = call i8 @llvm.ctpop.i8(i8 %777)
  %779 = and i8 %778, 1
  %780 = icmp eq i8 %779, 0
  store i1 %780, i1* %pf
  store i64 %765, i64* %rax
  store volatile i64 47280, i64* @assembly_address
  %781 = load i64* %rax
  %782 = inttoptr i64 %781 to i16*
  store i16 0, i16* %782
  br label %block_b8b5

block_b8b5:                                       ; preds = %block_b891, %740
  store volatile i64 47285, i64* @assembly_address
  %783 = load i32* %stack_var_-192
  %784 = zext i32 %783 to i64
  store i64 %784, i64* %rax
  store volatile i64 47291, i64* @assembly_address
  %785 = load i64* %rax
  %786 = trunc i64 %785 to i32
  %787 = load i32* %stack_var_-188
  %788 = sub i32 %786, %787
  %789 = and i32 %786, 15
  %790 = and i32 %787, 15
  %791 = sub i32 %789, %790
  %792 = icmp ugt i32 %791, 15
  %793 = icmp ult i32 %786, %787
  %794 = xor i32 %786, %787
  %795 = xor i32 %786, %788
  %796 = and i32 %794, %795
  %797 = icmp slt i32 %796, 0
  store i1 %792, i1* %az
  store i1 %793, i1* %cf
  store i1 %797, i1* %of
  %798 = icmp eq i32 %788, 0
  store i1 %798, i1* %zf
  %799 = icmp slt i32 %788, 0
  store i1 %799, i1* %sf
  %800 = trunc i32 %788 to i8
  %801 = call i8 @llvm.ctpop.i8(i8 %800)
  %802 = and i8 %801, 1
  %803 = icmp eq i8 %802, 0
  store i1 %803, i1* %pf
  store volatile i64 47297, i64* @assembly_address
  %804 = load i1* %zf
  %805 = icmp eq i1 %804, false
  br i1 %805, label %block_b891, label %block_b8c3

block_b8c3:                                       ; preds = %block_b8b5, %690
  store volatile i64 47299, i64* @assembly_address
  %806 = load i32* %stack_var_-204
  %807 = zext i32 %806 to i64
  store i64 %807, i64* %rax
  store volatile i64 47305, i64* @assembly_address
  %808 = load i64* %rax
  %809 = trunc i64 %808 to i32
  store i32 %809, i32* %stack_var_-180
  store volatile i64 47311, i64* @assembly_address
  store i64 15, i64* %rax
  store volatile i64 47316, i64* @assembly_address
  %810 = load i64* %rax
  %811 = trunc i64 %810 to i32
  %812 = load i32* %stack_var_-208
  %813 = sub i32 %811, %812
  %814 = and i32 %811, 15
  %815 = and i32 %812, 15
  %816 = sub i32 %814, %815
  %817 = icmp ugt i32 %816, 15
  %818 = icmp ult i32 %811, %812
  %819 = xor i32 %811, %812
  %820 = xor i32 %811, %813
  %821 = and i32 %819, %820
  %822 = icmp slt i32 %821, 0
  store i1 %817, i1* %az
  store i1 %818, i1* %cf
  store i1 %822, i1* %of
  %823 = icmp eq i32 %813, 0
  store i1 %823, i1* %zf
  %824 = icmp slt i32 %813, 0
  store i1 %824, i1* %sf
  %825 = trunc i32 %813 to i8
  %826 = call i8 @llvm.ctpop.i8(i8 %825)
  %827 = and i8 %826, 1
  %828 = icmp eq i8 %827, 0
  store i1 %828, i1* %pf
  %829 = zext i32 %813 to i64
  store i64 %829, i64* %rax
  store volatile i64 47322, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 47327, i64* @assembly_address
  %830 = load i64* %rax
  %831 = trunc i64 %830 to i32
  %832 = zext i32 %831 to i64
  store i64 %832, i64* %rcx
  store volatile i64 47329, i64* @assembly_address
  %833 = load i64* %rdx
  %834 = trunc i64 %833 to i32
  %835 = load i64* %rcx
  %836 = trunc i64 %835 to i8
  %837 = zext i8 %836 to i32
  %838 = and i32 %837, 31
  %839 = load i1* %of
  %840 = icmp eq i32 %838, 0
  br i1 %840, label %858, label %841

; <label>:841                                     ; preds = %block_b8c3
  %842 = shl i32 %834, %838
  %843 = icmp eq i32 %842, 0
  store i1 %843, i1* %zf
  %844 = icmp slt i32 %842, 0
  store i1 %844, i1* %sf
  %845 = trunc i32 %842 to i8
  %846 = call i8 @llvm.ctpop.i8(i8 %845)
  %847 = and i8 %846, 1
  %848 = icmp eq i8 %847, 0
  store i1 %848, i1* %pf
  %849 = zext i32 %842 to i64
  store i64 %849, i64* %rdx
  %850 = sub i32 %838, 1
  %851 = shl i32 %834, %850
  %852 = lshr i32 %851, 31
  %853 = trunc i32 %852 to i1
  store i1 %853, i1* %cf
  %854 = lshr i32 %842, 31
  %855 = icmp ne i32 %854, %852
  %856 = icmp eq i32 %838, 1
  %857 = select i1 %856, i1 %855, i1 %839
  store i1 %857, i1* %of
  br label %858

; <label>:858                                     ; preds = %block_b8c3, %841
  store volatile i64 47331, i64* @assembly_address
  %859 = load i64* %rdx
  %860 = trunc i64 %859 to i32
  %861 = zext i32 %860 to i64
  store i64 %861, i64* %rax
  store volatile i64 47333, i64* @assembly_address
  %862 = load i64* %rax
  %863 = trunc i64 %862 to i32
  store i32 %863, i32* %stack_var_-172
  store volatile i64 47339, i64* @assembly_address
  store i32 0, i32* %stack_var_-184
  store volatile i64 47349, i64* @assembly_address
  br label %block_bb39

block_b8fa:                                       ; preds = %block_bb39
  store volatile i64 47354, i64* @assembly_address
  %864 = load i32* %stack_var_-184
  %865 = zext i32 %864 to i64
  store i64 %865, i64* %rdx
  store volatile i64 47360, i64* @assembly_address
  %866 = load i8** %stack_var_-216
  %867 = ptrtoint i8* %866 to i64
  store i64 %867, i64* %rax
  store volatile i64 47367, i64* @assembly_address
  %868 = load i64* %rax
  %869 = load i64* %rdx
  %870 = add i64 %868, %869
  %871 = and i64 %868, 15
  %872 = and i64 %869, 15
  %873 = add i64 %871, %872
  %874 = icmp ugt i64 %873, 15
  %875 = icmp ult i64 %870, %868
  %876 = xor i64 %868, %870
  %877 = xor i64 %869, %870
  %878 = and i64 %876, %877
  %879 = icmp slt i64 %878, 0
  store i1 %874, i1* %az
  store i1 %875, i1* %cf
  store i1 %879, i1* %of
  %880 = icmp eq i64 %870, 0
  store i1 %880, i1* %zf
  %881 = icmp slt i64 %870, 0
  store i1 %881, i1* %sf
  %882 = trunc i64 %870 to i8
  %883 = call i8 @llvm.ctpop.i8(i8 %882)
  %884 = and i8 %883, 1
  %885 = icmp eq i8 %884, 0
  store i1 %885, i1* %pf
  store i64 %870, i64* %rax
  store volatile i64 47370, i64* @assembly_address
  %886 = load i64* %rax
  %887 = inttoptr i64 %886 to i8*
  %888 = load i8* %887
  %889 = zext i8 %888 to i64
  store i64 %889, i64* %rax
  store volatile i64 47373, i64* @assembly_address
  %890 = load i64* %rax
  %891 = trunc i64 %890 to i8
  %892 = zext i8 %891 to i64
  store i64 %892, i64* %rax
  store volatile i64 47376, i64* @assembly_address
  %893 = load i64* %rax
  %894 = trunc i64 %893 to i32
  store i32 %894, i32* %stack_var_-168
  store volatile i64 47382, i64* @assembly_address
  %895 = load i32* %stack_var_-168
  %896 = and i32 %895, 15
  %897 = icmp ugt i32 %896, 15
  %898 = icmp ult i32 %895, 0
  %899 = xor i32 %895, 0
  %900 = and i32 %899, 0
  %901 = icmp slt i32 %900, 0
  store i1 %897, i1* %az
  store i1 %898, i1* %cf
  store i1 %901, i1* %of
  %902 = icmp eq i32 %895, 0
  store i1 %902, i1* %zf
  %903 = icmp slt i32 %895, 0
  store i1 %903, i1* %sf
  %904 = trunc i32 %895 to i8
  %905 = call i8 @llvm.ctpop.i8(i8 %904)
  %906 = and i8 %905, 1
  %907 = icmp eq i8 %906, 0
  store i1 %907, i1* %pf
  store volatile i64 47389, i64* @assembly_address
  %908 = load i1* %zf
  br i1 %908, label %block_bb31, label %block_b923

block_b923:                                       ; preds = %block_b8fa
  store volatile i64 47395, i64* @assembly_address
  %909 = load i32* %stack_var_-168
  %910 = zext i32 %909 to i64
  store i64 %910, i64* %rax
  store volatile i64 47401, i64* @assembly_address
  %911 = load i64* %rbp
  %912 = load i64* %rax
  %913 = mul i64 %912, 2
  %914 = add i64 %911, -48
  %915 = add i64 %914, %913
  %916 = inttoptr i64 %915 to i16*
  %917 = load i16* %916
  %918 = zext i16 %917 to i64
  store i64 %918, i64* %rax
  store volatile i64 47406, i64* @assembly_address
  %919 = load i64* %rax
  %920 = trunc i64 %919 to i16
  %921 = zext i16 %920 to i64
  store i64 %921, i64* %rdx
  store volatile i64 47409, i64* @assembly_address
  %922 = load i32* %stack_var_-168
  %923 = zext i32 %922 to i64
  store i64 %923, i64* %rax
  store volatile i64 47415, i64* @assembly_address
  %924 = load i64* %rbp
  %925 = load i64* %rax
  %926 = mul i64 %925, 2
  %927 = add i64 %924, -96
  %928 = add i64 %927, %926
  %929 = inttoptr i64 %928 to i16*
  %930 = load i16* %929
  %931 = zext i16 %930 to i64
  store i64 %931, i64* %rax
  store volatile i64 47420, i64* @assembly_address
  %932 = load i64* %rax
  %933 = trunc i64 %932 to i16
  %934 = zext i16 %933 to i64
  store i64 %934, i64* %rax
  store volatile i64 47423, i64* @assembly_address
  %935 = load i64* %rax
  %936 = trunc i64 %935 to i32
  %937 = load i64* %rdx
  %938 = trunc i64 %937 to i32
  %939 = add i32 %936, %938
  %940 = and i32 %936, 15
  %941 = and i32 %938, 15
  %942 = add i32 %940, %941
  %943 = icmp ugt i32 %942, 15
  %944 = icmp ult i32 %939, %936
  %945 = xor i32 %936, %939
  %946 = xor i32 %938, %939
  %947 = and i32 %945, %946
  %948 = icmp slt i32 %947, 0
  store i1 %943, i1* %az
  store i1 %944, i1* %cf
  store i1 %948, i1* %of
  %949 = icmp eq i32 %939, 0
  store i1 %949, i1* %zf
  %950 = icmp slt i32 %939, 0
  store i1 %950, i1* %sf
  %951 = trunc i32 %939 to i8
  %952 = call i8 @llvm.ctpop.i8(i8 %951)
  %953 = and i8 %952, 1
  %954 = icmp eq i8 %953, 0
  store i1 %954, i1* %pf
  %955 = zext i32 %939 to i64
  store i64 %955, i64* %rax
  store volatile i64 47425, i64* @assembly_address
  %956 = load i64* %rax
  %957 = trunc i64 %956 to i32
  store i32 %957, i32* %stack_var_-164
  store volatile i64 47431, i64* @assembly_address
  %958 = load i32* %stack_var_-208
  %959 = zext i32 %958 to i64
  store i64 %959, i64* %rax
  store volatile i64 47437, i64* @assembly_address
  %960 = load i32* %stack_var_-168
  %961 = load i64* %rax
  %962 = trunc i64 %961 to i32
  %963 = sub i32 %960, %962
  %964 = and i32 %960, 15
  %965 = and i32 %962, 15
  %966 = sub i32 %964, %965
  %967 = icmp ugt i32 %966, 15
  %968 = icmp ult i32 %960, %962
  %969 = xor i32 %960, %962
  %970 = xor i32 %960, %963
  %971 = and i32 %969, %970
  %972 = icmp slt i32 %971, 0
  store i1 %967, i1* %az
  store i1 %968, i1* %cf
  store i1 %972, i1* %of
  %973 = icmp eq i32 %963, 0
  store i1 %973, i1* %zf
  %974 = icmp slt i32 %963, 0
  store i1 %974, i1* %sf
  %975 = trunc i32 %963 to i8
  %976 = call i8 @llvm.ctpop.i8(i8 %975)
  %977 = and i8 %976, 1
  %978 = icmp eq i8 %977, 0
  store i1 %978, i1* %pf
  store volatile i64 47443, i64* @assembly_address
  %979 = load i1* %cf
  %980 = load i1* %zf
  %981 = or i1 %979, %980
  %982 = icmp ne i1 %981, true
  br i1 %982, label %block_b9c7, label %block_b955

block_b955:                                       ; preds = %block_b923
  store volatile i64 47445, i64* @assembly_address
  %983 = load i32* %stack_var_-208
  %984 = zext i32 %983 to i64
  store i64 %984, i64* %rax
  store volatile i64 47451, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 47456, i64* @assembly_address
  %985 = load i64* %rax
  %986 = trunc i64 %985 to i32
  %987 = zext i32 %986 to i64
  store i64 %987, i64* %rcx
  store volatile i64 47458, i64* @assembly_address
  %988 = load i64* %rdx
  %989 = trunc i64 %988 to i32
  %990 = load i64* %rcx
  %991 = trunc i64 %990 to i8
  %992 = zext i8 %991 to i32
  %993 = and i32 %992, 31
  %994 = load i1* %of
  %995 = icmp eq i32 %993, 0
  br i1 %995, label %1013, label %996

; <label>:996                                     ; preds = %block_b955
  %997 = shl i32 %989, %993
  %998 = icmp eq i32 %997, 0
  store i1 %998, i1* %zf
  %999 = icmp slt i32 %997, 0
  store i1 %999, i1* %sf
  %1000 = trunc i32 %997 to i8
  %1001 = call i8 @llvm.ctpop.i8(i8 %1000)
  %1002 = and i8 %1001, 1
  %1003 = icmp eq i8 %1002, 0
  store i1 %1003, i1* %pf
  %1004 = zext i32 %997 to i64
  store i64 %1004, i64* %rdx
  %1005 = sub i32 %993, 1
  %1006 = shl i32 %989, %1005
  %1007 = lshr i32 %1006, 31
  %1008 = trunc i32 %1007 to i1
  store i1 %1008, i1* %cf
  %1009 = lshr i32 %997, 31
  %1010 = icmp ne i32 %1009, %1007
  %1011 = icmp eq i32 %993, 1
  %1012 = select i1 %1011, i1 %1010, i1 %994
  store i1 %1012, i1* %of
  br label %1013

; <label>:1013                                    ; preds = %block_b955, %996
  store volatile i64 47460, i64* @assembly_address
  %1014 = load i64* %rdx
  %1015 = trunc i64 %1014 to i32
  %1016 = zext i32 %1015 to i64
  store i64 %1016, i64* %rax
  store volatile i64 47462, i64* @assembly_address
  %1017 = load i32* %stack_var_-164
  %1018 = load i64* %rax
  %1019 = trunc i64 %1018 to i32
  %1020 = sub i32 %1017, %1019
  %1021 = and i32 %1017, 15
  %1022 = and i32 %1019, 15
  %1023 = sub i32 %1021, %1022
  %1024 = icmp ugt i32 %1023, 15
  %1025 = icmp ult i32 %1017, %1019
  %1026 = xor i32 %1017, %1019
  %1027 = xor i32 %1017, %1020
  %1028 = and i32 %1026, %1027
  %1029 = icmp slt i32 %1028, 0
  store i1 %1024, i1* %az
  store i1 %1025, i1* %cf
  store i1 %1029, i1* %of
  %1030 = icmp eq i32 %1020, 0
  store i1 %1030, i1* %zf
  %1031 = icmp slt i32 %1020, 0
  store i1 %1031, i1* %sf
  %1032 = trunc i32 %1020 to i8
  %1033 = call i8 @llvm.ctpop.i8(i8 %1032)
  %1034 = and i8 %1033, 1
  %1035 = icmp eq i8 %1034, 0
  store i1 %1035, i1* %pf
  store volatile i64 47468, i64* @assembly_address
  %1036 = load i1* %cf
  %1037 = load i1* %zf
  %1038 = or i1 %1036, %1037
  br i1 %1038, label %block_b97a, label %block_b96e

block_b96e:                                       ; preds = %1013
  store volatile i64 47470, i64* @assembly_address
  store i64 ptrtoint ([11 x i8]* @global_var_120bc to i64), i64* %rdi
  store volatile i64 47477, i64* @assembly_address
  %1039 = load i64* %rdi
  %1040 = inttoptr i64 %1039 to i8*
  %1041 = call i64 @gzip_error(i8* %1040)
  store i64 %1041, i64* %rax
  store i64 %1041, i64* %rax
  unreachable

block_b97a:                                       ; preds = %1013
  store volatile i64 47482, i64* @assembly_address
  %1042 = load i32* %stack_var_-168
  %1043 = zext i32 %1042 to i64
  store i64 %1043, i64* %rax
  store volatile i64 47488, i64* @assembly_address
  %1044 = load i64* %rbp
  %1045 = load i64* %rax
  %1046 = mul i64 %1045, 2
  %1047 = add i64 %1044, -48
  %1048 = add i64 %1047, %1046
  %1049 = inttoptr i64 %1048 to i16*
  %1050 = load i16* %1049
  %1051 = zext i16 %1050 to i64
  store i64 %1051, i64* %rax
  store volatile i64 47493, i64* @assembly_address
  %1052 = load i64* %rax
  %1053 = trunc i64 %1052 to i16
  %1054 = zext i16 %1053 to i64
  store i64 %1054, i64* %rax
  store volatile i64 47496, i64* @assembly_address
  %1055 = load i64* %rax
  %1056 = trunc i64 %1055 to i32
  store i32 %1056, i32* %stack_var_-192
  store volatile i64 47502, i64* @assembly_address
  br label %block_b9b4

block_b990:                                       ; preds = %block_b9b4
  store volatile i64 47504, i64* @assembly_address
  %1057 = load i32* %stack_var_-192
  %1058 = zext i32 %1057 to i64
  store i64 %1058, i64* %rax
  store volatile i64 47510, i64* @assembly_address
  %1059 = load i64* %rax
  %1060 = load i64* %rax
  %1061 = mul i64 %1060, 1
  %1062 = add i64 %1059, %1061
  store i64 %1062, i64* %rdx
  store volatile i64 47514, i64* @assembly_address
  %1063 = load i16** %stack_var_-224
  %1064 = ptrtoint i16* %1063 to i64
  store i64 %1064, i64* %rax
  store volatile i64 47521, i64* @assembly_address
  %1065 = load i64* %rax
  %1066 = load i64* %rdx
  %1067 = add i64 %1065, %1066
  %1068 = and i64 %1065, 15
  %1069 = and i64 %1066, 15
  %1070 = add i64 %1068, %1069
  %1071 = icmp ugt i64 %1070, 15
  %1072 = icmp ult i64 %1067, %1065
  %1073 = xor i64 %1065, %1067
  %1074 = xor i64 %1066, %1067
  %1075 = and i64 %1073, %1074
  %1076 = icmp slt i64 %1075, 0
  store i1 %1071, i1* %az
  store i1 %1072, i1* %cf
  store i1 %1076, i1* %of
  %1077 = icmp eq i64 %1067, 0
  store i1 %1077, i1* %zf
  %1078 = icmp slt i64 %1067, 0
  store i1 %1078, i1* %sf
  %1079 = trunc i64 %1067 to i8
  %1080 = call i8 @llvm.ctpop.i8(i8 %1079)
  %1081 = and i8 %1080, 1
  %1082 = icmp eq i8 %1081, 0
  store i1 %1082, i1* %pf
  store i64 %1067, i64* %rax
  store volatile i64 47524, i64* @assembly_address
  %1083 = load i32* %stack_var_-184
  %1084 = zext i32 %1083 to i64
  store i64 %1084, i64* %rdx
  store volatile i64 47530, i64* @assembly_address
  %1085 = load i64* %rdx
  %1086 = trunc i64 %1085 to i16
  %1087 = load i64* %rax
  %1088 = inttoptr i64 %1087 to i16*
  store i16 %1086, i16* %1088
  store volatile i64 47533, i64* @assembly_address
  %1089 = load i32* %stack_var_-192
  %1090 = add i32 %1089, 1
  %1091 = and i32 %1089, 15
  %1092 = add i32 %1091, 1
  %1093 = icmp ugt i32 %1092, 15
  %1094 = icmp ult i32 %1090, %1089
  %1095 = xor i32 %1089, %1090
  %1096 = xor i32 1, %1090
  %1097 = and i32 %1095, %1096
  %1098 = icmp slt i32 %1097, 0
  store i1 %1093, i1* %az
  store i1 %1094, i1* %cf
  store i1 %1098, i1* %of
  %1099 = icmp eq i32 %1090, 0
  store i1 %1099, i1* %zf
  %1100 = icmp slt i32 %1090, 0
  store i1 %1100, i1* %sf
  %1101 = trunc i32 %1090 to i8
  %1102 = call i8 @llvm.ctpop.i8(i8 %1101)
  %1103 = and i8 %1102, 1
  %1104 = icmp eq i8 %1103, 0
  store i1 %1104, i1* %pf
  store i32 %1090, i32* %stack_var_-192
  br label %block_b9b4

block_b9b4:                                       ; preds = %block_b990, %block_b97a
  store volatile i64 47540, i64* @assembly_address
  %1105 = load i32* %stack_var_-192
  %1106 = zext i32 %1105 to i64
  store i64 %1106, i64* %rax
  store volatile i64 47546, i64* @assembly_address
  %1107 = load i64* %rax
  %1108 = trunc i64 %1107 to i32
  %1109 = load i32* %stack_var_-164
  %1110 = sub i32 %1108, %1109
  %1111 = and i32 %1108, 15
  %1112 = and i32 %1109, 15
  %1113 = sub i32 %1111, %1112
  %1114 = icmp ugt i32 %1113, 15
  %1115 = icmp ult i32 %1108, %1109
  %1116 = xor i32 %1108, %1109
  %1117 = xor i32 %1108, %1110
  %1118 = and i32 %1116, %1117
  %1119 = icmp slt i32 %1118, 0
  store i1 %1114, i1* %az
  store i1 %1115, i1* %cf
  store i1 %1119, i1* %of
  %1120 = icmp eq i32 %1110, 0
  store i1 %1120, i1* %zf
  %1121 = icmp slt i32 %1110, 0
  store i1 %1121, i1* %sf
  %1122 = trunc i32 %1110 to i8
  %1123 = call i8 @llvm.ctpop.i8(i8 %1122)
  %1124 = and i8 %1123, 1
  %1125 = icmp eq i8 %1124, 0
  store i1 %1125, i1* %pf
  store volatile i64 47552, i64* @assembly_address
  %1126 = load i1* %cf
  br i1 %1126, label %block_b990, label %block_b9c2

block_b9c2:                                       ; preds = %block_b9b4
  store volatile i64 47554, i64* @assembly_address
  br label %block_bb1c

block_b9c7:                                       ; preds = %block_b923
  store volatile i64 47559, i64* @assembly_address
  %1127 = load i32* %stack_var_-168
  %1128 = zext i32 %1127 to i64
  store i64 %1128, i64* %rax
  store volatile i64 47565, i64* @assembly_address
  %1129 = load i64* %rbp
  %1130 = load i64* %rax
  %1131 = mul i64 %1130, 2
  %1132 = add i64 %1129, -48
  %1133 = add i64 %1132, %1131
  %1134 = inttoptr i64 %1133 to i16*
  %1135 = load i16* %1134
  %1136 = zext i16 %1135 to i64
  store i64 %1136, i64* %rax
  store volatile i64 47570, i64* @assembly_address
  %1137 = load i64* %rax
  %1138 = trunc i64 %1137 to i16
  %1139 = zext i16 %1138 to i64
  store i64 %1139, i64* %rax
  store volatile i64 47573, i64* @assembly_address
  %1140 = load i64* %rax
  %1141 = trunc i64 %1140 to i32
  store i32 %1141, i32* %stack_var_-188
  store volatile i64 47579, i64* @assembly_address
  %1142 = load i32* %stack_var_-176
  %1143 = zext i32 %1142 to i64
  store i64 %1143, i64* %rax
  store volatile i64 47585, i64* @assembly_address
  %1144 = load i32* %stack_var_-188
  %1145 = zext i32 %1144 to i64
  store i64 %1145, i64* %rdx
  store volatile i64 47591, i64* @assembly_address
  %1146 = load i64* %rax
  %1147 = trunc i64 %1146 to i32
  %1148 = zext i32 %1147 to i64
  store i64 %1148, i64* %rcx
  store volatile i64 47593, i64* @assembly_address
  %1149 = load i64* %rdx
  %1150 = trunc i64 %1149 to i32
  %1151 = load i64* %rcx
  %1152 = trunc i64 %1151 to i8
  %1153 = zext i8 %1152 to i32
  %1154 = and i32 %1153, 31
  %1155 = load i1* %of
  %1156 = icmp eq i32 %1154, 0
  br i1 %1156, label %1173, label %1157

; <label>:1157                                    ; preds = %block_b9c7
  %1158 = lshr i32 %1150, %1154
  %1159 = icmp eq i32 %1158, 0
  store i1 %1159, i1* %zf
  %1160 = icmp slt i32 %1158, 0
  store i1 %1160, i1* %sf
  %1161 = trunc i32 %1158 to i8
  %1162 = call i8 @llvm.ctpop.i8(i8 %1161)
  %1163 = and i8 %1162, 1
  %1164 = icmp eq i8 %1163, 0
  store i1 %1164, i1* %pf
  %1165 = zext i32 %1158 to i64
  store i64 %1165, i64* %rdx
  %1166 = sub i32 %1154, 1
  %1167 = shl i32 1, %1166
  %1168 = and i32 %1167, %1150
  %1169 = icmp ne i32 %1168, 0
  store i1 %1169, i1* %cf
  %1170 = icmp eq i32 %1154, 1
  %1171 = icmp slt i32 %1150, 0
  %1172 = select i1 %1170, i1 %1171, i1 %1155
  store i1 %1172, i1* %of
  br label %1173

; <label>:1173                                    ; preds = %block_b9c7, %1157
  store volatile i64 47595, i64* @assembly_address
  %1174 = load i64* %rdx
  %1175 = trunc i64 %1174 to i32
  %1176 = zext i32 %1175 to i64
  store i64 %1176, i64* %rax
  store volatile i64 47597, i64* @assembly_address
  %1177 = load i64* %rax
  %1178 = trunc i64 %1177 to i32
  %1179 = zext i32 %1178 to i64
  store i64 %1179, i64* %rax
  store volatile i64 47599, i64* @assembly_address
  %1180 = load i64* %rax
  %1181 = load i64* %rax
  %1182 = mul i64 %1181, 1
  %1183 = add i64 %1180, %1182
  store i64 %1183, i64* %rdx
  store volatile i64 47603, i64* @assembly_address
  %1184 = load i16** %stack_var_-224
  %1185 = ptrtoint i16* %1184 to i64
  store i64 %1185, i64* %rax
  store volatile i64 47610, i64* @assembly_address
  %1186 = load i64* %rax
  %1187 = load i64* %rdx
  %1188 = add i64 %1186, %1187
  %1189 = and i64 %1186, 15
  %1190 = and i64 %1187, 15
  %1191 = add i64 %1189, %1190
  %1192 = icmp ugt i64 %1191, 15
  %1193 = icmp ult i64 %1188, %1186
  %1194 = xor i64 %1186, %1188
  %1195 = xor i64 %1187, %1188
  %1196 = and i64 %1194, %1195
  %1197 = icmp slt i64 %1196, 0
  store i1 %1192, i1* %az
  store i1 %1193, i1* %cf
  store i1 %1197, i1* %of
  %1198 = icmp eq i64 %1188, 0
  store i1 %1198, i1* %zf
  %1199 = icmp slt i64 %1188, 0
  store i1 %1199, i1* %sf
  %1200 = trunc i64 %1188 to i8
  %1201 = call i8 @llvm.ctpop.i8(i8 %1200)
  %1202 = and i8 %1201, 1
  %1203 = icmp eq i8 %1202, 0
  store i1 %1203, i1* %pf
  store i64 %1188, i64* %rax
  store volatile i64 47613, i64* @assembly_address
  %1204 = load i64* %rax
  store i64 %1204, i64* %stack_var_-160
  store volatile i64 47620, i64* @assembly_address
  %1205 = load i32* %stack_var_-208
  %1206 = zext i32 %1205 to i64
  store i64 %1206, i64* %rax
  store volatile i64 47626, i64* @assembly_address
  %1207 = load i32* %stack_var_-168
  %1208 = zext i32 %1207 to i64
  store i64 %1208, i64* %rdx
  store volatile i64 47632, i64* @assembly_address
  %1209 = load i64* %rdx
  %1210 = trunc i64 %1209 to i32
  %1211 = load i64* %rax
  %1212 = trunc i64 %1211 to i32
  %1213 = sub i32 %1210, %1212
  %1214 = and i32 %1210, 15
  %1215 = and i32 %1212, 15
  %1216 = sub i32 %1214, %1215
  %1217 = icmp ugt i32 %1216, 15
  %1218 = icmp ult i32 %1210, %1212
  %1219 = xor i32 %1210, %1212
  %1220 = xor i32 %1210, %1213
  %1221 = and i32 %1219, %1220
  %1222 = icmp slt i32 %1221, 0
  store i1 %1217, i1* %az
  store i1 %1218, i1* %cf
  store i1 %1222, i1* %of
  %1223 = icmp eq i32 %1213, 0
  store i1 %1223, i1* %zf
  %1224 = icmp slt i32 %1213, 0
  store i1 %1224, i1* %sf
  %1225 = trunc i32 %1213 to i8
  %1226 = call i8 @llvm.ctpop.i8(i8 %1225)
  %1227 = and i8 %1226, 1
  %1228 = icmp eq i8 %1227, 0
  store i1 %1228, i1* %pf
  %1229 = zext i32 %1213 to i64
  store i64 %1229, i64* %rdx
  store volatile i64 47634, i64* @assembly_address
  %1230 = load i64* %rdx
  %1231 = trunc i64 %1230 to i32
  %1232 = zext i32 %1231 to i64
  store i64 %1232, i64* %rax
  store volatile i64 47636, i64* @assembly_address
  %1233 = load i64* %rax
  %1234 = trunc i64 %1233 to i32
  store i32 %1234, i32* %stack_var_-192
  store volatile i64 47642, i64* @assembly_address
  br label %block_bafd

block_ba1f:                                       ; preds = %block_bafd
  store volatile i64 47647, i64* @assembly_address
  %1235 = load i64* %stack_var_-160
  store i64 %1235, i64* %rax
  store volatile i64 47654, i64* @assembly_address
  %1236 = load i64* %rax
  %1237 = inttoptr i64 %1236 to i16*
  %1238 = load i16* %1237
  %1239 = zext i16 %1238 to i64
  store i64 %1239, i64* %rax
  store volatile i64 47657, i64* @assembly_address
  %1240 = load i64* %rax
  %1241 = trunc i64 %1240 to i16
  %1242 = load i64* %rax
  %1243 = trunc i64 %1242 to i16
  %1244 = and i16 %1241, %1243
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1245 = icmp eq i16 %1244, 0
  store i1 %1245, i1* %zf
  %1246 = icmp slt i16 %1244, 0
  store i1 %1246, i1* %sf
  %1247 = trunc i16 %1244 to i8
  %1248 = call i8 @llvm.ctpop.i8(i8 %1247)
  %1249 = and i8 %1248, 1
  %1250 = icmp eq i8 %1249, 0
  store i1 %1250, i1* %pf
  store volatile i64 47660, i64* @assembly_address
  %1251 = load i1* %zf
  %1252 = icmp eq i1 %1251, false
  br i1 %1252, label %block_ba92, label %block_ba2e

block_ba2e:                                       ; preds = %block_ba1f
  store volatile i64 47662, i64* @assembly_address
  %1253 = load i32* %stack_var_-180
  %1254 = zext i32 %1253 to i64
  store i64 %1254, i64* %rax
  store volatile i64 47668, i64* @assembly_address
  %1255 = load i64* %rax
  %1256 = load i64* %rax
  %1257 = mul i64 %1256, 1
  %1258 = add i64 %1255, %1257
  store i64 %1258, i64* %rdx
  store volatile i64 47672, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 47679, i64* @assembly_address
  %1259 = load i64* %rdx
  %1260 = load i64* %rax
  %1261 = mul i64 %1260, 1
  %1262 = add i64 %1259, %1261
  %1263 = inttoptr i64 %1262 to i16*
  store i16 0, i16* %1263
  store volatile i64 47685, i64* @assembly_address
  %1264 = load i32* %stack_var_-180
  %1265 = zext i32 %1264 to i64
  store i64 %1265, i64* %rax
  store volatile i64 47691, i64* @assembly_address
  %1266 = load i64* %rax
  %1267 = add i64 %1266, 32768
  %1268 = and i64 %1266, 15
  %1269 = icmp ugt i64 %1268, 15
  %1270 = icmp ult i64 %1267, %1266
  %1271 = xor i64 %1266, %1267
  %1272 = xor i64 32768, %1267
  %1273 = and i64 %1271, %1272
  %1274 = icmp slt i64 %1273, 0
  store i1 %1269, i1* %az
  store i1 %1270, i1* %cf
  store i1 %1274, i1* %of
  %1275 = icmp eq i64 %1267, 0
  store i1 %1275, i1* %zf
  %1276 = icmp slt i64 %1267, 0
  store i1 %1276, i1* %sf
  %1277 = trunc i64 %1267 to i8
  %1278 = call i8 @llvm.ctpop.i8(i8 %1277)
  %1279 = and i8 %1278, 1
  %1280 = icmp eq i8 %1279, 0
  store i1 %1280, i1* %pf
  store i64 %1267, i64* %rax
  store volatile i64 47697, i64* @assembly_address
  %1281 = load i64* %rax
  %1282 = load i64* %rax
  %1283 = mul i64 %1282, 1
  %1284 = add i64 %1281, %1283
  store i64 %1284, i64* %rdx
  store volatile i64 47701, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 47708, i64* @assembly_address
  %1285 = load i64* %rdx
  %1286 = load i64* %rax
  %1287 = add i64 %1285, %1286
  %1288 = and i64 %1285, 15
  %1289 = and i64 %1286, 15
  %1290 = add i64 %1288, %1289
  %1291 = icmp ugt i64 %1290, 15
  %1292 = icmp ult i64 %1287, %1285
  %1293 = xor i64 %1285, %1287
  %1294 = xor i64 %1286, %1287
  %1295 = and i64 %1293, %1294
  %1296 = icmp slt i64 %1295, 0
  store i1 %1291, i1* %az
  store i1 %1292, i1* %cf
  store i1 %1296, i1* %of
  %1297 = icmp eq i64 %1287, 0
  store i1 %1297, i1* %zf
  %1298 = icmp slt i64 %1287, 0
  store i1 %1298, i1* %sf
  %1299 = trunc i64 %1287 to i8
  %1300 = call i8 @llvm.ctpop.i8(i8 %1299)
  %1301 = and i8 %1300, 1
  %1302 = icmp eq i8 %1301, 0
  store i1 %1302, i1* %pf
  store i64 %1287, i64* %rdx
  store volatile i64 47711, i64* @assembly_address
  %1303 = load i32* %stack_var_-180
  %1304 = zext i32 %1303 to i64
  store i64 %1304, i64* %rax
  store volatile i64 47717, i64* @assembly_address
  %1305 = load i64* %rax
  %1306 = load i64* %rax
  %1307 = mul i64 %1306, 1
  %1308 = add i64 %1305, %1307
  store i64 %1308, i64* %rcx
  store volatile i64 47721, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 47728, i64* @assembly_address
  %1309 = load i64* %rcx
  %1310 = load i64* %rax
  %1311 = mul i64 %1310, 1
  %1312 = add i64 %1309, %1311
  %1313 = inttoptr i64 %1312 to i16*
  %1314 = load i16* %1313
  %1315 = zext i16 %1314 to i64
  store i64 %1315, i64* %rax
  store volatile i64 47732, i64* @assembly_address
  %1316 = load i64* %rax
  %1317 = trunc i64 %1316 to i16
  %1318 = load i64* %rdx
  %1319 = inttoptr i64 %1318 to i16*
  store i16 %1317, i16* %1319
  store volatile i64 47735, i64* @assembly_address
  %1320 = load i32* %stack_var_-180
  %1321 = zext i32 %1320 to i64
  store i64 %1321, i64* %rax
  store volatile i64 47741, i64* @assembly_address
  %1322 = load i64* %rax
  %1323 = add i64 %1322, 1
  %1324 = trunc i64 %1323 to i32
  %1325 = zext i32 %1324 to i64
  store i64 %1325, i64* %rdx
  store volatile i64 47744, i64* @assembly_address
  %1326 = load i64* %rdx
  %1327 = trunc i64 %1326 to i32
  store i32 %1327, i32* %stack_var_-180
  store volatile i64 47750, i64* @assembly_address
  %1328 = load i64* %rax
  %1329 = trunc i64 %1328 to i32
  %1330 = zext i32 %1329 to i64
  store i64 %1330, i64* %rdx
  store volatile i64 47752, i64* @assembly_address
  %1331 = load i64* %stack_var_-160
  store i64 %1331, i64* %rax
  store volatile i64 47759, i64* @assembly_address
  %1332 = load i64* %rdx
  %1333 = trunc i64 %1332 to i16
  %1334 = load i64* %rax
  %1335 = inttoptr i64 %1334 to i16*
  store i16 %1333, i16* %1335
  br label %block_ba92

block_ba92:                                       ; preds = %block_ba2e, %block_ba1f
  store volatile i64 47762, i64* @assembly_address
  %1336 = load i32* %stack_var_-188
  %1337 = zext i32 %1336 to i64
  store i64 %1337, i64* %rax
  store volatile i64 47768, i64* @assembly_address
  %1338 = load i64* %rax
  %1339 = trunc i64 %1338 to i32
  %1340 = load i32* %stack_var_-172
  %1341 = and i32 %1339, %1340
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1342 = icmp eq i32 %1341, 0
  store i1 %1342, i1* %zf
  %1343 = icmp slt i32 %1341, 0
  store i1 %1343, i1* %sf
  %1344 = trunc i32 %1341 to i8
  %1345 = call i8 @llvm.ctpop.i8(i8 %1344)
  %1346 = and i8 %1345, 1
  %1347 = icmp eq i8 %1346, 0
  store i1 %1347, i1* %pf
  %1348 = zext i32 %1341 to i64
  store i64 %1348, i64* %rax
  store volatile i64 47774, i64* @assembly_address
  %1349 = load i64* %rax
  %1350 = trunc i64 %1349 to i32
  %1351 = load i64* %rax
  %1352 = trunc i64 %1351 to i32
  %1353 = and i32 %1350, %1352
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1354 = icmp eq i32 %1353, 0
  store i1 %1354, i1* %zf
  %1355 = icmp slt i32 %1353, 0
  store i1 %1355, i1* %sf
  %1356 = trunc i32 %1353 to i8
  %1357 = call i8 @llvm.ctpop.i8(i8 %1356)
  %1358 = and i8 %1357, 1
  %1359 = icmp eq i8 %1358, 0
  store i1 %1359, i1* %pf
  store volatile i64 47776, i64* @assembly_address
  %1360 = load i1* %zf
  br i1 %1360, label %block_bacc, label %block_baa2

block_baa2:                                       ; preds = %block_ba92
  store volatile i64 47778, i64* @assembly_address
  %1361 = load i64* %stack_var_-160
  store i64 %1361, i64* %rax
  store volatile i64 47785, i64* @assembly_address
  %1362 = load i64* %rax
  %1363 = inttoptr i64 %1362 to i16*
  %1364 = load i16* %1363
  %1365 = zext i16 %1364 to i64
  store i64 %1365, i64* %rax
  store volatile i64 47788, i64* @assembly_address
  %1366 = load i64* %rax
  %1367 = trunc i64 %1366 to i16
  %1368 = zext i16 %1367 to i64
  store i64 %1368, i64* %rax
  store volatile i64 47791, i64* @assembly_address
  %1369 = load i64* %rax
  %1370 = add i64 %1369, 32768
  %1371 = and i64 %1369, 15
  %1372 = icmp ugt i64 %1371, 15
  %1373 = icmp ult i64 %1370, %1369
  %1374 = xor i64 %1369, %1370
  %1375 = xor i64 32768, %1370
  %1376 = and i64 %1374, %1375
  %1377 = icmp slt i64 %1376, 0
  store i1 %1372, i1* %az
  store i1 %1373, i1* %cf
  store i1 %1377, i1* %of
  %1378 = icmp eq i64 %1370, 0
  store i1 %1378, i1* %zf
  %1379 = icmp slt i64 %1370, 0
  store i1 %1379, i1* %sf
  %1380 = trunc i64 %1370 to i8
  %1381 = call i8 @llvm.ctpop.i8(i8 %1380)
  %1382 = and i8 %1381, 1
  %1383 = icmp eq i8 %1382, 0
  store i1 %1383, i1* %pf
  store i64 %1370, i64* %rax
  store volatile i64 47797, i64* @assembly_address
  %1384 = load i64* %rax
  %1385 = load i64* %rax
  %1386 = mul i64 %1385, 1
  %1387 = add i64 %1384, %1386
  store i64 %1387, i64* %rdx
  store volatile i64 47801, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 47808, i64* @assembly_address
  %1388 = load i64* %rax
  %1389 = load i64* %rdx
  %1390 = add i64 %1388, %1389
  %1391 = and i64 %1388, 15
  %1392 = and i64 %1389, 15
  %1393 = add i64 %1391, %1392
  %1394 = icmp ugt i64 %1393, 15
  %1395 = icmp ult i64 %1390, %1388
  %1396 = xor i64 %1388, %1390
  %1397 = xor i64 %1389, %1390
  %1398 = and i64 %1396, %1397
  %1399 = icmp slt i64 %1398, 0
  store i1 %1394, i1* %az
  store i1 %1395, i1* %cf
  store i1 %1399, i1* %of
  %1400 = icmp eq i64 %1390, 0
  store i1 %1400, i1* %zf
  %1401 = icmp slt i64 %1390, 0
  store i1 %1401, i1* %sf
  %1402 = trunc i64 %1390 to i8
  %1403 = call i8 @llvm.ctpop.i8(i8 %1402)
  %1404 = and i8 %1403, 1
  %1405 = icmp eq i8 %1404, 0
  store i1 %1405, i1* %pf
  store i64 %1390, i64* %rax
  store volatile i64 47811, i64* @assembly_address
  %1406 = load i64* %rax
  store i64 %1406, i64* %stack_var_-160
  store volatile i64 47818, i64* @assembly_address
  br label %block_baf0

block_bacc:                                       ; preds = %block_ba92
  store volatile i64 47820, i64* @assembly_address
  %1407 = load i64* %stack_var_-160
  store i64 %1407, i64* %rax
  store volatile i64 47827, i64* @assembly_address
  %1408 = load i64* %rax
  %1409 = inttoptr i64 %1408 to i16*
  %1410 = load i16* %1409
  %1411 = zext i16 %1410 to i64
  store i64 %1411, i64* %rax
  store volatile i64 47830, i64* @assembly_address
  %1412 = load i64* %rax
  %1413 = trunc i64 %1412 to i16
  %1414 = zext i16 %1413 to i64
  store i64 %1414, i64* %rax
  store volatile i64 47833, i64* @assembly_address
  %1415 = load i64* %rax
  %1416 = trunc i64 %1415 to i32
  %1417 = sext i32 %1416 to i64
  store i64 %1417, i64* %rax
  store volatile i64 47835, i64* @assembly_address
  %1418 = load i64* %rax
  %1419 = load i64* %rax
  %1420 = mul i64 %1419, 1
  %1421 = add i64 %1418, %1420
  store i64 %1421, i64* %rdx
  store volatile i64 47839, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 47846, i64* @assembly_address
  %1422 = load i64* %rax
  %1423 = load i64* %rdx
  %1424 = add i64 %1422, %1423
  %1425 = and i64 %1422, 15
  %1426 = and i64 %1423, 15
  %1427 = add i64 %1425, %1426
  %1428 = icmp ugt i64 %1427, 15
  %1429 = icmp ult i64 %1424, %1422
  %1430 = xor i64 %1422, %1424
  %1431 = xor i64 %1423, %1424
  %1432 = and i64 %1430, %1431
  %1433 = icmp slt i64 %1432, 0
  store i1 %1428, i1* %az
  store i1 %1429, i1* %cf
  store i1 %1433, i1* %of
  %1434 = icmp eq i64 %1424, 0
  store i1 %1434, i1* %zf
  %1435 = icmp slt i64 %1424, 0
  store i1 %1435, i1* %sf
  %1436 = trunc i64 %1424 to i8
  %1437 = call i8 @llvm.ctpop.i8(i8 %1436)
  %1438 = and i8 %1437, 1
  %1439 = icmp eq i8 %1438, 0
  store i1 %1439, i1* %pf
  store i64 %1424, i64* %rax
  store volatile i64 47849, i64* @assembly_address
  %1440 = load i64* %rax
  store i64 %1440, i64* %stack_var_-160
  br label %block_baf0

block_baf0:                                       ; preds = %block_bacc, %block_baa2
  store volatile i64 47856, i64* @assembly_address
  %1441 = load i32* %stack_var_-188
  %1442 = load i1* %of
  %1443 = shl i32 %1441, 1
  %1444 = icmp eq i32 %1443, 0
  store i1 %1444, i1* %zf
  %1445 = icmp slt i32 %1443, 0
  store i1 %1445, i1* %sf
  %1446 = trunc i32 %1443 to i8
  %1447 = call i8 @llvm.ctpop.i8(i8 %1446)
  %1448 = and i8 %1447, 1
  %1449 = icmp eq i8 %1448, 0
  store i1 %1449, i1* %pf
  store i32 %1443, i32* %stack_var_-188
  %1450 = shl i32 %1441, 0
  %1451 = lshr i32 %1450, 31
  %1452 = trunc i32 %1451 to i1
  store i1 %1452, i1* %cf
  %1453 = lshr i32 %1443, 31
  %1454 = icmp ne i32 %1453, %1451
  %1455 = select i1 true, i1 %1454, i1 %1442
  store i1 %1455, i1* %of
  store volatile i64 47862, i64* @assembly_address
  %1456 = load i32* %stack_var_-192
  %1457 = sub i32 %1456, 1
  %1458 = and i32 %1456, 15
  %1459 = sub i32 %1458, 1
  %1460 = icmp ugt i32 %1459, 15
  %1461 = icmp ult i32 %1456, 1
  %1462 = xor i32 %1456, 1
  %1463 = xor i32 %1456, %1457
  %1464 = and i32 %1462, %1463
  %1465 = icmp slt i32 %1464, 0
  store i1 %1460, i1* %az
  store i1 %1461, i1* %cf
  store i1 %1465, i1* %of
  %1466 = icmp eq i32 %1457, 0
  store i1 %1466, i1* %zf
  %1467 = icmp slt i32 %1457, 0
  store i1 %1467, i1* %sf
  %1468 = trunc i32 %1457 to i8
  %1469 = call i8 @llvm.ctpop.i8(i8 %1468)
  %1470 = and i8 %1469, 1
  %1471 = icmp eq i8 %1470, 0
  store i1 %1471, i1* %pf
  store i32 %1457, i32* %stack_var_-192
  br label %block_bafd

block_bafd:                                       ; preds = %block_baf0, %1173
  store volatile i64 47869, i64* @assembly_address
  %1472 = load i32* %stack_var_-192
  %1473 = and i32 %1472, 15
  %1474 = icmp ugt i32 %1473, 15
  %1475 = icmp ult i32 %1472, 0
  %1476 = xor i32 %1472, 0
  %1477 = and i32 %1476, 0
  %1478 = icmp slt i32 %1477, 0
  store i1 %1474, i1* %az
  store i1 %1475, i1* %cf
  store i1 %1478, i1* %of
  %1479 = icmp eq i32 %1472, 0
  store i1 %1479, i1* %zf
  %1480 = icmp slt i32 %1472, 0
  store i1 %1480, i1* %sf
  %1481 = trunc i32 %1472 to i8
  %1482 = call i8 @llvm.ctpop.i8(i8 %1481)
  %1483 = and i8 %1482, 1
  %1484 = icmp eq i8 %1483, 0
  store i1 %1484, i1* %pf
  store volatile i64 47876, i64* @assembly_address
  %1485 = load i1* %zf
  %1486 = icmp eq i1 %1485, false
  br i1 %1486, label %block_ba1f, label %block_bb0a

block_bb0a:                                       ; preds = %block_bafd
  store volatile i64 47882, i64* @assembly_address
  %1487 = load i32* %stack_var_-184
  %1488 = zext i32 %1487 to i64
  store i64 %1488, i64* %rax
  store volatile i64 47888, i64* @assembly_address
  %1489 = load i64* %rax
  %1490 = trunc i64 %1489 to i32
  %1491 = zext i32 %1490 to i64
  store i64 %1491, i64* %rdx
  store volatile i64 47890, i64* @assembly_address
  %1492 = load i64* %stack_var_-160
  store i64 %1492, i64* %rax
  store volatile i64 47897, i64* @assembly_address
  %1493 = load i64* %rdx
  %1494 = trunc i64 %1493 to i16
  %1495 = load i64* %rax
  %1496 = inttoptr i64 %1495 to i16*
  store i16 %1494, i16* %1496
  br label %block_bb1c

block_bb1c:                                       ; preds = %block_bb0a, %block_b9c2
  store volatile i64 47900, i64* @assembly_address
  %1497 = load i32* %stack_var_-164
  %1498 = zext i32 %1497 to i64
  store i64 %1498, i64* %rax
  store volatile i64 47906, i64* @assembly_address
  %1499 = load i64* %rax
  %1500 = trunc i64 %1499 to i32
  %1501 = zext i32 %1500 to i64
  store i64 %1501, i64* %rdx
  store volatile i64 47908, i64* @assembly_address
  %1502 = load i32* %stack_var_-168
  %1503 = zext i32 %1502 to i64
  store i64 %1503, i64* %rax
  store volatile i64 47914, i64* @assembly_address
  %1504 = load i64* %rdx
  %1505 = trunc i64 %1504 to i16
  %1506 = load i64* %rbp
  %1507 = load i64* %rax
  %1508 = mul i64 %1507, 2
  %1509 = add i64 %1506, -48
  %1510 = add i64 %1509, %1508
  %1511 = inttoptr i64 %1510 to i16*
  store i16 %1505, i16* %1511
  store volatile i64 47919, i64* @assembly_address
  br label %block_bb32

block_bb31:                                       ; preds = %block_b8fa
  store volatile i64 47921, i64* @assembly_address
  br label %block_bb32

block_bb32:                                       ; preds = %block_bb31, %block_bb1c
  store volatile i64 47922, i64* @assembly_address
  %1512 = load i32* %stack_var_-184
  %1513 = add i32 %1512, 1
  %1514 = and i32 %1512, 15
  %1515 = add i32 %1514, 1
  %1516 = icmp ugt i32 %1515, 15
  %1517 = icmp ult i32 %1513, %1512
  %1518 = xor i32 %1512, %1513
  %1519 = xor i32 1, %1513
  %1520 = and i32 %1518, %1519
  %1521 = icmp slt i32 %1520, 0
  store i1 %1516, i1* %az
  store i1 %1517, i1* %cf
  store i1 %1521, i1* %of
  %1522 = icmp eq i32 %1513, 0
  store i1 %1522, i1* %zf
  %1523 = icmp slt i32 %1513, 0
  store i1 %1523, i1* %sf
  %1524 = trunc i32 %1513 to i8
  %1525 = call i8 @llvm.ctpop.i8(i8 %1524)
  %1526 = and i8 %1525, 1
  %1527 = icmp eq i8 %1526, 0
  store i1 %1527, i1* %pf
  store i32 %1513, i32* %stack_var_-184
  br label %block_bb39

block_bb39:                                       ; preds = %block_bb32, %858
  store volatile i64 47929, i64* @assembly_address
  %1528 = load i32* %stack_var_-204
  %1529 = zext i32 %1528 to i64
  store i64 %1529, i64* %rax
  store volatile i64 47935, i64* @assembly_address
  %1530 = load i32* %stack_var_-184
  %1531 = load i64* %rax
  %1532 = trunc i64 %1531 to i32
  %1533 = sub i32 %1530, %1532
  %1534 = and i32 %1530, 15
  %1535 = and i32 %1532, 15
  %1536 = sub i32 %1534, %1535
  %1537 = icmp ugt i32 %1536, 15
  %1538 = icmp ult i32 %1530, %1532
  %1539 = xor i32 %1530, %1532
  %1540 = xor i32 %1530, %1533
  %1541 = and i32 %1539, %1540
  %1542 = icmp slt i32 %1541, 0
  store i1 %1537, i1* %az
  store i1 %1538, i1* %cf
  store i1 %1542, i1* %of
  %1543 = icmp eq i32 %1533, 0
  store i1 %1543, i1* %zf
  %1544 = icmp slt i32 %1533, 0
  store i1 %1544, i1* %sf
  %1545 = trunc i32 %1533 to i8
  %1546 = call i8 @llvm.ctpop.i8(i8 %1545)
  %1547 = and i8 %1546, 1
  %1548 = icmp eq i8 %1547, 0
  store i1 %1548, i1* %pf
  store volatile i64 47941, i64* @assembly_address
  %1549 = load i1* %cf
  br i1 %1549, label %block_b8fa, label %block_bb4b

block_bb4b:                                       ; preds = %block_bb39
  store volatile i64 47947, i64* @assembly_address
  store volatile i64 47948, i64* @assembly_address
  %1550 = load i64* %stack_var_-16
  store i64 %1550, i64* %rax
  store volatile i64 47952, i64* @assembly_address
  %1551 = load i64* %rax
  %1552 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %1553 = xor i64 %1551, %1552
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1554 = icmp eq i64 %1553, 0
  store i1 %1554, i1* %zf
  %1555 = icmp slt i64 %1553, 0
  store i1 %1555, i1* %sf
  %1556 = trunc i64 %1553 to i8
  %1557 = call i8 @llvm.ctpop.i8(i8 %1556)
  %1558 = and i8 %1557, 1
  %1559 = icmp eq i8 %1558, 0
  store i1 %1559, i1* %pf
  store i64 %1553, i64* %rax
  store volatile i64 47961, i64* @assembly_address
  %1560 = load i1* %zf
  br i1 %1560, label %block_bb60, label %block_bb5b

block_bb5b:                                       ; preds = %block_bb4b
  store volatile i64 47963, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_bb60:                                       ; preds = %block_bb4b
  store volatile i64 47968, i64* @assembly_address
  %1561 = load i64* %stack_var_-8
  store i64 %1561, i64* %rbp
  %1562 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1562, i64* %rsp
  store volatile i64 47969, i64* @assembly_address
  %1563 = load i64* %rax
  ret i64 %1563
}

declare i64 @222(i64, i8*, i64, i64*)

declare i64 @223(i64, i64*, i32, i64*)

declare i64 @224(i64, i64*, i64, i16*)

declare i64 @225(i64, i64*, i64, i64*)

define i64 @read_pt_len(i32 %arg1, i64 %arg2, i64 %arg3) {
block_bb62:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %1 = alloca i32
  %stack_var_-24 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-12 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i64
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i64
  %8 = alloca i32
  %9 = alloca i32
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i32
  %13 = alloca i32
  %14 = alloca i64
  store volatile i64 47970, i64* @assembly_address
  %15 = load i64* %rbp
  store i64 %15, i64* %stack_var_-8
  %16 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %16, i64* %rsp
  store volatile i64 47971, i64* @assembly_address
  %17 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %17, i64* %rbp
  store volatile i64 47974, i64* @assembly_address
  %18 = load i64* %rsp
  %19 = sub i64 %18, 32
  %20 = and i64 %18, 15
  %21 = icmp ugt i64 %20, 15
  %22 = icmp ult i64 %18, 32
  %23 = xor i64 %18, 32
  %24 = xor i64 %18, %19
  %25 = and i64 %23, %24
  %26 = icmp slt i64 %25, 0
  store i1 %21, i1* %az
  store i1 %22, i1* %cf
  store i1 %26, i1* %of
  %27 = icmp eq i64 %19, 0
  store i1 %27, i1* %zf
  %28 = icmp slt i64 %19, 0
  store i1 %28, i1* %sf
  %29 = trunc i64 %19 to i8
  %30 = call i8 @llvm.ctpop.i8(i8 %29)
  %31 = and i8 %30, 1
  %32 = icmp eq i8 %31, 0
  store i1 %32, i1* %pf
  %33 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %33, i64* %rsp
  store volatile i64 47978, i64* @assembly_address
  %34 = load i64* %rdi
  %35 = trunc i64 %34 to i32
  store i32 %35, i32* %stack_var_-28
  store volatile i64 47981, i64* @assembly_address
  %36 = load i64* %rsi
  %37 = trunc i64 %36 to i32
  store i32 %37, i32* %stack_var_-32
  store volatile i64 47984, i64* @assembly_address
  %38 = load i64* %rdx
  %39 = trunc i64 %38 to i32
  store i32 %39, i32* %stack_var_-36
  store volatile i64 47987, i64* @assembly_address
  %40 = load i32* %stack_var_-32
  %41 = zext i32 %40 to i64
  store i64 %41, i64* %rax
  store volatile i64 47990, i64* @assembly_address
  %42 = load i64* %rax
  %43 = trunc i64 %42 to i32
  %44 = zext i32 %43 to i64
  store i64 %44, i64* %rdi
  store volatile i64 47992, i64* @assembly_address
  %45 = load i64* %rdi
  %46 = trunc i64 %45 to i32
  %47 = call i64 @getbits(i32 %46)
  store i64 %47, i64* %rax
  store i64 %47, i64* %rax
  store volatile i64 47997, i64* @assembly_address
  %48 = load i64* %rax
  %49 = trunc i64 %48 to i32
  store i32 %49, i32* %stack_var_-12
  store volatile i64 48000, i64* @assembly_address
  %50 = load i32* %stack_var_-12
  %51 = and i32 %50, 15
  %52 = icmp ugt i32 %51, 15
  %53 = icmp ult i32 %50, 0
  %54 = xor i32 %50, 0
  %55 = and i32 %54, 0
  %56 = icmp slt i32 %55, 0
  store i1 %52, i1* %az
  store i1 %53, i1* %cf
  store i1 %56, i1* %of
  %57 = icmp eq i32 %50, 0
  store i1 %57, i1* %zf
  %58 = icmp slt i32 %50, 0
  store i1 %58, i1* %sf
  %59 = trunc i32 %50 to i8
  %60 = call i8 @llvm.ctpop.i8(i8 %59)
  %61 = and i8 %60, 1
  %62 = icmp eq i8 %61, 0
  store i1 %62, i1* %pf
  store volatile i64 48004, i64* @assembly_address
  %63 = load i1* %zf
  %64 = icmp eq i1 %63, false
  br i1 %64, label %block_bbed, label %block_bb86

block_bb86:                                       ; preds = %block_bb62
  store volatile i64 48006, i64* @assembly_address
  %65 = load i32* %stack_var_-32
  %66 = zext i32 %65 to i64
  store i64 %66, i64* %rax
  store volatile i64 48009, i64* @assembly_address
  %67 = load i64* %rax
  %68 = trunc i64 %67 to i32
  %69 = zext i32 %68 to i64
  store i64 %69, i64* %rdi
  store volatile i64 48011, i64* @assembly_address
  %70 = load i64* %rdi
  %71 = trunc i64 %70 to i32
  %72 = call i64 @getbits(i32 %71)
  store i64 %72, i64* %rax
  store i64 %72, i64* %rax
  store volatile i64 48016, i64* @assembly_address
  %73 = load i64* %rax
  %74 = trunc i64 %73 to i32
  store i32 %74, i32* %stack_var_-20
  store volatile i64 48019, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 48026, i64* @assembly_address
  br label %block_bbb1

block_bb9c:                                       ; preds = %block_bbb1
  store volatile i64 48028, i64* @assembly_address
  %75 = load i32* %stack_var_-24
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 48031, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i32
  %79 = sext i32 %78 to i64
  store i64 %79, i64* %rdx
  store volatile i64 48034, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f00 to i64), i64* %rax
  store volatile i64 48041, i64* @assembly_address
  %80 = load i64* %rdx
  %81 = load i64* %rax
  %82 = mul i64 %81, 1
  %83 = add i64 %80, %82
  %84 = inttoptr i64 %83 to i8*
  store i8 0, i8* %84
  store volatile i64 48045, i64* @assembly_address
  %85 = load i32* %stack_var_-24
  %86 = add i32 %85, 1
  %87 = and i32 %85, 15
  %88 = add i32 %87, 1
  %89 = icmp ugt i32 %88, 15
  %90 = icmp ult i32 %86, %85
  %91 = xor i32 %85, %86
  %92 = xor i32 1, %86
  %93 = and i32 %91, %92
  %94 = icmp slt i32 %93, 0
  store i1 %89, i1* %az
  store i1 %90, i1* %cf
  store i1 %94, i1* %of
  %95 = icmp eq i32 %86, 0
  store i1 %95, i1* %zf
  %96 = icmp slt i32 %86, 0
  store i1 %96, i1* %sf
  %97 = trunc i32 %86 to i8
  %98 = call i8 @llvm.ctpop.i8(i8 %97)
  %99 = and i8 %98, 1
  %100 = icmp eq i8 %99, 0
  store i1 %100, i1* %pf
  store i32 %86, i32* %stack_var_-24
  br label %block_bbb1

block_bbb1:                                       ; preds = %block_bb9c, %block_bb86
  store volatile i64 48049, i64* @assembly_address
  %101 = load i32* %stack_var_-24
  %102 = zext i32 %101 to i64
  store i64 %102, i64* %rax
  store volatile i64 48052, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  %105 = load i32* %stack_var_-28
  %106 = trunc i64 %103 to i32
  store i32 %106, i32* %13
  store i32 %105, i32* %12
  %107 = sub i32 %104, %105
  %108 = and i32 %104, 15
  %109 = and i32 %105, 15
  %110 = sub i32 %108, %109
  %111 = icmp ugt i32 %110, 15
  %112 = icmp ult i32 %104, %105
  %113 = xor i32 %104, %105
  %114 = xor i32 %104, %107
  %115 = and i32 %113, %114
  %116 = icmp slt i32 %115, 0
  store i1 %111, i1* %az
  store i1 %112, i1* %cf
  store i1 %116, i1* %of
  %117 = icmp eq i32 %107, 0
  store i1 %117, i1* %zf
  %118 = icmp slt i32 %107, 0
  store i1 %118, i1* %sf
  %119 = trunc i32 %107 to i8
  %120 = call i8 @llvm.ctpop.i8(i8 %119)
  %121 = and i8 %120, 1
  %122 = icmp eq i8 %121, 0
  store i1 %122, i1* %pf
  store volatile i64 48055, i64* @assembly_address
  %123 = load i32* %13
  %124 = sext i32 %123 to i64
  %125 = load i32* %12
  %126 = trunc i64 %124 to i32
  %127 = icmp slt i32 %126, %125
  br i1 %127, label %block_bb9c, label %block_bbb9

block_bbb9:                                       ; preds = %block_bbb1
  store volatile i64 48057, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 48064, i64* @assembly_address
  br label %block_bbdf

block_bbc2:                                       ; preds = %block_bbdf
  store volatile i64 48066, i64* @assembly_address
  %128 = load i32* %stack_var_-20
  %129 = zext i32 %128 to i64
  store i64 %129, i64* %rax
  store volatile i64 48069, i64* @assembly_address
  %130 = load i64* %rax
  %131 = trunc i64 %130 to i32
  %132 = zext i32 %131 to i64
  store i64 %132, i64* %rcx
  store volatile i64 48071, i64* @assembly_address
  %133 = load i32* %stack_var_-24
  %134 = zext i32 %133 to i64
  store i64 %134, i64* %rax
  store volatile i64 48074, i64* @assembly_address
  %135 = load i64* %rax
  %136 = trunc i64 %135 to i32
  %137 = sext i32 %136 to i64
  store i64 %137, i64* %rax
  store volatile i64 48076, i64* @assembly_address
  %138 = load i64* %rax
  %139 = load i64* %rax
  %140 = mul i64 %139, 1
  %141 = add i64 %138, %140
  store i64 %141, i64* %rdx
  store volatile i64 48080, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f40 to i64), i64* %rax
  store volatile i64 48087, i64* @assembly_address
  %142 = load i64* %rcx
  %143 = trunc i64 %142 to i16
  %144 = load i64* %rdx
  %145 = load i64* %rax
  %146 = mul i64 %145, 1
  %147 = add i64 %144, %146
  %148 = inttoptr i64 %147 to i16*
  store i16 %143, i16* %148
  store volatile i64 48091, i64* @assembly_address
  %149 = load i32* %stack_var_-24
  %150 = add i32 %149, 1
  %151 = and i32 %149, 15
  %152 = add i32 %151, 1
  %153 = icmp ugt i32 %152, 15
  %154 = icmp ult i32 %150, %149
  %155 = xor i32 %149, %150
  %156 = xor i32 1, %150
  %157 = and i32 %155, %156
  %158 = icmp slt i32 %157, 0
  store i1 %153, i1* %az
  store i1 %154, i1* %cf
  store i1 %158, i1* %of
  %159 = icmp eq i32 %150, 0
  store i1 %159, i1* %zf
  %160 = icmp slt i32 %150, 0
  store i1 %160, i1* %sf
  %161 = trunc i32 %150 to i8
  %162 = call i8 @llvm.ctpop.i8(i8 %161)
  %163 = and i8 %162, 1
  %164 = icmp eq i8 %163, 0
  store i1 %164, i1* %pf
  store i32 %150, i32* %stack_var_-24
  br label %block_bbdf

block_bbdf:                                       ; preds = %block_bbc2, %block_bbb9
  store volatile i64 48095, i64* @assembly_address
  %165 = load i32* %stack_var_-24
  store i32 %165, i32* %11
  store i32 255, i32* %10
  %166 = sub i32 %165, 255
  %167 = and i32 %165, 15
  %168 = sub i32 %167, 15
  %169 = icmp ugt i32 %168, 15
  %170 = icmp ult i32 %165, 255
  %171 = xor i32 %165, 255
  %172 = xor i32 %165, %166
  %173 = and i32 %171, %172
  %174 = icmp slt i32 %173, 0
  store i1 %169, i1* %az
  store i1 %170, i1* %cf
  store i1 %174, i1* %of
  %175 = icmp eq i32 %166, 0
  store i1 %175, i1* %zf
  %176 = icmp slt i32 %166, 0
  store i1 %176, i1* %sf
  %177 = trunc i32 %166 to i8
  %178 = call i8 @llvm.ctpop.i8(i8 %177)
  %179 = and i8 %178, 1
  %180 = icmp eq i8 %179, 0
  store i1 %180, i1* %pf
  store volatile i64 48102, i64* @assembly_address
  %181 = load i32* %11
  %182 = load i32* %10
  %183 = icmp sle i32 %181, %182
  br i1 %183, label %block_bbc2, label %block_bbe8

block_bbe8:                                       ; preds = %block_bbdf
  store volatile i64 48104, i64* @assembly_address
  br label %block_bcf7

block_bbed:                                       ; preds = %block_bb62
  store volatile i64 48109, i64* @assembly_address
  store i32 0, i32* %stack_var_-24
  store volatile i64 48116, i64* @assembly_address
  br label %block_bcad

block_bbf9:                                       ; preds = %block_bcad
  store volatile i64 48121, i64* @assembly_address
  %184 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %185 = zext i16 %184 to i64
  store i64 %185, i64* %rax
  store volatile i64 48128, i64* @assembly_address
  %186 = load i64* %rax
  %187 = trunc i64 %186 to i16
  %188 = load i1* %of
  %189 = lshr i16 %187, 13
  %190 = icmp eq i16 %189, 0
  store i1 %190, i1* %zf
  %191 = icmp slt i16 %189, 0
  store i1 %191, i1* %sf
  %192 = trunc i16 %189 to i8
  %193 = call i8 @llvm.ctpop.i8(i8 %192)
  %194 = and i8 %193, 1
  %195 = icmp eq i8 %194, 0
  store i1 %195, i1* %pf
  %196 = zext i16 %189 to i64
  %197 = load i64* %rax
  %198 = and i64 %197, -65536
  %199 = or i64 %198, %196
  store i64 %199, i64* %rax
  %200 = and i16 4096, %187
  %201 = icmp ne i16 %200, 0
  store i1 %201, i1* %cf
  %202 = icmp slt i16 %187, 0
  %203 = select i1 false, i1 %202, i1 %188
  store i1 %203, i1* %of
  store volatile i64 48132, i64* @assembly_address
  %204 = load i64* %rax
  %205 = trunc i64 %204 to i16
  %206 = zext i16 %205 to i64
  store i64 %206, i64* %rax
  store volatile i64 48135, i64* @assembly_address
  %207 = load i64* %rax
  %208 = trunc i64 %207 to i32
  store i32 %208, i32* %stack_var_-20
  store volatile i64 48138, i64* @assembly_address
  %209 = load i32* %stack_var_-20
  %210 = sub i32 %209, 7
  %211 = and i32 %209, 15
  %212 = sub i32 %211, 7
  %213 = icmp ugt i32 %212, 15
  %214 = icmp ult i32 %209, 7
  %215 = xor i32 %209, 7
  %216 = xor i32 %209, %210
  %217 = and i32 %215, %216
  %218 = icmp slt i32 %217, 0
  store i1 %213, i1* %az
  store i1 %214, i1* %cf
  store i1 %218, i1* %of
  %219 = icmp eq i32 %210, 0
  store i1 %219, i1* %zf
  %220 = icmp slt i32 %210, 0
  store i1 %220, i1* %sf
  %221 = trunc i32 %210 to i8
  %222 = call i8 @llvm.ctpop.i8(i8 %221)
  %223 = and i8 %222, 1
  %224 = icmp eq i8 %223, 0
  store i1 %224, i1* %pf
  store volatile i64 48142, i64* @assembly_address
  %225 = load i1* %zf
  %226 = icmp eq i1 %225, false
  br i1 %226, label %block_bc43, label %block_bc10

block_bc10:                                       ; preds = %block_bbf9
  store volatile i64 48144, i64* @assembly_address
  %227 = inttoptr i32 ptrtoint ([2 x i8]* @global_var_1000 to i32) to i8*
  store i8* %227, i8** %stack_var_-16
  store volatile i64 48151, i64* @assembly_address
  br label %block_bc20

block_bc19:                                       ; preds = %block_bc20
  store volatile i64 48153, i64* @assembly_address
  %228 = load i8** %stack_var_-16
  %229 = ptrtoint i8* %228 to i32
  %230 = load i1* %of
  %231 = lshr i32 %229, 1
  %232 = icmp eq i32 %231, 0
  store i1 %232, i1* %zf
  %233 = icmp slt i32 %231, 0
  store i1 %233, i1* %sf
  %234 = trunc i32 %231 to i8
  %235 = call i8 @llvm.ctpop.i8(i8 %234)
  %236 = and i8 %235, 1
  %237 = icmp eq i8 %236, 0
  store i1 %237, i1* %pf
  %238 = inttoptr i32 %231 to i8*
  store i8* %238, i8** %stack_var_-16
  %239 = and i32 1, %229
  %240 = icmp ne i32 %239, 0
  store i1 %240, i1* %cf
  %241 = icmp slt i32 %229, 0
  %242 = select i1 true, i1 %241, i1 %230
  store i1 %242, i1* %of
  store volatile i64 48156, i64* @assembly_address
  %243 = load i32* %stack_var_-20
  %244 = add i32 %243, 1
  %245 = and i32 %243, 15
  %246 = add i32 %245, 1
  %247 = icmp ugt i32 %246, 15
  %248 = icmp ult i32 %244, %243
  %249 = xor i32 %243, %244
  %250 = xor i32 1, %244
  %251 = and i32 %249, %250
  %252 = icmp slt i32 %251, 0
  store i1 %247, i1* %az
  store i1 %248, i1* %cf
  store i1 %252, i1* %of
  %253 = icmp eq i32 %244, 0
  store i1 %253, i1* %zf
  %254 = icmp slt i32 %244, 0
  store i1 %254, i1* %sf
  %255 = trunc i32 %244 to i8
  %256 = call i8 @llvm.ctpop.i8(i8 %255)
  %257 = and i8 %256, 1
  %258 = icmp eq i8 %257, 0
  store i1 %258, i1* %pf
  store i32 %244, i32* %stack_var_-20
  br label %block_bc20

block_bc20:                                       ; preds = %block_bc19, %block_bc10
  store volatile i64 48160, i64* @assembly_address
  %259 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %260 = zext i16 %259 to i64
  store i64 %260, i64* %rax
  store volatile i64 48167, i64* @assembly_address
  %261 = load i64* %rax
  %262 = trunc i64 %261 to i16
  %263 = zext i16 %262 to i64
  store i64 %263, i64* %rax
  store volatile i64 48170, i64* @assembly_address
  %264 = load i64* %rax
  %265 = trunc i64 %264 to i32
  %266 = load i8** %stack_var_-16
  %267 = ptrtoint i8* %266 to i32
  %268 = and i32 %265, %267
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %269 = icmp eq i32 %268, 0
  store i1 %269, i1* %zf
  %270 = icmp slt i32 %268, 0
  store i1 %270, i1* %sf
  %271 = trunc i32 %268 to i8
  %272 = call i8 @llvm.ctpop.i8(i8 %271)
  %273 = and i8 %272, 1
  %274 = icmp eq i8 %273, 0
  store i1 %274, i1* %pf
  %275 = zext i32 %268 to i64
  store i64 %275, i64* %rax
  store volatile i64 48173, i64* @assembly_address
  %276 = load i64* %rax
  %277 = trunc i64 %276 to i32
  %278 = load i64* %rax
  %279 = trunc i64 %278 to i32
  %280 = and i32 %277, %279
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %281 = icmp eq i32 %280, 0
  store i1 %281, i1* %zf
  %282 = icmp slt i32 %280, 0
  store i1 %282, i1* %sf
  %283 = trunc i32 %280 to i8
  %284 = call i8 @llvm.ctpop.i8(i8 %283)
  %285 = and i8 %284, 1
  %286 = icmp eq i8 %285, 0
  store i1 %286, i1* %pf
  store volatile i64 48175, i64* @assembly_address
  %287 = load i1* %zf
  %288 = icmp eq i1 %287, false
  br i1 %288, label %block_bc19, label %block_bc31

block_bc31:                                       ; preds = %block_bc20
  store volatile i64 48177, i64* @assembly_address
  %289 = load i32* %stack_var_-20
  store i32 %289, i32* %9
  store i32 16, i32* %8
  %290 = sub i32 %289, 16
  %291 = and i32 %289, 15
  %292 = icmp ugt i32 %291, 15
  %293 = icmp ult i32 %289, 16
  %294 = xor i32 %289, 16
  %295 = xor i32 %289, %290
  %296 = and i32 %294, %295
  %297 = icmp slt i32 %296, 0
  store i1 %292, i1* %az
  store i1 %293, i1* %cf
  store i1 %297, i1* %of
  %298 = icmp eq i32 %290, 0
  store i1 %298, i1* %zf
  %299 = icmp slt i32 %290, 0
  store i1 %299, i1* %sf
  %300 = trunc i32 %290 to i8
  %301 = call i8 @llvm.ctpop.i8(i8 %300)
  %302 = and i8 %301, 1
  %303 = icmp eq i8 %302, 0
  store i1 %303, i1* %pf
  store volatile i64 48181, i64* @assembly_address
  %304 = load i32* %9
  %305 = load i32* %8
  %306 = icmp sle i32 %304, %305
  br i1 %306, label %block_bc43, label %block_bc37

block_bc37:                                       ; preds = %block_bc31
  store volatile i64 48183, i64* @assembly_address
  store i64 ptrtoint ([11 x i8]* @global_var_120bc to i64), i64* %rdi
  store volatile i64 48190, i64* @assembly_address
  %307 = load i64* %rdi
  %308 = inttoptr i64 %307 to i8*
  %309 = call i64 @gzip_error(i8* %308)
  store i64 %309, i64* %rax
  store i64 %309, i64* %rax
  unreachable

block_bc43:                                       ; preds = %block_bc31, %block_bbf9
  store volatile i64 48195, i64* @assembly_address
  store i64 6, i64* %rax
  store volatile i64 48200, i64* @assembly_address
  %310 = load i32* %stack_var_-20
  %311 = sub i32 %310, 6
  %312 = and i32 %310, 15
  %313 = sub i32 %312, 6
  %314 = icmp ugt i32 %313, 15
  %315 = icmp ult i32 %310, 6
  %316 = xor i32 %310, 6
  %317 = xor i32 %310, %311
  %318 = and i32 %316, %317
  %319 = icmp slt i32 %318, 0
  store i1 %314, i1* %az
  store i1 %315, i1* %cf
  store i1 %319, i1* %of
  %320 = icmp eq i32 %311, 0
  store i1 %320, i1* %zf
  %321 = icmp slt i32 %311, 0
  store i1 %321, i1* %sf
  %322 = trunc i32 %311 to i8
  %323 = call i8 @llvm.ctpop.i8(i8 %322)
  %324 = and i8 %323, 1
  %325 = icmp eq i8 %324, 0
  store i1 %325, i1* %pf
  store volatile i64 48204, i64* @assembly_address
  %326 = load i1* %sf
  %327 = load i1* %of
  %328 = icmp eq i1 %326, %327
  %329 = load i64* %rax
  %330 = trunc i64 %329 to i32
  %331 = load i32* %stack_var_-20
  %332 = select i1 %328, i32 %331, i32 %330
  %333 = zext i32 %332 to i64
  store i64 %333, i64* %rax
  store volatile i64 48208, i64* @assembly_address
  %334 = load i64* %rax
  %335 = trunc i64 %334 to i32
  %336 = sub i32 %335, 3
  %337 = and i32 %335, 15
  %338 = sub i32 %337, 3
  %339 = icmp ugt i32 %338, 15
  %340 = icmp ult i32 %335, 3
  %341 = xor i32 %335, 3
  %342 = xor i32 %335, %336
  %343 = and i32 %341, %342
  %344 = icmp slt i32 %343, 0
  store i1 %339, i1* %az
  store i1 %340, i1* %cf
  store i1 %344, i1* %of
  %345 = icmp eq i32 %336, 0
  store i1 %345, i1* %zf
  %346 = icmp slt i32 %336, 0
  store i1 %346, i1* %sf
  %347 = trunc i32 %336 to i8
  %348 = call i8 @llvm.ctpop.i8(i8 %347)
  %349 = and i8 %348, 1
  %350 = icmp eq i8 %349, 0
  store i1 %350, i1* %pf
  %351 = zext i32 %336 to i64
  store i64 %351, i64* %rax
  store volatile i64 48211, i64* @assembly_address
  %352 = load i64* %rax
  %353 = trunc i64 %352 to i32
  %354 = zext i32 %353 to i64
  store i64 %354, i64* %rdi
  store volatile i64 48213, i64* @assembly_address
  %355 = load i64* %rdi
  %356 = trunc i64 %355 to i32
  %357 = call i64 @fillbuf(i32 %356)
  store i64 %357, i64* %rax
  store i64 %357, i64* %rax
  store volatile i64 48218, i64* @assembly_address
  %358 = load i32* %stack_var_-24
  %359 = zext i32 %358 to i64
  store i64 %359, i64* %rax
  store volatile i64 48221, i64* @assembly_address
  %360 = load i64* %rax
  %361 = add i64 %360, 1
  %362 = trunc i64 %361 to i32
  %363 = zext i32 %362 to i64
  store i64 %363, i64* %rdx
  store volatile i64 48224, i64* @assembly_address
  %364 = load i64* %rdx
  %365 = trunc i64 %364 to i32
  store i32 %365, i32* %stack_var_-24
  store volatile i64 48227, i64* @assembly_address
  %366 = load i32* %stack_var_-20
  %367 = zext i32 %366 to i64
  store i64 %367, i64* %rdx
  store volatile i64 48230, i64* @assembly_address
  %368 = load i64* %rdx
  %369 = trunc i64 %368 to i32
  %370 = zext i32 %369 to i64
  store i64 %370, i64* %rcx
  store volatile i64 48232, i64* @assembly_address
  %371 = load i64* %rax
  %372 = trunc i64 %371 to i32
  %373 = sext i32 %372 to i64
  store i64 %373, i64* %rdx
  store volatile i64 48235, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f00 to i64), i64* %rax
  store volatile i64 48242, i64* @assembly_address
  %374 = load i64* %rcx
  %375 = trunc i64 %374 to i8
  %376 = load i64* %rdx
  %377 = load i64* %rax
  %378 = mul i64 %377, 1
  %379 = add i64 %376, %378
  %380 = inttoptr i64 %379 to i8*
  store i8 %375, i8* %380
  store volatile i64 48245, i64* @assembly_address
  %381 = load i32* %stack_var_-24
  %382 = zext i32 %381 to i64
  store i64 %382, i64* %rax
  store volatile i64 48248, i64* @assembly_address
  %383 = load i64* %rax
  %384 = trunc i64 %383 to i32
  %385 = load i32* %stack_var_-36
  %386 = sub i32 %384, %385
  %387 = and i32 %384, 15
  %388 = and i32 %385, 15
  %389 = sub i32 %387, %388
  %390 = icmp ugt i32 %389, 15
  %391 = icmp ult i32 %384, %385
  %392 = xor i32 %384, %385
  %393 = xor i32 %384, %386
  %394 = and i32 %392, %393
  %395 = icmp slt i32 %394, 0
  store i1 %390, i1* %az
  store i1 %391, i1* %cf
  store i1 %395, i1* %of
  %396 = icmp eq i32 %386, 0
  store i1 %396, i1* %zf
  %397 = icmp slt i32 %386, 0
  store i1 %397, i1* %sf
  %398 = trunc i32 %386 to i8
  %399 = call i8 @llvm.ctpop.i8(i8 %398)
  %400 = and i8 %399, 1
  %401 = icmp eq i8 %400, 0
  store i1 %401, i1* %pf
  store volatile i64 48251, i64* @assembly_address
  %402 = load i1* %zf
  %403 = icmp eq i1 %402, false
  br i1 %403, label %block_bcad, label %block_bc7d

block_bc7d:                                       ; preds = %block_bc43
  store volatile i64 48253, i64* @assembly_address
  store i64 2, i64* %rdi
  store volatile i64 48258, i64* @assembly_address
  %404 = load i64* %rdi
  %405 = trunc i64 %404 to i32
  %406 = call i64 @getbits(i32 %405)
  store i64 %406, i64* %rax
  store i64 %406, i64* %rax
  store volatile i64 48263, i64* @assembly_address
  %407 = load i64* %rax
  %408 = trunc i64 %407 to i32
  store i32 %408, i32* %stack_var_-20
  store volatile i64 48266, i64* @assembly_address
  br label %block_bca3

block_bc8c:                                       ; preds = %block_bca3
  store volatile i64 48268, i64* @assembly_address
  %409 = load i32* %stack_var_-24
  %410 = zext i32 %409 to i64
  store i64 %410, i64* %rax
  store volatile i64 48271, i64* @assembly_address
  %411 = load i64* %rax
  %412 = add i64 %411, 1
  %413 = trunc i64 %412 to i32
  %414 = zext i32 %413 to i64
  store i64 %414, i64* %rdx
  store volatile i64 48274, i64* @assembly_address
  %415 = load i64* %rdx
  %416 = trunc i64 %415 to i32
  store i32 %416, i32* %stack_var_-24
  store volatile i64 48277, i64* @assembly_address
  %417 = load i64* %rax
  %418 = trunc i64 %417 to i32
  %419 = sext i32 %418 to i64
  store i64 %419, i64* %rdx
  store volatile i64 48280, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f00 to i64), i64* %rax
  store volatile i64 48287, i64* @assembly_address
  %420 = load i64* %rdx
  %421 = load i64* %rax
  %422 = mul i64 %421, 1
  %423 = add i64 %420, %422
  %424 = inttoptr i64 %423 to i8*
  store i8 0, i8* %424
  br label %block_bca3

block_bca3:                                       ; preds = %block_bc8c, %block_bc7d
  store volatile i64 48291, i64* @assembly_address
  %425 = load i32* %stack_var_-20
  %426 = sub i32 %425, 1
  %427 = and i32 %425, 15
  %428 = sub i32 %427, 1
  %429 = icmp ugt i32 %428, 15
  %430 = icmp ult i32 %425, 1
  %431 = xor i32 %425, 1
  %432 = xor i32 %425, %426
  %433 = and i32 %431, %432
  %434 = icmp slt i32 %433, 0
  store i1 %429, i1* %az
  store i1 %430, i1* %cf
  store i1 %434, i1* %of
  %435 = icmp eq i32 %426, 0
  store i1 %435, i1* %zf
  %436 = icmp slt i32 %426, 0
  store i1 %436, i1* %sf
  %437 = trunc i32 %426 to i8
  %438 = call i8 @llvm.ctpop.i8(i8 %437)
  %439 = and i8 %438, 1
  %440 = icmp eq i8 %439, 0
  store i1 %440, i1* %pf
  store i32 %426, i32* %stack_var_-20
  store volatile i64 48295, i64* @assembly_address
  %441 = load i32* %stack_var_-20
  %442 = and i32 %441, 15
  %443 = icmp ugt i32 %442, 15
  %444 = icmp ult i32 %441, 0
  %445 = xor i32 %441, 0
  %446 = and i32 %445, 0
  %447 = icmp slt i32 %446, 0
  store i1 %443, i1* %az
  store i1 %444, i1* %cf
  store i1 %447, i1* %of
  %448 = icmp eq i32 %441, 0
  store i1 %448, i1* %zf
  %449 = icmp slt i32 %441, 0
  store i1 %449, i1* %sf
  %450 = trunc i32 %441 to i8
  %451 = call i8 @llvm.ctpop.i8(i8 %450)
  %452 = and i8 %451, 1
  %453 = icmp eq i8 %452, 0
  store i1 %453, i1* %pf
  store volatile i64 48299, i64* @assembly_address
  %454 = load i1* %sf
  %455 = icmp eq i1 %454, false
  br i1 %455, label %block_bc8c, label %block_bcad

block_bcad:                                       ; preds = %block_bca3, %block_bc43, %block_bbed
  store volatile i64 48301, i64* @assembly_address
  %456 = load i32* %stack_var_-24
  %457 = zext i32 %456 to i64
  store i64 %457, i64* %rax
  store volatile i64 48304, i64* @assembly_address
  %458 = load i64* %rax
  %459 = trunc i64 %458 to i32
  %460 = load i32* %stack_var_-12
  %461 = trunc i64 %458 to i32
  store i32 %461, i32* %6
  store i32 %460, i32* %5
  %462 = sub i32 %459, %460
  %463 = and i32 %459, 15
  %464 = and i32 %460, 15
  %465 = sub i32 %463, %464
  %466 = icmp ugt i32 %465, 15
  %467 = icmp ult i32 %459, %460
  %468 = xor i32 %459, %460
  %469 = xor i32 %459, %462
  %470 = and i32 %468, %469
  %471 = icmp slt i32 %470, 0
  store i1 %466, i1* %az
  store i1 %467, i1* %cf
  store i1 %471, i1* %of
  %472 = icmp eq i32 %462, 0
  store i1 %472, i1* %zf
  %473 = icmp slt i32 %462, 0
  store i1 %473, i1* %sf
  %474 = trunc i32 %462 to i8
  %475 = call i8 @llvm.ctpop.i8(i8 %474)
  %476 = and i8 %475, 1
  %477 = icmp eq i8 %476, 0
  store i1 %477, i1* %pf
  store volatile i64 48307, i64* @assembly_address
  %478 = load i32* %6
  %479 = sext i32 %478 to i64
  %480 = load i32* %5
  %481 = trunc i64 %479 to i32
  %482 = icmp slt i32 %481, %480
  br i1 %482, label %block_bbf9, label %block_bcb9

block_bcb9:                                       ; preds = %block_bcad
  store volatile i64 48313, i64* @assembly_address
  br label %block_bcd2

block_bcbb:                                       ; preds = %block_bcd2
  store volatile i64 48315, i64* @assembly_address
  %483 = load i32* %stack_var_-24
  %484 = zext i32 %483 to i64
  store i64 %484, i64* %rax
  store volatile i64 48318, i64* @assembly_address
  %485 = load i64* %rax
  %486 = add i64 %485, 1
  %487 = trunc i64 %486 to i32
  %488 = zext i32 %487 to i64
  store i64 %488, i64* %rdx
  store volatile i64 48321, i64* @assembly_address
  %489 = load i64* %rdx
  %490 = trunc i64 %489 to i32
  store i32 %490, i32* %stack_var_-24
  store volatile i64 48324, i64* @assembly_address
  %491 = load i64* %rax
  %492 = trunc i64 %491 to i32
  %493 = sext i32 %492 to i64
  store i64 %493, i64* %rdx
  store volatile i64 48327, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f00 to i64), i64* %rax
  store volatile i64 48334, i64* @assembly_address
  %494 = load i64* %rdx
  %495 = load i64* %rax
  %496 = mul i64 %495, 1
  %497 = add i64 %494, %496
  %498 = inttoptr i64 %497 to i8*
  store i8 0, i8* %498
  br label %block_bcd2

block_bcd2:                                       ; preds = %block_bcbb, %block_bcb9
  store volatile i64 48338, i64* @assembly_address
  %499 = load i32* %stack_var_-24
  %500 = zext i32 %499 to i64
  store i64 %500, i64* %rax
  store volatile i64 48341, i64* @assembly_address
  %501 = load i64* %rax
  %502 = trunc i64 %501 to i32
  %503 = load i32* %stack_var_-28
  %504 = trunc i64 %501 to i32
  store i32 %504, i32* %3
  store i32 %503, i32* %2
  %505 = sub i32 %502, %503
  %506 = and i32 %502, 15
  %507 = and i32 %503, 15
  %508 = sub i32 %506, %507
  %509 = icmp ugt i32 %508, 15
  %510 = icmp ult i32 %502, %503
  %511 = xor i32 %502, %503
  %512 = xor i32 %502, %505
  %513 = and i32 %511, %512
  %514 = icmp slt i32 %513, 0
  store i1 %509, i1* %az
  store i1 %510, i1* %cf
  store i1 %514, i1* %of
  %515 = icmp eq i32 %505, 0
  store i1 %515, i1* %zf
  %516 = icmp slt i32 %505, 0
  store i1 %516, i1* %sf
  %517 = trunc i32 %505 to i8
  %518 = call i8 @llvm.ctpop.i8(i8 %517)
  %519 = and i8 %518, 1
  %520 = icmp eq i8 %519, 0
  store i1 %520, i1* %pf
  store volatile i64 48344, i64* @assembly_address
  %521 = load i32* %3
  %522 = sext i32 %521 to i64
  %523 = load i32* %2
  %524 = trunc i64 %522 to i32
  %525 = icmp slt i32 %524, %523
  br i1 %525, label %block_bcbb, label %block_bcda

block_bcda:                                       ; preds = %block_bcd2
  store volatile i64 48346, i64* @assembly_address
  %526 = load i32* %stack_var_-28
  %527 = zext i32 %526 to i64
  store i64 %527, i64* %rax
  store volatile i64 48349, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f40 to i64), i64* %rcx
  store volatile i64 48356, i64* @assembly_address
  store i64 8, i64* %rdx
  store volatile i64 48361, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f00 to i64), i64* %rsi
  store volatile i64 48368, i64* @assembly_address
  %528 = load i64* %rax
  %529 = trunc i64 %528 to i32
  %530 = zext i32 %529 to i64
  store i64 %530, i64* %rdi
  store volatile i64 48370, i64* @assembly_address
  %531 = load i64* %rdi
  %532 = load i64* %rsi
  %533 = inttoptr i64 %532 to i64*
  %534 = load i64* %rdx
  %535 = load i64* %rcx
  %536 = inttoptr i64 %535 to i64*
  %537 = trunc i64 %531 to i32
  %538 = call i64 @make_table(i32 %537, i64* %533, i64 %534, i64* %536)
  store i64 %538, i64* %rax
  store i64 %538, i64* %rax
  br label %block_bcf7

block_bcf7:                                       ; preds = %block_bcda, %block_bbe8
  store volatile i64 48375, i64* @assembly_address
  store volatile i64 48376, i64* @assembly_address
  %539 = load i64* %stack_var_-8
  store i64 %539, i64* %rbp
  %540 = ptrtoint i64* %stack_var_0 to i64
  store i64 %540, i64* %rsp
  store volatile i64 48377, i64* @assembly_address
  %541 = load i64* %rax
  ret i64 %541
}

declare i64 @226(i64, i32, i64)

declare i64 @227(i64, i64, i32)

declare i64 @228(i64, i64, i64)

define i64 @read_c_len() {
block_bcfa:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i32
  %0 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i64
  %6 = alloca i32
  %7 = alloca i32
  %8 = alloca i32
  %9 = alloca i32
  %10 = alloca i32
  %11 = alloca i32
  %12 = alloca i64*
  %13 = alloca i32
  %14 = alloca i32
  %15 = alloca i32
  %16 = alloca i32
  store volatile i64 48378, i64* @assembly_address
  %17 = load i64* %rbp
  store i64 %17, i64* %stack_var_-8
  %18 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %18, i64* %rsp
  store volatile i64 48379, i64* @assembly_address
  %19 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %19, i64* %rbp
  store volatile i64 48382, i64* @assembly_address
  %20 = load i64* %rsp
  %21 = sub i64 %20, 16
  %22 = and i64 %20, 15
  %23 = icmp ugt i64 %22, 15
  %24 = icmp ult i64 %20, 16
  %25 = xor i64 %20, 16
  %26 = xor i64 %20, %21
  %27 = and i64 %25, %26
  %28 = icmp slt i64 %27, 0
  store i1 %23, i1* %az
  store i1 %24, i1* %cf
  store i1 %28, i1* %of
  %29 = icmp eq i64 %21, 0
  store i1 %29, i1* %zf
  %30 = icmp slt i64 %21, 0
  store i1 %30, i1* %sf
  %31 = trunc i64 %21 to i8
  %32 = call i8 @llvm.ctpop.i8(i8 %31)
  %33 = and i8 %32, 1
  %34 = icmp eq i8 %33, 0
  store i1 %34, i1* %pf
  %35 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %35, i64* %rsp
  store volatile i64 48386, i64* @assembly_address
  store i64 9, i64* %rdi
  store volatile i64 48391, i64* @assembly_address
  %36 = load i64* %rdi
  %37 = trunc i64 %36 to i32
  %38 = call i64 @getbits(i32 %37)
  store i64 %38, i64* %rax
  store i64 %38, i64* %rax
  store volatile i64 48396, i64* @assembly_address
  %39 = load i64* %rax
  %40 = trunc i64 %39 to i32
  store i32 %40, i32* %stack_var_-12
  store volatile i64 48399, i64* @assembly_address
  %41 = load i32* %stack_var_-12
  %42 = and i32 %41, 15
  %43 = icmp ugt i32 %42, 15
  %44 = icmp ult i32 %41, 0
  %45 = xor i32 %41, 0
  %46 = and i32 %45, 0
  %47 = icmp slt i32 %46, 0
  store i1 %43, i1* %az
  store i1 %44, i1* %cf
  store i1 %47, i1* %of
  %48 = icmp eq i32 %41, 0
  store i1 %48, i1* %zf
  %49 = icmp slt i32 %41, 0
  store i1 %49, i1* %sf
  %50 = trunc i32 %41 to i8
  %51 = call i8 @llvm.ctpop.i8(i8 %50)
  %52 = and i8 %51, 1
  %53 = icmp eq i8 %52, 0
  store i1 %53, i1* %pf
  store volatile i64 48403, i64* @assembly_address
  %54 = load i1* %zf
  %55 = icmp eq i1 %54, false
  br i1 %55, label %block_bd7d, label %block_bd15

block_bd15:                                       ; preds = %block_bcfa
  store volatile i64 48405, i64* @assembly_address
  store i64 9, i64* %rdi
  store volatile i64 48410, i64* @assembly_address
  %56 = load i64* %rdi
  %57 = trunc i64 %56 to i32
  %58 = call i64 @getbits(i32 %57)
  store i64 %58, i64* %rax
  store i64 %58, i64* %rax
  store volatile i64 48415, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  store i32 %60, i32* %stack_var_-20
  store volatile i64 48418, i64* @assembly_address
  %61 = sext i32 0 to i64
  %62 = trunc i64 %61 to i32
  store i32 %62, i32* %stack_var_-24
  store volatile i64 48425, i64* @assembly_address
  br label %block_bd40

block_bd2b:                                       ; preds = %block_bd40
  store volatile i64 48427, i64* @assembly_address
  %63 = load i32* %stack_var_-24
  %64 = sext i32 %63 to i64
  %65 = trunc i64 %64 to i32
  %66 = zext i32 %65 to i64
  store i64 %66, i64* %rax
  store volatile i64 48430, i64* @assembly_address
  %67 = load i64* %rax
  %68 = trunc i64 %67 to i32
  %69 = sext i32 %68 to i64
  store i64 %69, i64* %rdx
  store volatile i64 48433, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 48440, i64* @assembly_address
  %70 = load i64* %rdx
  %71 = load i64* %rax
  %72 = mul i64 %71, 1
  %73 = add i64 %70, %72
  %74 = inttoptr i64 %73 to i8*
  store i8 0, i8* %74
  store volatile i64 48444, i64* @assembly_address
  %75 = load i32* %stack_var_-24
  %76 = sext i32 %75 to i64
  %77 = trunc i64 %76 to i32
  %78 = add i32 %77, 1
  %79 = and i32 %77, 15
  %80 = add i32 %79, 1
  %81 = icmp ugt i32 %80, 15
  %82 = icmp ult i32 %78, %77
  %83 = xor i32 %77, %78
  %84 = xor i32 1, %78
  %85 = and i32 %83, %84
  %86 = icmp slt i32 %85, 0
  store i1 %81, i1* %az
  store i1 %82, i1* %cf
  store i1 %86, i1* %of
  %87 = icmp eq i32 %78, 0
  store i1 %87, i1* %zf
  %88 = icmp slt i32 %78, 0
  store i1 %88, i1* %sf
  %89 = trunc i32 %78 to i8
  %90 = call i8 @llvm.ctpop.i8(i8 %89)
  %91 = and i8 %90, 1
  %92 = icmp eq i8 %91, 0
  store i1 %92, i1* %pf
  %93 = sext i32 %78 to i64
  %94 = trunc i64 %93 to i32
  store i32 %94, i32* %stack_var_-24
  br label %block_bd40

block_bd40:                                       ; preds = %block_bd2b, %block_bd15
  store volatile i64 48448, i64* @assembly_address
  %95 = load i32* %stack_var_-24
  %96 = sext i32 %95 to i64
  %97 = trunc i64 %96 to i32
  store i32 %97, i32* %16
  store i32 509, i32* %15
  %98 = sub i32 %97, 509
  %99 = and i32 %97, 15
  %100 = sub i32 %99, 13
  %101 = icmp ugt i32 %100, 15
  %102 = icmp ult i32 %97, 509
  %103 = xor i32 %97, 509
  %104 = xor i32 %97, %98
  %105 = and i32 %103, %104
  %106 = icmp slt i32 %105, 0
  store i1 %101, i1* %az
  store i1 %102, i1* %cf
  store i1 %106, i1* %of
  %107 = icmp eq i32 %98, 0
  store i1 %107, i1* %zf
  %108 = icmp slt i32 %98, 0
  store i1 %108, i1* %sf
  %109 = trunc i32 %98 to i8
  %110 = call i8 @llvm.ctpop.i8(i8 %109)
  %111 = and i8 %110, 1
  %112 = icmp eq i8 %111, 0
  store i1 %112, i1* %pf
  store volatile i64 48455, i64* @assembly_address
  %113 = load i32* %16
  %114 = load i32* %15
  %115 = icmp sle i32 %113, %114
  br i1 %115, label %block_bd2b, label %block_bd49

block_bd49:                                       ; preds = %block_bd40
  store volatile i64 48457, i64* @assembly_address
  %116 = sext i32 0 to i64
  %117 = trunc i64 %116 to i32
  store i32 %117, i32* %stack_var_-24
  store volatile i64 48464, i64* @assembly_address
  br label %block_bd6f

block_bd52:                                       ; preds = %block_bd6f
  store volatile i64 48466, i64* @assembly_address
  %118 = load i32* %stack_var_-20
  %119 = zext i32 %118 to i64
  store i64 %119, i64* %rax
  store volatile i64 48469, i64* @assembly_address
  %120 = load i64* %rax
  %121 = trunc i64 %120 to i32
  %122 = zext i32 %121 to i64
  store i64 %122, i64* %rcx
  store volatile i64 48471, i64* @assembly_address
  %123 = load i32* %stack_var_-24
  %124 = sext i32 %123 to i64
  %125 = trunc i64 %124 to i32
  %126 = zext i32 %125 to i64
  store i64 %126, i64* %rax
  store volatile i64 48474, i64* @assembly_address
  %127 = load i64* %rax
  %128 = trunc i64 %127 to i32
  %129 = sext i32 %128 to i64
  store i64 %129, i64* %rax
  store volatile i64 48476, i64* @assembly_address
  %130 = load i64* %rax
  %131 = load i64* %rax
  %132 = mul i64 %131, 1
  %133 = add i64 %130, %132
  store i64 %133, i64* %rdx
  store volatile i64 48480, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_23a880 to i64), i64* %rax
  store volatile i64 48487, i64* @assembly_address
  %134 = load i64* %rcx
  %135 = trunc i64 %134 to i16
  %136 = load i64* %rdx
  %137 = load i64* %rax
  %138 = mul i64 %137, 1
  %139 = add i64 %136, %138
  %140 = inttoptr i64 %139 to i16*
  store i16 %135, i16* %140
  store volatile i64 48491, i64* @assembly_address
  %141 = load i32* %stack_var_-24
  %142 = sext i32 %141 to i64
  %143 = trunc i64 %142 to i32
  %144 = add i32 %143, 1
  %145 = and i32 %143, 15
  %146 = add i32 %145, 1
  %147 = icmp ugt i32 %146, 15
  %148 = icmp ult i32 %144, %143
  %149 = xor i32 %143, %144
  %150 = xor i32 1, %144
  %151 = and i32 %149, %150
  %152 = icmp slt i32 %151, 0
  store i1 %147, i1* %az
  store i1 %148, i1* %cf
  store i1 %152, i1* %of
  %153 = icmp eq i32 %144, 0
  store i1 %153, i1* %zf
  %154 = icmp slt i32 %144, 0
  store i1 %154, i1* %sf
  %155 = trunc i32 %144 to i8
  %156 = call i8 @llvm.ctpop.i8(i8 %155)
  %157 = and i8 %156, 1
  %158 = icmp eq i8 %157, 0
  store i1 %158, i1* %pf
  %159 = sext i32 %144 to i64
  %160 = trunc i64 %159 to i32
  store i32 %160, i32* %stack_var_-24
  br label %block_bd6f

block_bd6f:                                       ; preds = %block_bd52, %block_bd49
  store volatile i64 48495, i64* @assembly_address
  %161 = load i32* %stack_var_-24
  %162 = sext i32 %161 to i64
  %163 = trunc i64 %162 to i32
  store i32 %163, i32* %14
  %164 = inttoptr i32 ptrtoint (i64** @global_var_fff to i32) to i64*
  store i64* %164, i64** %12
  %165 = sub i32 %163, 4095
  %166 = and i32 %163, 15
  %167 = sub i32 %166, 15
  %168 = icmp ugt i32 %167, 15
  %169 = icmp ult i32 %163, 4095
  %170 = xor i32 %163, 4095
  %171 = xor i32 %163, %165
  %172 = and i32 %170, %171
  %173 = icmp slt i32 %172, 0
  store i1 %168, i1* %az
  store i1 %169, i1* %cf
  store i1 %173, i1* %of
  %174 = icmp eq i32 %165, 0
  store i1 %174, i1* %zf
  %175 = icmp slt i32 %165, 0
  store i1 %175, i1* %sf
  %176 = trunc i32 %165 to i8
  %177 = call i8 @llvm.ctpop.i8(i8 %176)
  %178 = and i8 %177, 1
  %179 = icmp eq i8 %178, 0
  store i1 %179, i1* %pf
  store volatile i64 48502, i64* @assembly_address
  %180 = load i32* %14
  %181 = load i64** %12
  %182 = ptrtoint i64* %181 to i32
  %183 = icmp sle i32 %180, %182
  br i1 %183, label %block_bd52, label %block_bd78

block_bd78:                                       ; preds = %block_bd6f
  store volatile i64 48504, i64* @assembly_address
  br label %block_bef9

block_bd7d:                                       ; preds = %block_bcfa
  store volatile i64 48509, i64* @assembly_address
  %184 = sext i32 0 to i64
  %185 = trunc i64 %184 to i32
  store i32 %185, i32* %stack_var_-24
  store volatile i64 48516, i64* @assembly_address
  br label %block_beae

block_bd89:                                       ; preds = %block_beae
  store volatile i64 48521, i64* @assembly_address
  %186 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %187 = zext i16 %186 to i64
  store i64 %187, i64* %rax
  store volatile i64 48528, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i16
  %190 = load i1* %of
  %191 = lshr i16 %189, 8
  %192 = icmp eq i16 %191, 0
  store i1 %192, i1* %zf
  %193 = icmp slt i16 %191, 0
  store i1 %193, i1* %sf
  %194 = trunc i16 %191 to i8
  %195 = call i8 @llvm.ctpop.i8(i8 %194)
  %196 = and i8 %195, 1
  %197 = icmp eq i8 %196, 0
  store i1 %197, i1* %pf
  %198 = zext i16 %191 to i64
  %199 = load i64* %rax
  %200 = and i64 %199, -65536
  %201 = or i64 %200, %198
  store i64 %201, i64* %rax
  %202 = and i16 128, %189
  %203 = icmp ne i16 %202, 0
  store i1 %203, i1* %cf
  %204 = icmp slt i16 %189, 0
  %205 = select i1 false, i1 %204, i1 %190
  store i1 %205, i1* %of
  store volatile i64 48532, i64* @assembly_address
  %206 = load i64* %rax
  %207 = trunc i64 %206 to i16
  %208 = zext i16 %207 to i64
  store i64 %208, i64* %rax
  store volatile i64 48535, i64* @assembly_address
  %209 = load i64* %rax
  %210 = trunc i64 %209 to i32
  %211 = sext i32 %210 to i64
  store i64 %211, i64* %rax
  store volatile i64 48537, i64* @assembly_address
  %212 = load i64* %rax
  %213 = load i64* %rax
  %214 = mul i64 %213, 1
  %215 = add i64 %212, %214
  store i64 %215, i64* %rdx
  store volatile i64 48541, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f40 to i64), i64* %rax
  store volatile i64 48548, i64* @assembly_address
  %216 = load i64* %rdx
  %217 = load i64* %rax
  %218 = mul i64 %217, 1
  %219 = add i64 %216, %218
  %220 = inttoptr i64 %219 to i16*
  %221 = load i16* %220
  %222 = zext i16 %221 to i64
  store i64 %222, i64* %rax
  store volatile i64 48552, i64* @assembly_address
  %223 = load i64* %rax
  %224 = trunc i64 %223 to i16
  %225 = zext i16 %224 to i64
  store i64 %225, i64* %rax
  store volatile i64 48555, i64* @assembly_address
  %226 = load i64* %rax
  %227 = trunc i64 %226 to i32
  store i32 %227, i32* %stack_var_-20
  store volatile i64 48558, i64* @assembly_address
  %228 = load i32* %stack_var_-20
  store i32 %228, i32* %11
  store i32 18, i32* %10
  %229 = sub i32 %228, 18
  %230 = and i32 %228, 15
  %231 = sub i32 %230, 2
  %232 = icmp ugt i32 %231, 15
  %233 = icmp ult i32 %228, 18
  %234 = xor i32 %228, 18
  %235 = xor i32 %228, %229
  %236 = and i32 %234, %235
  %237 = icmp slt i32 %236, 0
  store i1 %232, i1* %az
  store i1 %233, i1* %cf
  store i1 %237, i1* %of
  %238 = icmp eq i32 %229, 0
  store i1 %238, i1* %zf
  %239 = icmp slt i32 %229, 0
  store i1 %239, i1* %sf
  %240 = trunc i32 %229 to i8
  %241 = call i8 @llvm.ctpop.i8(i8 %240)
  %242 = and i8 %241, 1
  %243 = icmp eq i8 %242, 0
  store i1 %243, i1* %pf
  store volatile i64 48562, i64* @assembly_address
  %244 = load i32* %11
  %245 = load i32* %10
  %246 = icmp sle i32 %244, %245
  br i1 %246, label %block_be13, label %block_bdb4

block_bdb4:                                       ; preds = %block_bd89
  store volatile i64 48564, i64* @assembly_address
  store i32 128, i32* %stack_var_-16
  br label %block_bdbb

block_bdbb:                                       ; preds = %block_be0a, %block_bdb4
  store volatile i64 48571, i64* @assembly_address
  %247 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %248 = zext i16 %247 to i64
  store i64 %248, i64* %rax
  store volatile i64 48578, i64* @assembly_address
  %249 = load i64* %rax
  %250 = trunc i64 %249 to i16
  %251 = zext i16 %250 to i64
  store i64 %251, i64* %rax
  store volatile i64 48581, i64* @assembly_address
  %252 = load i64* %rax
  %253 = trunc i64 %252 to i32
  %254 = load i32* %stack_var_-16
  %255 = and i32 %253, %254
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %256 = icmp eq i32 %255, 0
  store i1 %256, i1* %zf
  %257 = icmp slt i32 %255, 0
  store i1 %257, i1* %sf
  %258 = trunc i32 %255 to i8
  %259 = call i8 @llvm.ctpop.i8(i8 %258)
  %260 = and i8 %259, 1
  %261 = icmp eq i8 %260, 0
  store i1 %261, i1* %pf
  %262 = zext i32 %255 to i64
  store i64 %262, i64* %rax
  store volatile i64 48584, i64* @assembly_address
  %263 = load i64* %rax
  %264 = trunc i64 %263 to i32
  %265 = load i64* %rax
  %266 = trunc i64 %265 to i32
  %267 = and i32 %264, %266
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %268 = icmp eq i32 %267, 0
  store i1 %268, i1* %zf
  %269 = icmp slt i32 %267, 0
  store i1 %269, i1* %sf
  %270 = trunc i32 %267 to i8
  %271 = call i8 @llvm.ctpop.i8(i8 %270)
  %272 = and i8 %271, 1
  %273 = icmp eq i8 %272, 0
  store i1 %273, i1* %pf
  store volatile i64 48586, i64* @assembly_address
  %274 = load i1* %zf
  br i1 %274, label %block_bdf0, label %block_bdcc

block_bdcc:                                       ; preds = %block_bdbb
  store volatile i64 48588, i64* @assembly_address
  %275 = load i32* %stack_var_-20
  %276 = zext i32 %275 to i64
  store i64 %276, i64* %rax
  store volatile i64 48591, i64* @assembly_address
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i32
  %279 = sext i32 %278 to i64
  store i64 %279, i64* %rax
  store volatile i64 48593, i64* @assembly_address
  %280 = load i64* %rax
  %281 = add i64 %280, 32768
  %282 = and i64 %280, 15
  %283 = icmp ugt i64 %282, 15
  %284 = icmp ult i64 %281, %280
  %285 = xor i64 %280, %281
  %286 = xor i64 32768, %281
  %287 = and i64 %285, %286
  %288 = icmp slt i64 %287, 0
  store i1 %283, i1* %az
  store i1 %284, i1* %cf
  store i1 %288, i1* %of
  %289 = icmp eq i64 %281, 0
  store i1 %289, i1* %zf
  %290 = icmp slt i64 %281, 0
  store i1 %290, i1* %sf
  %291 = trunc i64 %281 to i8
  %292 = call i8 @llvm.ctpop.i8(i8 %291)
  %293 = and i8 %292, 1
  %294 = icmp eq i8 %293, 0
  store i1 %294, i1* %pf
  store i64 %281, i64* %rax
  store volatile i64 48599, i64* @assembly_address
  %295 = load i64* %rax
  %296 = load i64* %rax
  %297 = mul i64 %296, 1
  %298 = add i64 %295, %297
  store i64 %298, i64* %rdx
  store volatile i64 48603, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 48610, i64* @assembly_address
  %299 = load i64* %rax
  %300 = load i64* %rdx
  %301 = add i64 %299, %300
  %302 = and i64 %299, 15
  %303 = and i64 %300, 15
  %304 = add i64 %302, %303
  %305 = icmp ugt i64 %304, 15
  %306 = icmp ult i64 %301, %299
  %307 = xor i64 %299, %301
  %308 = xor i64 %300, %301
  %309 = and i64 %307, %308
  %310 = icmp slt i64 %309, 0
  store i1 %305, i1* %az
  store i1 %306, i1* %cf
  store i1 %310, i1* %of
  %311 = icmp eq i64 %301, 0
  store i1 %311, i1* %zf
  %312 = icmp slt i64 %301, 0
  store i1 %312, i1* %sf
  %313 = trunc i64 %301 to i8
  %314 = call i8 @llvm.ctpop.i8(i8 %313)
  %315 = and i8 %314, 1
  %316 = icmp eq i8 %315, 0
  store i1 %316, i1* %pf
  store i64 %301, i64* %rax
  store volatile i64 48613, i64* @assembly_address
  %317 = load i64* %rax
  %318 = inttoptr i64 %317 to i16*
  %319 = load i16* %318
  %320 = zext i16 %319 to i64
  store i64 %320, i64* %rax
  store volatile i64 48616, i64* @assembly_address
  %321 = load i64* %rax
  %322 = trunc i64 %321 to i16
  %323 = zext i16 %322 to i64
  store i64 %323, i64* %rax
  store volatile i64 48619, i64* @assembly_address
  %324 = load i64* %rax
  %325 = trunc i64 %324 to i32
  store i32 %325, i32* %stack_var_-20
  store volatile i64 48622, i64* @assembly_address
  br label %block_be0a

block_bdf0:                                       ; preds = %block_bdbb
  store volatile i64 48624, i64* @assembly_address
  %326 = load i32* %stack_var_-20
  %327 = zext i32 %326 to i64
  store i64 %327, i64* %rax
  store volatile i64 48627, i64* @assembly_address
  %328 = load i64* %rax
  %329 = trunc i64 %328 to i32
  %330 = sext i32 %329 to i64
  store i64 %330, i64* %rax
  store volatile i64 48629, i64* @assembly_address
  %331 = load i64* %rax
  %332 = load i64* %rax
  %333 = mul i64 %332, 1
  %334 = add i64 %331, %333
  store i64 %334, i64* %rdx
  store volatile i64 48633, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 48640, i64* @assembly_address
  %335 = load i64* %rdx
  %336 = load i64* %rax
  %337 = mul i64 %336, 1
  %338 = add i64 %335, %337
  %339 = inttoptr i64 %338 to i16*
  %340 = load i16* %339
  %341 = zext i16 %340 to i64
  store i64 %341, i64* %rax
  store volatile i64 48644, i64* @assembly_address
  %342 = load i64* %rax
  %343 = trunc i64 %342 to i16
  %344 = zext i16 %343 to i64
  store i64 %344, i64* %rax
  store volatile i64 48647, i64* @assembly_address
  %345 = load i64* %rax
  %346 = trunc i64 %345 to i32
  store i32 %346, i32* %stack_var_-20
  br label %block_be0a

block_be0a:                                       ; preds = %block_bdf0, %block_bdcc
  store volatile i64 48650, i64* @assembly_address
  %347 = load i32* %stack_var_-16
  %348 = load i1* %of
  %349 = lshr i32 %347, 1
  %350 = icmp eq i32 %349, 0
  store i1 %350, i1* %zf
  %351 = icmp slt i32 %349, 0
  store i1 %351, i1* %sf
  %352 = trunc i32 %349 to i8
  %353 = call i8 @llvm.ctpop.i8(i8 %352)
  %354 = and i8 %353, 1
  %355 = icmp eq i8 %354, 0
  store i1 %355, i1* %pf
  store i32 %349, i32* %stack_var_-16
  %356 = and i32 1, %347
  %357 = icmp ne i32 %356, 0
  store i1 %357, i1* %cf
  %358 = icmp slt i32 %347, 0
  %359 = select i1 true, i1 %358, i1 %348
  store i1 %359, i1* %of
  store volatile i64 48653, i64* @assembly_address
  %360 = load i32* %stack_var_-20
  store i32 %360, i32* %9
  store i32 18, i32* %8
  %361 = sub i32 %360, 18
  %362 = and i32 %360, 15
  %363 = sub i32 %362, 2
  %364 = icmp ugt i32 %363, 15
  %365 = icmp ult i32 %360, 18
  %366 = xor i32 %360, 18
  %367 = xor i32 %360, %361
  %368 = and i32 %366, %367
  %369 = icmp slt i32 %368, 0
  store i1 %364, i1* %az
  store i1 %365, i1* %cf
  store i1 %369, i1* %of
  %370 = icmp eq i32 %361, 0
  store i1 %370, i1* %zf
  %371 = icmp slt i32 %361, 0
  store i1 %371, i1* %sf
  %372 = trunc i32 %361 to i8
  %373 = call i8 @llvm.ctpop.i8(i8 %372)
  %374 = and i8 %373, 1
  %375 = icmp eq i8 %374, 0
  store i1 %375, i1* %pf
  store volatile i64 48657, i64* @assembly_address
  %376 = load i32* %9
  %377 = load i32* %8
  %378 = icmp sgt i32 %376, %377
  br i1 %378, label %block_bdbb, label %block_be13

block_be13:                                       ; preds = %block_be0a, %block_bd89
  store volatile i64 48659, i64* @assembly_address
  %379 = load i32* %stack_var_-20
  %380 = zext i32 %379 to i64
  store i64 %380, i64* %rax
  store volatile i64 48662, i64* @assembly_address
  %381 = load i64* %rax
  %382 = trunc i64 %381 to i32
  %383 = sext i32 %382 to i64
  store i64 %383, i64* %rdx
  store volatile i64 48665, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f00 to i64), i64* %rax
  store volatile i64 48672, i64* @assembly_address
  %384 = load i64* %rdx
  %385 = load i64* %rax
  %386 = mul i64 %385, 1
  %387 = add i64 %384, %386
  %388 = inttoptr i64 %387 to i8*
  %389 = load i8* %388
  %390 = zext i8 %389 to i64
  store i64 %390, i64* %rax
  store volatile i64 48676, i64* @assembly_address
  %391 = load i64* %rax
  %392 = trunc i64 %391 to i8
  %393 = zext i8 %392 to i64
  store i64 %393, i64* %rax
  store volatile i64 48679, i64* @assembly_address
  %394 = load i64* %rax
  %395 = trunc i64 %394 to i32
  %396 = zext i32 %395 to i64
  store i64 %396, i64* %rdi
  store volatile i64 48681, i64* @assembly_address
  %397 = load i64* %rdi
  %398 = trunc i64 %397 to i32
  %399 = call i64 @fillbuf(i32 %398)
  store i64 %399, i64* %rax
  store i64 %399, i64* %rax
  store volatile i64 48686, i64* @assembly_address
  %400 = load i32* %stack_var_-20
  store i32 %400, i32* %7
  store i32 2, i32* %6
  %401 = sub i32 %400, 2
  %402 = and i32 %400, 15
  %403 = sub i32 %402, 2
  %404 = icmp ugt i32 %403, 15
  %405 = icmp ult i32 %400, 2
  %406 = xor i32 %400, 2
  %407 = xor i32 %400, %401
  %408 = and i32 %406, %407
  %409 = icmp slt i32 %408, 0
  store i1 %404, i1* %az
  store i1 %405, i1* %cf
  store i1 %409, i1* %of
  %410 = icmp eq i32 %401, 0
  store i1 %410, i1* %zf
  %411 = icmp slt i32 %401, 0
  store i1 %411, i1* %sf
  %412 = trunc i32 %401 to i8
  %413 = call i8 @llvm.ctpop.i8(i8 %412)
  %414 = and i8 %413, 1
  %415 = icmp eq i8 %414, 0
  store i1 %415, i1* %pf
  store volatile i64 48690, i64* @assembly_address
  %416 = load i32* %7
  %417 = load i32* %6
  %418 = icmp sgt i32 %416, %417
  br i1 %418, label %block_be90, label %block_be34

block_be34:                                       ; preds = %block_be13
  store volatile i64 48692, i64* @assembly_address
  %419 = load i32* %stack_var_-20
  %420 = and i32 %419, 15
  %421 = icmp ugt i32 %420, 15
  %422 = icmp ult i32 %419, 0
  %423 = xor i32 %419, 0
  %424 = and i32 %423, 0
  %425 = icmp slt i32 %424, 0
  store i1 %421, i1* %az
  store i1 %422, i1* %cf
  store i1 %425, i1* %of
  %426 = icmp eq i32 %419, 0
  store i1 %426, i1* %zf
  %427 = icmp slt i32 %419, 0
  store i1 %427, i1* %sf
  %428 = trunc i32 %419 to i8
  %429 = call i8 @llvm.ctpop.i8(i8 %428)
  %430 = and i8 %429, 1
  %431 = icmp eq i8 %430, 0
  store i1 %431, i1* %pf
  store volatile i64 48696, i64* @assembly_address
  %432 = load i1* %zf
  %433 = icmp eq i1 %432, false
  br i1 %433, label %block_be43, label %block_be3a

block_be3a:                                       ; preds = %block_be34
  store volatile i64 48698, i64* @assembly_address
  store i32 1, i32* %stack_var_-20
  store volatile i64 48705, i64* @assembly_address
  br label %block_be84

block_be43:                                       ; preds = %block_be34
  store volatile i64 48707, i64* @assembly_address
  %434 = load i32* %stack_var_-20
  %435 = sub i32 %434, 1
  %436 = and i32 %434, 15
  %437 = sub i32 %436, 1
  %438 = icmp ugt i32 %437, 15
  %439 = icmp ult i32 %434, 1
  %440 = xor i32 %434, 1
  %441 = xor i32 %434, %435
  %442 = and i32 %440, %441
  %443 = icmp slt i32 %442, 0
  store i1 %438, i1* %az
  store i1 %439, i1* %cf
  store i1 %443, i1* %of
  %444 = icmp eq i32 %435, 0
  store i1 %444, i1* %zf
  %445 = icmp slt i32 %435, 0
  store i1 %445, i1* %sf
  %446 = trunc i32 %435 to i8
  %447 = call i8 @llvm.ctpop.i8(i8 %446)
  %448 = and i8 %447, 1
  %449 = icmp eq i8 %448, 0
  store i1 %449, i1* %pf
  store volatile i64 48711, i64* @assembly_address
  %450 = load i1* %zf
  %451 = icmp eq i1 %450, false
  br i1 %451, label %block_be5b, label %block_be49

block_be49:                                       ; preds = %block_be43
  store volatile i64 48713, i64* @assembly_address
  store i64 4, i64* %rdi
  store volatile i64 48718, i64* @assembly_address
  %452 = load i64* %rdi
  %453 = trunc i64 %452 to i32
  %454 = call i64 @getbits(i32 %453)
  store i64 %454, i64* %rax
  store i64 %454, i64* %rax
  store volatile i64 48723, i64* @assembly_address
  %455 = load i64* %rax
  %456 = trunc i64 %455 to i32
  %457 = add i32 %456, 3
  %458 = and i32 %456, 15
  %459 = add i32 %458, 3
  %460 = icmp ugt i32 %459, 15
  %461 = icmp ult i32 %457, %456
  %462 = xor i32 %456, %457
  %463 = xor i32 3, %457
  %464 = and i32 %462, %463
  %465 = icmp slt i32 %464, 0
  store i1 %460, i1* %az
  store i1 %461, i1* %cf
  store i1 %465, i1* %of
  %466 = icmp eq i32 %457, 0
  store i1 %466, i1* %zf
  %467 = icmp slt i32 %457, 0
  store i1 %467, i1* %sf
  %468 = trunc i32 %457 to i8
  %469 = call i8 @llvm.ctpop.i8(i8 %468)
  %470 = and i8 %469, 1
  %471 = icmp eq i8 %470, 0
  store i1 %471, i1* %pf
  %472 = zext i32 %457 to i64
  store i64 %472, i64* %rax
  store volatile i64 48726, i64* @assembly_address
  %473 = load i64* %rax
  %474 = trunc i64 %473 to i32
  store i32 %474, i32* %stack_var_-20
  store volatile i64 48729, i64* @assembly_address
  br label %block_be84

block_be5b:                                       ; preds = %block_be43
  store volatile i64 48731, i64* @assembly_address
  store i64 9, i64* %rdi
  store volatile i64 48736, i64* @assembly_address
  %475 = load i64* %rdi
  %476 = trunc i64 %475 to i32
  %477 = call i64 @getbits(i32 %476)
  store i64 %477, i64* %rax
  store i64 %477, i64* %rax
  store volatile i64 48741, i64* @assembly_address
  %478 = load i64* %rax
  %479 = trunc i64 %478 to i32
  %480 = add i32 %479, 20
  %481 = and i32 %479, 15
  %482 = add i32 %481, 4
  %483 = icmp ugt i32 %482, 15
  %484 = icmp ult i32 %480, %479
  %485 = xor i32 %479, %480
  %486 = xor i32 20, %480
  %487 = and i32 %485, %486
  %488 = icmp slt i32 %487, 0
  store i1 %483, i1* %az
  store i1 %484, i1* %cf
  store i1 %488, i1* %of
  %489 = icmp eq i32 %480, 0
  store i1 %489, i1* %zf
  %490 = icmp slt i32 %480, 0
  store i1 %490, i1* %sf
  %491 = trunc i32 %480 to i8
  %492 = call i8 @llvm.ctpop.i8(i8 %491)
  %493 = and i8 %492, 1
  %494 = icmp eq i8 %493, 0
  store i1 %494, i1* %pf
  %495 = zext i32 %480 to i64
  store i64 %495, i64* %rax
  store volatile i64 48744, i64* @assembly_address
  %496 = load i64* %rax
  %497 = trunc i64 %496 to i32
  store i32 %497, i32* %stack_var_-20
  store volatile i64 48747, i64* @assembly_address
  br label %block_be84

block_be6d:                                       ; preds = %block_be84
  store volatile i64 48749, i64* @assembly_address
  %498 = load i32* %stack_var_-24
  %499 = sext i32 %498 to i64
  %500 = trunc i64 %499 to i32
  %501 = zext i32 %500 to i64
  store i64 %501, i64* %rax
  store volatile i64 48752, i64* @assembly_address
  %502 = load i64* %rax
  %503 = add i64 %502, 1
  %504 = trunc i64 %503 to i32
  %505 = zext i32 %504 to i64
  store i64 %505, i64* %rdx
  store volatile i64 48755, i64* @assembly_address
  %506 = load i64* %rdx
  %507 = trunc i64 %506 to i32
  %508 = sext i32 %507 to i64
  %509 = trunc i64 %508 to i32
  store i32 %509, i32* %stack_var_-24
  store volatile i64 48758, i64* @assembly_address
  %510 = load i64* %rax
  %511 = trunc i64 %510 to i32
  %512 = sext i32 %511 to i64
  store i64 %512, i64* %rdx
  store volatile i64 48761, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 48768, i64* @assembly_address
  %513 = load i64* %rdx
  %514 = load i64* %rax
  %515 = mul i64 %514, 1
  %516 = add i64 %513, %515
  %517 = inttoptr i64 %516 to i8*
  store i8 0, i8* %517
  br label %block_be84

block_be84:                                       ; preds = %block_be6d, %block_be5b, %block_be49, %block_be3a
  store volatile i64 48772, i64* @assembly_address
  %518 = load i32* %stack_var_-20
  %519 = sub i32 %518, 1
  %520 = and i32 %518, 15
  %521 = sub i32 %520, 1
  %522 = icmp ugt i32 %521, 15
  %523 = icmp ult i32 %518, 1
  %524 = xor i32 %518, 1
  %525 = xor i32 %518, %519
  %526 = and i32 %524, %525
  %527 = icmp slt i32 %526, 0
  store i1 %522, i1* %az
  store i1 %523, i1* %cf
  store i1 %527, i1* %of
  %528 = icmp eq i32 %519, 0
  store i1 %528, i1* %zf
  %529 = icmp slt i32 %519, 0
  store i1 %529, i1* %sf
  %530 = trunc i32 %519 to i8
  %531 = call i8 @llvm.ctpop.i8(i8 %530)
  %532 = and i8 %531, 1
  %533 = icmp eq i8 %532, 0
  store i1 %533, i1* %pf
  store i32 %519, i32* %stack_var_-20
  store volatile i64 48776, i64* @assembly_address
  %534 = load i32* %stack_var_-20
  %535 = and i32 %534, 15
  %536 = icmp ugt i32 %535, 15
  %537 = icmp ult i32 %534, 0
  %538 = xor i32 %534, 0
  %539 = and i32 %538, 0
  %540 = icmp slt i32 %539, 0
  store i1 %536, i1* %az
  store i1 %537, i1* %cf
  store i1 %540, i1* %of
  %541 = icmp eq i32 %534, 0
  store i1 %541, i1* %zf
  %542 = icmp slt i32 %534, 0
  store i1 %542, i1* %sf
  %543 = trunc i32 %534 to i8
  %544 = call i8 @llvm.ctpop.i8(i8 %543)
  %545 = and i8 %544, 1
  %546 = icmp eq i8 %545, 0
  store i1 %546, i1* %pf
  store volatile i64 48780, i64* @assembly_address
  %547 = load i1* %sf
  %548 = icmp eq i1 %547, false
  br i1 %548, label %block_be6d, label %block_be8e

block_be8e:                                       ; preds = %block_be84
  store volatile i64 48782, i64* @assembly_address
  br label %block_beae

block_be90:                                       ; preds = %block_be13
  store volatile i64 48784, i64* @assembly_address
  %549 = load i32* %stack_var_-20
  %550 = zext i32 %549 to i64
  store i64 %550, i64* %rax
  store volatile i64 48787, i64* @assembly_address
  %551 = load i64* %rax
  %552 = trunc i64 %551 to i32
  %553 = zext i32 %552 to i64
  store i64 %553, i64* %rcx
  store volatile i64 48789, i64* @assembly_address
  %554 = load i32* %stack_var_-24
  %555 = sext i32 %554 to i64
  %556 = trunc i64 %555 to i32
  %557 = zext i32 %556 to i64
  store i64 %557, i64* %rax
  store volatile i64 48792, i64* @assembly_address
  %558 = load i64* %rax
  %559 = add i64 %558, 1
  %560 = trunc i64 %559 to i32
  %561 = zext i32 %560 to i64
  store i64 %561, i64* %rdx
  store volatile i64 48795, i64* @assembly_address
  %562 = load i64* %rdx
  %563 = trunc i64 %562 to i32
  %564 = sext i32 %563 to i64
  %565 = trunc i64 %564 to i32
  store i32 %565, i32* %stack_var_-24
  store volatile i64 48798, i64* @assembly_address
  %566 = load i64* %rcx
  %567 = trunc i64 %566 to i32
  %568 = sub i32 %567, 2
  %569 = and i32 %567, 15
  %570 = sub i32 %569, 2
  %571 = icmp ugt i32 %570, 15
  %572 = icmp ult i32 %567, 2
  %573 = xor i32 %567, 2
  %574 = xor i32 %567, %568
  %575 = and i32 %573, %574
  %576 = icmp slt i32 %575, 0
  store i1 %571, i1* %az
  store i1 %572, i1* %cf
  store i1 %576, i1* %of
  %577 = icmp eq i32 %568, 0
  store i1 %577, i1* %zf
  %578 = icmp slt i32 %568, 0
  store i1 %578, i1* %sf
  %579 = trunc i32 %568 to i8
  %580 = call i8 @llvm.ctpop.i8(i8 %579)
  %581 = and i8 %580, 1
  %582 = icmp eq i8 %581, 0
  store i1 %582, i1* %pf
  %583 = zext i32 %568 to i64
  store i64 %583, i64* %rcx
  store volatile i64 48801, i64* @assembly_address
  %584 = load i64* %rax
  %585 = trunc i64 %584 to i32
  %586 = sext i32 %585 to i64
  store i64 %586, i64* %rdx
  store volatile i64 48804, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 48811, i64* @assembly_address
  %587 = load i64* %rcx
  %588 = trunc i64 %587 to i8
  %589 = load i64* %rdx
  %590 = load i64* %rax
  %591 = mul i64 %590, 1
  %592 = add i64 %589, %591
  %593 = inttoptr i64 %592 to i8*
  store i8 %588, i8* %593
  br label %block_beae

block_beae:                                       ; preds = %block_be90, %block_be8e, %block_bd7d
  store volatile i64 48814, i64* @assembly_address
  %594 = load i32* %stack_var_-24
  %595 = sext i32 %594 to i64
  %596 = trunc i64 %595 to i32
  %597 = zext i32 %596 to i64
  store i64 %597, i64* %rax
  store volatile i64 48817, i64* @assembly_address
  %598 = load i64* %rax
  %599 = trunc i64 %598 to i32
  %600 = load i32* %stack_var_-12
  %601 = trunc i64 %598 to i32
  store i32 %601, i32* %4
  store i32 %600, i32* %3
  %602 = sub i32 %599, %600
  %603 = and i32 %599, 15
  %604 = and i32 %600, 15
  %605 = sub i32 %603, %604
  %606 = icmp ugt i32 %605, 15
  %607 = icmp ult i32 %599, %600
  %608 = xor i32 %599, %600
  %609 = xor i32 %599, %602
  %610 = and i32 %608, %609
  %611 = icmp slt i32 %610, 0
  store i1 %606, i1* %az
  store i1 %607, i1* %cf
  store i1 %611, i1* %of
  %612 = icmp eq i32 %602, 0
  store i1 %612, i1* %zf
  %613 = icmp slt i32 %602, 0
  store i1 %613, i1* %sf
  %614 = trunc i32 %602 to i8
  %615 = call i8 @llvm.ctpop.i8(i8 %614)
  %616 = and i8 %615, 1
  %617 = icmp eq i8 %616, 0
  store i1 %617, i1* %pf
  store volatile i64 48820, i64* @assembly_address
  %618 = load i32* %4
  %619 = sext i32 %618 to i64
  %620 = load i32* %3
  %621 = trunc i64 %619 to i32
  %622 = icmp slt i32 %621, %620
  br i1 %622, label %block_bd89, label %block_beba

block_beba:                                       ; preds = %block_beae
  store volatile i64 48826, i64* @assembly_address
  br label %block_bed3

block_bebc:                                       ; preds = %block_bed3
  store volatile i64 48828, i64* @assembly_address
  %623 = load i32* %stack_var_-24
  %624 = sext i32 %623 to i64
  %625 = trunc i64 %624 to i32
  %626 = zext i32 %625 to i64
  store i64 %626, i64* %rax
  store volatile i64 48831, i64* @assembly_address
  %627 = load i64* %rax
  %628 = add i64 %627, 1
  %629 = trunc i64 %628 to i32
  %630 = zext i32 %629 to i64
  store i64 %630, i64* %rdx
  store volatile i64 48834, i64* @assembly_address
  %631 = load i64* %rdx
  %632 = trunc i64 %631 to i32
  %633 = sext i32 %632 to i64
  %634 = trunc i64 %633 to i32
  store i32 %634, i32* %stack_var_-24
  store volatile i64 48837, i64* @assembly_address
  %635 = load i64* %rax
  %636 = trunc i64 %635 to i32
  %637 = sext i32 %636 to i64
  store i64 %637, i64* %rdx
  store volatile i64 48840, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 48847, i64* @assembly_address
  %638 = load i64* %rdx
  %639 = load i64* %rax
  %640 = mul i64 %639, 1
  %641 = add i64 %638, %640
  %642 = inttoptr i64 %641 to i8*
  store i8 0, i8* %642
  br label %block_bed3

block_bed3:                                       ; preds = %block_bebc, %block_beba
  store volatile i64 48851, i64* @assembly_address
  %643 = load i32* %stack_var_-24
  %644 = sext i32 %643 to i64
  %645 = trunc i64 %644 to i32
  store i32 %645, i32* %2
  store i32 509, i32* %1
  %646 = sub i32 %645, 509
  %647 = and i32 %645, 15
  %648 = sub i32 %647, 13
  %649 = icmp ugt i32 %648, 15
  %650 = icmp ult i32 %645, 509
  %651 = xor i32 %645, 509
  %652 = xor i32 %645, %646
  %653 = and i32 %651, %652
  %654 = icmp slt i32 %653, 0
  store i1 %649, i1* %az
  store i1 %650, i1* %cf
  store i1 %654, i1* %of
  %655 = icmp eq i32 %646, 0
  store i1 %655, i1* %zf
  %656 = icmp slt i32 %646, 0
  store i1 %656, i1* %sf
  %657 = trunc i32 %646 to i8
  %658 = call i8 @llvm.ctpop.i8(i8 %657)
  %659 = and i8 %658, 1
  %660 = icmp eq i8 %659, 0
  store i1 %660, i1* %pf
  store volatile i64 48858, i64* @assembly_address
  %661 = load i32* %2
  %662 = load i32* %1
  %663 = icmp sle i32 %661, %662
  br i1 %663, label %block_bebc, label %block_bedc

block_bedc:                                       ; preds = %block_bed3
  store volatile i64 48860, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_23a880 to i64), i64* %rcx
  store volatile i64 48867, i64* @assembly_address
  store i64 12, i64* %rdx
  store volatile i64 48872, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rsi
  store volatile i64 48879, i64* @assembly_address
  store i64 510, i64* %rdi
  store volatile i64 48884, i64* @assembly_address
  %664 = load i64* %rdi
  %665 = load i64* %rsi
  %666 = inttoptr i64 %665 to i64*
  %667 = load i64* %rdx
  %668 = load i64* %rcx
  %669 = inttoptr i64 %668 to i64*
  %670 = trunc i64 %664 to i32
  %671 = call i64 @make_table(i32 %670, i64* %666, i64 %667, i64* %669)
  store i64 %671, i64* %rax
  store i64 %671, i64* %rax
  br label %block_bef9

block_bef9:                                       ; preds = %block_bedc, %block_bd78
  store volatile i64 48889, i64* @assembly_address
  store volatile i64 48890, i64* @assembly_address
  %672 = load i64* %stack_var_-8
  store i64 %672, i64* %rbp
  %673 = ptrtoint i64* %stack_var_0 to i64
  store i64 %673, i64* %rsp
  store volatile i64 48891, i64* @assembly_address
  %674 = load i64* %rax
  %675 = load i64* %rax
  ret i64 %675
}

define i64 @decode_c() {
block_befc:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 48892, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 48893, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 48896, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 16
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 16
  %8 = xor i64 %3, 16
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %18, i64* %rsp
  store volatile i64 48900, i64* @assembly_address
  %19 = load i32* bitcast (i64* @global_var_219f20 to i32*)
  %20 = zext i32 %19 to i64
  store i64 %20, i64* %rax
  store volatile i64 48906, i64* @assembly_address
  %21 = load i64* %rax
  %22 = trunc i64 %21 to i32
  %23 = load i64* %rax
  %24 = trunc i64 %23 to i32
  %25 = and i32 %22, %24
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %26 = icmp eq i32 %25, 0
  store i1 %26, i1* %zf
  %27 = icmp slt i32 %25, 0
  store i1 %27, i1* %sf
  %28 = trunc i32 %25 to i8
  %29 = call i8 @llvm.ctpop.i8(i8 %28)
  %30 = and i8 %29, 1
  %31 = icmp eq i8 %30, 0
  store i1 %31, i1* %pf
  store volatile i64 48908, i64* @assembly_address
  %32 = load i1* %zf
  %33 = icmp eq i1 %32, false
  br i1 %33, label %block_bf5f, label %block_bf0e

block_bf0e:                                       ; preds = %block_befc
  store volatile i64 48910, i64* @assembly_address
  store i64 16, i64* %rdi
  store volatile i64 48915, i64* @assembly_address
  %34 = load i64* %rdi
  %35 = trunc i64 %34 to i32
  %36 = call i64 @getbits(i32 %35)
  store i64 %36, i64* %rax
  store i64 %36, i64* %rax
  store volatile i64 48920, i64* @assembly_address
  %37 = load i64* %rax
  %38 = trunc i64 %37 to i32
  store i32 %38, i32* bitcast (i64* @global_var_219f20 to i32*)
  store volatile i64 48926, i64* @assembly_address
  %39 = load i32* bitcast (i64* @global_var_219f20 to i32*)
  %40 = zext i32 %39 to i64
  store i64 %40, i64* %rax
  store volatile i64 48932, i64* @assembly_address
  %41 = load i64* %rax
  %42 = trunc i64 %41 to i32
  %43 = load i64* %rax
  %44 = trunc i64 %43 to i32
  %45 = and i32 %42, %44
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %46 = icmp eq i32 %45, 0
  store i1 %46, i1* %zf
  %47 = icmp slt i32 %45, 0
  store i1 %47, i1* %sf
  %48 = trunc i32 %45 to i8
  %49 = call i8 @llvm.ctpop.i8(i8 %48)
  %50 = and i8 %49, 1
  %51 = icmp eq i8 %50, 0
  store i1 %51, i1* %pf
  store volatile i64 48934, i64* @assembly_address
  %52 = load i1* %zf
  %53 = icmp eq i1 %52, false
  br i1 %53, label %block_bf32, label %block_bf28

block_bf28:                                       ; preds = %block_bf0e
  store volatile i64 48936, i64* @assembly_address
  store i64 510, i64* %rax
  store volatile i64 48941, i64* @assembly_address
  br label %block_c015

block_bf32:                                       ; preds = %block_bf0e
  store volatile i64 48946, i64* @assembly_address
  store i64 3, i64* %rdx
  store volatile i64 48951, i64* @assembly_address
  store i64 5, i64* %rsi
  store volatile i64 48956, i64* @assembly_address
  store i64 19, i64* %rdi
  store volatile i64 48961, i64* @assembly_address
  %54 = load i64* %rdi
  %55 = load i64* %rsi
  %56 = load i64* %rdx
  %57 = trunc i64 %54 to i32
  %58 = call i64 @read_pt_len(i32 %57, i64 %55, i64 %56)
  store i64 %58, i64* %rax
  store i64 %58, i64* %rax
  store volatile i64 48966, i64* @assembly_address
  %59 = call i64 @read_c_len()
  store i64 %59, i64* %rax
  store i64 %59, i64* %rax
  store i64 %59, i64* %rax
  store volatile i64 48971, i64* @assembly_address
  store i64 4294967295, i64* %rdx
  store volatile i64 48976, i64* @assembly_address
  store i64 4, i64* %rsi
  store volatile i64 48981, i64* @assembly_address
  store i64 14, i64* %rdi
  store volatile i64 48986, i64* @assembly_address
  %60 = load i64* %rdi
  %61 = load i64* %rsi
  %62 = load i64* %rdx
  %63 = trunc i64 %60 to i32
  %64 = call i64 @read_pt_len(i32 %63, i64 %61, i64 %62)
  store i64 %64, i64* %rax
  store i64 %64, i64* %rax
  br label %block_bf5f

block_bf5f:                                       ; preds = %block_bf32, %block_befc
  store volatile i64 48991, i64* @assembly_address
  %65 = load i32* bitcast (i64* @global_var_219f20 to i32*)
  %66 = zext i32 %65 to i64
  store i64 %66, i64* %rax
  store volatile i64 48997, i64* @assembly_address
  %67 = load i64* %rax
  %68 = trunc i64 %67 to i32
  %69 = sub i32 %68, 1
  %70 = and i32 %68, 15
  %71 = sub i32 %70, 1
  %72 = icmp ugt i32 %71, 15
  %73 = icmp ult i32 %68, 1
  %74 = xor i32 %68, 1
  %75 = xor i32 %68, %69
  %76 = and i32 %74, %75
  %77 = icmp slt i32 %76, 0
  store i1 %72, i1* %az
  store i1 %73, i1* %cf
  store i1 %77, i1* %of
  %78 = icmp eq i32 %69, 0
  store i1 %78, i1* %zf
  %79 = icmp slt i32 %69, 0
  store i1 %79, i1* %sf
  %80 = trunc i32 %69 to i8
  %81 = call i8 @llvm.ctpop.i8(i8 %80)
  %82 = and i8 %81, 1
  %83 = icmp eq i8 %82, 0
  store i1 %83, i1* %pf
  %84 = zext i32 %69 to i64
  store i64 %84, i64* %rax
  store volatile i64 49000, i64* @assembly_address
  %85 = load i64* %rax
  %86 = trunc i64 %85 to i32
  store i32 %86, i32* bitcast (i64* @global_var_219f20 to i32*)
  store volatile i64 49006, i64* @assembly_address
  %87 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %88 = zext i16 %87 to i64
  store i64 %88, i64* %rax
  store volatile i64 49013, i64* @assembly_address
  %89 = load i64* %rax
  %90 = trunc i64 %89 to i16
  %91 = load i1* %of
  %92 = lshr i16 %90, 4
  %93 = icmp eq i16 %92, 0
  store i1 %93, i1* %zf
  %94 = icmp slt i16 %92, 0
  store i1 %94, i1* %sf
  %95 = trunc i16 %92 to i8
  %96 = call i8 @llvm.ctpop.i8(i8 %95)
  %97 = and i8 %96, 1
  %98 = icmp eq i8 %97, 0
  store i1 %98, i1* %pf
  %99 = zext i16 %92 to i64
  %100 = load i64* %rax
  %101 = and i64 %100, -65536
  %102 = or i64 %101, %99
  store i64 %102, i64* %rax
  %103 = and i16 8, %90
  %104 = icmp ne i16 %103, 0
  store i1 %104, i1* %cf
  %105 = icmp slt i16 %90, 0
  %106 = select i1 false, i1 %105, i1 %91
  store i1 %106, i1* %of
  store volatile i64 49017, i64* @assembly_address
  %107 = load i64* %rax
  %108 = trunc i64 %107 to i16
  %109 = zext i16 %108 to i64
  store i64 %109, i64* %rax
  store volatile i64 49020, i64* @assembly_address
  %110 = load i64* %rax
  %111 = trunc i64 %110 to i32
  %112 = sext i32 %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 49022, i64* @assembly_address
  %113 = load i64* %rax
  %114 = load i64* %rax
  %115 = mul i64 %114, 1
  %116 = add i64 %113, %115
  store i64 %116, i64* %rdx
  store volatile i64 49026, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_23a880 to i64), i64* %rax
  store volatile i64 49033, i64* @assembly_address
  %117 = load i64* %rdx
  %118 = load i64* %rax
  %119 = mul i64 %118, 1
  %120 = add i64 %117, %119
  %121 = inttoptr i64 %120 to i16*
  %122 = load i16* %121
  %123 = zext i16 %122 to i64
  store i64 %123, i64* %rax
  store volatile i64 49037, i64* @assembly_address
  %124 = load i64* %rax
  %125 = trunc i64 %124 to i16
  %126 = zext i16 %125 to i64
  store i64 %126, i64* %rax
  store volatile i64 49040, i64* @assembly_address
  %127 = load i64* %rax
  %128 = trunc i64 %127 to i32
  store i32 %128, i32* %stack_var_-16
  store volatile i64 49043, i64* @assembly_address
  %129 = load i32* %stack_var_-16
  %130 = sub i32 %129, 509
  %131 = and i32 %129, 15
  %132 = sub i32 %131, 13
  %133 = icmp ugt i32 %132, 15
  %134 = icmp ult i32 %129, 509
  %135 = xor i32 %129, 509
  %136 = xor i32 %129, %130
  %137 = and i32 %135, %136
  %138 = icmp slt i32 %137, 0
  store i1 %133, i1* %az
  store i1 %134, i1* %cf
  store i1 %138, i1* %of
  %139 = icmp eq i32 %130, 0
  store i1 %139, i1* %zf
  %140 = icmp slt i32 %130, 0
  store i1 %140, i1* %sf
  %141 = trunc i32 %130 to i8
  %142 = call i8 @llvm.ctpop.i8(i8 %141)
  %143 = and i8 %142, 1
  %144 = icmp eq i8 %143, 0
  store i1 %144, i1* %pf
  store volatile i64 49050, i64* @assembly_address
  %145 = load i1* %cf
  %146 = load i1* %zf
  %147 = or i1 %145, %146
  br i1 %147, label %block_bffa, label %block_bf9c

block_bf9c:                                       ; preds = %block_bf5f
  store volatile i64 49052, i64* @assembly_address
  store i32 8, i32* %stack_var_-12
  br label %block_bfa3

block_bfa3:                                       ; preds = %block_bfee, %block_bf9c
  store volatile i64 49059, i64* @assembly_address
  %148 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %149 = zext i16 %148 to i64
  store i64 %149, i64* %rax
  store volatile i64 49066, i64* @assembly_address
  %150 = load i64* %rax
  %151 = trunc i64 %150 to i16
  %152 = zext i16 %151 to i64
  store i64 %152, i64* %rax
  store volatile i64 49069, i64* @assembly_address
  %153 = load i64* %rax
  %154 = trunc i64 %153 to i32
  %155 = load i32* %stack_var_-12
  %156 = and i32 %154, %155
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %157 = icmp eq i32 %156, 0
  store i1 %157, i1* %zf
  %158 = icmp slt i32 %156, 0
  store i1 %158, i1* %sf
  %159 = trunc i32 %156 to i8
  %160 = call i8 @llvm.ctpop.i8(i8 %159)
  %161 = and i8 %160, 1
  %162 = icmp eq i8 %161, 0
  store i1 %162, i1* %pf
  %163 = zext i32 %156 to i64
  store i64 %163, i64* %rax
  store volatile i64 49072, i64* @assembly_address
  %164 = load i64* %rax
  %165 = trunc i64 %164 to i32
  %166 = load i64* %rax
  %167 = trunc i64 %166 to i32
  %168 = and i32 %165, %167
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %169 = icmp eq i32 %168, 0
  store i1 %169, i1* %zf
  %170 = icmp slt i32 %168, 0
  store i1 %170, i1* %sf
  %171 = trunc i32 %168 to i8
  %172 = call i8 @llvm.ctpop.i8(i8 %171)
  %173 = and i8 %172, 1
  %174 = icmp eq i8 %173, 0
  store i1 %174, i1* %pf
  store volatile i64 49074, i64* @assembly_address
  %175 = load i1* %zf
  br i1 %175, label %block_bfd6, label %block_bfb4

block_bfb4:                                       ; preds = %block_bfa3
  store volatile i64 49076, i64* @assembly_address
  %176 = load i32* %stack_var_-16
  %177 = zext i32 %176 to i64
  store i64 %177, i64* %rax
  store volatile i64 49079, i64* @assembly_address
  %178 = load i64* %rax
  %179 = add i64 %178, 32768
  %180 = and i64 %178, 15
  %181 = icmp ugt i64 %180, 15
  %182 = icmp ult i64 %179, %178
  %183 = xor i64 %178, %179
  %184 = xor i64 32768, %179
  %185 = and i64 %183, %184
  %186 = icmp slt i64 %185, 0
  store i1 %181, i1* %az
  store i1 %182, i1* %cf
  store i1 %186, i1* %of
  %187 = icmp eq i64 %179, 0
  store i1 %187, i1* %zf
  %188 = icmp slt i64 %179, 0
  store i1 %188, i1* %sf
  %189 = trunc i64 %179 to i8
  %190 = call i8 @llvm.ctpop.i8(i8 %189)
  %191 = and i8 %190, 1
  %192 = icmp eq i8 %191, 0
  store i1 %192, i1* %pf
  store i64 %179, i64* %rax
  store volatile i64 49085, i64* @assembly_address
  %193 = load i64* %rax
  %194 = load i64* %rax
  %195 = mul i64 %194, 1
  %196 = add i64 %193, %195
  store i64 %196, i64* %rdx
  store volatile i64 49089, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 49096, i64* @assembly_address
  %197 = load i64* %rax
  %198 = load i64* %rdx
  %199 = add i64 %197, %198
  %200 = and i64 %197, 15
  %201 = and i64 %198, 15
  %202 = add i64 %200, %201
  %203 = icmp ugt i64 %202, 15
  %204 = icmp ult i64 %199, %197
  %205 = xor i64 %197, %199
  %206 = xor i64 %198, %199
  %207 = and i64 %205, %206
  %208 = icmp slt i64 %207, 0
  store i1 %203, i1* %az
  store i1 %204, i1* %cf
  store i1 %208, i1* %of
  %209 = icmp eq i64 %199, 0
  store i1 %209, i1* %zf
  %210 = icmp slt i64 %199, 0
  store i1 %210, i1* %sf
  %211 = trunc i64 %199 to i8
  %212 = call i8 @llvm.ctpop.i8(i8 %211)
  %213 = and i8 %212, 1
  %214 = icmp eq i8 %213, 0
  store i1 %214, i1* %pf
  store i64 %199, i64* %rax
  store volatile i64 49099, i64* @assembly_address
  %215 = load i64* %rax
  %216 = inttoptr i64 %215 to i16*
  %217 = load i16* %216
  %218 = zext i16 %217 to i64
  store i64 %218, i64* %rax
  store volatile i64 49102, i64* @assembly_address
  %219 = load i64* %rax
  %220 = trunc i64 %219 to i16
  %221 = zext i16 %220 to i64
  store i64 %221, i64* %rax
  store volatile i64 49105, i64* @assembly_address
  %222 = load i64* %rax
  %223 = trunc i64 %222 to i32
  store i32 %223, i32* %stack_var_-16
  store volatile i64 49108, i64* @assembly_address
  br label %block_bfee

block_bfd6:                                       ; preds = %block_bfa3
  store volatile i64 49110, i64* @assembly_address
  %224 = load i32* %stack_var_-16
  %225 = zext i32 %224 to i64
  store i64 %225, i64* %rax
  store volatile i64 49113, i64* @assembly_address
  %226 = load i64* %rax
  %227 = load i64* %rax
  %228 = mul i64 %227, 1
  %229 = add i64 %226, %228
  store i64 %229, i64* %rdx
  store volatile i64 49117, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 49124, i64* @assembly_address
  %230 = load i64* %rdx
  %231 = load i64* %rax
  %232 = mul i64 %231, 1
  %233 = add i64 %230, %232
  %234 = inttoptr i64 %233 to i16*
  %235 = load i16* %234
  %236 = zext i16 %235 to i64
  store i64 %236, i64* %rax
  store volatile i64 49128, i64* @assembly_address
  %237 = load i64* %rax
  %238 = trunc i64 %237 to i16
  %239 = zext i16 %238 to i64
  store i64 %239, i64* %rax
  store volatile i64 49131, i64* @assembly_address
  %240 = load i64* %rax
  %241 = trunc i64 %240 to i32
  store i32 %241, i32* %stack_var_-16
  br label %block_bfee

block_bfee:                                       ; preds = %block_bfd6, %block_bfb4
  store volatile i64 49134, i64* @assembly_address
  %242 = load i32* %stack_var_-12
  %243 = load i1* %of
  %244 = lshr i32 %242, 1
  %245 = icmp eq i32 %244, 0
  store i1 %245, i1* %zf
  %246 = icmp slt i32 %244, 0
  store i1 %246, i1* %sf
  %247 = trunc i32 %244 to i8
  %248 = call i8 @llvm.ctpop.i8(i8 %247)
  %249 = and i8 %248, 1
  %250 = icmp eq i8 %249, 0
  store i1 %250, i1* %pf
  store i32 %244, i32* %stack_var_-12
  %251 = and i32 1, %242
  %252 = icmp ne i32 %251, 0
  store i1 %252, i1* %cf
  %253 = icmp slt i32 %242, 0
  %254 = select i1 true, i1 %253, i1 %243
  store i1 %254, i1* %of
  store volatile i64 49137, i64* @assembly_address
  %255 = load i32* %stack_var_-16
  %256 = sub i32 %255, 509
  %257 = and i32 %255, 15
  %258 = sub i32 %257, 13
  %259 = icmp ugt i32 %258, 15
  %260 = icmp ult i32 %255, 509
  %261 = xor i32 %255, 509
  %262 = xor i32 %255, %256
  %263 = and i32 %261, %262
  %264 = icmp slt i32 %263, 0
  store i1 %259, i1* %az
  store i1 %260, i1* %cf
  store i1 %264, i1* %of
  %265 = icmp eq i32 %256, 0
  store i1 %265, i1* %zf
  %266 = icmp slt i32 %256, 0
  store i1 %266, i1* %sf
  %267 = trunc i32 %256 to i8
  %268 = call i8 @llvm.ctpop.i8(i8 %267)
  %269 = and i8 %268, 1
  %270 = icmp eq i8 %269, 0
  store i1 %270, i1* %pf
  store volatile i64 49144, i64* @assembly_address
  %271 = load i1* %cf
  %272 = load i1* %zf
  %273 = or i1 %271, %272
  %274 = icmp ne i1 %273, true
  br i1 %274, label %block_bfa3, label %block_bffa

block_bffa:                                       ; preds = %block_bfee, %block_bf5f
  store volatile i64 49146, i64* @assembly_address
  %275 = load i32* %stack_var_-16
  %276 = zext i32 %275 to i64
  store i64 %276, i64* %rdx
  store volatile i64 49149, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 49156, i64* @assembly_address
  %277 = load i64* %rdx
  %278 = load i64* %rax
  %279 = mul i64 %278, 1
  %280 = add i64 %277, %279
  %281 = inttoptr i64 %280 to i8*
  %282 = load i8* %281
  %283 = zext i8 %282 to i64
  store i64 %283, i64* %rax
  store volatile i64 49160, i64* @assembly_address
  %284 = load i64* %rax
  %285 = trunc i64 %284 to i8
  %286 = zext i8 %285 to i64
  store i64 %286, i64* %rax
  store volatile i64 49163, i64* @assembly_address
  %287 = load i64* %rax
  %288 = trunc i64 %287 to i32
  %289 = zext i32 %288 to i64
  store i64 %289, i64* %rdi
  store volatile i64 49165, i64* @assembly_address
  %290 = load i64* %rdi
  %291 = trunc i64 %290 to i32
  %292 = call i64 @fillbuf(i32 %291)
  store i64 %292, i64* %rax
  store i64 %292, i64* %rax
  store volatile i64 49170, i64* @assembly_address
  %293 = load i32* %stack_var_-16
  %294 = zext i32 %293 to i64
  store i64 %294, i64* %rax
  br label %block_c015

block_c015:                                       ; preds = %block_bffa, %block_bf28
  store volatile i64 49173, i64* @assembly_address
  %295 = load i64* %stack_var_-8
  store i64 %295, i64* %rbp
  %296 = ptrtoint i64* %stack_var_0 to i64
  store i64 %296, i64* %rsp
  store volatile i64 49174, i64* @assembly_address
  %297 = load i64* %rax
  %298 = load i64* %rax
  ret i64 %298
}

define i64 @decode_p() {
block_c017:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 49175, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 49176, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 49179, i64* @assembly_address
  %3 = load i64* %rbx
  store i64 %3, i64* %stack_var_-16
  %4 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %4, i64* %rsp
  store volatile i64 49180, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 24
  %7 = and i64 %5, 15
  %8 = sub i64 %7, 8
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %5, 24
  %11 = xor i64 %5, 24
  %12 = xor i64 %5, %6
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %6, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %6, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %6 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 49184, i64* @assembly_address
  %22 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %23 = zext i16 %22 to i64
  store i64 %23, i64* %rax
  store volatile i64 49191, i64* @assembly_address
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i16
  %26 = load i1* %of
  %27 = lshr i16 %25, 8
  %28 = icmp eq i16 %27, 0
  store i1 %28, i1* %zf
  %29 = icmp slt i16 %27, 0
  store i1 %29, i1* %sf
  %30 = trunc i16 %27 to i8
  %31 = call i8 @llvm.ctpop.i8(i8 %30)
  %32 = and i8 %31, 1
  %33 = icmp eq i8 %32, 0
  store i1 %33, i1* %pf
  %34 = zext i16 %27 to i64
  %35 = load i64* %rax
  %36 = and i64 %35, -65536
  %37 = or i64 %36, %34
  store i64 %37, i64* %rax
  %38 = and i16 128, %25
  %39 = icmp ne i16 %38, 0
  store i1 %39, i1* %cf
  %40 = icmp slt i16 %25, 0
  %41 = select i1 false, i1 %40, i1 %26
  store i1 %41, i1* %of
  store volatile i64 49195, i64* @assembly_address
  %42 = load i64* %rax
  %43 = trunc i64 %42 to i16
  %44 = zext i16 %43 to i64
  store i64 %44, i64* %rax
  store volatile i64 49198, i64* @assembly_address
  %45 = load i64* %rax
  %46 = trunc i64 %45 to i32
  %47 = sext i32 %46 to i64
  store i64 %47, i64* %rax
  store volatile i64 49200, i64* @assembly_address
  %48 = load i64* %rax
  %49 = load i64* %rax
  %50 = mul i64 %49, 1
  %51 = add i64 %48, %50
  store i64 %51, i64* %rdx
  store volatile i64 49204, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f40 to i64), i64* %rax
  store volatile i64 49211, i64* @assembly_address
  %52 = load i64* %rdx
  %53 = load i64* %rax
  %54 = mul i64 %53, 1
  %55 = add i64 %52, %54
  %56 = inttoptr i64 %55 to i16*
  %57 = load i16* %56
  %58 = zext i16 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 49215, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i16
  %61 = zext i16 %60 to i64
  store i64 %61, i64* %rax
  store volatile i64 49218, i64* @assembly_address
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i32
  store i32 %63, i32* %stack_var_-32
  store volatile i64 49221, i64* @assembly_address
  %64 = load i32* %stack_var_-32
  %65 = sub i32 %64, 13
  %66 = and i32 %64, 15
  %67 = sub i32 %66, 13
  %68 = icmp ugt i32 %67, 15
  %69 = icmp ult i32 %64, 13
  %70 = xor i32 %64, 13
  %71 = xor i32 %64, %65
  %72 = and i32 %70, %71
  %73 = icmp slt i32 %72, 0
  store i1 %68, i1* %az
  store i1 %69, i1* %cf
  store i1 %73, i1* %of
  %74 = icmp eq i32 %65, 0
  store i1 %74, i1* %zf
  %75 = icmp slt i32 %65, 0
  store i1 %75, i1* %sf
  %76 = trunc i32 %65 to i8
  %77 = call i8 @llvm.ctpop.i8(i8 %76)
  %78 = and i8 %77, 1
  %79 = icmp eq i8 %78, 0
  store i1 %79, i1* %pf
  store volatile i64 49225, i64* @assembly_address
  %80 = load i1* %cf
  %81 = load i1* %zf
  %82 = or i1 %80, %81
  br i1 %82, label %block_c0a6, label %block_c04b

block_c04b:                                       ; preds = %block_c017
  store volatile i64 49227, i64* @assembly_address
  store i32 128, i32* %stack_var_-28
  br label %block_c052

block_c052:                                       ; preds = %block_c09d, %block_c04b
  store volatile i64 49234, i64* @assembly_address
  %83 = load i16* bitcast (i64* @global_var_21a140 to i16*)
  %84 = zext i16 %83 to i64
  store i64 %84, i64* %rax
  store volatile i64 49241, i64* @assembly_address
  %85 = load i64* %rax
  %86 = trunc i64 %85 to i16
  %87 = zext i16 %86 to i64
  store i64 %87, i64* %rax
  store volatile i64 49244, i64* @assembly_address
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  %90 = load i32* %stack_var_-28
  %91 = and i32 %89, %90
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %92 = icmp eq i32 %91, 0
  store i1 %92, i1* %zf
  %93 = icmp slt i32 %91, 0
  store i1 %93, i1* %sf
  %94 = trunc i32 %91 to i8
  %95 = call i8 @llvm.ctpop.i8(i8 %94)
  %96 = and i8 %95, 1
  %97 = icmp eq i8 %96, 0
  store i1 %97, i1* %pf
  %98 = zext i32 %91 to i64
  store i64 %98, i64* %rax
  store volatile i64 49247, i64* @assembly_address
  %99 = load i64* %rax
  %100 = trunc i64 %99 to i32
  %101 = load i64* %rax
  %102 = trunc i64 %101 to i32
  %103 = and i32 %100, %102
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %104 = icmp eq i32 %103, 0
  store i1 %104, i1* %zf
  %105 = icmp slt i32 %103, 0
  store i1 %105, i1* %sf
  %106 = trunc i32 %103 to i8
  %107 = call i8 @llvm.ctpop.i8(i8 %106)
  %108 = and i8 %107, 1
  %109 = icmp eq i8 %108, 0
  store i1 %109, i1* %pf
  store volatile i64 49249, i64* @assembly_address
  %110 = load i1* %zf
  br i1 %110, label %block_c085, label %block_c063

block_c063:                                       ; preds = %block_c052
  store volatile i64 49251, i64* @assembly_address
  %111 = load i32* %stack_var_-32
  %112 = zext i32 %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 49254, i64* @assembly_address
  %113 = load i64* %rax
  %114 = add i64 %113, 32768
  %115 = and i64 %113, 15
  %116 = icmp ugt i64 %115, 15
  %117 = icmp ult i64 %114, %113
  %118 = xor i64 %113, %114
  %119 = xor i64 32768, %114
  %120 = and i64 %118, %119
  %121 = icmp slt i64 %120, 0
  store i1 %116, i1* %az
  store i1 %117, i1* %cf
  store i1 %121, i1* %of
  %122 = icmp eq i64 %114, 0
  store i1 %122, i1* %zf
  %123 = icmp slt i64 %114, 0
  store i1 %123, i1* %sf
  %124 = trunc i64 %114 to i8
  %125 = call i8 @llvm.ctpop.i8(i8 %124)
  %126 = and i8 %125, 1
  %127 = icmp eq i8 %126, 0
  store i1 %127, i1* %pf
  store i64 %114, i64* %rax
  store volatile i64 49260, i64* @assembly_address
  %128 = load i64* %rax
  %129 = load i64* %rax
  %130 = mul i64 %129, 1
  %131 = add i64 %128, %130
  store i64 %131, i64* %rdx
  store volatile i64 49264, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 49271, i64* @assembly_address
  %132 = load i64* %rax
  %133 = load i64* %rdx
  %134 = add i64 %132, %133
  %135 = and i64 %132, 15
  %136 = and i64 %133, 15
  %137 = add i64 %135, %136
  %138 = icmp ugt i64 %137, 15
  %139 = icmp ult i64 %134, %132
  %140 = xor i64 %132, %134
  %141 = xor i64 %133, %134
  %142 = and i64 %140, %141
  %143 = icmp slt i64 %142, 0
  store i1 %138, i1* %az
  store i1 %139, i1* %cf
  store i1 %143, i1* %of
  %144 = icmp eq i64 %134, 0
  store i1 %144, i1* %zf
  %145 = icmp slt i64 %134, 0
  store i1 %145, i1* %sf
  %146 = trunc i64 %134 to i8
  %147 = call i8 @llvm.ctpop.i8(i8 %146)
  %148 = and i8 %147, 1
  %149 = icmp eq i8 %148, 0
  store i1 %149, i1* %pf
  store i64 %134, i64* %rax
  store volatile i64 49274, i64* @assembly_address
  %150 = load i64* %rax
  %151 = inttoptr i64 %150 to i16*
  %152 = load i16* %151
  %153 = zext i16 %152 to i64
  store i64 %153, i64* %rax
  store volatile i64 49277, i64* @assembly_address
  %154 = load i64* %rax
  %155 = trunc i64 %154 to i16
  %156 = zext i16 %155 to i64
  store i64 %156, i64* %rax
  store volatile i64 49280, i64* @assembly_address
  %157 = load i64* %rax
  %158 = trunc i64 %157 to i32
  store i32 %158, i32* %stack_var_-32
  store volatile i64 49283, i64* @assembly_address
  br label %block_c09d

block_c085:                                       ; preds = %block_c052
  store volatile i64 49285, i64* @assembly_address
  %159 = load i32* %stack_var_-32
  %160 = zext i32 %159 to i64
  store i64 %160, i64* %rax
  store volatile i64 49288, i64* @assembly_address
  %161 = load i64* %rax
  %162 = load i64* %rax
  %163 = mul i64 %162, 1
  %164 = add i64 %161, %163
  store i64 %164, i64* %rdx
  store volatile i64 49292, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 49299, i64* @assembly_address
  %165 = load i64* %rdx
  %166 = load i64* %rax
  %167 = mul i64 %166, 1
  %168 = add i64 %165, %167
  %169 = inttoptr i64 %168 to i16*
  %170 = load i16* %169
  %171 = zext i16 %170 to i64
  store i64 %171, i64* %rax
  store volatile i64 49303, i64* @assembly_address
  %172 = load i64* %rax
  %173 = trunc i64 %172 to i16
  %174 = zext i16 %173 to i64
  store i64 %174, i64* %rax
  store volatile i64 49306, i64* @assembly_address
  %175 = load i64* %rax
  %176 = trunc i64 %175 to i32
  store i32 %176, i32* %stack_var_-32
  br label %block_c09d

block_c09d:                                       ; preds = %block_c085, %block_c063
  store volatile i64 49309, i64* @assembly_address
  %177 = load i32* %stack_var_-28
  %178 = load i1* %of
  %179 = lshr i32 %177, 1
  %180 = icmp eq i32 %179, 0
  store i1 %180, i1* %zf
  %181 = icmp slt i32 %179, 0
  store i1 %181, i1* %sf
  %182 = trunc i32 %179 to i8
  %183 = call i8 @llvm.ctpop.i8(i8 %182)
  %184 = and i8 %183, 1
  %185 = icmp eq i8 %184, 0
  store i1 %185, i1* %pf
  store i32 %179, i32* %stack_var_-28
  %186 = and i32 1, %177
  %187 = icmp ne i32 %186, 0
  store i1 %187, i1* %cf
  %188 = icmp slt i32 %177, 0
  %189 = select i1 true, i1 %188, i1 %178
  store i1 %189, i1* %of
  store volatile i64 49312, i64* @assembly_address
  %190 = load i32* %stack_var_-32
  %191 = sub i32 %190, 13
  %192 = and i32 %190, 15
  %193 = sub i32 %192, 13
  %194 = icmp ugt i32 %193, 15
  %195 = icmp ult i32 %190, 13
  %196 = xor i32 %190, 13
  %197 = xor i32 %190, %191
  %198 = and i32 %196, %197
  %199 = icmp slt i32 %198, 0
  store i1 %194, i1* %az
  store i1 %195, i1* %cf
  store i1 %199, i1* %of
  %200 = icmp eq i32 %191, 0
  store i1 %200, i1* %zf
  %201 = icmp slt i32 %191, 0
  store i1 %201, i1* %sf
  %202 = trunc i32 %191 to i8
  %203 = call i8 @llvm.ctpop.i8(i8 %202)
  %204 = and i8 %203, 1
  %205 = icmp eq i8 %204, 0
  store i1 %205, i1* %pf
  store volatile i64 49316, i64* @assembly_address
  %206 = load i1* %cf
  %207 = load i1* %zf
  %208 = or i1 %206, %207
  %209 = icmp ne i1 %208, true
  br i1 %209, label %block_c052, label %block_c0a6

block_c0a6:                                       ; preds = %block_c09d, %block_c017
  store volatile i64 49318, i64* @assembly_address
  %210 = load i32* %stack_var_-32
  %211 = zext i32 %210 to i64
  store i64 %211, i64* %rdx
  store volatile i64 49321, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_219f00 to i64), i64* %rax
  store volatile i64 49328, i64* @assembly_address
  %212 = load i64* %rdx
  %213 = load i64* %rax
  %214 = mul i64 %213, 1
  %215 = add i64 %212, %214
  %216 = inttoptr i64 %215 to i8*
  %217 = load i8* %216
  %218 = zext i8 %217 to i64
  store i64 %218, i64* %rax
  store volatile i64 49332, i64* @assembly_address
  %219 = load i64* %rax
  %220 = trunc i64 %219 to i8
  %221 = zext i8 %220 to i64
  store i64 %221, i64* %rax
  store volatile i64 49335, i64* @assembly_address
  %222 = load i64* %rax
  %223 = trunc i64 %222 to i32
  %224 = zext i32 %223 to i64
  store i64 %224, i64* %rdi
  store volatile i64 49337, i64* @assembly_address
  %225 = load i64* %rdi
  %226 = trunc i64 %225 to i32
  %227 = call i64 @fillbuf(i32 %226)
  store i64 %227, i64* %rax
  store i64 %227, i64* %rax
  store volatile i64 49342, i64* @assembly_address
  %228 = load i32* %stack_var_-32
  %229 = and i32 %228, 15
  %230 = icmp ugt i32 %229, 15
  %231 = icmp ult i32 %228, 0
  %232 = xor i32 %228, 0
  %233 = and i32 %232, 0
  %234 = icmp slt i32 %233, 0
  store i1 %230, i1* %az
  store i1 %231, i1* %cf
  store i1 %234, i1* %of
  %235 = icmp eq i32 %228, 0
  store i1 %235, i1* %zf
  %236 = icmp slt i32 %228, 0
  store i1 %236, i1* %sf
  %237 = trunc i32 %228 to i8
  %238 = call i8 @llvm.ctpop.i8(i8 %237)
  %239 = and i8 %238, 1
  %240 = icmp eq i8 %239, 0
  store i1 %240, i1* %pf
  store volatile i64 49346, i64* @assembly_address
  %241 = load i1* %zf
  br i1 %241, label %block_c0e7, label %block_c0c4

block_c0c4:                                       ; preds = %block_c0a6
  store volatile i64 49348, i64* @assembly_address
  %242 = load i32* %stack_var_-32
  %243 = zext i32 %242 to i64
  store i64 %243, i64* %rax
  store volatile i64 49351, i64* @assembly_address
  %244 = load i64* %rax
  %245 = trunc i64 %244 to i32
  %246 = sub i32 %245, 1
  %247 = and i32 %245, 15
  %248 = sub i32 %247, 1
  %249 = icmp ugt i32 %248, 15
  %250 = icmp ult i32 %245, 1
  %251 = xor i32 %245, 1
  %252 = xor i32 %245, %246
  %253 = and i32 %251, %252
  %254 = icmp slt i32 %253, 0
  store i1 %249, i1* %az
  store i1 %250, i1* %cf
  store i1 %254, i1* %of
  %255 = icmp eq i32 %246, 0
  store i1 %255, i1* %zf
  %256 = icmp slt i32 %246, 0
  store i1 %256, i1* %sf
  %257 = trunc i32 %246 to i8
  %258 = call i8 @llvm.ctpop.i8(i8 %257)
  %259 = and i8 %258, 1
  %260 = icmp eq i8 %259, 0
  store i1 %260, i1* %pf
  %261 = zext i32 %246 to i64
  store i64 %261, i64* %rax
  store volatile i64 49354, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 49359, i64* @assembly_address
  %262 = load i64* %rdx
  %263 = trunc i64 %262 to i32
  %264 = zext i32 %263 to i64
  store i64 %264, i64* %rbx
  store volatile i64 49361, i64* @assembly_address
  %265 = load i64* %rax
  %266 = trunc i64 %265 to i32
  %267 = zext i32 %266 to i64
  store i64 %267, i64* %rcx
  store volatile i64 49363, i64* @assembly_address
  %268 = load i64* %rbx
  %269 = trunc i64 %268 to i32
  %270 = load i64* %rcx
  %271 = trunc i64 %270 to i8
  %272 = zext i8 %271 to i32
  %273 = and i32 %272, 31
  %274 = load i1* %of
  %275 = icmp eq i32 %273, 0
  br i1 %275, label %293, label %276

; <label>:276                                     ; preds = %block_c0c4
  %277 = shl i32 %269, %273
  %278 = icmp eq i32 %277, 0
  store i1 %278, i1* %zf
  %279 = icmp slt i32 %277, 0
  store i1 %279, i1* %sf
  %280 = trunc i32 %277 to i8
  %281 = call i8 @llvm.ctpop.i8(i8 %280)
  %282 = and i8 %281, 1
  %283 = icmp eq i8 %282, 0
  store i1 %283, i1* %pf
  %284 = zext i32 %277 to i64
  store i64 %284, i64* %rbx
  %285 = sub i32 %273, 1
  %286 = shl i32 %269, %285
  %287 = lshr i32 %286, 31
  %288 = trunc i32 %287 to i1
  store i1 %288, i1* %cf
  %289 = lshr i32 %277, 31
  %290 = icmp ne i32 %289, %287
  %291 = icmp eq i32 %273, 1
  %292 = select i1 %291, i1 %290, i1 %274
  store i1 %292, i1* %of
  br label %293

; <label>:293                                     ; preds = %block_c0c4, %276
  store volatile i64 49365, i64* @assembly_address
  %294 = load i32* %stack_var_-32
  %295 = zext i32 %294 to i64
  store i64 %295, i64* %rax
  store volatile i64 49368, i64* @assembly_address
  %296 = load i64* %rax
  %297 = trunc i64 %296 to i32
  %298 = sub i32 %297, 1
  %299 = and i32 %297, 15
  %300 = sub i32 %299, 1
  %301 = icmp ugt i32 %300, 15
  %302 = icmp ult i32 %297, 1
  %303 = xor i32 %297, 1
  %304 = xor i32 %297, %298
  %305 = and i32 %303, %304
  %306 = icmp slt i32 %305, 0
  store i1 %301, i1* %az
  store i1 %302, i1* %cf
  store i1 %306, i1* %of
  %307 = icmp eq i32 %298, 0
  store i1 %307, i1* %zf
  %308 = icmp slt i32 %298, 0
  store i1 %308, i1* %sf
  %309 = trunc i32 %298 to i8
  %310 = call i8 @llvm.ctpop.i8(i8 %309)
  %311 = and i8 %310, 1
  %312 = icmp eq i8 %311, 0
  store i1 %312, i1* %pf
  %313 = zext i32 %298 to i64
  store i64 %313, i64* %rax
  store volatile i64 49371, i64* @assembly_address
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i32
  %316 = zext i32 %315 to i64
  store i64 %316, i64* %rdi
  store volatile i64 49373, i64* @assembly_address
  %317 = load i64* %rdi
  %318 = trunc i64 %317 to i32
  %319 = call i64 @getbits(i32 %318)
  store i64 %319, i64* %rax
  store i64 %319, i64* %rax
  store volatile i64 49378, i64* @assembly_address
  %320 = load i64* %rax
  %321 = trunc i64 %320 to i32
  %322 = load i64* %rbx
  %323 = trunc i64 %322 to i32
  %324 = add i32 %321, %323
  %325 = and i32 %321, 15
  %326 = and i32 %323, 15
  %327 = add i32 %325, %326
  %328 = icmp ugt i32 %327, 15
  %329 = icmp ult i32 %324, %321
  %330 = xor i32 %321, %324
  %331 = xor i32 %323, %324
  %332 = and i32 %330, %331
  %333 = icmp slt i32 %332, 0
  store i1 %328, i1* %az
  store i1 %329, i1* %cf
  store i1 %333, i1* %of
  %334 = icmp eq i32 %324, 0
  store i1 %334, i1* %zf
  %335 = icmp slt i32 %324, 0
  store i1 %335, i1* %sf
  %336 = trunc i32 %324 to i8
  %337 = call i8 @llvm.ctpop.i8(i8 %336)
  %338 = and i8 %337, 1
  %339 = icmp eq i8 %338, 0
  store i1 %339, i1* %pf
  %340 = zext i32 %324 to i64
  store i64 %340, i64* %rax
  store volatile i64 49380, i64* @assembly_address
  %341 = load i64* %rax
  %342 = trunc i64 %341 to i32
  store i32 %342, i32* %stack_var_-32
  br label %block_c0e7

block_c0e7:                                       ; preds = %293, %block_c0a6
  store volatile i64 49383, i64* @assembly_address
  %343 = load i32* %stack_var_-32
  %344 = zext i32 %343 to i64
  store i64 %344, i64* %rax
  store volatile i64 49386, i64* @assembly_address
  %345 = load i64* %rsp
  %346 = add i64 %345, 24
  %347 = and i64 %345, 15
  %348 = add i64 %347, 8
  %349 = icmp ugt i64 %348, 15
  %350 = icmp ult i64 %346, %345
  %351 = xor i64 %345, %346
  %352 = xor i64 24, %346
  %353 = and i64 %351, %352
  %354 = icmp slt i64 %353, 0
  store i1 %349, i1* %az
  store i1 %350, i1* %cf
  store i1 %354, i1* %of
  %355 = icmp eq i64 %346, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i64 %346, 0
  store i1 %356, i1* %sf
  %357 = trunc i64 %346 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  %361 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %361, i64* %rsp
  store volatile i64 49390, i64* @assembly_address
  %362 = load i64* %stack_var_-16
  store i64 %362, i64* %rbx
  %363 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %363, i64* %rsp
  store volatile i64 49391, i64* @assembly_address
  %364 = load i64* %stack_var_-8
  store i64 %364, i64* %rbp
  %365 = ptrtoint i64* %stack_var_0 to i64
  store i64 %365, i64* %rsp
  store volatile i64 49392, i64* @assembly_address
  %366 = load i64* %rax
  %367 = load i64* %rax
  ret i64 %367
}

define i64 @huf_decode_start() {
block_c0f1:
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 49393, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 49394, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 49397, i64* @assembly_address
  %3 = call i64 @init_getbits()
  store i64 %3, i64* %rax
  store i64 %3, i64* %rax
  store i64 %3, i64* %rax
  store volatile i64 49402, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_219f20 to i32*)
  store volatile i64 49412, i64* @assembly_address
  store volatile i64 49413, i64* @assembly_address
  %4 = load i64* %stack_var_-8
  store i64 %4, i64* %rbp
  %5 = ptrtoint i64* %stack_var_0 to i64
  store i64 %5, i64* %rsp
  store volatile i64 49414, i64* @assembly_address
  %6 = load i64* %rax
  %7 = load i64* %rax
  ret i64 %7
}

define i64 @decode_start(i64 %arg1, i64 %arg2) {
block_c107:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 49415, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 49416, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 49419, i64* @assembly_address
  %3 = call i64 @huf_decode_start()
  store i64 %3, i64* %rax
  store i64 %3, i64* %rax
  store i64 %3, i64* %rax
  store volatile i64 49424, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a14c to i32*)
  store volatile i64 49434, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a150 to i32*)
  store volatile i64 49444, i64* @assembly_address
  store volatile i64 49445, i64* @assembly_address
  %4 = load i64* %stack_var_-8
  store i64 %4, i64* %rbp
  %5 = ptrtoint i64* %stack_var_0 to i64
  store i64 %5, i64* %rsp
  store volatile i64 49446, i64* @assembly_address
  %6 = load i64* %rax
  %7 = load i64* %rax
  ret i64 %7
}

define i64 @decode(i32 %arg1, i64* %arg2) {
block_c127:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i8*
  %2 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 49447, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 49448, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 49451, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i8** %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 49455, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-28
  store volatile i64 49458, i64* @assembly_address
  %24 = load i64* %rsi
  %25 = inttoptr i64 %24 to i8*
  store i8* %25, i8** %stack_var_-40
  store volatile i64 49462, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 49469, i64* @assembly_address
  br label %block_c185

block_c13f:                                       ; preds = %block_c185
  store volatile i64 49471, i64* @assembly_address
  %26 = load i32* bitcast (i64* @global_var_21a154 to i32*)
  %27 = zext i32 %26 to i64
  store i64 %27, i64* %rax
  store volatile i64 49477, i64* @assembly_address
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  %30 = zext i32 %29 to i64
  store i64 %30, i64* %rdx
  store volatile i64 49479, i64* @assembly_address
  %31 = load i8** %stack_var_-40
  %32 = ptrtoint i8* %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 49483, i64* @assembly_address
  %33 = load i64* %rax
  %34 = load i64* %rdx
  %35 = add i64 %33, %34
  %36 = and i64 %33, 15
  %37 = and i64 %34, 15
  %38 = add i64 %36, %37
  %39 = icmp ugt i64 %38, 15
  %40 = icmp ult i64 %35, %33
  %41 = xor i64 %33, %35
  %42 = xor i64 %34, %35
  %43 = and i64 %41, %42
  %44 = icmp slt i64 %43, 0
  store i1 %39, i1* %az
  store i1 %40, i1* %cf
  store i1 %44, i1* %of
  %45 = icmp eq i64 %35, 0
  store i1 %45, i1* %zf
  %46 = icmp slt i64 %35, 0
  store i1 %46, i1* %sf
  %47 = trunc i64 %35 to i8
  %48 = call i8 @llvm.ctpop.i8(i8 %47)
  %49 = and i8 %48, 1
  %50 = icmp eq i8 %49, 0
  store i1 %50, i1* %pf
  store i64 %35, i64* %rax
  store volatile i64 49486, i64* @assembly_address
  %51 = load i32* %stack_var_-16
  %52 = zext i32 %51 to i64
  store i64 %52, i64* %rcx
  store volatile i64 49489, i64* @assembly_address
  %53 = load i8** %stack_var_-40
  %54 = ptrtoint i8* %53 to i64
  store i64 %54, i64* %rdx
  store volatile i64 49493, i64* @assembly_address
  %55 = load i64* %rdx
  %56 = load i64* %rcx
  %57 = add i64 %55, %56
  %58 = and i64 %55, 15
  %59 = and i64 %56, 15
  %60 = add i64 %58, %59
  %61 = icmp ugt i64 %60, 15
  %62 = icmp ult i64 %57, %55
  %63 = xor i64 %55, %57
  %64 = xor i64 %56, %57
  %65 = and i64 %63, %64
  %66 = icmp slt i64 %65, 0
  store i1 %61, i1* %az
  store i1 %62, i1* %cf
  store i1 %66, i1* %of
  %67 = icmp eq i64 %57, 0
  store i1 %67, i1* %zf
  %68 = icmp slt i64 %57, 0
  store i1 %68, i1* %sf
  %69 = trunc i64 %57 to i8
  %70 = call i8 @llvm.ctpop.i8(i8 %69)
  %71 = and i8 %70, 1
  %72 = icmp eq i8 %71, 0
  store i1 %72, i1* %pf
  store i64 %57, i64* %rdx
  store volatile i64 49496, i64* @assembly_address
  %73 = load i64* %rax
  %74 = inttoptr i64 %73 to i8*
  %75 = load i8* %74
  %76 = zext i8 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 49499, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i8
  %79 = load i64* %rdx
  %80 = inttoptr i64 %79 to i8*
  store i8 %78, i8* %80
  store volatile i64 49501, i64* @assembly_address
  %81 = load i32* bitcast (i64* @global_var_21a154 to i32*)
  %82 = zext i32 %81 to i64
  store i64 %82, i64* %rax
  store volatile i64 49507, i64* @assembly_address
  %83 = load i64* %rax
  %84 = trunc i64 %83 to i32
  %85 = add i32 %84, 1
  %86 = and i32 %84, 15
  %87 = add i32 %86, 1
  %88 = icmp ugt i32 %87, 15
  %89 = icmp ult i32 %85, %84
  %90 = xor i32 %84, %85
  %91 = xor i32 1, %85
  %92 = and i32 %90, %91
  %93 = icmp slt i32 %92, 0
  store i1 %88, i1* %az
  store i1 %89, i1* %cf
  store i1 %93, i1* %of
  %94 = icmp eq i32 %85, 0
  store i1 %94, i1* %zf
  %95 = icmp slt i32 %85, 0
  store i1 %95, i1* %sf
  %96 = trunc i32 %85 to i8
  %97 = call i8 @llvm.ctpop.i8(i8 %96)
  %98 = and i8 %97, 1
  %99 = icmp eq i8 %98, 0
  store i1 %99, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a155 to i64), i64* %rax
  store volatile i64 49510, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i32
  %102 = and i32 %101, 8191
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %103 = icmp eq i32 %102, 0
  store i1 %103, i1* %zf
  %104 = icmp slt i32 %102, 0
  store i1 %104, i1* %sf
  %105 = trunc i32 %102 to i8
  %106 = call i8 @llvm.ctpop.i8(i8 %105)
  %107 = and i8 %106, 1
  %108 = icmp eq i8 %107, 0
  store i1 %108, i1* %pf
  %109 = zext i32 %102 to i64
  store i64 %109, i64* %rax
  store volatile i64 49515, i64* @assembly_address
  %110 = load i64* %rax
  %111 = trunc i64 %110 to i32
  store i32 %111, i32* bitcast (i64* @global_var_21a154 to i32*)
  store volatile i64 49521, i64* @assembly_address
  %112 = load i32* %stack_var_-16
  %113 = add i32 %112, 1
  %114 = and i32 %112, 15
  %115 = add i32 %114, 1
  %116 = icmp ugt i32 %115, 15
  %117 = icmp ult i32 %113, %112
  %118 = xor i32 %112, %113
  %119 = xor i32 1, %113
  %120 = and i32 %118, %119
  %121 = icmp slt i32 %120, 0
  store i1 %116, i1* %az
  store i1 %117, i1* %cf
  store i1 %121, i1* %of
  %122 = icmp eq i32 %113, 0
  store i1 %122, i1* %zf
  %123 = icmp slt i32 %113, 0
  store i1 %123, i1* %sf
  %124 = trunc i32 %113 to i8
  %125 = call i8 @llvm.ctpop.i8(i8 %124)
  %126 = and i8 %125, 1
  %127 = icmp eq i8 %126, 0
  store i1 %127, i1* %pf
  store i32 %113, i32* %stack_var_-16
  store volatile i64 49525, i64* @assembly_address
  %128 = load i32* %stack_var_-16
  %129 = zext i32 %128 to i64
  store i64 %129, i64* %rax
  store volatile i64 49528, i64* @assembly_address
  %130 = load i64* %rax
  %131 = trunc i64 %130 to i32
  %132 = load i32* %stack_var_-28
  %133 = sub i32 %131, %132
  %134 = and i32 %131, 15
  %135 = and i32 %132, 15
  %136 = sub i32 %134, %135
  %137 = icmp ugt i32 %136, 15
  %138 = icmp ult i32 %131, %132
  %139 = xor i32 %131, %132
  %140 = xor i32 %131, %133
  %141 = and i32 %139, %140
  %142 = icmp slt i32 %141, 0
  store i1 %137, i1* %az
  store i1 %138, i1* %cf
  store i1 %142, i1* %of
  %143 = icmp eq i32 %133, 0
  store i1 %143, i1* %zf
  %144 = icmp slt i32 %133, 0
  store i1 %144, i1* %sf
  %145 = trunc i32 %133 to i8
  %146 = call i8 @llvm.ctpop.i8(i8 %145)
  %147 = and i8 %146, 1
  %148 = icmp eq i8 %147, 0
  store i1 %148, i1* %pf
  store volatile i64 49531, i64* @assembly_address
  %149 = load i1* %zf
  %150 = icmp eq i1 %149, false
  br i1 %150, label %block_c185, label %block_c17d

block_c17d:                                       ; preds = %block_c13f
  store volatile i64 49533, i64* @assembly_address
  %151 = load i32* %stack_var_-16
  %152 = zext i32 %151 to i64
  store i64 %152, i64* %rax
  store volatile i64 49536, i64* @assembly_address
  br label %block_c278

block_c185:                                       ; preds = %block_c13f, %block_c127
  store volatile i64 49541, i64* @assembly_address
  %153 = load i32* bitcast (i64* @global_var_21a14c to i32*)
  %154 = zext i32 %153 to i64
  store i64 %154, i64* %rax
  store volatile i64 49547, i64* @assembly_address
  %155 = load i64* %rax
  %156 = trunc i64 %155 to i32
  %157 = sub i32 %156, 1
  %158 = and i32 %156, 15
  %159 = sub i32 %158, 1
  %160 = icmp ugt i32 %159, 15
  %161 = icmp ult i32 %156, 1
  %162 = xor i32 %156, 1
  %163 = xor i32 %156, %157
  %164 = and i32 %162, %163
  %165 = icmp slt i32 %164, 0
  store i1 %160, i1* %az
  store i1 %161, i1* %cf
  store i1 %165, i1* %of
  %166 = icmp eq i32 %157, 0
  store i1 %166, i1* %zf
  %167 = icmp slt i32 %157, 0
  store i1 %167, i1* %sf
  %168 = trunc i32 %157 to i8
  %169 = call i8 @llvm.ctpop.i8(i8 %168)
  %170 = and i8 %169, 1
  %171 = icmp eq i8 %170, 0
  store i1 %171, i1* %pf
  %172 = zext i32 %157 to i64
  store i64 %172, i64* %rax
  store volatile i64 49550, i64* @assembly_address
  %173 = load i64* %rax
  %174 = trunc i64 %173 to i32
  store i32 %174, i32* bitcast (i64* @global_var_21a14c to i32*)
  store volatile i64 49556, i64* @assembly_address
  %175 = load i32* bitcast (i64* @global_var_21a14c to i32*)
  %176 = zext i32 %175 to i64
  store i64 %176, i64* %rax
  store volatile i64 49562, i64* @assembly_address
  %177 = load i64* %rax
  %178 = trunc i64 %177 to i32
  %179 = load i64* %rax
  %180 = trunc i64 %179 to i32
  %181 = and i32 %178, %180
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %182 = icmp eq i32 %181, 0
  store i1 %182, i1* %zf
  %183 = icmp slt i32 %181, 0
  store i1 %183, i1* %sf
  %184 = trunc i32 %181 to i8
  %185 = call i8 @llvm.ctpop.i8(i8 %184)
  %186 = and i8 %185, 1
  %187 = icmp eq i8 %186, 0
  store i1 %187, i1* %pf
  store volatile i64 49564, i64* @assembly_address
  %188 = load i1* %sf
  %189 = icmp eq i1 %188, false
  br i1 %189, label %block_c13f, label %block_c19e

block_c19e:                                       ; preds = %block_c273, %block_c1ca, %block_c185
  store volatile i64 49566, i64* @assembly_address
  %190 = call i64 @decode_c()
  store i64 %190, i64* %rax
  store i64 %190, i64* %rax
  store i64 %190, i64* %rax
  store volatile i64 49571, i64* @assembly_address
  %191 = load i64* %rax
  %192 = trunc i64 %191 to i32
  store i32 %192, i32* %stack_var_-12
  store volatile i64 49574, i64* @assembly_address
  %193 = load i32* %stack_var_-12
  %194 = sub i32 %193, 510
  %195 = and i32 %193, 15
  %196 = sub i32 %195, 14
  %197 = icmp ugt i32 %196, 15
  %198 = icmp ult i32 %193, 510
  %199 = xor i32 %193, 510
  %200 = xor i32 %193, %194
  %201 = and i32 %199, %200
  %202 = icmp slt i32 %201, 0
  store i1 %197, i1* %az
  store i1 %198, i1* %cf
  store i1 %202, i1* %of
  %203 = icmp eq i32 %194, 0
  store i1 %203, i1* %zf
  %204 = icmp slt i32 %194, 0
  store i1 %204, i1* %sf
  %205 = trunc i32 %194 to i8
  %206 = call i8 @llvm.ctpop.i8(i8 %205)
  %207 = and i8 %206, 1
  %208 = icmp eq i8 %207, 0
  store i1 %208, i1* %pf
  store volatile i64 49581, i64* @assembly_address
  %209 = load i1* %zf
  %210 = icmp eq i1 %209, false
  br i1 %210, label %block_c1c1, label %block_c1af

block_c1af:                                       ; preds = %block_c19e
  store volatile i64 49583, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21a150 to i32*)
  store volatile i64 49593, i64* @assembly_address
  %211 = load i32* %stack_var_-16
  %212 = zext i32 %211 to i64
  store i64 %212, i64* %rax
  store volatile i64 49596, i64* @assembly_address
  br label %block_c278

block_c1c1:                                       ; preds = %block_c19e
  store volatile i64 49601, i64* @assembly_address
  %213 = load i32* %stack_var_-12
  %214 = sub i32 %213, 255
  %215 = and i32 %213, 15
  %216 = sub i32 %215, 15
  %217 = icmp ugt i32 %216, 15
  %218 = icmp ult i32 %213, 255
  %219 = xor i32 %213, 255
  %220 = xor i32 %213, %214
  %221 = and i32 %219, %220
  %222 = icmp slt i32 %221, 0
  store i1 %217, i1* %az
  store i1 %218, i1* %cf
  store i1 %222, i1* %of
  %223 = icmp eq i32 %214, 0
  store i1 %223, i1* %zf
  %224 = icmp slt i32 %214, 0
  store i1 %224, i1* %sf
  %225 = trunc i32 %214 to i8
  %226 = call i8 @llvm.ctpop.i8(i8 %225)
  %227 = and i8 %226, 1
  %228 = icmp eq i8 %227, 0
  store i1 %228, i1* %pf
  store volatile i64 49608, i64* @assembly_address
  %229 = load i1* %cf
  %230 = load i1* %zf
  %231 = or i1 %229, %230
  %232 = icmp ne i1 %231, true
  br i1 %232, label %block_c1ed, label %block_c1ca

block_c1ca:                                       ; preds = %block_c1c1
  store volatile i64 49610, i64* @assembly_address
  %233 = load i32* %stack_var_-16
  %234 = zext i32 %233 to i64
  store i64 %234, i64* %rdx
  store volatile i64 49613, i64* @assembly_address
  %235 = load i8** %stack_var_-40
  %236 = ptrtoint i8* %235 to i64
  store i64 %236, i64* %rax
  store volatile i64 49617, i64* @assembly_address
  %237 = load i64* %rax
  %238 = load i64* %rdx
  %239 = add i64 %237, %238
  %240 = and i64 %237, 15
  %241 = and i64 %238, 15
  %242 = add i64 %240, %241
  %243 = icmp ugt i64 %242, 15
  %244 = icmp ult i64 %239, %237
  %245 = xor i64 %237, %239
  %246 = xor i64 %238, %239
  %247 = and i64 %245, %246
  %248 = icmp slt i64 %247, 0
  store i1 %243, i1* %az
  store i1 %244, i1* %cf
  store i1 %248, i1* %of
  %249 = icmp eq i64 %239, 0
  store i1 %249, i1* %zf
  %250 = icmp slt i64 %239, 0
  store i1 %250, i1* %sf
  %251 = trunc i64 %239 to i8
  %252 = call i8 @llvm.ctpop.i8(i8 %251)
  %253 = and i8 %252, 1
  %254 = icmp eq i8 %253, 0
  store i1 %254, i1* %pf
  store i64 %239, i64* %rax
  store volatile i64 49620, i64* @assembly_address
  %255 = load i32* %stack_var_-12
  %256 = zext i32 %255 to i64
  store i64 %256, i64* %rdx
  store volatile i64 49623, i64* @assembly_address
  %257 = load i64* %rdx
  %258 = trunc i64 %257 to i8
  %259 = load i64* %rax
  %260 = inttoptr i64 %259 to i8*
  store i8 %258, i8* %260
  store volatile i64 49625, i64* @assembly_address
  %261 = load i32* %stack_var_-16
  %262 = add i32 %261, 1
  %263 = and i32 %261, 15
  %264 = add i32 %263, 1
  %265 = icmp ugt i32 %264, 15
  %266 = icmp ult i32 %262, %261
  %267 = xor i32 %261, %262
  %268 = xor i32 1, %262
  %269 = and i32 %267, %268
  %270 = icmp slt i32 %269, 0
  store i1 %265, i1* %az
  store i1 %266, i1* %cf
  store i1 %270, i1* %of
  %271 = icmp eq i32 %262, 0
  store i1 %271, i1* %zf
  %272 = icmp slt i32 %262, 0
  store i1 %272, i1* %sf
  %273 = trunc i32 %262 to i8
  %274 = call i8 @llvm.ctpop.i8(i8 %273)
  %275 = and i8 %274, 1
  %276 = icmp eq i8 %275, 0
  store i1 %276, i1* %pf
  store i32 %262, i32* %stack_var_-16
  store volatile i64 49629, i64* @assembly_address
  %277 = load i32* %stack_var_-16
  %278 = zext i32 %277 to i64
  store i64 %278, i64* %rax
  store volatile i64 49632, i64* @assembly_address
  %279 = load i64* %rax
  %280 = trunc i64 %279 to i32
  %281 = load i32* %stack_var_-28
  %282 = sub i32 %280, %281
  %283 = and i32 %280, 15
  %284 = and i32 %281, 15
  %285 = sub i32 %283, %284
  %286 = icmp ugt i32 %285, 15
  %287 = icmp ult i32 %280, %281
  %288 = xor i32 %280, %281
  %289 = xor i32 %280, %282
  %290 = and i32 %288, %289
  %291 = icmp slt i32 %290, 0
  store i1 %286, i1* %az
  store i1 %287, i1* %cf
  store i1 %291, i1* %of
  %292 = icmp eq i32 %282, 0
  store i1 %292, i1* %zf
  %293 = icmp slt i32 %282, 0
  store i1 %293, i1* %sf
  %294 = trunc i32 %282 to i8
  %295 = call i8 @llvm.ctpop.i8(i8 %294)
  %296 = and i8 %295, 1
  %297 = icmp eq i8 %296, 0
  store i1 %297, i1* %pf
  store volatile i64 49635, i64* @assembly_address
  %298 = load i1* %zf
  %299 = icmp eq i1 %298, false
  br i1 %299, label %block_c19e, label %block_c1e5

block_c1e5:                                       ; preds = %block_c1ca
  store volatile i64 49637, i64* @assembly_address
  %300 = load i32* %stack_var_-16
  %301 = zext i32 %300 to i64
  store i64 %301, i64* %rax
  store volatile i64 49640, i64* @assembly_address
  br label %block_c278

block_c1ed:                                       ; preds = %block_c1c1
  store volatile i64 49645, i64* @assembly_address
  %302 = load i32* %stack_var_-12
  %303 = zext i32 %302 to i64
  store i64 %303, i64* %rax
  store volatile i64 49648, i64* @assembly_address
  %304 = load i64* %rax
  %305 = trunc i64 %304 to i32
  %306 = sub i32 %305, 253
  %307 = and i32 %305, 15
  %308 = sub i32 %307, 13
  %309 = icmp ugt i32 %308, 15
  %310 = icmp ult i32 %305, 253
  %311 = xor i32 %305, 253
  %312 = xor i32 %305, %306
  %313 = and i32 %311, %312
  %314 = icmp slt i32 %313, 0
  store i1 %309, i1* %az
  store i1 %310, i1* %cf
  store i1 %314, i1* %of
  %315 = icmp eq i32 %306, 0
  store i1 %315, i1* %zf
  %316 = icmp slt i32 %306, 0
  store i1 %316, i1* %sf
  %317 = trunc i32 %306 to i8
  %318 = call i8 @llvm.ctpop.i8(i8 %317)
  %319 = and i8 %318, 1
  %320 = icmp eq i8 %319, 0
  store i1 %320, i1* %pf
  %321 = zext i32 %306 to i64
  store i64 %321, i64* %rax
  store volatile i64 49653, i64* @assembly_address
  %322 = load i64* %rax
  %323 = trunc i64 %322 to i32
  store i32 %323, i32* bitcast (i64* @global_var_21a14c to i32*)
  store volatile i64 49659, i64* @assembly_address
  %324 = call i64 @decode_p()
  store i64 %324, i64* %rax
  store i64 %324, i64* %rax
  store i64 %324, i64* %rax
  store volatile i64 49664, i64* @assembly_address
  %325 = load i64* %rax
  %326 = trunc i64 %325 to i32
  %327 = zext i32 %326 to i64
  store i64 %327, i64* %rdx
  store volatile i64 49666, i64* @assembly_address
  %328 = load i32* %stack_var_-16
  %329 = zext i32 %328 to i64
  store i64 %329, i64* %rax
  store volatile i64 49669, i64* @assembly_address
  %330 = load i64* %rax
  %331 = trunc i64 %330 to i32
  %332 = load i64* %rdx
  %333 = trunc i64 %332 to i32
  %334 = sub i32 %331, %333
  %335 = and i32 %331, 15
  %336 = and i32 %333, 15
  %337 = sub i32 %335, %336
  %338 = icmp ugt i32 %337, 15
  %339 = icmp ult i32 %331, %333
  %340 = xor i32 %331, %333
  %341 = xor i32 %331, %334
  %342 = and i32 %340, %341
  %343 = icmp slt i32 %342, 0
  store i1 %338, i1* %az
  store i1 %339, i1* %cf
  store i1 %343, i1* %of
  %344 = icmp eq i32 %334, 0
  store i1 %344, i1* %zf
  %345 = icmp slt i32 %334, 0
  store i1 %345, i1* %sf
  %346 = trunc i32 %334 to i8
  %347 = call i8 @llvm.ctpop.i8(i8 %346)
  %348 = and i8 %347, 1
  %349 = icmp eq i8 %348, 0
  store i1 %349, i1* %pf
  %350 = zext i32 %334 to i64
  store i64 %350, i64* %rax
  store volatile i64 49671, i64* @assembly_address
  %351 = load i64* %rax
  %352 = trunc i64 %351 to i32
  %353 = sub i32 %352, 1
  %354 = and i32 %352, 15
  %355 = sub i32 %354, 1
  %356 = icmp ugt i32 %355, 15
  %357 = icmp ult i32 %352, 1
  %358 = xor i32 %352, 1
  %359 = xor i32 %352, %353
  %360 = and i32 %358, %359
  %361 = icmp slt i32 %360, 0
  store i1 %356, i1* %az
  store i1 %357, i1* %cf
  store i1 %361, i1* %of
  %362 = icmp eq i32 %353, 0
  store i1 %362, i1* %zf
  %363 = icmp slt i32 %353, 0
  store i1 %363, i1* %sf
  %364 = trunc i32 %353 to i8
  %365 = call i8 @llvm.ctpop.i8(i8 %364)
  %366 = and i8 %365, 1
  %367 = icmp eq i8 %366, 0
  store i1 %367, i1* %pf
  %368 = zext i32 %353 to i64
  store i64 %368, i64* %rax
  store volatile i64 49674, i64* @assembly_address
  %369 = load i64* %rax
  %370 = trunc i64 %369 to i32
  %371 = and i32 %370, 8191
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %372 = icmp eq i32 %371, 0
  store i1 %372, i1* %zf
  %373 = icmp slt i32 %371, 0
  store i1 %373, i1* %sf
  %374 = trunc i32 %371 to i8
  %375 = call i8 @llvm.ctpop.i8(i8 %374)
  %376 = and i8 %375, 1
  %377 = icmp eq i8 %376, 0
  store i1 %377, i1* %pf
  %378 = zext i32 %371 to i64
  store i64 %378, i64* %rax
  store volatile i64 49679, i64* @assembly_address
  %379 = load i64* %rax
  %380 = trunc i64 %379 to i32
  store i32 %380, i32* bitcast (i64* @global_var_21a154 to i32*)
  store volatile i64 49685, i64* @assembly_address
  br label %block_c25a

block_c217:                                       ; preds = %block_c25a
  store volatile i64 49687, i64* @assembly_address
  %381 = load i32* bitcast (i64* @global_var_21a154 to i32*)
  %382 = zext i32 %381 to i64
  store i64 %382, i64* %rax
  store volatile i64 49693, i64* @assembly_address
  %383 = load i64* %rax
  %384 = trunc i64 %383 to i32
  %385 = zext i32 %384 to i64
  store i64 %385, i64* %rdx
  store volatile i64 49695, i64* @assembly_address
  %386 = load i8** %stack_var_-40
  %387 = ptrtoint i8* %386 to i64
  store i64 %387, i64* %rax
  store volatile i64 49699, i64* @assembly_address
  %388 = load i64* %rax
  %389 = load i64* %rdx
  %390 = add i64 %388, %389
  %391 = and i64 %388, 15
  %392 = and i64 %389, 15
  %393 = add i64 %391, %392
  %394 = icmp ugt i64 %393, 15
  %395 = icmp ult i64 %390, %388
  %396 = xor i64 %388, %390
  %397 = xor i64 %389, %390
  %398 = and i64 %396, %397
  %399 = icmp slt i64 %398, 0
  store i1 %394, i1* %az
  store i1 %395, i1* %cf
  store i1 %399, i1* %of
  %400 = icmp eq i64 %390, 0
  store i1 %400, i1* %zf
  %401 = icmp slt i64 %390, 0
  store i1 %401, i1* %sf
  %402 = trunc i64 %390 to i8
  %403 = call i8 @llvm.ctpop.i8(i8 %402)
  %404 = and i8 %403, 1
  %405 = icmp eq i8 %404, 0
  store i1 %405, i1* %pf
  store i64 %390, i64* %rax
  store volatile i64 49702, i64* @assembly_address
  %406 = load i32* %stack_var_-16
  %407 = zext i32 %406 to i64
  store i64 %407, i64* %rcx
  store volatile i64 49705, i64* @assembly_address
  %408 = load i8** %stack_var_-40
  %409 = ptrtoint i8* %408 to i64
  store i64 %409, i64* %rdx
  store volatile i64 49709, i64* @assembly_address
  %410 = load i64* %rdx
  %411 = load i64* %rcx
  %412 = add i64 %410, %411
  %413 = and i64 %410, 15
  %414 = and i64 %411, 15
  %415 = add i64 %413, %414
  %416 = icmp ugt i64 %415, 15
  %417 = icmp ult i64 %412, %410
  %418 = xor i64 %410, %412
  %419 = xor i64 %411, %412
  %420 = and i64 %418, %419
  %421 = icmp slt i64 %420, 0
  store i1 %416, i1* %az
  store i1 %417, i1* %cf
  store i1 %421, i1* %of
  %422 = icmp eq i64 %412, 0
  store i1 %422, i1* %zf
  %423 = icmp slt i64 %412, 0
  store i1 %423, i1* %sf
  %424 = trunc i64 %412 to i8
  %425 = call i8 @llvm.ctpop.i8(i8 %424)
  %426 = and i8 %425, 1
  %427 = icmp eq i8 %426, 0
  store i1 %427, i1* %pf
  store i64 %412, i64* %rdx
  store volatile i64 49712, i64* @assembly_address
  %428 = load i64* %rax
  %429 = inttoptr i64 %428 to i8*
  %430 = load i8* %429
  %431 = zext i8 %430 to i64
  store i64 %431, i64* %rax
  store volatile i64 49715, i64* @assembly_address
  %432 = load i64* %rax
  %433 = trunc i64 %432 to i8
  %434 = load i64* %rdx
  %435 = inttoptr i64 %434 to i8*
  store i8 %433, i8* %435
  store volatile i64 49717, i64* @assembly_address
  %436 = load i32* bitcast (i64* @global_var_21a154 to i32*)
  %437 = zext i32 %436 to i64
  store i64 %437, i64* %rax
  store volatile i64 49723, i64* @assembly_address
  %438 = load i64* %rax
  %439 = trunc i64 %438 to i32
  %440 = add i32 %439, 1
  %441 = and i32 %439, 15
  %442 = add i32 %441, 1
  %443 = icmp ugt i32 %442, 15
  %444 = icmp ult i32 %440, %439
  %445 = xor i32 %439, %440
  %446 = xor i32 1, %440
  %447 = and i32 %445, %446
  %448 = icmp slt i32 %447, 0
  store i1 %443, i1* %az
  store i1 %444, i1* %cf
  store i1 %448, i1* %of
  %449 = icmp eq i32 %440, 0
  store i1 %449, i1* %zf
  %450 = icmp slt i32 %440, 0
  store i1 %450, i1* %sf
  %451 = trunc i32 %440 to i8
  %452 = call i8 @llvm.ctpop.i8(i8 %451)
  %453 = and i8 %452, 1
  %454 = icmp eq i8 %453, 0
  store i1 %454, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a155 to i64), i64* %rax
  store volatile i64 49726, i64* @assembly_address
  %455 = load i64* %rax
  %456 = trunc i64 %455 to i32
  %457 = and i32 %456, 8191
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %458 = icmp eq i32 %457, 0
  store i1 %458, i1* %zf
  %459 = icmp slt i32 %457, 0
  store i1 %459, i1* %sf
  %460 = trunc i32 %457 to i8
  %461 = call i8 @llvm.ctpop.i8(i8 %460)
  %462 = and i8 %461, 1
  %463 = icmp eq i8 %462, 0
  store i1 %463, i1* %pf
  %464 = zext i32 %457 to i64
  store i64 %464, i64* %rax
  store volatile i64 49731, i64* @assembly_address
  %465 = load i64* %rax
  %466 = trunc i64 %465 to i32
  store i32 %466, i32* bitcast (i64* @global_var_21a154 to i32*)
  store volatile i64 49737, i64* @assembly_address
  %467 = load i32* %stack_var_-16
  %468 = add i32 %467, 1
  %469 = and i32 %467, 15
  %470 = add i32 %469, 1
  %471 = icmp ugt i32 %470, 15
  %472 = icmp ult i32 %468, %467
  %473 = xor i32 %467, %468
  %474 = xor i32 1, %468
  %475 = and i32 %473, %474
  %476 = icmp slt i32 %475, 0
  store i1 %471, i1* %az
  store i1 %472, i1* %cf
  store i1 %476, i1* %of
  %477 = icmp eq i32 %468, 0
  store i1 %477, i1* %zf
  %478 = icmp slt i32 %468, 0
  store i1 %478, i1* %sf
  %479 = trunc i32 %468 to i8
  %480 = call i8 @llvm.ctpop.i8(i8 %479)
  %481 = and i8 %480, 1
  %482 = icmp eq i8 %481, 0
  store i1 %482, i1* %pf
  store i32 %468, i32* %stack_var_-16
  store volatile i64 49741, i64* @assembly_address
  %483 = load i32* %stack_var_-16
  %484 = zext i32 %483 to i64
  store i64 %484, i64* %rax
  store volatile i64 49744, i64* @assembly_address
  %485 = load i64* %rax
  %486 = trunc i64 %485 to i32
  %487 = load i32* %stack_var_-28
  %488 = sub i32 %486, %487
  %489 = and i32 %486, 15
  %490 = and i32 %487, 15
  %491 = sub i32 %489, %490
  %492 = icmp ugt i32 %491, 15
  %493 = icmp ult i32 %486, %487
  %494 = xor i32 %486, %487
  %495 = xor i32 %486, %488
  %496 = and i32 %494, %495
  %497 = icmp slt i32 %496, 0
  store i1 %492, i1* %az
  store i1 %493, i1* %cf
  store i1 %497, i1* %of
  %498 = icmp eq i32 %488, 0
  store i1 %498, i1* %zf
  %499 = icmp slt i32 %488, 0
  store i1 %499, i1* %sf
  %500 = trunc i32 %488 to i8
  %501 = call i8 @llvm.ctpop.i8(i8 %500)
  %502 = and i8 %501, 1
  %503 = icmp eq i8 %502, 0
  store i1 %503, i1* %pf
  store volatile i64 49747, i64* @assembly_address
  %504 = load i1* %zf
  %505 = icmp eq i1 %504, false
  br i1 %505, label %block_c25a, label %block_c255

block_c255:                                       ; preds = %block_c217
  store volatile i64 49749, i64* @assembly_address
  %506 = load i32* %stack_var_-16
  %507 = zext i32 %506 to i64
  store i64 %507, i64* %rax
  store volatile i64 49752, i64* @assembly_address
  br label %block_c278

block_c25a:                                       ; preds = %block_c217, %block_c1ed
  store volatile i64 49754, i64* @assembly_address
  %508 = load i32* bitcast (i64* @global_var_21a14c to i32*)
  %509 = zext i32 %508 to i64
  store i64 %509, i64* %rax
  store volatile i64 49760, i64* @assembly_address
  %510 = load i64* %rax
  %511 = trunc i64 %510 to i32
  %512 = sub i32 %511, 1
  %513 = and i32 %511, 15
  %514 = sub i32 %513, 1
  %515 = icmp ugt i32 %514, 15
  %516 = icmp ult i32 %511, 1
  %517 = xor i32 %511, 1
  %518 = xor i32 %511, %512
  %519 = and i32 %517, %518
  %520 = icmp slt i32 %519, 0
  store i1 %515, i1* %az
  store i1 %516, i1* %cf
  store i1 %520, i1* %of
  %521 = icmp eq i32 %512, 0
  store i1 %521, i1* %zf
  %522 = icmp slt i32 %512, 0
  store i1 %522, i1* %sf
  %523 = trunc i32 %512 to i8
  %524 = call i8 @llvm.ctpop.i8(i8 %523)
  %525 = and i8 %524, 1
  %526 = icmp eq i8 %525, 0
  store i1 %526, i1* %pf
  %527 = zext i32 %512 to i64
  store i64 %527, i64* %rax
  store volatile i64 49763, i64* @assembly_address
  %528 = load i64* %rax
  %529 = trunc i64 %528 to i32
  store i32 %529, i32* bitcast (i64* @global_var_21a14c to i32*)
  store volatile i64 49769, i64* @assembly_address
  %530 = load i32* bitcast (i64* @global_var_21a14c to i32*)
  %531 = zext i32 %530 to i64
  store i64 %531, i64* %rax
  store volatile i64 49775, i64* @assembly_address
  %532 = load i64* %rax
  %533 = trunc i64 %532 to i32
  %534 = load i64* %rax
  %535 = trunc i64 %534 to i32
  %536 = and i32 %533, %535
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %537 = icmp eq i32 %536, 0
  store i1 %537, i1* %zf
  %538 = icmp slt i32 %536, 0
  store i1 %538, i1* %sf
  %539 = trunc i32 %536 to i8
  %540 = call i8 @llvm.ctpop.i8(i8 %539)
  %541 = and i8 %540, 1
  %542 = icmp eq i8 %541, 0
  store i1 %542, i1* %pf
  store volatile i64 49777, i64* @assembly_address
  %543 = load i1* %sf
  %544 = icmp eq i1 %543, false
  br i1 %544, label %block_c217, label %block_c273

block_c273:                                       ; preds = %block_c25a
  store volatile i64 49779, i64* @assembly_address
  br label %block_c19e

block_c278:                                       ; preds = %block_c255, %block_c1e5, %block_c17d, %block_c1af
  store volatile i64 49784, i64* @assembly_address
  %545 = load i64* %stack_var_-8
  store i64 %545, i64* %rbp
  %546 = ptrtoint i64* %stack_var_0 to i64
  store i64 %546, i64* %rsp
  store volatile i64 49785, i64* @assembly_address
  %547 = load i64* %rax
  ret i64 %547
}

declare i64 @229(i64, i8*)

declare i64 @230(i64, i64*)

define i64 @unlzh(i32 %arg1, i64 %arg2) {
block_c27a:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 49786, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 49787, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 49790, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 49794, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-28
  store volatile i64 49797, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-32
  store volatile i64 49800, i64* @assembly_address
  %24 = load i32* %stack_var_-28
  %25 = zext i32 %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 49803, i64* @assembly_address
  %26 = load i64* %rax
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* bitcast (i64* @global_var_24f0a0 to i32*)
  store volatile i64 49809, i64* @assembly_address
  %28 = load i32* %stack_var_-32
  %29 = zext i32 %28 to i64
  store i64 %29, i64* %rax
  store volatile i64 49812, i64* @assembly_address
  %30 = load i64* %rax
  %31 = trunc i64 %30 to i32
  store i32 %31, i32* bitcast (i64* @global_var_24a880 to i32*)
  store volatile i64 49818, i64* @assembly_address
  %32 = load i64* %rdi
  %33 = load i64* %rsi
  %34 = call i64 @decode_start(i64 %32, i64 %33)
  store i64 %34, i64* %rax
  store i64 %34, i64* %rax
  store i64 %34, i64* %rax
  store volatile i64 49823, i64* @assembly_address
  br label %block_c2d9

block_c2a1:                                       ; preds = %block_c2d9
  store volatile i64 49825, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rsi
  store volatile i64 49832, i64* @assembly_address
  store i64 8192, i64* %rdi
  store volatile i64 49837, i64* @assembly_address
  %35 = load i64* %rdi
  %36 = load i64* %rsi
  %37 = inttoptr i64 %36 to i64*
  %38 = trunc i64 %35 to i32
  %39 = call i64 @decode(i32 %38, i64* %37)
  store i64 %39, i64* %rax
  store i64 %39, i64* %rax
  store volatile i64 49842, i64* @assembly_address
  %40 = load i64* %rax
  %41 = trunc i64 %40 to i32
  store i32 %41, i32* %stack_var_-12
  store volatile i64 49845, i64* @assembly_address
  %42 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %43 = zext i32 %42 to i64
  store i64 %43, i64* %rax
  store volatile i64 49851, i64* @assembly_address
  %44 = load i64* %rax
  %45 = trunc i64 %44 to i32
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = and i32 %45, %47
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %49 = icmp eq i32 %48, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i32 %48, 0
  store i1 %50, i1* %sf
  %51 = trunc i32 %48 to i8
  %52 = call i8 @llvm.ctpop.i8(i8 %51)
  %53 = and i8 %52, 1
  %54 = icmp eq i8 %53, 0
  store i1 %54, i1* %pf
  store volatile i64 49853, i64* @assembly_address
  %55 = load i1* %zf
  %56 = icmp eq i1 %55, false
  br i1 %56, label %block_c2d9, label %block_c2bf

block_c2bf:                                       ; preds = %block_c2a1
  store volatile i64 49855, i64* @assembly_address
  %57 = load i32* %stack_var_-12
  %58 = and i32 %57, 15
  %59 = icmp ugt i32 %58, 15
  %60 = icmp ult i32 %57, 0
  %61 = xor i32 %57, 0
  %62 = and i32 %61, 0
  %63 = icmp slt i32 %62, 0
  store i1 %59, i1* %az
  store i1 %60, i1* %cf
  store i1 %63, i1* %of
  %64 = icmp eq i32 %57, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i32 %57, 0
  store i1 %65, i1* %sf
  %66 = trunc i32 %57 to i8
  %67 = call i8 @llvm.ctpop.i8(i8 %66)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  store volatile i64 49859, i64* @assembly_address
  %70 = load i1* %zf
  br i1 %70, label %block_c2d9, label %block_c2c5

block_c2c5:                                       ; preds = %block_c2bf
  store volatile i64 49861, i64* @assembly_address
  %71 = load i32* %stack_var_-12
  %72 = zext i32 %71 to i64
  store i64 %72, i64* %rdx
  store volatile i64 49864, i64* @assembly_address
  %73 = load i32* %stack_var_-32
  %74 = zext i32 %73 to i64
  store i64 %74, i64* %rax
  store volatile i64 49867, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rsi
  store volatile i64 49874, i64* @assembly_address
  %75 = load i64* %rax
  %76 = trunc i64 %75 to i32
  %77 = zext i32 %76 to i64
  store i64 %77, i64* %rdi
  store volatile i64 49876, i64* @assembly_address
  %78 = load i64* %rdi
  %79 = load i64* %rsi
  %80 = inttoptr i64 %79 to i8*
  %81 = load i64* %rdx
  %82 = trunc i64 %78 to i32
  %83 = call i64 @write_buf(i32 %82, i8* %80, i64 %81)
  store i64 %83, i64* %rax
  store i64 %83, i64* %rax
  br label %block_c2d9

block_c2d9:                                       ; preds = %block_c2c5, %block_c2bf, %block_c2a1, %block_c27a
  store volatile i64 49881, i64* @assembly_address
  %84 = load i32* bitcast (i64* @global_var_21a150 to i32*)
  %85 = zext i32 %84 to i64
  store i64 %85, i64* %rax
  store volatile i64 49887, i64* @assembly_address
  %86 = load i64* %rax
  %87 = trunc i64 %86 to i32
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  %90 = and i32 %87, %89
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %91 = icmp eq i32 %90, 0
  store i1 %91, i1* %zf
  %92 = icmp slt i32 %90, 0
  store i1 %92, i1* %sf
  %93 = trunc i32 %90 to i8
  %94 = call i8 @llvm.ctpop.i8(i8 %93)
  %95 = and i8 %94, 1
  %96 = icmp eq i8 %95, 0
  store i1 %96, i1* %pf
  store volatile i64 49889, i64* @assembly_address
  %97 = load i1* %zf
  br i1 %97, label %block_c2a1, label %block_c2e3

block_c2e3:                                       ; preds = %block_c2d9
  store volatile i64 49891, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 49896, i64* @assembly_address
  %98 = load i64* %stack_var_-8
  store i64 %98, i64* %rbp
  %99 = ptrtoint i64* %stack_var_0 to i64
  store i64 %99, i64* %rsp
  store volatile i64 49897, i64* @assembly_address
  %100 = load i64* %rax
  ret i64 %100
}

declare i64 @231(i64, i32)

declare i64 @232(i64, i64)

define i64 @unlzw(i32 %arg1, i64 %arg2) {
block_c2ea:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-100 = alloca i64*
  %1 = alloca i32
  %stack_var_-88 = alloca i64
  %stack_var_-16 = alloca i32
  %2 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-104 = alloca i32
  %stack_var_-92 = alloca i32
  %stack_var_-96 = alloca i32
  %stack_var_-80 = alloca i32
  %3 = alloca i64
  %stack_var_-56 = alloca i32
  %4 = alloca i64
  %stack_var_-64 = alloca i32
  %5 = alloca i64
  %stack_var_-120 = alloca i64*
  %6 = alloca i32
  %stack_var_-124 = alloca i32
  %stack_var_-72 = alloca i32
  %7 = alloca i64
  %stack_var_-116 = alloca i32
  %stack_var_-48 = alloca i64
  %stack_var_-112 = alloca i32
  %stack_var_-108 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-144 = alloca i32
  %stack_var_-140 = alloca i32
  %stack_var_-152 = alloca i64
  %stack_var_-8 = alloca i64
  %8 = alloca i32
  %9 = alloca i64*
  %10 = alloca i32
  %11 = alloca i64
  %12 = alloca i64
  %13 = alloca i32
  %14 = alloca i64
  %15 = alloca i32
  %16 = alloca i64
  %17 = alloca i32
  %18 = alloca i64*
  %19 = alloca i32
  %20 = alloca i32
  %21 = alloca i64*
  %22 = alloca i32
  %23 = alloca i64
  %24 = alloca i64*
  %25 = alloca i32
  %26 = alloca i32
  %27 = alloca i64
  %28 = alloca i32
  %29 = alloca i64*
  %30 = alloca i32
  %31 = alloca i32
  %32 = alloca i64
  %33 = alloca i32
  %34 = alloca i64
  %35 = alloca i32
  %36 = alloca i64
  %37 = alloca i32
  %38 = alloca i64
  %39 = alloca i64
  %40 = alloca i32
  %41 = alloca i64
  %42 = alloca i32
  %43 = alloca i64
  %44 = alloca i32
  %45 = alloca i64
  %46 = alloca i32
  %47 = alloca i32
  %48 = alloca i64
  %49 = alloca i32
  %50 = alloca i32
  %51 = alloca i64
  store volatile i64 49898, i64* @assembly_address
  %52 = load i64* %rbp
  store i64 %52, i64* %stack_var_-8
  %53 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %53, i64* %rsp
  store volatile i64 49899, i64* @assembly_address
  %54 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %54, i64* %rbp
  store volatile i64 49902, i64* @assembly_address
  %55 = load i64* %rsp
  %56 = sub i64 %55, 144
  %57 = and i64 %55, 15
  %58 = icmp ugt i64 %57, 15
  %59 = icmp ult i64 %55, 144
  %60 = xor i64 %55, 144
  %61 = xor i64 %55, %56
  %62 = and i64 %60, %61
  %63 = icmp slt i64 %62, 0
  store i1 %58, i1* %az
  store i1 %59, i1* %cf
  store i1 %63, i1* %of
  %64 = icmp eq i64 %56, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i64 %56, 0
  store i1 %65, i1* %sf
  %66 = trunc i64 %56 to i8
  %67 = call i8 @llvm.ctpop.i8(i8 %66)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  %70 = ptrtoint i64* %stack_var_-152 to i64
  store i64 %70, i64* %rsp
  store volatile i64 49909, i64* @assembly_address
  %71 = load i64* %rdi
  %72 = trunc i64 %71 to i32
  store i32 %72, i32* %stack_var_-140
  store volatile i64 49915, i64* @assembly_address
  %73 = load i64* %rsi
  %74 = trunc i64 %73 to i32
  store i32 %74, i32* %stack_var_-144
  store volatile i64 49921, i64* @assembly_address
  %75 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rdx
  store volatile i64 49927, i64* @assembly_address
  %77 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %78 = zext i32 %77 to i64
  store i64 %78, i64* %rax
  store volatile i64 49933, i64* @assembly_address
  %79 = load i64* %rdx
  %80 = trunc i64 %79 to i32
  %81 = load i64* %rax
  %82 = trunc i64 %81 to i32
  %83 = sub i32 %80, %82
  %84 = and i32 %80, 15
  %85 = and i32 %82, 15
  %86 = sub i32 %84, %85
  %87 = icmp ugt i32 %86, 15
  %88 = icmp ult i32 %80, %82
  %89 = xor i32 %80, %82
  %90 = xor i32 %80, %83
  %91 = and i32 %89, %90
  %92 = icmp slt i32 %91, 0
  store i1 %87, i1* %az
  store i1 %88, i1* %cf
  store i1 %92, i1* %of
  %93 = icmp eq i32 %83, 0
  store i1 %93, i1* %zf
  %94 = icmp slt i32 %83, 0
  store i1 %94, i1* %sf
  %95 = trunc i32 %83 to i8
  %96 = call i8 @llvm.ctpop.i8(i8 %95)
  %97 = and i8 %96, 1
  %98 = icmp eq i8 %97, 0
  store i1 %98, i1* %pf
  store volatile i64 49935, i64* @assembly_address
  %99 = load i1* %cf
  %100 = icmp eq i1 %99, false
  br i1 %100, label %block_c332, label %block_c311

block_c311:                                       ; preds = %block_c2ea
  store volatile i64 49937, i64* @assembly_address
  %101 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %102 = zext i32 %101 to i64
  store i64 %102, i64* %rax
  store volatile i64 49943, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 49946, i64* @assembly_address
  %103 = load i64* %rdx
  %104 = trunc i64 %103 to i32
  store i32 %104, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 49952, i64* @assembly_address
  %105 = load i64* %rax
  %106 = trunc i64 %105 to i32
  %107 = zext i32 %106 to i64
  store i64 %107, i64* %rdx
  store volatile i64 49954, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 49961, i64* @assembly_address
  %108 = load i64* %rdx
  %109 = load i64* %rax
  %110 = mul i64 %109, 1
  %111 = add i64 %108, %110
  %112 = inttoptr i64 %111 to i8*
  %113 = load i8* %112
  %114 = zext i8 %113 to i64
  store i64 %114, i64* %rax
  store volatile i64 49965, i64* @assembly_address
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i8
  %117 = zext i8 %116 to i64
  store i64 %117, i64* %rax
  store volatile i64 49968, i64* @assembly_address
  br label %block_c33c

block_c332:                                       ; preds = %block_c2ea
  store volatile i64 49970, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 49975, i64* @assembly_address
  %118 = load i64* %rdi
  %119 = trunc i64 %118 to i32
  %120 = call i64 @fill_inbuf(i32 %119)
  store i64 %120, i64* %rax
  store i64 %120, i64* %rax
  br label %block_c33c

block_c33c:                                       ; preds = %block_c332, %block_c311
  store volatile i64 49980, i64* @assembly_address
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i32
  store i32 %122, i32* bitcast (i64* @global_var_216098 to i32*)
  store volatile i64 49986, i64* @assembly_address
  %123 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %124 = zext i32 %123 to i64
  store i64 %124, i64* %rax
  store volatile i64 49992, i64* @assembly_address
  %125 = load i64* %rax
  %126 = trunc i64 %125 to i32
  %127 = and i32 %126, 128
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %128 = icmp eq i32 %127, 0
  store i1 %128, i1* %zf
  %129 = icmp slt i32 %127, 0
  store i1 %129, i1* %sf
  %130 = trunc i32 %127 to i8
  %131 = call i8 @llvm.ctpop.i8(i8 %130)
  %132 = and i8 %131, 1
  %133 = icmp eq i8 %132, 0
  store i1 %133, i1* %pf
  %134 = zext i32 %127 to i64
  store i64 %134, i64* %rax
  store volatile i64 49997, i64* @assembly_address
  %135 = load i64* %rax
  %136 = trunc i64 %135 to i32
  store i32 %136, i32* bitcast (i64* @global_var_216548 to i32*)
  store volatile i64 50003, i64* @assembly_address
  %137 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %138 = zext i32 %137 to i64
  store i64 %138, i64* %rax
  store volatile i64 50009, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  %141 = and i32 %140, 96
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %142 = icmp eq i32 %141, 0
  store i1 %142, i1* %zf
  %143 = icmp slt i32 %141, 0
  store i1 %143, i1* %sf
  %144 = trunc i32 %141 to i8
  %145 = call i8 @llvm.ctpop.i8(i8 %144)
  %146 = and i8 %145, 1
  %147 = icmp eq i8 %146, 0
  store i1 %147, i1* %pf
  %148 = zext i32 %141 to i64
  store i64 %148, i64* %rax
  store volatile i64 50012, i64* @assembly_address
  %149 = load i64* %rax
  %150 = trunc i64 %149 to i32
  %151 = load i64* %rax
  %152 = trunc i64 %151 to i32
  %153 = and i32 %150, %152
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %154 = icmp eq i32 %153, 0
  store i1 %154, i1* %zf
  %155 = icmp slt i32 %153, 0
  store i1 %155, i1* %sf
  %156 = trunc i32 %153 to i8
  %157 = call i8 @llvm.ctpop.i8(i8 %156)
  %158 = and i8 %157, 1
  %159 = icmp eq i8 %158, 0
  store i1 %159, i1* %pf
  store volatile i64 50014, i64* @assembly_address
  %160 = load i1* %zf
  br i1 %160, label %block_c3b5, label %block_c360

block_c360:                                       ; preds = %block_c33c
  store volatile i64 50016, i64* @assembly_address
  %161 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %162 = zext i32 %161 to i64
  store i64 %162, i64* %rax
  store volatile i64 50022, i64* @assembly_address
  %163 = load i64* %rax
  %164 = trunc i64 %163 to i32
  %165 = load i64* %rax
  %166 = trunc i64 %165 to i32
  %167 = and i32 %164, %166
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %168 = icmp eq i32 %167, 0
  store i1 %168, i1* %zf
  %169 = icmp slt i32 %167, 0
  store i1 %169, i1* %sf
  %170 = trunc i32 %167 to i8
  %171 = call i8 @llvm.ctpop.i8(i8 %170)
  %172 = and i8 %171, 1
  %173 = icmp eq i8 %172, 0
  store i1 %173, i1* %pf
  store volatile i64 50024, i64* @assembly_address
  %174 = load i1* %zf
  %175 = icmp eq i1 %174, false
  br i1 %175, label %block_c3a1, label %block_c36a

block_c36a:                                       ; preds = %block_c360
  store volatile i64 50026, i64* @assembly_address
  %176 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %177 = zext i32 %176 to i64
  store i64 %177, i64* %rax
  store volatile i64 50032, i64* @assembly_address
  %178 = load i64* %rax
  %179 = trunc i64 %178 to i32
  %180 = and i32 %179, 96
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %181 = icmp eq i32 %180, 0
  store i1 %181, i1* %zf
  %182 = icmp slt i32 %180, 0
  store i1 %182, i1* %sf
  %183 = trunc i32 %180 to i8
  %184 = call i8 @llvm.ctpop.i8(i8 %183)
  %185 = and i8 %184, 1
  %186 = icmp eq i8 %185, 0
  store i1 %186, i1* %pf
  %187 = zext i32 %180 to i64
  store i64 %187, i64* %rax
  store volatile i64 50035, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  %190 = zext i32 %189 to i64
  store i64 %190, i64* %rcx
  store volatile i64 50037, i64* @assembly_address
  %191 = load i64* @global_var_25f4c8
  store i64 %191, i64* %rdx
  store volatile i64 50044, i64* @assembly_address
  %192 = load i64* @global_var_216580
  store i64 %192, i64* %rax
  store volatile i64 50051, i64* @assembly_address
  %193 = load i64* %rcx
  %194 = trunc i64 %193 to i32
  %195 = zext i32 %194 to i64
  store i64 %195, i64* %r8
  store volatile i64 50054, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 50061, i64* @assembly_address
  store i64 ptrtoint ([38 x i8]* @global_var_120c8 to i64), i64* %rsi
  store volatile i64 50068, i64* @assembly_address
  %196 = load i64* %rax
  store i64 %196, i64* %rdi
  store volatile i64 50071, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 50076, i64* @assembly_address
  %197 = load i64* %rdi
  %198 = inttoptr i64 %197 to %_IO_FILE*
  %199 = load i64* %rsi
  %200 = inttoptr i64 %199 to i8*
  %201 = load i64* %rdx
  %202 = inttoptr i64 %201 to i8*
  %203 = load i64* %rcx
  %204 = inttoptr i64 %203 to i8*
  %205 = load i64* %r8
  %206 = trunc i64 %205 to i32
  %207 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %198, i8* %200, i8* %202, i8* %204, i32 %206)
  %208 = sext i32 %207 to i64
  store i64 %208, i64* %rax
  %209 = sext i32 %207 to i64
  store i64 %209, i64* %rax
  br label %block_c3a1

block_c3a1:                                       ; preds = %block_c36a, %block_c360
  store volatile i64 50081, i64* @assembly_address
  %210 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %211 = zext i32 %210 to i64
  store i64 %211, i64* %rax
  store volatile i64 50087, i64* @assembly_address
  %212 = load i64* %rax
  %213 = trunc i64 %212 to i32
  %214 = load i64* %rax
  %215 = trunc i64 %214 to i32
  %216 = and i32 %213, %215
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %217 = icmp eq i32 %216, 0
  store i1 %217, i1* %zf
  %218 = icmp slt i32 %216, 0
  store i1 %218, i1* %sf
  %219 = trunc i32 %216 to i8
  %220 = call i8 @llvm.ctpop.i8(i8 %219)
  %221 = and i8 %220, 1
  %222 = icmp eq i8 %221, 0
  store i1 %222, i1* %pf
  store volatile i64 50089, i64* @assembly_address
  %223 = load i1* %zf
  %224 = icmp eq i1 %223, false
  br i1 %224, label %block_c3b5, label %block_c3ab

block_c3ab:                                       ; preds = %block_c3a1
  store volatile i64 50091, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_c3b5

block_c3b5:                                       ; preds = %block_c3ab, %block_c3a1, %block_c33c
  store volatile i64 50101, i64* @assembly_address
  %225 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %226 = zext i32 %225 to i64
  store i64 %226, i64* %rax
  store volatile i64 50107, i64* @assembly_address
  %227 = load i64* %rax
  %228 = trunc i64 %227 to i32
  %229 = and i32 %228, 31
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %230 = icmp eq i32 %229, 0
  store i1 %230, i1* %zf
  %231 = icmp slt i32 %229, 0
  store i1 %231, i1* %sf
  %232 = trunc i32 %229 to i8
  %233 = call i8 @llvm.ctpop.i8(i8 %232)
  %234 = and i8 %233, 1
  %235 = icmp eq i8 %234, 0
  store i1 %235, i1* %pf
  %236 = zext i32 %229 to i64
  store i64 %236, i64* %rax
  store volatile i64 50110, i64* @assembly_address
  %237 = load i64* %rax
  %238 = trunc i64 %237 to i32
  store i32 %238, i32* bitcast (i64* @global_var_216098 to i32*)
  store volatile i64 50116, i64* @assembly_address
  %239 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %240 = zext i32 %239 to i64
  store i64 %240, i64* %rax
  store volatile i64 50122, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 50127, i64* @assembly_address
  %241 = load i64* %rax
  %242 = trunc i64 %241 to i32
  %243 = zext i32 %242 to i64
  store i64 %243, i64* %rcx
  store volatile i64 50129, i64* @assembly_address
  %244 = load i64* %rdx
  %245 = load i64* %rcx
  %246 = trunc i64 %245 to i8
  %247 = zext i8 %246 to i64
  %248 = and i64 %247, 63
  %249 = load i1* %of
  %250 = icmp eq i64 %248, 0
  br i1 %250, label %267, label %251

; <label>:251                                     ; preds = %block_c3b5
  %252 = shl i64 %244, %248
  %253 = icmp eq i64 %252, 0
  store i1 %253, i1* %zf
  %254 = icmp slt i64 %252, 0
  store i1 %254, i1* %sf
  %255 = trunc i64 %252 to i8
  %256 = call i8 @llvm.ctpop.i8(i8 %255)
  %257 = and i8 %256, 1
  %258 = icmp eq i8 %257, 0
  store i1 %258, i1* %pf
  store i64 %252, i64* %rdx
  %259 = sub i64 %248, 1
  %260 = shl i64 %244, %259
  %261 = lshr i64 %260, 63
  %262 = trunc i64 %261 to i1
  store i1 %262, i1* %cf
  %263 = lshr i64 %252, 63
  %264 = icmp ne i64 %263, %261
  %265 = icmp eq i64 %248, 1
  %266 = select i1 %265, i1 %264, i1 %249
  store i1 %266, i1* %of
  br label %267

; <label>:267                                     ; preds = %block_c3b5, %251
  store volatile i64 50132, i64* @assembly_address
  %268 = load i64* %rdx
  store i64 %268, i64* %rax
  store volatile i64 50135, i64* @assembly_address
  %269 = load i64* %rax
  store i64 %269, i64* %stack_var_-40
  store volatile i64 50139, i64* @assembly_address
  %270 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %271 = zext i32 %270 to i64
  store i64 %271, i64* %rax
  store volatile i64 50145, i64* @assembly_address
  %272 = load i64* %rax
  %273 = trunc i64 %272 to i32
  %274 = trunc i64 %272 to i32
  store i32 %274, i32* %50
  store i32 16, i32* %49
  %275 = sub i32 %273, 16
  %276 = and i32 %273, 15
  %277 = icmp ugt i32 %276, 15
  %278 = icmp ult i32 %273, 16
  %279 = xor i32 %273, 16
  %280 = xor i32 %273, %275
  %281 = and i32 %279, %280
  %282 = icmp slt i32 %281, 0
  store i1 %277, i1* %az
  store i1 %278, i1* %cf
  store i1 %282, i1* %of
  %283 = icmp eq i32 %275, 0
  store i1 %283, i1* %zf
  %284 = icmp slt i32 %275, 0
  store i1 %284, i1* %sf
  %285 = trunc i32 %275 to i8
  %286 = call i8 @llvm.ctpop.i8(i8 %285)
  %287 = and i8 %286, 1
  %288 = icmp eq i8 %287, 0
  store i1 %288, i1* %pf
  store volatile i64 50148, i64* @assembly_address
  %289 = load i32* %50
  %290 = sext i32 %289 to i64
  %291 = load i32* %49
  %292 = trunc i64 %290 to i32
  %293 = icmp sle i32 %292, %291
  br i1 %293, label %block_c432, label %block_c3e6

block_c3e6:                                       ; preds = %267
  store volatile i64 50150, i64* @assembly_address
  %294 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %295 = zext i32 %294 to i64
  store i64 %295, i64* %rcx
  store volatile i64 50156, i64* @assembly_address
  %296 = load i64* @global_var_25f4c8
  store i64 %296, i64* %rdx
  store volatile i64 50163, i64* @assembly_address
  %297 = load i64* @global_var_216580
  store i64 %297, i64* %rax
  store volatile i64 50170, i64* @assembly_address
  store i64 16, i64* %r9
  store volatile i64 50176, i64* @assembly_address
  %298 = load i64* %rcx
  %299 = trunc i64 %298 to i32
  %300 = zext i32 %299 to i64
  store i64 %300, i64* %r8
  store volatile i64 50179, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 50186, i64* @assembly_address
  store i64 ptrtoint ([59 x i8]* @global_var_120f0 to i64), i64* %rsi
  store volatile i64 50193, i64* @assembly_address
  %301 = load i64* %rax
  store i64 %301, i64* %rdi
  store volatile i64 50196, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 50201, i64* @assembly_address
  %302 = load i64* %rdi
  %303 = inttoptr i64 %302 to %_IO_FILE*
  %304 = load i64* %rsi
  %305 = inttoptr i64 %304 to i8*
  %306 = load i64* %rdx
  %307 = inttoptr i64 %306 to i8*
  %308 = load i64* %rcx
  %309 = inttoptr i64 %308 to i8*
  %310 = load i64* %r8
  %311 = load i64* %r9
  %312 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %303, i8* %305, i8* %307, i8* %309, i64 %310, i64 %311)
  %313 = sext i32 %312 to i64
  store i64 %313, i64* %rax
  %314 = sext i32 %312 to i64
  store i64 %314, i64* %rax
  store volatile i64 50206, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 50216, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 50221, i64* @assembly_address
  br label %block_caf8

block_c432:                                       ; preds = %267
  store volatile i64 50226, i64* @assembly_address
  %315 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %316 = zext i32 %315 to i64
  store i64 %316, i64* %rax
  store volatile i64 50232, i64* @assembly_address
  %317 = load i64* %rax
  %318 = trunc i64 %317 to i32
  store i32 %318, i32* %stack_var_-108
  store volatile i64 50235, i64* @assembly_address
  store i32 9, i32* %stack_var_-112
  store volatile i64 50242, i64* @assembly_address
  %319 = load i32* %stack_var_-112
  %320 = zext i32 %319 to i64
  store i64 %320, i64* %rax
  store volatile i64 50245, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 50250, i64* @assembly_address
  %321 = load i64* %rax
  %322 = trunc i64 %321 to i32
  %323 = zext i32 %322 to i64
  store i64 %323, i64* %rcx
  store volatile i64 50252, i64* @assembly_address
  %324 = load i64* %rdx
  %325 = load i64* %rcx
  %326 = trunc i64 %325 to i8
  %327 = zext i8 %326 to i64
  %328 = and i64 %327, 63
  %329 = load i1* %of
  %330 = icmp eq i64 %328, 0
  br i1 %330, label %347, label %331

; <label>:331                                     ; preds = %block_c432
  %332 = shl i64 %324, %328
  %333 = icmp eq i64 %332, 0
  store i1 %333, i1* %zf
  %334 = icmp slt i64 %332, 0
  store i1 %334, i1* %sf
  %335 = trunc i64 %332 to i8
  %336 = call i8 @llvm.ctpop.i8(i8 %335)
  %337 = and i8 %336, 1
  %338 = icmp eq i8 %337, 0
  store i1 %338, i1* %pf
  store i64 %332, i64* %rdx
  %339 = sub i64 %328, 1
  %340 = shl i64 %324, %339
  %341 = lshr i64 %340, 63
  %342 = trunc i64 %341 to i1
  store i1 %342, i1* %cf
  %343 = lshr i64 %332, 63
  %344 = icmp ne i64 %343, %341
  %345 = icmp eq i64 %328, 1
  %346 = select i1 %345, i1 %344, i1 %329
  store i1 %346, i1* %of
  br label %347

; <label>:347                                     ; preds = %block_c432, %331
  store volatile i64 50255, i64* @assembly_address
  %348 = load i64* %rdx
  store i64 %348, i64* %rax
  store volatile i64 50258, i64* @assembly_address
  %349 = load i64* %rax
  %350 = sub i64 %349, 1
  %351 = and i64 %349, 15
  %352 = sub i64 %351, 1
  %353 = icmp ugt i64 %352, 15
  %354 = icmp ult i64 %349, 1
  %355 = xor i64 %349, 1
  %356 = xor i64 %349, %350
  %357 = and i64 %355, %356
  %358 = icmp slt i64 %357, 0
  store i1 %353, i1* %az
  store i1 %354, i1* %cf
  store i1 %358, i1* %of
  %359 = icmp eq i64 %350, 0
  store i1 %359, i1* %zf
  %360 = icmp slt i64 %350, 0
  store i1 %360, i1* %sf
  %361 = trunc i64 %350 to i8
  %362 = call i8 @llvm.ctpop.i8(i8 %361)
  %363 = and i8 %362, 1
  %364 = icmp eq i8 %363, 0
  store i1 %364, i1* %pf
  store i64 %350, i64* %rax
  store volatile i64 50262, i64* @assembly_address
  %365 = load i64* %rax
  store i64 %365, i64* %stack_var_-48
  store volatile i64 50266, i64* @assembly_address
  %366 = load i32* %stack_var_-112
  %367 = zext i32 %366 to i64
  store i64 %367, i64* %rax
  store volatile i64 50269, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 50274, i64* @assembly_address
  %368 = load i64* %rax
  %369 = trunc i64 %368 to i32
  %370 = zext i32 %369 to i64
  store i64 %370, i64* %rcx
  store volatile i64 50276, i64* @assembly_address
  %371 = load i64* %rdx
  %372 = trunc i64 %371 to i32
  %373 = load i64* %rcx
  %374 = trunc i64 %373 to i8
  %375 = zext i8 %374 to i32
  %376 = and i32 %375, 31
  %377 = load i1* %of
  %378 = icmp eq i32 %376, 0
  br i1 %378, label %396, label %379

; <label>:379                                     ; preds = %347
  %380 = shl i32 %372, %376
  %381 = icmp eq i32 %380, 0
  store i1 %381, i1* %zf
  %382 = icmp slt i32 %380, 0
  store i1 %382, i1* %sf
  %383 = trunc i32 %380 to i8
  %384 = call i8 @llvm.ctpop.i8(i8 %383)
  %385 = and i8 %384, 1
  %386 = icmp eq i8 %385, 0
  store i1 %386, i1* %pf
  %387 = zext i32 %380 to i64
  store i64 %387, i64* %rdx
  %388 = sub i32 %376, 1
  %389 = shl i32 %372, %388
  %390 = lshr i32 %389, 31
  %391 = trunc i32 %390 to i1
  store i1 %391, i1* %cf
  %392 = lshr i32 %380, 31
  %393 = icmp ne i32 %392, %390
  %394 = icmp eq i32 %376, 1
  %395 = select i1 %394, i1 %393, i1 %377
  store i1 %395, i1* %of
  br label %396

; <label>:396                                     ; preds = %347, %379
  store volatile i64 50278, i64* @assembly_address
  %397 = load i64* %rdx
  %398 = trunc i64 %397 to i32
  %399 = zext i32 %398 to i64
  store i64 %399, i64* %rax
  store volatile i64 50280, i64* @assembly_address
  %400 = load i64* %rax
  %401 = trunc i64 %400 to i32
  %402 = sub i32 %401, 1
  %403 = and i32 %401, 15
  %404 = sub i32 %403, 1
  %405 = icmp ugt i32 %404, 15
  %406 = icmp ult i32 %401, 1
  %407 = xor i32 %401, 1
  %408 = xor i32 %401, %402
  %409 = and i32 %407, %408
  %410 = icmp slt i32 %409, 0
  store i1 %405, i1* %az
  store i1 %406, i1* %cf
  store i1 %410, i1* %of
  %411 = icmp eq i32 %402, 0
  store i1 %411, i1* %zf
  %412 = icmp slt i32 %402, 0
  store i1 %412, i1* %sf
  %413 = trunc i32 %402 to i8
  %414 = call i8 @llvm.ctpop.i8(i8 %413)
  %415 = and i8 %414, 1
  %416 = icmp eq i8 %415, 0
  store i1 %416, i1* %pf
  %417 = zext i32 %402 to i64
  store i64 %417, i64* %rax
  store volatile i64 50283, i64* @assembly_address
  %418 = load i64* %rax
  %419 = trunc i64 %418 to i32
  store i32 %419, i32* %stack_var_-116
  store volatile i64 50286, i64* @assembly_address
  %420 = trunc i64 -1 to i32
  store i32 %420, i32* %stack_var_-72
  store volatile i64 50294, i64* @assembly_address
  store i32 0, i32* %stack_var_-124
  store volatile i64 50301, i64* @assembly_address
  %421 = inttoptr i32 0 to i64*
  store i64* %421, i64** %stack_var_-120
  store volatile i64 50308, i64* @assembly_address
  %422 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %423 = zext i32 %422 to i64
  store i64 %423, i64* %rax
  store volatile i64 50314, i64* @assembly_address
  %424 = load i64* %rax
  %425 = trunc i64 %424 to i32
  %426 = load i1* %of
  %427 = shl i32 %425, 3
  %428 = icmp eq i32 %427, 0
  store i1 %428, i1* %zf
  %429 = icmp slt i32 %427, 0
  store i1 %429, i1* %sf
  %430 = trunc i32 %427 to i8
  %431 = call i8 @llvm.ctpop.i8(i8 %430)
  %432 = and i8 %431, 1
  %433 = icmp eq i8 %432, 0
  store i1 %433, i1* %pf
  %434 = zext i32 %427 to i64
  store i64 %434, i64* %rax
  %435 = shl i32 %425, 2
  %436 = lshr i32 %435, 31
  %437 = trunc i32 %436 to i1
  store i1 %437, i1* %cf
  %438 = lshr i32 %427, 31
  %439 = icmp ne i32 %438, %436
  %440 = select i1 false, i1 %439, i1 %426
  store i1 %440, i1* %of
  store volatile i64 50317, i64* @assembly_address
  %441 = load i64* %rax
  %442 = trunc i64 %441 to i32
  %443 = zext i32 %442 to i64
  store i64 %443, i64* %rax
  store volatile i64 50319, i64* @assembly_address
  %444 = load i64* %rax
  %445 = trunc i64 %444 to i32
  store i32 %445, i32* %stack_var_-64
  store volatile i64 50323, i64* @assembly_address
  %446 = load i32* bitcast (i64* @global_var_216548 to i32*)
  %447 = zext i32 %446 to i64
  store i64 %447, i64* %rax
  store volatile i64 50329, i64* @assembly_address
  %448 = load i64* %rax
  %449 = trunc i64 %448 to i32
  %450 = load i64* %rax
  %451 = trunc i64 %450 to i32
  %452 = and i32 %449, %451
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %453 = icmp eq i32 %452, 0
  store i1 %453, i1* %zf
  %454 = icmp slt i32 %452, 0
  store i1 %454, i1* %sf
  %455 = trunc i32 %452 to i8
  %456 = call i8 @llvm.ctpop.i8(i8 %455)
  %457 = and i8 %456, 1
  %458 = icmp eq i8 %457, 0
  store i1 %458, i1* %pf
  store volatile i64 50331, i64* @assembly_address
  %459 = load i1* %zf
  br i1 %459, label %block_c4a4, label %block_c49d

block_c49d:                                       ; preds = %396
  store volatile i64 50333, i64* @assembly_address
  store i64 257, i64* %rax
  store volatile i64 50338, i64* @assembly_address
  br label %block_c4a9

block_c4a4:                                       ; preds = %396
  store volatile i64 50340, i64* @assembly_address
  store i64 256, i64* %rax
  br label %block_c4a9

block_c4a9:                                       ; preds = %block_c4a4, %block_c49d
  store volatile i64 50345, i64* @assembly_address
  %460 = load i64* %rax
  %461 = trunc i64 %460 to i32
  store i32 %461, i32* %stack_var_-56
  store volatile i64 50349, i64* @assembly_address
  store i64 256, i64* %rdx
  store volatile i64 50354, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 50359, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rdi
  store volatile i64 50366, i64* @assembly_address
  %462 = load i64* %rdi
  %463 = inttoptr i64 %462 to i64*
  %464 = load i64* %rsi
  %465 = trunc i64 %464 to i32
  %466 = load i64* %rdx
  %467 = trunc i64 %466 to i32
  %468 = call i64* @memset(i64* %463, i32 %465, i32 %467)
  %469 = ptrtoint i64* %468 to i64
  store i64 %469, i64* %rax
  %470 = ptrtoint i64* %468 to i64
  store i64 %470, i64* %rax
  store volatile i64 50371, i64* @assembly_address
  %471 = trunc i64 255 to i32
  store i32 %471, i32* %stack_var_-80
  store volatile i64 50379, i64* @assembly_address
  br label %block_c4e8

block_c4cd:                                       ; preds = %block_c4e8
  store volatile i64 50381, i64* @assembly_address
  %472 = load i32* %stack_var_-80
  %473 = sext i32 %472 to i64
  store i64 %473, i64* %rax
  store volatile i64 50385, i64* @assembly_address
  %474 = load i64* %rax
  %475 = trunc i64 %474 to i32
  %476 = zext i32 %475 to i64
  store i64 %476, i64* %rcx
  store volatile i64 50387, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdx
  store volatile i64 50394, i64* @assembly_address
  %477 = load i32* %stack_var_-80
  %478 = sext i32 %477 to i64
  store i64 %478, i64* %rax
  store volatile i64 50398, i64* @assembly_address
  %479 = load i64* %rax
  %480 = load i64* %rdx
  %481 = add i64 %479, %480
  %482 = and i64 %479, 15
  %483 = and i64 %480, 15
  %484 = add i64 %482, %483
  %485 = icmp ugt i64 %484, 15
  %486 = icmp ult i64 %481, %479
  %487 = xor i64 %479, %481
  %488 = xor i64 %480, %481
  %489 = and i64 %487, %488
  %490 = icmp slt i64 %489, 0
  store i1 %485, i1* %az
  store i1 %486, i1* %cf
  store i1 %490, i1* %of
  %491 = icmp eq i64 %481, 0
  store i1 %491, i1* %zf
  %492 = icmp slt i64 %481, 0
  store i1 %492, i1* %sf
  %493 = trunc i64 %481 to i8
  %494 = call i8 @llvm.ctpop.i8(i8 %493)
  %495 = and i8 %494, 1
  %496 = icmp eq i8 %495, 0
  store i1 %496, i1* %pf
  store i64 %481, i64* %rax
  store volatile i64 50401, i64* @assembly_address
  %497 = load i64* %rcx
  %498 = trunc i64 %497 to i8
  %499 = load i64* %rax
  %500 = inttoptr i64 %499 to i8*
  store i8 %498, i8* %500
  store volatile i64 50403, i64* @assembly_address
  %501 = load i32* %stack_var_-80
  %502 = sext i32 %501 to i64
  %503 = sub i64 %502, 1
  %504 = and i64 %502, 15
  %505 = sub i64 %504, 1
  %506 = icmp ugt i64 %505, 15
  %507 = icmp ult i64 %502, 1
  %508 = xor i64 %502, 1
  %509 = xor i64 %502, %503
  %510 = and i64 %508, %509
  %511 = icmp slt i64 %510, 0
  store i1 %506, i1* %az
  store i1 %507, i1* %cf
  store i1 %511, i1* %of
  %512 = icmp eq i64 %503, 0
  store i1 %512, i1* %zf
  %513 = icmp slt i64 %503, 0
  store i1 %513, i1* %sf
  %514 = trunc i64 %503 to i8
  %515 = call i8 @llvm.ctpop.i8(i8 %514)
  %516 = and i8 %515, 1
  %517 = icmp eq i8 %516, 0
  store i1 %517, i1* %pf
  %518 = trunc i64 %503 to i32
  store i32 %518, i32* %stack_var_-80
  br label %block_c4e8

block_c4e8:                                       ; preds = %block_c4cd, %block_c4a9
  store volatile i64 50408, i64* @assembly_address
  %519 = load i32* %stack_var_-80
  %520 = sext i32 %519 to i64
  %521 = and i64 %520, 15
  %522 = icmp ugt i64 %521, 15
  %523 = icmp ult i64 %520, 0
  %524 = xor i64 %520, 0
  %525 = and i64 %524, 0
  %526 = icmp slt i64 %525, 0
  store i1 %522, i1* %az
  store i1 %523, i1* %cf
  store i1 %526, i1* %of
  %527 = icmp eq i64 %520, 0
  store i1 %527, i1* %zf
  %528 = icmp slt i64 %520, 0
  store i1 %528, i1* %sf
  %529 = trunc i64 %520 to i8
  %530 = call i8 @llvm.ctpop.i8(i8 %529)
  %531 = and i8 %530, 1
  %532 = icmp eq i8 %531, 0
  store i1 %532, i1* %pf
  store volatile i64 50413, i64* @assembly_address
  %533 = load i1* %sf
  %534 = icmp eq i1 %533, false
  br i1 %534, label %block_c4cd, label %block_c4ef

block_c4ef:                                       ; preds = %block_caab, %block_c4e8
  store volatile i64 50415, i64* @assembly_address
  br label %block_c4f0

block_c4f0:                                       ; preds = %1820, %1241, %block_c4ef
  store volatile i64 50416, i64* @assembly_address
  %535 = load i32* %stack_var_-64
  %536 = sext i32 %535 to i64
  store i64 %536, i64* %rax
  store volatile i64 50420, i64* @assembly_address
  %537 = load i64* %rax
  %538 = load i1* %of
  %539 = ashr i64 %537, 3
  %540 = icmp eq i64 %539, 0
  store i1 %540, i1* %zf
  %541 = icmp slt i64 %539, 0
  store i1 %541, i1* %sf
  %542 = trunc i64 %539 to i8
  %543 = call i8 @llvm.ctpop.i8(i8 %542)
  %544 = and i8 %543, 1
  %545 = icmp eq i8 %544, 0
  store i1 %545, i1* %pf
  store i64 %539, i64* %rax
  %546 = and i64 4, %537
  %547 = icmp ne i64 %546, 0
  store i1 %547, i1* %cf
  %548 = select i1 false, i1 false, i1 %538
  store i1 %548, i1* %of
  store volatile i64 50424, i64* @assembly_address
  %549 = load i64* %rax
  %550 = trunc i64 %549 to i32
  store i32 %550, i32* %stack_var_-96
  store volatile i64 50427, i64* @assembly_address
  %551 = load i32* %stack_var_-96
  %552 = zext i32 %551 to i64
  store i64 %552, i64* %rdx
  store volatile i64 50430, i64* @assembly_address
  %553 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %554 = zext i32 %553 to i64
  store i64 %554, i64* %rax
  store volatile i64 50436, i64* @assembly_address
  %555 = load i64* %rdx
  %556 = trunc i64 %555 to i32
  %557 = load i64* %rax
  %558 = trunc i64 %557 to i32
  %559 = sub i32 %556, %558
  %560 = and i32 %556, 15
  %561 = and i32 %558, 15
  %562 = sub i32 %560, %561
  %563 = icmp ugt i32 %562, 15
  %564 = icmp ult i32 %556, %558
  %565 = xor i32 %556, %558
  %566 = xor i32 %556, %559
  %567 = and i32 %565, %566
  %568 = icmp slt i32 %567, 0
  store i1 %563, i1* %az
  store i1 %564, i1* %cf
  store i1 %568, i1* %of
  %569 = icmp eq i32 %559, 0
  store i1 %569, i1* %zf
  %570 = icmp slt i32 %559, 0
  store i1 %570, i1* %sf
  %571 = trunc i32 %559 to i8
  %572 = call i8 @llvm.ctpop.i8(i8 %571)
  %573 = and i8 %572, 1
  %574 = icmp eq i8 %573, 0
  store i1 %574, i1* %pf
  store volatile i64 50438, i64* @assembly_address
  %575 = load i1* %cf
  %576 = load i1* %zf
  %577 = or i1 %575, %576
  %578 = icmp ne i1 %577, true
  br i1 %578, label %block_c517, label %block_c508

block_c508:                                       ; preds = %block_c4f0
  store volatile i64 50440, i64* @assembly_address
  %579 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %580 = zext i32 %579 to i64
  store i64 %580, i64* %rdx
  store volatile i64 50446, i64* @assembly_address
  %581 = load i32* %stack_var_-96
  %582 = zext i32 %581 to i64
  store i64 %582, i64* %rax
  store volatile i64 50449, i64* @assembly_address
  %583 = load i64* %rdx
  %584 = trunc i64 %583 to i32
  %585 = load i64* %rax
  %586 = trunc i64 %585 to i32
  %587 = sub i32 %584, %586
  %588 = and i32 %584, 15
  %589 = and i32 %586, 15
  %590 = sub i32 %588, %589
  %591 = icmp ugt i32 %590, 15
  %592 = icmp ult i32 %584, %586
  %593 = xor i32 %584, %586
  %594 = xor i32 %584, %587
  %595 = and i32 %593, %594
  %596 = icmp slt i32 %595, 0
  store i1 %591, i1* %az
  store i1 %592, i1* %cf
  store i1 %596, i1* %of
  %597 = icmp eq i32 %587, 0
  store i1 %597, i1* %zf
  %598 = icmp slt i32 %587, 0
  store i1 %598, i1* %sf
  %599 = trunc i32 %587 to i8
  %600 = call i8 @llvm.ctpop.i8(i8 %599)
  %601 = and i8 %600, 1
  %602 = icmp eq i8 %601, 0
  store i1 %602, i1* %pf
  %603 = zext i32 %587 to i64
  store i64 %603, i64* %rdx
  store volatile i64 50451, i64* @assembly_address
  %604 = load i64* %rdx
  %605 = trunc i64 %604 to i32
  %606 = zext i32 %605 to i64
  store i64 %606, i64* %rax
  store volatile i64 50453, i64* @assembly_address
  br label %block_c51c

block_c517:                                       ; preds = %block_c4f0
  store volatile i64 50455, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_c51c

block_c51c:                                       ; preds = %block_c517, %block_c508
  store volatile i64 50460, i64* @assembly_address
  %607 = load i64* %rax
  %608 = trunc i64 %607 to i32
  store i32 %608, i32* %stack_var_-92
  store volatile i64 50463, i64* @assembly_address
  store i32 0, i32* %stack_var_-104
  store volatile i64 50470, i64* @assembly_address
  br label %block_c552

block_c528:                                       ; preds = %block_c552
  store volatile i64 50472, i64* @assembly_address
  %609 = load i32* %stack_var_-104
  %610 = zext i32 %609 to i64
  store i64 %610, i64* %rdx
  store volatile i64 50475, i64* @assembly_address
  %611 = load i32* %stack_var_-96
  %612 = zext i32 %611 to i64
  store i64 %612, i64* %rax
  store volatile i64 50478, i64* @assembly_address
  %613 = load i64* %rax
  %614 = trunc i64 %613 to i32
  %615 = load i64* %rdx
  %616 = trunc i64 %615 to i32
  %617 = add i32 %614, %616
  %618 = and i32 %614, 15
  %619 = and i32 %616, 15
  %620 = add i32 %618, %619
  %621 = icmp ugt i32 %620, 15
  %622 = icmp ult i32 %617, %614
  %623 = xor i32 %614, %617
  %624 = xor i32 %616, %617
  %625 = and i32 %623, %624
  %626 = icmp slt i32 %625, 0
  store i1 %621, i1* %az
  store i1 %622, i1* %cf
  store i1 %626, i1* %of
  %627 = icmp eq i32 %617, 0
  store i1 %627, i1* %zf
  %628 = icmp slt i32 %617, 0
  store i1 %628, i1* %sf
  %629 = trunc i32 %617 to i8
  %630 = call i8 @llvm.ctpop.i8(i8 %629)
  %631 = and i8 %630, 1
  %632 = icmp eq i8 %631, 0
  store i1 %632, i1* %pf
  %633 = zext i32 %617 to i64
  store i64 %633, i64* %rax
  store volatile i64 50480, i64* @assembly_address
  %634 = load i64* %rax
  %635 = trunc i64 %634 to i32
  %636 = sext i32 %635 to i64
  store i64 %636, i64* %rdx
  store volatile i64 50483, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 50490, i64* @assembly_address
  %637 = load i64* %rdx
  %638 = load i64* %rax
  %639 = mul i64 %638, 1
  %640 = add i64 %637, %639
  %641 = inttoptr i64 %640 to i8*
  %642 = load i8* %641
  %643 = zext i8 %642 to i64
  store i64 %643, i64* %rcx
  store volatile i64 50494, i64* @assembly_address
  %644 = load i32* %stack_var_-104
  %645 = zext i32 %644 to i64
  store i64 %645, i64* %rax
  store volatile i64 50497, i64* @assembly_address
  %646 = load i64* %rax
  %647 = trunc i64 %646 to i32
  %648 = sext i32 %647 to i64
  store i64 %648, i64* %rdx
  store volatile i64 50500, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 50507, i64* @assembly_address
  %649 = load i64* %rcx
  %650 = trunc i64 %649 to i8
  %651 = load i64* %rdx
  %652 = load i64* %rax
  %653 = mul i64 %652, 1
  %654 = add i64 %651, %653
  %655 = inttoptr i64 %654 to i8*
  store i8 %650, i8* %655
  store volatile i64 50510, i64* @assembly_address
  %656 = load i32* %stack_var_-104
  %657 = add i32 %656, 1
  %658 = and i32 %656, 15
  %659 = add i32 %658, 1
  %660 = icmp ugt i32 %659, 15
  %661 = icmp ult i32 %657, %656
  %662 = xor i32 %656, %657
  %663 = xor i32 1, %657
  %664 = and i32 %662, %663
  %665 = icmp slt i32 %664, 0
  store i1 %660, i1* %az
  store i1 %661, i1* %cf
  store i1 %665, i1* %of
  %666 = icmp eq i32 %657, 0
  store i1 %666, i1* %zf
  %667 = icmp slt i32 %657, 0
  store i1 %667, i1* %sf
  %668 = trunc i32 %657 to i8
  %669 = call i8 @llvm.ctpop.i8(i8 %668)
  %670 = and i8 %669, 1
  %671 = icmp eq i8 %670, 0
  store i1 %671, i1* %pf
  store i32 %657, i32* %stack_var_-104
  br label %block_c552

block_c552:                                       ; preds = %block_c528, %block_c51c
  store volatile i64 50514, i64* @assembly_address
  %672 = load i32* %stack_var_-104
  %673 = zext i32 %672 to i64
  store i64 %673, i64* %rax
  store volatile i64 50517, i64* @assembly_address
  %674 = load i64* %rax
  %675 = trunc i64 %674 to i32
  %676 = load i32* %stack_var_-92
  %677 = trunc i64 %674 to i32
  store i32 %677, i32* %47
  store i32 %676, i32* %46
  %678 = sub i32 %675, %676
  %679 = and i32 %675, 15
  %680 = and i32 %676, 15
  %681 = sub i32 %679, %680
  %682 = icmp ugt i32 %681, 15
  %683 = icmp ult i32 %675, %676
  %684 = xor i32 %675, %676
  %685 = xor i32 %675, %678
  %686 = and i32 %684, %685
  %687 = icmp slt i32 %686, 0
  store i1 %682, i1* %az
  store i1 %683, i1* %cf
  store i1 %687, i1* %of
  %688 = icmp eq i32 %678, 0
  store i1 %688, i1* %zf
  %689 = icmp slt i32 %678, 0
  store i1 %689, i1* %sf
  %690 = trunc i32 %678 to i8
  %691 = call i8 @llvm.ctpop.i8(i8 %690)
  %692 = and i8 %691, 1
  %693 = icmp eq i8 %692, 0
  store i1 %693, i1* %pf
  store volatile i64 50520, i64* @assembly_address
  %694 = load i32* %47
  %695 = sext i32 %694 to i64
  %696 = load i32* %46
  %697 = trunc i64 %695 to i32
  %698 = icmp slt i32 %697, %696
  br i1 %698, label %block_c528, label %block_c55a

block_c55a:                                       ; preds = %block_c552
  store volatile i64 50522, i64* @assembly_address
  %699 = load i32* %stack_var_-92
  %700 = zext i32 %699 to i64
  store i64 %700, i64* %rax
  store volatile i64 50525, i64* @assembly_address
  %701 = load i64* %rax
  %702 = trunc i64 %701 to i32
  store i32 %702, i32* bitcast (i64* @global_var_25f4e4 to i32*)
  store volatile i64 50531, i64* @assembly_address
  %703 = trunc i64 0 to i32
  store i32 %703, i32* %stack_var_-64
  store volatile i64 50539, i64* @assembly_address
  %704 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %705 = zext i32 %704 to i64
  store i64 %705, i64* %rax
  store volatile i64 50545, i64* @assembly_address
  %706 = load i64* %rax
  %707 = trunc i64 %706 to i32
  %708 = sub i32 %707, 63
  %709 = and i32 %707, 15
  %710 = sub i32 %709, 15
  %711 = icmp ugt i32 %710, 15
  %712 = icmp ult i32 %707, 63
  %713 = xor i32 %707, 63
  %714 = xor i32 %707, %708
  %715 = and i32 %713, %714
  %716 = icmp slt i32 %715, 0
  store i1 %711, i1* %az
  store i1 %712, i1* %cf
  store i1 %716, i1* %of
  %717 = icmp eq i32 %708, 0
  store i1 %717, i1* %zf
  %718 = icmp slt i32 %708, 0
  store i1 %718, i1* %sf
  %719 = trunc i32 %708 to i8
  %720 = call i8 @llvm.ctpop.i8(i8 %719)
  %721 = and i8 %720, 1
  %722 = icmp eq i8 %721, 0
  store i1 %722, i1* %pf
  store volatile i64 50548, i64* @assembly_address
  %723 = load i1* %cf
  %724 = load i1* %zf
  %725 = or i1 %723, %724
  %726 = icmp ne i1 %725, true
  br i1 %726, label %block_c5d4, label %block_c576

block_c576:                                       ; preds = %block_c55a
  store volatile i64 50550, i64* @assembly_address
  %727 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %728 = zext i32 %727 to i64
  store i64 %728, i64* %rax
  store volatile i64 50556, i64* @assembly_address
  %729 = load i64* %rax
  %730 = trunc i64 %729 to i32
  %731 = zext i32 %730 to i64
  store i64 %731, i64* %rdx
  store volatile i64 50558, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 50565, i64* @assembly_address
  %732 = load i64* %rdx
  %733 = load i64* %rax
  %734 = mul i64 %733, 1
  %735 = add i64 %732, %734
  store i64 %735, i64* %rcx
  store volatile i64 50569, i64* @assembly_address
  %736 = load i32* %stack_var_-140
  %737 = zext i32 %736 to i64
  store i64 %737, i64* %rax
  store volatile i64 50575, i64* @assembly_address
  store i64 32768, i64* %rdx
  store volatile i64 50580, i64* @assembly_address
  %738 = load i64* %rcx
  store i64 %738, i64* %rsi
  store volatile i64 50583, i64* @assembly_address
  %739 = load i64* %rax
  %740 = trunc i64 %739 to i32
  %741 = zext i32 %740 to i64
  store i64 %741, i64* %rdi
  store volatile i64 50585, i64* @assembly_address
  %742 = load i64* %rdi
  %743 = load i64* %rsi
  %744 = load i64* %rdx
  %745 = trunc i64 %742 to i32
  %746 = call i64 @read_buffer(i32 %745, i64 %743, i64 %744)
  store i64 %746, i64* %rax
  store i64 %746, i64* %rax
  store volatile i64 50590, i64* @assembly_address
  %747 = load i64* %rax
  %748 = trunc i64 %747 to i32
  store i32 %748, i32* %stack_var_-108
  store volatile i64 50593, i64* @assembly_address
  %749 = load i32* %stack_var_-108
  %750 = sub i32 %749, -1
  %751 = and i32 %749, 15
  %752 = sub i32 %751, 15
  %753 = icmp ugt i32 %752, 15
  %754 = icmp ult i32 %749, -1
  %755 = xor i32 %749, -1
  %756 = xor i32 %749, %750
  %757 = and i32 %755, %756
  %758 = icmp slt i32 %757, 0
  store i1 %753, i1* %az
  store i1 %754, i1* %cf
  store i1 %758, i1* %of
  %759 = icmp eq i32 %750, 0
  store i1 %759, i1* %zf
  %760 = icmp slt i32 %750, 0
  store i1 %760, i1* %sf
  %761 = trunc i32 %750 to i8
  %762 = call i8 @llvm.ctpop.i8(i8 %761)
  %763 = and i8 %762, 1
  %764 = icmp eq i8 %763, 0
  store i1 %764, i1* %pf
  store volatile i64 50597, i64* @assembly_address
  %765 = load i1* %zf
  %766 = icmp eq i1 %765, false
  br i1 %766, label %block_c5ac, label %block_c5a7

block_c5a7:                                       ; preds = %block_c576
  store volatile i64 50599, i64* @assembly_address
  %767 = call i64 @read_error()
  store i64 %767, i64* %rax
  store i64 %767, i64* %rax
  store i64 %767, i64* %rax
  unreachable

block_c5ac:                                       ; preds = %block_c576
  store volatile i64 50604, i64* @assembly_address
  %768 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %769 = zext i32 %768 to i64
  store i64 %769, i64* %rdx
  store volatile i64 50610, i64* @assembly_address
  %770 = load i32* %stack_var_-108
  %771 = zext i32 %770 to i64
  store i64 %771, i64* %rax
  store volatile i64 50613, i64* @assembly_address
  %772 = load i64* %rax
  %773 = trunc i64 %772 to i32
  %774 = load i64* %rdx
  %775 = trunc i64 %774 to i32
  %776 = add i32 %773, %775
  %777 = and i32 %773, 15
  %778 = and i32 %775, 15
  %779 = add i32 %777, %778
  %780 = icmp ugt i32 %779, 15
  %781 = icmp ult i32 %776, %773
  %782 = xor i32 %773, %776
  %783 = xor i32 %775, %776
  %784 = and i32 %782, %783
  %785 = icmp slt i32 %784, 0
  store i1 %780, i1* %az
  store i1 %781, i1* %cf
  store i1 %785, i1* %of
  %786 = icmp eq i32 %776, 0
  store i1 %786, i1* %zf
  %787 = icmp slt i32 %776, 0
  store i1 %787, i1* %sf
  %788 = trunc i32 %776 to i8
  %789 = call i8 @llvm.ctpop.i8(i8 %788)
  %790 = and i8 %789, 1
  %791 = icmp eq i8 %790, 0
  store i1 %791, i1* %pf
  %792 = zext i32 %776 to i64
  store i64 %792, i64* %rax
  store volatile i64 50615, i64* @assembly_address
  %793 = load i64* %rax
  %794 = trunc i64 %793 to i32
  store i32 %794, i32* bitcast (i64* @global_var_25f4e4 to i32*)
  store volatile i64 50621, i64* @assembly_address
  %795 = load i32* %stack_var_-108
  %796 = zext i32 %795 to i64
  store i64 %796, i64* %rax
  store volatile i64 50624, i64* @assembly_address
  %797 = load i64* %rax
  %798 = trunc i64 %797 to i32
  %799 = sext i32 %798 to i64
  store i64 %799, i64* %rdx
  store volatile i64 50627, i64* @assembly_address
  %800 = load i64* @global_var_21a860
  store i64 %800, i64* %rax
  store volatile i64 50634, i64* @assembly_address
  %801 = load i64* %rax
  %802 = load i64* %rdx
  %803 = add i64 %801, %802
  %804 = and i64 %801, 15
  %805 = and i64 %802, 15
  %806 = add i64 %804, %805
  %807 = icmp ugt i64 %806, 15
  %808 = icmp ult i64 %803, %801
  %809 = xor i64 %801, %803
  %810 = xor i64 %802, %803
  %811 = and i64 %809, %810
  %812 = icmp slt i64 %811, 0
  store i1 %807, i1* %az
  store i1 %808, i1* %cf
  store i1 %812, i1* %of
  %813 = icmp eq i64 %803, 0
  store i1 %813, i1* %zf
  %814 = icmp slt i64 %803, 0
  store i1 %814, i1* %sf
  %815 = trunc i64 %803 to i8
  %816 = call i8 @llvm.ctpop.i8(i8 %815)
  %817 = and i8 %816, 1
  %818 = icmp eq i8 %817, 0
  store i1 %818, i1* %pf
  store i64 %803, i64* %rax
  store volatile i64 50637, i64* @assembly_address
  %819 = load i64* %rax
  store i64 %819, i64* @global_var_21a860
  br label %block_c5d4

block_c5d4:                                       ; preds = %block_c5ac, %block_c55a
  store volatile i64 50644, i64* @assembly_address
  %820 = load i32* %stack_var_-108
  %821 = and i32 %820, 15
  %822 = icmp ugt i32 %821, 15
  %823 = icmp ult i32 %820, 0
  %824 = xor i32 %820, 0
  %825 = and i32 %824, 0
  %826 = icmp slt i32 %825, 0
  store i1 %822, i1* %az
  store i1 %823, i1* %cf
  store i1 %826, i1* %of
  %827 = icmp eq i32 %820, 0
  store i1 %827, i1* %zf
  %828 = icmp slt i32 %820, 0
  store i1 %828, i1* %sf
  %829 = trunc i32 %820 to i8
  %830 = call i8 @llvm.ctpop.i8(i8 %829)
  %831 = and i8 %830, 1
  %832 = icmp eq i8 %831, 0
  store i1 %832, i1* %pf
  store volatile i64 50648, i64* @assembly_address
  %833 = load i1* %zf
  br i1 %833, label %block_c602, label %block_c5da

block_c5da:                                       ; preds = %block_c5d4
  store volatile i64 50650, i64* @assembly_address
  %834 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %835 = zext i32 %834 to i64
  store i64 %835, i64* %rax
  store volatile i64 50656, i64* @assembly_address
  %836 = load i64* %rax
  %837 = trunc i64 %836 to i32
  %838 = zext i32 %837 to i64
  store i64 %838, i64* %rsi
  store volatile i64 50658, i64* @assembly_address
  %839 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %840 = zext i32 %839 to i64
  store i64 %840, i64* %rax
  store volatile i64 50664, i64* @assembly_address
  %841 = load i32* %stack_var_-112
  %842 = zext i32 %841 to i64
  store i64 %842, i64* %rcx
  store volatile i64 50667, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 50672, i64* @assembly_address
  %843 = load i64* %rcx
  %844 = trunc i64 %843 to i32
  %845 = load i64* %rax
  %846 = trunc i64 %845 to i32
  %847 = zext i32 %846 to i64
  %848 = load i64* %rdx
  %849 = trunc i64 %848 to i32
  %850 = zext i32 %849 to i64
  %851 = shl i64 %850, 32
  %852 = or i64 %851, %847
  %853 = zext i32 %844 to i64
  %854 = udiv i64 %852, %853
  %855 = trunc i64 %854 to i32
  %856 = zext i32 %855 to i64
  store i64 %856, i64* %rax
  %857 = urem i64 %852, %853
  %858 = trunc i64 %857 to i32
  %859 = zext i32 %858 to i64
  store i64 %859, i64* %rdx
  store volatile i64 50674, i64* @assembly_address
  %860 = load i64* %rdx
  %861 = trunc i64 %860 to i32
  %862 = zext i32 %861 to i64
  store i64 %862, i64* %rax
  store volatile i64 50676, i64* @assembly_address
  %863 = load i64* %rax
  %864 = trunc i64 %863 to i32
  %865 = zext i32 %864 to i64
  store i64 %865, i64* %rax
  store volatile i64 50678, i64* @assembly_address
  %866 = load i64* %rsi
  %867 = load i64* %rax
  %868 = sub i64 %866, %867
  %869 = and i64 %866, 15
  %870 = and i64 %867, 15
  %871 = sub i64 %869, %870
  %872 = icmp ugt i64 %871, 15
  %873 = icmp ult i64 %866, %867
  %874 = xor i64 %866, %867
  %875 = xor i64 %866, %868
  %876 = and i64 %874, %875
  %877 = icmp slt i64 %876, 0
  store i1 %872, i1* %az
  store i1 %873, i1* %cf
  store i1 %877, i1* %of
  %878 = icmp eq i64 %868, 0
  store i1 %878, i1* %zf
  %879 = icmp slt i64 %868, 0
  store i1 %879, i1* %sf
  %880 = trunc i64 %868 to i8
  %881 = call i8 @llvm.ctpop.i8(i8 %880)
  %882 = and i8 %881, 1
  %883 = icmp eq i8 %882, 0
  store i1 %883, i1* %pf
  store i64 %868, i64* %rsi
  store volatile i64 50681, i64* @assembly_address
  %884 = load i64* %rsi
  store i64 %884, i64* %rax
  store volatile i64 50684, i64* @assembly_address
  %885 = load i64* %rax
  %886 = load i1* %of
  %887 = shl i64 %885, 3
  %888 = icmp eq i64 %887, 0
  store i1 %888, i1* %zf
  %889 = icmp slt i64 %887, 0
  store i1 %889, i1* %sf
  %890 = trunc i64 %887 to i8
  %891 = call i8 @llvm.ctpop.i8(i8 %890)
  %892 = and i8 %891, 1
  %893 = icmp eq i8 %892, 0
  store i1 %893, i1* %pf
  store i64 %887, i64* %rax
  %894 = shl i64 %885, 2
  %895 = lshr i64 %894, 63
  %896 = trunc i64 %895 to i1
  store i1 %896, i1* %cf
  %897 = lshr i64 %887, 63
  %898 = icmp ne i64 %897, %895
  %899 = select i1 false, i1 %898, i1 %886
  store i1 %899, i1* %of
  store volatile i64 50688, i64* @assembly_address
  br label %block_c620

block_c602:                                       ; preds = %block_c5d4
  store volatile i64 50690, i64* @assembly_address
  %900 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %901 = zext i32 %900 to i64
  store i64 %901, i64* %rax
  store volatile i64 50696, i64* @assembly_address
  %902 = load i64* %rax
  %903 = trunc i64 %902 to i32
  %904 = zext i32 %903 to i64
  store i64 %904, i64* %rax
  store volatile i64 50698, i64* @assembly_address
  %905 = load i64* %rax
  %906 = mul i64 %905, 8
  store i64 %906, i64* %rdx
  store volatile i64 50706, i64* @assembly_address
  %907 = load i32* %stack_var_-112
  %908 = zext i32 %907 to i64
  store i64 %908, i64* %rax
  store volatile i64 50709, i64* @assembly_address
  %909 = load i64* %rax
  %910 = trunc i64 %909 to i32
  %911 = sub i32 %910, 1
  %912 = and i32 %910, 15
  %913 = sub i32 %912, 1
  %914 = icmp ugt i32 %913, 15
  %915 = icmp ult i32 %910, 1
  %916 = xor i32 %910, 1
  %917 = xor i32 %910, %911
  %918 = and i32 %916, %917
  %919 = icmp slt i32 %918, 0
  store i1 %914, i1* %az
  store i1 %915, i1* %cf
  store i1 %919, i1* %of
  %920 = icmp eq i32 %911, 0
  store i1 %920, i1* %zf
  %921 = icmp slt i32 %911, 0
  store i1 %921, i1* %sf
  %922 = trunc i32 %911 to i8
  %923 = call i8 @llvm.ctpop.i8(i8 %922)
  %924 = and i8 %923, 1
  %925 = icmp eq i8 %924, 0
  store i1 %925, i1* %pf
  %926 = zext i32 %911 to i64
  store i64 %926, i64* %rax
  store volatile i64 50712, i64* @assembly_address
  %927 = load i64* %rax
  %928 = trunc i64 %927 to i32
  %929 = sext i32 %928 to i64
  store i64 %929, i64* %rax
  store volatile i64 50714, i64* @assembly_address
  %930 = load i64* %rdx
  %931 = load i64* %rax
  %932 = sub i64 %930, %931
  %933 = and i64 %930, 15
  %934 = and i64 %931, 15
  %935 = sub i64 %933, %934
  %936 = icmp ugt i64 %935, 15
  %937 = icmp ult i64 %930, %931
  %938 = xor i64 %930, %931
  %939 = xor i64 %930, %932
  %940 = and i64 %938, %939
  %941 = icmp slt i64 %940, 0
  store i1 %936, i1* %az
  store i1 %937, i1* %cf
  store i1 %941, i1* %of
  %942 = icmp eq i64 %932, 0
  store i1 %942, i1* %zf
  %943 = icmp slt i64 %932, 0
  store i1 %943, i1* %sf
  %944 = trunc i64 %932 to i8
  %945 = call i8 @llvm.ctpop.i8(i8 %944)
  %946 = and i8 %945, 1
  %947 = icmp eq i8 %946, 0
  store i1 %947, i1* %pf
  store i64 %932, i64* %rdx
  store volatile i64 50717, i64* @assembly_address
  %948 = load i64* %rdx
  store i64 %948, i64* %rax
  br label %block_c620

block_c620:                                       ; preds = %block_c602, %block_c5da
  store volatile i64 50720, i64* @assembly_address
  %949 = load i64* %rax
  store i64 %949, i64* %stack_var_-32
  store volatile i64 50724, i64* @assembly_address
  br label %block_ca9d

block_c629:                                       ; preds = %block_ca9d
  store volatile i64 50729, i64* @assembly_address
  %950 = load i32* %stack_var_-56
  %951 = sext i32 %950 to i64
  store i64 %951, i64* %rax
  store volatile i64 50733, i64* @assembly_address
  %952 = load i64* %rax
  %953 = load i64* %stack_var_-48
  %954 = trunc i64 %952 to i32
  store i32 %954, i32* %44
  %955 = trunc i64 %953 to i32
  store i32 %955, i32* %42
  %956 = sub i64 %952, %953
  %957 = and i64 %952, 15
  %958 = and i64 %953, 15
  %959 = sub i64 %957, %958
  %960 = icmp ugt i64 %959, 15
  %961 = icmp ult i64 %952, %953
  %962 = xor i64 %952, %953
  %963 = xor i64 %952, %956
  %964 = and i64 %962, %963
  %965 = icmp slt i64 %964, 0
  store i1 %960, i1* %az
  store i1 %961, i1* %cf
  store i1 %965, i1* %of
  %966 = icmp eq i64 %956, 0
  store i1 %966, i1* %zf
  %967 = icmp slt i64 %956, 0
  store i1 %967, i1* %sf
  %968 = trunc i64 %956 to i8
  %969 = call i8 @llvm.ctpop.i8(i8 %968)
  %970 = and i8 %969, 1
  %971 = icmp eq i8 %970, 0
  store i1 %971, i1* %pf
  store volatile i64 50737, i64* @assembly_address
  %972 = load i32* %44
  %973 = sext i32 %972 to i64
  %974 = load i32* %42
  %975 = sext i32 %974 to i64
  %976 = icmp sle i64 %973, %975
  br i1 %976, label %block_c6c3, label %block_c637

block_c637:                                       ; preds = %block_c629
  store volatile i64 50743, i64* @assembly_address
  %977 = load i32* %stack_var_-64
  %978 = sext i32 %977 to i64
  store i64 %978, i64* %rax
  store volatile i64 50747, i64* @assembly_address
  %979 = load i64* %rax
  %980 = add i64 %979, -1
  store i64 %980, i64* %rdi
  store volatile i64 50751, i64* @assembly_address
  %981 = load i32* %stack_var_-112
  %982 = zext i32 %981 to i64
  store i64 %982, i64* %rax
  store volatile i64 50754, i64* @assembly_address
  %983 = load i64* %rax
  %984 = trunc i64 %983 to i32
  %985 = load i1* %of
  %986 = shl i32 %984, 3
  %987 = icmp eq i32 %986, 0
  store i1 %987, i1* %zf
  %988 = icmp slt i32 %986, 0
  store i1 %988, i1* %sf
  %989 = trunc i32 %986 to i8
  %990 = call i8 @llvm.ctpop.i8(i8 %989)
  %991 = and i8 %990, 1
  %992 = icmp eq i8 %991, 0
  store i1 %992, i1* %pf
  %993 = zext i32 %986 to i64
  store i64 %993, i64* %rax
  %994 = shl i32 %984, 2
  %995 = lshr i32 %994, 31
  %996 = trunc i32 %995 to i1
  store i1 %996, i1* %cf
  %997 = lshr i32 %986, 31
  %998 = icmp ne i32 %997, %995
  %999 = select i1 false, i1 %998, i1 %985
  store i1 %999, i1* %of
  store volatile i64 50757, i64* @assembly_address
  %1000 = load i64* %rax
  %1001 = trunc i64 %1000 to i32
  %1002 = sext i32 %1001 to i64
  store i64 %1002, i64* %rcx
  store volatile i64 50760, i64* @assembly_address
  %1003 = load i32* %stack_var_-64
  %1004 = sext i32 %1003 to i64
  store i64 %1004, i64* %rax
  store volatile i64 50764, i64* @assembly_address
  %1005 = load i64* %rax
  %1006 = add i64 %1005, -1
  store i64 %1006, i64* %rdx
  store volatile i64 50768, i64* @assembly_address
  %1007 = load i32* %stack_var_-112
  %1008 = zext i32 %1007 to i64
  store i64 %1008, i64* %rax
  store volatile i64 50771, i64* @assembly_address
  %1009 = load i64* %rax
  %1010 = trunc i64 %1009 to i32
  %1011 = load i1* %of
  %1012 = shl i32 %1010, 3
  %1013 = icmp eq i32 %1012, 0
  store i1 %1013, i1* %zf
  %1014 = icmp slt i32 %1012, 0
  store i1 %1014, i1* %sf
  %1015 = trunc i32 %1012 to i8
  %1016 = call i8 @llvm.ctpop.i8(i8 %1015)
  %1017 = and i8 %1016, 1
  %1018 = icmp eq i8 %1017, 0
  store i1 %1018, i1* %pf
  %1019 = zext i32 %1012 to i64
  store i64 %1019, i64* %rax
  %1020 = shl i32 %1010, 2
  %1021 = lshr i32 %1020, 31
  %1022 = trunc i32 %1021 to i1
  store i1 %1022, i1* %cf
  %1023 = lshr i32 %1012, 31
  %1024 = icmp ne i32 %1023, %1021
  %1025 = select i1 false, i1 %1024, i1 %1011
  store i1 %1025, i1* %of
  store volatile i64 50774, i64* @assembly_address
  %1026 = load i64* %rax
  %1027 = trunc i64 %1026 to i32
  %1028 = sext i32 %1027 to i64
  store i64 %1028, i64* %rax
  store volatile i64 50776, i64* @assembly_address
  %1029 = load i64* %rax
  %1030 = load i64* %rdx
  %1031 = add i64 %1029, %1030
  %1032 = and i64 %1029, 15
  %1033 = and i64 %1030, 15
  %1034 = add i64 %1032, %1033
  %1035 = icmp ugt i64 %1034, 15
  %1036 = icmp ult i64 %1031, %1029
  %1037 = xor i64 %1029, %1031
  %1038 = xor i64 %1030, %1031
  %1039 = and i64 %1037, %1038
  %1040 = icmp slt i64 %1039, 0
  store i1 %1035, i1* %az
  store i1 %1036, i1* %cf
  store i1 %1040, i1* %of
  %1041 = icmp eq i64 %1031, 0
  store i1 %1041, i1* %zf
  %1042 = icmp slt i64 %1031, 0
  store i1 %1042, i1* %sf
  %1043 = trunc i64 %1031 to i8
  %1044 = call i8 @llvm.ctpop.i8(i8 %1043)
  %1045 = and i8 %1044, 1
  %1046 = icmp eq i8 %1045, 0
  store i1 %1046, i1* %pf
  store i64 %1031, i64* %rax
  store volatile i64 50779, i64* @assembly_address
  %1047 = load i32* %stack_var_-112
  %1048 = zext i32 %1047 to i64
  store i64 %1048, i64* %rdx
  store volatile i64 50782, i64* @assembly_address
  %1049 = load i64* %rdx
  %1050 = trunc i64 %1049 to i32
  %1051 = load i1* %of
  %1052 = shl i32 %1050, 3
  %1053 = icmp eq i32 %1052, 0
  store i1 %1053, i1* %zf
  %1054 = icmp slt i32 %1052, 0
  store i1 %1054, i1* %sf
  %1055 = trunc i32 %1052 to i8
  %1056 = call i8 @llvm.ctpop.i8(i8 %1055)
  %1057 = and i8 %1056, 1
  %1058 = icmp eq i8 %1057, 0
  store i1 %1058, i1* %pf
  %1059 = zext i32 %1052 to i64
  store i64 %1059, i64* %rdx
  %1060 = shl i32 %1050, 2
  %1061 = lshr i32 %1060, 31
  %1062 = trunc i32 %1061 to i1
  store i1 %1062, i1* %cf
  %1063 = lshr i32 %1052, 31
  %1064 = icmp ne i32 %1063, %1061
  %1065 = select i1 false, i1 %1064, i1 %1051
  store i1 %1065, i1* %of
  store volatile i64 50785, i64* @assembly_address
  %1066 = load i64* %rdx
  %1067 = trunc i64 %1066 to i32
  %1068 = sext i32 %1067 to i64
  store i64 %1068, i64* %rsi
  store volatile i64 50788, i64* @assembly_address
  %1069 = load i64* %rax
  %1070 = ashr i64 %1069, 63
  store i64 %1070, i64* %rdx
  store volatile i64 50790, i64* @assembly_address
  %1071 = load i64* %rsi
  %1072 = load i64* %rax
  %1073 = zext i64 %1072 to i128
  %1074 = load i64* %rdx
  %1075 = zext i64 %1074 to i128
  %1076 = shl i128 %1075, 64
  %1077 = or i128 %1076, %1073
  %1078 = zext i64 %1071 to i128
  %1079 = sdiv i128 %1077, %1078
  %1080 = trunc i128 %1079 to i64
  store i64 %1080, i64* %rax
  %1081 = srem i128 %1077, %1078
  %1082 = trunc i128 %1081 to i64
  store i64 %1082, i64* %rdx
  store volatile i64 50793, i64* @assembly_address
  %1083 = load i64* %rdx
  store i64 %1083, i64* %rax
  store volatile i64 50796, i64* @assembly_address
  %1084 = load i64* %rcx
  %1085 = load i64* %rax
  %1086 = sub i64 %1084, %1085
  %1087 = and i64 %1084, 15
  %1088 = and i64 %1085, 15
  %1089 = sub i64 %1087, %1088
  %1090 = icmp ugt i64 %1089, 15
  %1091 = icmp ult i64 %1084, %1085
  %1092 = xor i64 %1084, %1085
  %1093 = xor i64 %1084, %1086
  %1094 = and i64 %1092, %1093
  %1095 = icmp slt i64 %1094, 0
  store i1 %1090, i1* %az
  store i1 %1091, i1* %cf
  store i1 %1095, i1* %of
  %1096 = icmp eq i64 %1086, 0
  store i1 %1096, i1* %zf
  %1097 = icmp slt i64 %1086, 0
  store i1 %1097, i1* %sf
  %1098 = trunc i64 %1086 to i8
  %1099 = call i8 @llvm.ctpop.i8(i8 %1098)
  %1100 = and i8 %1099, 1
  %1101 = icmp eq i8 %1100, 0
  store i1 %1101, i1* %pf
  store i64 %1086, i64* %rcx
  store volatile i64 50799, i64* @assembly_address
  %1102 = load i64* %rcx
  store i64 %1102, i64* %rax
  store volatile i64 50802, i64* @assembly_address
  %1103 = load i64* %rax
  %1104 = load i64* %rdi
  %1105 = add i64 %1103, %1104
  %1106 = and i64 %1103, 15
  %1107 = and i64 %1104, 15
  %1108 = add i64 %1106, %1107
  %1109 = icmp ugt i64 %1108, 15
  %1110 = icmp ult i64 %1105, %1103
  %1111 = xor i64 %1103, %1105
  %1112 = xor i64 %1104, %1105
  %1113 = and i64 %1111, %1112
  %1114 = icmp slt i64 %1113, 0
  store i1 %1109, i1* %az
  store i1 %1110, i1* %cf
  store i1 %1114, i1* %of
  %1115 = icmp eq i64 %1105, 0
  store i1 %1115, i1* %zf
  %1116 = icmp slt i64 %1105, 0
  store i1 %1116, i1* %sf
  %1117 = trunc i64 %1105 to i8
  %1118 = call i8 @llvm.ctpop.i8(i8 %1117)
  %1119 = and i8 %1118, 1
  %1120 = icmp eq i8 %1119, 0
  store i1 %1120, i1* %pf
  store i64 %1105, i64* %rax
  store volatile i64 50805, i64* @assembly_address
  %1121 = load i64* %rax
  %1122 = trunc i64 %1121 to i32
  store i32 %1122, i32* %stack_var_-64
  store volatile i64 50809, i64* @assembly_address
  %1123 = load i32* %stack_var_-112
  %1124 = add i32 %1123, 1
  %1125 = and i32 %1123, 15
  %1126 = add i32 %1125, 1
  %1127 = icmp ugt i32 %1126, 15
  %1128 = icmp ult i32 %1124, %1123
  %1129 = xor i32 %1123, %1124
  %1130 = xor i32 1, %1124
  %1131 = and i32 %1129, %1130
  %1132 = icmp slt i32 %1131, 0
  store i1 %1127, i1* %az
  store i1 %1128, i1* %cf
  store i1 %1132, i1* %of
  %1133 = icmp eq i32 %1124, 0
  store i1 %1133, i1* %zf
  %1134 = icmp slt i32 %1124, 0
  store i1 %1134, i1* %sf
  %1135 = trunc i32 %1124 to i8
  %1136 = call i8 @llvm.ctpop.i8(i8 %1135)
  %1137 = and i8 %1136, 1
  %1138 = icmp eq i8 %1137, 0
  store i1 %1138, i1* %pf
  store i32 %1124, i32* %stack_var_-112
  store volatile i64 50813, i64* @assembly_address
  %1139 = load i32* bitcast (i64* @global_var_216098 to i32*)
  %1140 = zext i32 %1139 to i64
  store i64 %1140, i64* %rax
  store volatile i64 50819, i64* @assembly_address
  %1141 = load i32* %stack_var_-112
  %1142 = load i64* %rax
  %1143 = trunc i64 %1142 to i32
  %1144 = sub i32 %1141, %1143
  %1145 = and i32 %1141, 15
  %1146 = and i32 %1143, 15
  %1147 = sub i32 %1145, %1146
  %1148 = icmp ugt i32 %1147, 15
  %1149 = icmp ult i32 %1141, %1143
  %1150 = xor i32 %1141, %1143
  %1151 = xor i32 %1141, %1144
  %1152 = and i32 %1150, %1151
  %1153 = icmp slt i32 %1152, 0
  store i1 %1148, i1* %az
  store i1 %1149, i1* %cf
  store i1 %1153, i1* %of
  %1154 = icmp eq i32 %1144, 0
  store i1 %1154, i1* %zf
  %1155 = icmp slt i32 %1144, 0
  store i1 %1155, i1* %sf
  %1156 = trunc i32 %1144 to i8
  %1157 = call i8 @llvm.ctpop.i8(i8 %1156)
  %1158 = and i8 %1157, 1
  %1159 = icmp eq i8 %1158, 0
  store i1 %1159, i1* %pf
  store volatile i64 50822, i64* @assembly_address
  %1160 = load i1* %zf
  %1161 = icmp eq i1 %1160, false
  br i1 %1161, label %block_c692, label %block_c688

block_c688:                                       ; preds = %block_c637
  store volatile i64 50824, i64* @assembly_address
  %1162 = load i64* %stack_var_-40
  store i64 %1162, i64* %rax
  store volatile i64 50828, i64* @assembly_address
  %1163 = load i64* %rax
  store i64 %1163, i64* %stack_var_-48
  store volatile i64 50832, i64* @assembly_address
  br label %block_c6aa

block_c692:                                       ; preds = %block_c637
  store volatile i64 50834, i64* @assembly_address
  %1164 = load i32* %stack_var_-112
  %1165 = zext i32 %1164 to i64
  store i64 %1165, i64* %rax
  store volatile i64 50837, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 50842, i64* @assembly_address
  %1166 = load i64* %rax
  %1167 = trunc i64 %1166 to i32
  %1168 = zext i32 %1167 to i64
  store i64 %1168, i64* %rcx
  store volatile i64 50844, i64* @assembly_address
  %1169 = load i64* %rdx
  %1170 = load i64* %rcx
  %1171 = trunc i64 %1170 to i8
  %1172 = zext i8 %1171 to i64
  %1173 = and i64 %1172, 63
  %1174 = load i1* %of
  %1175 = icmp eq i64 %1173, 0
  br i1 %1175, label %1192, label %1176

; <label>:1176                                    ; preds = %block_c692
  %1177 = shl i64 %1169, %1173
  %1178 = icmp eq i64 %1177, 0
  store i1 %1178, i1* %zf
  %1179 = icmp slt i64 %1177, 0
  store i1 %1179, i1* %sf
  %1180 = trunc i64 %1177 to i8
  %1181 = call i8 @llvm.ctpop.i8(i8 %1180)
  %1182 = and i8 %1181, 1
  %1183 = icmp eq i8 %1182, 0
  store i1 %1183, i1* %pf
  store i64 %1177, i64* %rdx
  %1184 = sub i64 %1173, 1
  %1185 = shl i64 %1169, %1184
  %1186 = lshr i64 %1185, 63
  %1187 = trunc i64 %1186 to i1
  store i1 %1187, i1* %cf
  %1188 = lshr i64 %1177, 63
  %1189 = icmp ne i64 %1188, %1186
  %1190 = icmp eq i64 %1173, 1
  %1191 = select i1 %1190, i1 %1189, i1 %1174
  store i1 %1191, i1* %of
  br label %1192

; <label>:1192                                    ; preds = %block_c692, %1176
  store volatile i64 50847, i64* @assembly_address
  %1193 = load i64* %rdx
  store i64 %1193, i64* %rax
  store volatile i64 50850, i64* @assembly_address
  %1194 = load i64* %rax
  %1195 = sub i64 %1194, 1
  %1196 = and i64 %1194, 15
  %1197 = sub i64 %1196, 1
  %1198 = icmp ugt i64 %1197, 15
  %1199 = icmp ult i64 %1194, 1
  %1200 = xor i64 %1194, 1
  %1201 = xor i64 %1194, %1195
  %1202 = and i64 %1200, %1201
  %1203 = icmp slt i64 %1202, 0
  store i1 %1198, i1* %az
  store i1 %1199, i1* %cf
  store i1 %1203, i1* %of
  %1204 = icmp eq i64 %1195, 0
  store i1 %1204, i1* %zf
  %1205 = icmp slt i64 %1195, 0
  store i1 %1205, i1* %sf
  %1206 = trunc i64 %1195 to i8
  %1207 = call i8 @llvm.ctpop.i8(i8 %1206)
  %1208 = and i8 %1207, 1
  %1209 = icmp eq i8 %1208, 0
  store i1 %1209, i1* %pf
  store i64 %1195, i64* %rax
  store volatile i64 50854, i64* @assembly_address
  %1210 = load i64* %rax
  store i64 %1210, i64* %stack_var_-48
  br label %block_c6aa

block_c6aa:                                       ; preds = %1192, %block_c688
  store volatile i64 50858, i64* @assembly_address
  %1211 = load i32* %stack_var_-112
  %1212 = zext i32 %1211 to i64
  store i64 %1212, i64* %rax
  store volatile i64 50861, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 50866, i64* @assembly_address
  %1213 = load i64* %rax
  %1214 = trunc i64 %1213 to i32
  %1215 = zext i32 %1214 to i64
  store i64 %1215, i64* %rcx
  store volatile i64 50868, i64* @assembly_address
  %1216 = load i64* %rdx
  %1217 = trunc i64 %1216 to i32
  %1218 = load i64* %rcx
  %1219 = trunc i64 %1218 to i8
  %1220 = zext i8 %1219 to i32
  %1221 = and i32 %1220, 31
  %1222 = load i1* %of
  %1223 = icmp eq i32 %1221, 0
  br i1 %1223, label %1241, label %1224

; <label>:1224                                    ; preds = %block_c6aa
  %1225 = shl i32 %1217, %1221
  %1226 = icmp eq i32 %1225, 0
  store i1 %1226, i1* %zf
  %1227 = icmp slt i32 %1225, 0
  store i1 %1227, i1* %sf
  %1228 = trunc i32 %1225 to i8
  %1229 = call i8 @llvm.ctpop.i8(i8 %1228)
  %1230 = and i8 %1229, 1
  %1231 = icmp eq i8 %1230, 0
  store i1 %1231, i1* %pf
  %1232 = zext i32 %1225 to i64
  store i64 %1232, i64* %rdx
  %1233 = sub i32 %1221, 1
  %1234 = shl i32 %1217, %1233
  %1235 = lshr i32 %1234, 31
  %1236 = trunc i32 %1235 to i1
  store i1 %1236, i1* %cf
  %1237 = lshr i32 %1225, 31
  %1238 = icmp ne i32 %1237, %1235
  %1239 = icmp eq i32 %1221, 1
  %1240 = select i1 %1239, i1 %1238, i1 %1222
  store i1 %1240, i1* %of
  br label %1241

; <label>:1241                                    ; preds = %block_c6aa, %1224
  store volatile i64 50870, i64* @assembly_address
  %1242 = load i64* %rdx
  %1243 = trunc i64 %1242 to i32
  %1244 = zext i32 %1243 to i64
  store i64 %1244, i64* %rax
  store volatile i64 50872, i64* @assembly_address
  %1245 = load i64* %rax
  %1246 = trunc i64 %1245 to i32
  %1247 = sub i32 %1246, 1
  %1248 = and i32 %1246, 15
  %1249 = sub i32 %1248, 1
  %1250 = icmp ugt i32 %1249, 15
  %1251 = icmp ult i32 %1246, 1
  %1252 = xor i32 %1246, 1
  %1253 = xor i32 %1246, %1247
  %1254 = and i32 %1252, %1253
  %1255 = icmp slt i32 %1254, 0
  store i1 %1250, i1* %az
  store i1 %1251, i1* %cf
  store i1 %1255, i1* %of
  %1256 = icmp eq i32 %1247, 0
  store i1 %1256, i1* %zf
  %1257 = icmp slt i32 %1247, 0
  store i1 %1257, i1* %sf
  %1258 = trunc i32 %1247 to i8
  %1259 = call i8 @llvm.ctpop.i8(i8 %1258)
  %1260 = and i8 %1259, 1
  %1261 = icmp eq i8 %1260, 0
  store i1 %1261, i1* %pf
  %1262 = zext i32 %1247 to i64
  store i64 %1262, i64* %rax
  store volatile i64 50875, i64* @assembly_address
  %1263 = load i64* %rax
  %1264 = trunc i64 %1263 to i32
  store i32 %1264, i32* %stack_var_-116
  store volatile i64 50878, i64* @assembly_address
  br label %block_c4f0

block_c6c3:                                       ; preds = %block_c629
  store volatile i64 50883, i64* @assembly_address
  %1265 = load i32* %stack_var_-64
  %1266 = sext i32 %1265 to i64
  store i64 %1266, i64* %rax
  store volatile i64 50887, i64* @assembly_address
  %1267 = load i64* %rax
  %1268 = load i1* %of
  %1269 = ashr i64 %1267, 3
  %1270 = icmp eq i64 %1269, 0
  store i1 %1270, i1* %zf
  %1271 = icmp slt i64 %1269, 0
  store i1 %1271, i1* %sf
  %1272 = trunc i64 %1269 to i8
  %1273 = call i8 @llvm.ctpop.i8(i8 %1272)
  %1274 = and i8 %1273, 1
  %1275 = icmp eq i8 %1274, 0
  store i1 %1275, i1* %pf
  store i64 %1269, i64* %rax
  %1276 = and i64 4, %1267
  %1277 = icmp ne i64 %1276, 0
  store i1 %1277, i1* %cf
  %1278 = select i1 false, i1 false, i1 %1268
  store i1 %1278, i1* %of
  store volatile i64 50891, i64* @assembly_address
  %1279 = load i64* %rax
  store i64 %1279, i64* %rdx
  store volatile i64 50894, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 50901, i64* @assembly_address
  %1280 = load i64* %rax
  %1281 = load i64* %rdx
  %1282 = add i64 %1280, %1281
  %1283 = and i64 %1280, 15
  %1284 = and i64 %1281, 15
  %1285 = add i64 %1283, %1284
  %1286 = icmp ugt i64 %1285, 15
  %1287 = icmp ult i64 %1282, %1280
  %1288 = xor i64 %1280, %1282
  %1289 = xor i64 %1281, %1282
  %1290 = and i64 %1288, %1289
  %1291 = icmp slt i64 %1290, 0
  store i1 %1286, i1* %az
  store i1 %1287, i1* %cf
  store i1 %1291, i1* %of
  %1292 = icmp eq i64 %1282, 0
  store i1 %1292, i1* %zf
  %1293 = icmp slt i64 %1282, 0
  store i1 %1293, i1* %sf
  %1294 = trunc i64 %1282 to i8
  %1295 = call i8 @llvm.ctpop.i8(i8 %1294)
  %1296 = and i8 %1295, 1
  %1297 = icmp eq i8 %1296, 0
  store i1 %1297, i1* %pf
  store i64 %1282, i64* %rax
  store volatile i64 50904, i64* @assembly_address
  %1298 = load i64* %rax
  store i64 %1298, i64* %stack_var_-24
  store volatile i64 50908, i64* @assembly_address
  %1299 = load i64* %stack_var_-24
  store i64 %1299, i64* %rax
  store volatile i64 50912, i64* @assembly_address
  %1300 = load i64* %rax
  %1301 = inttoptr i64 %1300 to i8*
  %1302 = load i8* %1301
  %1303 = zext i8 %1302 to i64
  store i64 %1303, i64* %rax
  store volatile i64 50915, i64* @assembly_address
  %1304 = load i64* %rax
  %1305 = trunc i64 %1304 to i8
  %1306 = zext i8 %1305 to i64
  store i64 %1306, i64* %rax
  store volatile i64 50918, i64* @assembly_address
  %1307 = load i64* %stack_var_-24
  store i64 %1307, i64* %rdx
  store volatile i64 50922, i64* @assembly_address
  %1308 = load i64* %rdx
  %1309 = add i64 %1308, 1
  %1310 = and i64 %1308, 15
  %1311 = add i64 %1310, 1
  %1312 = icmp ugt i64 %1311, 15
  %1313 = icmp ult i64 %1309, %1308
  %1314 = xor i64 %1308, %1309
  %1315 = xor i64 1, %1309
  %1316 = and i64 %1314, %1315
  %1317 = icmp slt i64 %1316, 0
  store i1 %1312, i1* %az
  store i1 %1313, i1* %cf
  store i1 %1317, i1* %of
  %1318 = icmp eq i64 %1309, 0
  store i1 %1318, i1* %zf
  %1319 = icmp slt i64 %1309, 0
  store i1 %1319, i1* %sf
  %1320 = trunc i64 %1309 to i8
  %1321 = call i8 @llvm.ctpop.i8(i8 %1320)
  %1322 = and i8 %1321, 1
  %1323 = icmp eq i8 %1322, 0
  store i1 %1323, i1* %pf
  store i64 %1309, i64* %rdx
  store volatile i64 50926, i64* @assembly_address
  %1324 = load i64* %rdx
  %1325 = inttoptr i64 %1324 to i8*
  %1326 = load i8* %1325
  %1327 = zext i8 %1326 to i64
  store i64 %1327, i64* %rdx
  store volatile i64 50929, i64* @assembly_address
  %1328 = load i64* %rdx
  %1329 = trunc i64 %1328 to i8
  %1330 = zext i8 %1329 to i64
  store i64 %1330, i64* %rdx
  store volatile i64 50932, i64* @assembly_address
  %1331 = load i64* %rdx
  %1332 = load i1* %of
  %1333 = shl i64 %1331, 8
  %1334 = icmp eq i64 %1333, 0
  store i1 %1334, i1* %zf
  %1335 = icmp slt i64 %1333, 0
  store i1 %1335, i1* %sf
  %1336 = trunc i64 %1333 to i8
  %1337 = call i8 @llvm.ctpop.i8(i8 %1336)
  %1338 = and i8 %1337, 1
  %1339 = icmp eq i8 %1338, 0
  store i1 %1339, i1* %pf
  store i64 %1333, i64* %rdx
  %1340 = shl i64 %1331, 7
  %1341 = lshr i64 %1340, 63
  %1342 = trunc i64 %1341 to i1
  store i1 %1342, i1* %cf
  %1343 = lshr i64 %1333, 63
  %1344 = icmp ne i64 %1343, %1341
  %1345 = select i1 false, i1 %1344, i1 %1332
  store i1 %1345, i1* %of
  store volatile i64 50936, i64* @assembly_address
  %1346 = load i64* %rdx
  %1347 = load i64* %rax
  %1348 = or i64 %1346, %1347
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1349 = icmp eq i64 %1348, 0
  store i1 %1349, i1* %zf
  %1350 = icmp slt i64 %1348, 0
  store i1 %1350, i1* %sf
  %1351 = trunc i64 %1348 to i8
  %1352 = call i8 @llvm.ctpop.i8(i8 %1351)
  %1353 = and i8 %1352, 1
  %1354 = icmp eq i8 %1353, 0
  store i1 %1354, i1* %pf
  store i64 %1348, i64* %rdx
  store volatile i64 50939, i64* @assembly_address
  %1355 = load i64* %stack_var_-24
  store i64 %1355, i64* %rax
  store volatile i64 50943, i64* @assembly_address
  %1356 = load i64* %rax
  %1357 = add i64 %1356, 2
  %1358 = and i64 %1356, 15
  %1359 = add i64 %1358, 2
  %1360 = icmp ugt i64 %1359, 15
  %1361 = icmp ult i64 %1357, %1356
  %1362 = xor i64 %1356, %1357
  %1363 = xor i64 2, %1357
  %1364 = and i64 %1362, %1363
  %1365 = icmp slt i64 %1364, 0
  store i1 %1360, i1* %az
  store i1 %1361, i1* %cf
  store i1 %1365, i1* %of
  %1366 = icmp eq i64 %1357, 0
  store i1 %1366, i1* %zf
  %1367 = icmp slt i64 %1357, 0
  store i1 %1367, i1* %sf
  %1368 = trunc i64 %1357 to i8
  %1369 = call i8 @llvm.ctpop.i8(i8 %1368)
  %1370 = and i8 %1369, 1
  %1371 = icmp eq i8 %1370, 0
  store i1 %1371, i1* %pf
  store i64 %1357, i64* %rax
  store volatile i64 50947, i64* @assembly_address
  %1372 = load i64* %rax
  %1373 = inttoptr i64 %1372 to i8*
  %1374 = load i8* %1373
  %1375 = zext i8 %1374 to i64
  store i64 %1375, i64* %rax
  store volatile i64 50950, i64* @assembly_address
  %1376 = load i64* %rax
  %1377 = trunc i64 %1376 to i8
  %1378 = zext i8 %1377 to i64
  store i64 %1378, i64* %rax
  store volatile i64 50953, i64* @assembly_address
  %1379 = load i64* %rax
  %1380 = load i1* %of
  %1381 = shl i64 %1379, 16
  %1382 = icmp eq i64 %1381, 0
  store i1 %1382, i1* %zf
  %1383 = icmp slt i64 %1381, 0
  store i1 %1383, i1* %sf
  %1384 = trunc i64 %1381 to i8
  %1385 = call i8 @llvm.ctpop.i8(i8 %1384)
  %1386 = and i8 %1385, 1
  %1387 = icmp eq i8 %1386, 0
  store i1 %1387, i1* %pf
  store i64 %1381, i64* %rax
  %1388 = shl i64 %1379, 15
  %1389 = lshr i64 %1388, 63
  %1390 = trunc i64 %1389 to i1
  store i1 %1390, i1* %cf
  %1391 = lshr i64 %1381, 63
  %1392 = icmp ne i64 %1391, %1389
  %1393 = select i1 false, i1 %1392, i1 %1380
  store i1 %1393, i1* %of
  store volatile i64 50957, i64* @assembly_address
  %1394 = load i64* %rdx
  %1395 = load i64* %rax
  %1396 = or i64 %1394, %1395
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1397 = icmp eq i64 %1396, 0
  store i1 %1397, i1* %zf
  %1398 = icmp slt i64 %1396, 0
  store i1 %1398, i1* %sf
  %1399 = trunc i64 %1396 to i8
  %1400 = call i8 @llvm.ctpop.i8(i8 %1399)
  %1401 = and i8 %1400, 1
  %1402 = icmp eq i8 %1401, 0
  store i1 %1402, i1* %pf
  store i64 %1396, i64* %rdx
  store volatile i64 50960, i64* @assembly_address
  %1403 = load i32* %stack_var_-64
  %1404 = sext i32 %1403 to i64
  store i64 %1404, i64* %rax
  store volatile i64 50964, i64* @assembly_address
  %1405 = load i64* %rax
  %1406 = trunc i64 %1405 to i32
  %1407 = and i32 %1406, 7
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1408 = icmp eq i32 %1407, 0
  store i1 %1408, i1* %zf
  %1409 = icmp slt i32 %1407, 0
  store i1 %1409, i1* %sf
  %1410 = trunc i32 %1407 to i8
  %1411 = call i8 @llvm.ctpop.i8(i8 %1410)
  %1412 = and i8 %1411, 1
  %1413 = icmp eq i8 %1412, 0
  store i1 %1413, i1* %pf
  %1414 = zext i32 %1407 to i64
  store i64 %1414, i64* %rax
  store volatile i64 50967, i64* @assembly_address
  %1415 = load i64* %rax
  %1416 = trunc i64 %1415 to i32
  %1417 = zext i32 %1416 to i64
  store i64 %1417, i64* %rcx
  store volatile i64 50969, i64* @assembly_address
  %1418 = load i64* %rdx
  %1419 = load i64* %rcx
  %1420 = trunc i64 %1419 to i8
  %1421 = zext i8 %1420 to i64
  %1422 = and i64 %1421, 63
  %1423 = load i1* %of
  %1424 = icmp eq i64 %1422, 0
  br i1 %1424, label %1439, label %1425

; <label>:1425                                    ; preds = %block_c6c3
  %1426 = ashr i64 %1418, %1422
  %1427 = icmp eq i64 %1426, 0
  store i1 %1427, i1* %zf
  %1428 = icmp slt i64 %1426, 0
  store i1 %1428, i1* %sf
  %1429 = trunc i64 %1426 to i8
  %1430 = call i8 @llvm.ctpop.i8(i8 %1429)
  %1431 = and i8 %1430, 1
  %1432 = icmp eq i8 %1431, 0
  store i1 %1432, i1* %pf
  store i64 %1426, i64* %rdx
  %1433 = sub i64 %1422, 1
  %1434 = shl i64 1, %1433
  %1435 = and i64 %1434, %1418
  %1436 = icmp ne i64 %1435, 0
  store i1 %1436, i1* %cf
  %1437 = icmp eq i64 %1422, 1
  %1438 = select i1 %1437, i1 false, i1 %1423
  store i1 %1438, i1* %of
  br label %1439

; <label>:1439                                    ; preds = %block_c6c3, %1425
  store volatile i64 50972, i64* @assembly_address
  %1440 = load i32* %stack_var_-116
  %1441 = zext i32 %1440 to i64
  store i64 %1441, i64* %rax
  store volatile i64 50975, i64* @assembly_address
  %1442 = load i64* %rax
  %1443 = load i64* %rdx
  %1444 = and i64 %1442, %1443
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1445 = icmp eq i64 %1444, 0
  store i1 %1445, i1* %zf
  %1446 = icmp slt i64 %1444, 0
  store i1 %1446, i1* %sf
  %1447 = trunc i64 %1444 to i8
  %1448 = call i8 @llvm.ctpop.i8(i8 %1447)
  %1449 = and i8 %1448, 1
  %1450 = icmp eq i8 %1449, 0
  store i1 %1450, i1* %pf
  store i64 %1444, i64* %rax
  store volatile i64 50978, i64* @assembly_address
  %1451 = load i64* %rax
  %1452 = trunc i64 %1451 to i32
  store i32 %1452, i32* %stack_var_-80
  store volatile i64 50982, i64* @assembly_address
  %1453 = load i32* %stack_var_-112
  %1454 = zext i32 %1453 to i64
  store i64 %1454, i64* %rax
  store volatile i64 50985, i64* @assembly_address
  %1455 = load i64* %rax
  %1456 = trunc i64 %1455 to i32
  %1457 = sext i32 %1456 to i64
  store i64 %1457, i64* %rax
  store volatile i64 50987, i64* @assembly_address
  %1458 = load i32* %stack_var_-64
  %1459 = sext i32 %1458 to i64
  %1460 = load i64* %rax
  %1461 = add i64 %1459, %1460
  %1462 = and i64 %1459, 15
  %1463 = and i64 %1460, 15
  %1464 = add i64 %1462, %1463
  %1465 = icmp ugt i64 %1464, 15
  %1466 = icmp ult i64 %1461, %1459
  %1467 = xor i64 %1459, %1461
  %1468 = xor i64 %1460, %1461
  %1469 = and i64 %1467, %1468
  %1470 = icmp slt i64 %1469, 0
  store i1 %1465, i1* %az
  store i1 %1466, i1* %cf
  store i1 %1470, i1* %of
  %1471 = icmp eq i64 %1461, 0
  store i1 %1471, i1* %zf
  %1472 = icmp slt i64 %1461, 0
  store i1 %1472, i1* %sf
  %1473 = trunc i64 %1461 to i8
  %1474 = call i8 @llvm.ctpop.i8(i8 %1473)
  %1475 = and i8 %1474, 1
  %1476 = icmp eq i8 %1475, 0
  store i1 %1476, i1* %pf
  %1477 = trunc i64 %1461 to i32
  store i32 %1477, i32* %stack_var_-64
  store volatile i64 50991, i64* @assembly_address
  %1478 = load i32* %stack_var_-72
  %1479 = sext i32 %1478 to i64
  %1480 = sub i64 %1479, -1
  %1481 = and i64 %1479, 15
  %1482 = sub i64 %1481, 15
  %1483 = icmp ugt i64 %1482, 15
  %1484 = icmp ult i64 %1479, -1
  %1485 = xor i64 %1479, -1
  %1486 = xor i64 %1479, %1480
  %1487 = and i64 %1485, %1486
  %1488 = icmp slt i64 %1487, 0
  store i1 %1483, i1* %az
  store i1 %1484, i1* %cf
  store i1 %1488, i1* %of
  %1489 = icmp eq i64 %1480, 0
  store i1 %1489, i1* %zf
  %1490 = icmp slt i64 %1480, 0
  store i1 %1490, i1* %sf
  %1491 = trunc i64 %1480 to i8
  %1492 = call i8 @llvm.ctpop.i8(i8 %1491)
  %1493 = and i8 %1492, 1
  %1494 = icmp eq i8 %1493, 0
  store i1 %1494, i1* %pf
  store volatile i64 50996, i64* @assembly_address
  %1495 = load i1* %zf
  %1496 = icmp eq i1 %1495, false
  br i1 %1496, label %block_c77b, label %block_c736

block_c736:                                       ; preds = %1439
  store volatile i64 50998, i64* @assembly_address
  %1497 = load i32* %stack_var_-80
  %1498 = sext i32 %1497 to i64
  %1499 = trunc i64 %1498 to i32
  store i32 %1499, i32* %40
  store i64 255, i64* %39
  %1500 = sub i64 %1498, 255
  %1501 = and i64 %1498, 15
  %1502 = sub i64 %1501, 15
  %1503 = icmp ugt i64 %1502, 15
  %1504 = icmp ult i64 %1498, 255
  %1505 = xor i64 %1498, 255
  %1506 = xor i64 %1498, %1500
  %1507 = and i64 %1505, %1506
  %1508 = icmp slt i64 %1507, 0
  store i1 %1503, i1* %az
  store i1 %1504, i1* %cf
  store i1 %1508, i1* %of
  %1509 = icmp eq i64 %1500, 0
  store i1 %1509, i1* %zf
  %1510 = icmp slt i64 %1500, 0
  store i1 %1510, i1* %sf
  %1511 = trunc i64 %1500 to i8
  %1512 = call i8 @llvm.ctpop.i8(i8 %1511)
  %1513 = and i8 %1512, 1
  %1514 = icmp eq i8 %1513, 0
  store i1 %1514, i1* %pf
  store volatile i64 51006, i64* @assembly_address
  %1515 = load i32* %40
  %1516 = sext i32 %1515 to i64
  %1517 = load i64* %39
  %1518 = icmp sle i64 %1516, %1517
  br i1 %1518, label %block_c74c, label %block_c740

block_c740:                                       ; preds = %block_c736
  store volatile i64 51008, i64* @assembly_address
  store i64 ptrtoint ([15 x i8]* @global_var_1212b to i64), i64* %rdi
  store volatile i64 51015, i64* @assembly_address
  %1519 = load i64* %rdi
  %1520 = inttoptr i64 %1519 to i8*
  %1521 = call i64 @gzip_error(i8* %1520)
  store i64 %1521, i64* %rax
  store i64 %1521, i64* %rax
  unreachable

block_c74c:                                       ; preds = %block_c736
  store volatile i64 51020, i64* @assembly_address
  %1522 = load i32* %stack_var_-80
  %1523 = sext i32 %1522 to i64
  store i64 %1523, i64* %rax
  store volatile i64 51024, i64* @assembly_address
  %1524 = load i64* %rax
  %1525 = trunc i64 %1524 to i32
  store i32 %1525, i32* %stack_var_-72
  store volatile i64 51028, i64* @assembly_address
  %1526 = load i32* %stack_var_-72
  %1527 = sext i32 %1526 to i64
  store i64 %1527, i64* %rax
  store volatile i64 51032, i64* @assembly_address
  %1528 = load i64* %rax
  %1529 = trunc i64 %1528 to i32
  store i32 %1529, i32* %stack_var_-124
  store volatile i64 51035, i64* @assembly_address
  %1530 = load i64** %stack_var_-120
  %1531 = ptrtoint i64* %1530 to i32
  %1532 = zext i32 %1531 to i64
  store i64 %1532, i64* %rax
  store volatile i64 51038, i64* @assembly_address
  %1533 = load i64* %rax
  %1534 = add i64 %1533, 1
  %1535 = trunc i64 %1534 to i32
  %1536 = zext i32 %1535 to i64
  store i64 %1536, i64* %rdx
  store volatile i64 51041, i64* @assembly_address
  %1537 = load i64* %rdx
  %1538 = trunc i64 %1537 to i32
  %1539 = inttoptr i32 %1538 to i64*
  store i64* %1539, i64** %stack_var_-120
  store volatile i64 51044, i64* @assembly_address
  %1540 = load i32* %stack_var_-124
  %1541 = zext i32 %1540 to i64
  store i64 %1541, i64* %rdx
  store volatile i64 51047, i64* @assembly_address
  %1542 = load i64* %rdx
  %1543 = trunc i64 %1542 to i32
  %1544 = zext i32 %1543 to i64
  store i64 %1544, i64* %rcx
  store volatile i64 51049, i64* @assembly_address
  %1545 = load i64* %rax
  %1546 = trunc i64 %1545 to i32
  %1547 = sext i32 %1546 to i64
  store i64 %1547, i64* %rdx
  store volatile i64 51052, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 51059, i64* @assembly_address
  %1548 = load i64* %rcx
  %1549 = trunc i64 %1548 to i8
  %1550 = load i64* %rdx
  %1551 = load i64* %rax
  %1552 = mul i64 %1551, 1
  %1553 = add i64 %1550, %1552
  %1554 = inttoptr i64 %1553 to i8*
  store i8 %1549, i8* %1554
  store volatile i64 51062, i64* @assembly_address
  br label %block_ca9d

block_c77b:                                       ; preds = %1439
  store volatile i64 51067, i64* @assembly_address
  %1555 = load i32* %stack_var_-80
  %1556 = sext i32 %1555 to i64
  %1557 = sub i64 %1556, 256
  %1558 = and i64 %1556, 15
  %1559 = icmp ugt i64 %1558, 15
  %1560 = icmp ult i64 %1556, 256
  %1561 = xor i64 %1556, 256
  %1562 = xor i64 %1556, %1557
  %1563 = and i64 %1561, %1562
  %1564 = icmp slt i64 %1563, 0
  store i1 %1559, i1* %az
  store i1 %1560, i1* %cf
  store i1 %1564, i1* %of
  %1565 = icmp eq i64 %1557, 0
  store i1 %1565, i1* %zf
  %1566 = icmp slt i64 %1557, 0
  store i1 %1566, i1* %sf
  %1567 = trunc i64 %1557 to i8
  %1568 = call i8 @llvm.ctpop.i8(i8 %1567)
  %1569 = and i8 %1568, 1
  %1570 = icmp eq i8 %1569, 0
  store i1 %1570, i1* %pf
  store volatile i64 51075, i64* @assembly_address
  %1571 = load i1* %zf
  %1572 = icmp eq i1 %1571, false
  br i1 %1572, label %block_c82f, label %block_c789

block_c789:                                       ; preds = %block_c77b
  store volatile i64 51081, i64* @assembly_address
  %1573 = load i32* bitcast (i64* @global_var_216548 to i32*)
  %1574 = zext i32 %1573 to i64
  store i64 %1574, i64* %rax
  store volatile i64 51087, i64* @assembly_address
  %1575 = load i64* %rax
  %1576 = trunc i64 %1575 to i32
  %1577 = load i64* %rax
  %1578 = trunc i64 %1577 to i32
  %1579 = and i32 %1576, %1578
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1580 = icmp eq i32 %1579, 0
  store i1 %1580, i1* %zf
  %1581 = icmp slt i32 %1579, 0
  store i1 %1581, i1* %sf
  %1582 = trunc i32 %1579 to i8
  %1583 = call i8 @llvm.ctpop.i8(i8 %1582)
  %1584 = and i8 %1583, 1
  %1585 = icmp eq i8 %1584, 0
  store i1 %1585, i1* %pf
  store volatile i64 51089, i64* @assembly_address
  %1586 = load i1* %zf
  br i1 %1586, label %block_c82f, label %block_c797

block_c797:                                       ; preds = %block_c789
  store volatile i64 51095, i64* @assembly_address
  store i64 256, i64* %rdx
  store volatile i64 51100, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 51105, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rdi
  store volatile i64 51112, i64* @assembly_address
  %1587 = load i64* %rdi
  %1588 = inttoptr i64 %1587 to i64*
  %1589 = load i64* %rsi
  %1590 = trunc i64 %1589 to i32
  %1591 = load i64* %rdx
  %1592 = trunc i64 %1591 to i32
  %1593 = call i64* @memset(i64* %1588, i32 %1590, i32 %1592)
  %1594 = ptrtoint i64* %1593 to i64
  store i64 %1594, i64* %rax
  %1595 = ptrtoint i64* %1593 to i64
  store i64 %1595, i64* %rax
  store volatile i64 51117, i64* @assembly_address
  %1596 = trunc i64 256 to i32
  store i32 %1596, i32* %stack_var_-56
  store volatile i64 51125, i64* @assembly_address
  %1597 = load i32* %stack_var_-64
  %1598 = sext i32 %1597 to i64
  store i64 %1598, i64* %rax
  store volatile i64 51129, i64* @assembly_address
  %1599 = load i64* %rax
  %1600 = add i64 %1599, -1
  store i64 %1600, i64* %rdi
  store volatile i64 51133, i64* @assembly_address
  %1601 = load i32* %stack_var_-112
  %1602 = zext i32 %1601 to i64
  store i64 %1602, i64* %rax
  store volatile i64 51136, i64* @assembly_address
  %1603 = load i64* %rax
  %1604 = trunc i64 %1603 to i32
  %1605 = load i1* %of
  %1606 = shl i32 %1604, 3
  %1607 = icmp eq i32 %1606, 0
  store i1 %1607, i1* %zf
  %1608 = icmp slt i32 %1606, 0
  store i1 %1608, i1* %sf
  %1609 = trunc i32 %1606 to i8
  %1610 = call i8 @llvm.ctpop.i8(i8 %1609)
  %1611 = and i8 %1610, 1
  %1612 = icmp eq i8 %1611, 0
  store i1 %1612, i1* %pf
  %1613 = zext i32 %1606 to i64
  store i64 %1613, i64* %rax
  %1614 = shl i32 %1604, 2
  %1615 = lshr i32 %1614, 31
  %1616 = trunc i32 %1615 to i1
  store i1 %1616, i1* %cf
  %1617 = lshr i32 %1606, 31
  %1618 = icmp ne i32 %1617, %1615
  %1619 = select i1 false, i1 %1618, i1 %1605
  store i1 %1619, i1* %of
  store volatile i64 51139, i64* @assembly_address
  %1620 = load i64* %rax
  %1621 = trunc i64 %1620 to i32
  %1622 = sext i32 %1621 to i64
  store i64 %1622, i64* %rcx
  store volatile i64 51142, i64* @assembly_address
  %1623 = load i32* %stack_var_-64
  %1624 = sext i32 %1623 to i64
  store i64 %1624, i64* %rax
  store volatile i64 51146, i64* @assembly_address
  %1625 = load i64* %rax
  %1626 = add i64 %1625, -1
  store i64 %1626, i64* %rdx
  store volatile i64 51150, i64* @assembly_address
  %1627 = load i32* %stack_var_-112
  %1628 = zext i32 %1627 to i64
  store i64 %1628, i64* %rax
  store volatile i64 51153, i64* @assembly_address
  %1629 = load i64* %rax
  %1630 = trunc i64 %1629 to i32
  %1631 = load i1* %of
  %1632 = shl i32 %1630, 3
  %1633 = icmp eq i32 %1632, 0
  store i1 %1633, i1* %zf
  %1634 = icmp slt i32 %1632, 0
  store i1 %1634, i1* %sf
  %1635 = trunc i32 %1632 to i8
  %1636 = call i8 @llvm.ctpop.i8(i8 %1635)
  %1637 = and i8 %1636, 1
  %1638 = icmp eq i8 %1637, 0
  store i1 %1638, i1* %pf
  %1639 = zext i32 %1632 to i64
  store i64 %1639, i64* %rax
  %1640 = shl i32 %1630, 2
  %1641 = lshr i32 %1640, 31
  %1642 = trunc i32 %1641 to i1
  store i1 %1642, i1* %cf
  %1643 = lshr i32 %1632, 31
  %1644 = icmp ne i32 %1643, %1641
  %1645 = select i1 false, i1 %1644, i1 %1631
  store i1 %1645, i1* %of
  store volatile i64 51156, i64* @assembly_address
  %1646 = load i64* %rax
  %1647 = trunc i64 %1646 to i32
  %1648 = sext i32 %1647 to i64
  store i64 %1648, i64* %rax
  store volatile i64 51158, i64* @assembly_address
  %1649 = load i64* %rax
  %1650 = load i64* %rdx
  %1651 = add i64 %1649, %1650
  %1652 = and i64 %1649, 15
  %1653 = and i64 %1650, 15
  %1654 = add i64 %1652, %1653
  %1655 = icmp ugt i64 %1654, 15
  %1656 = icmp ult i64 %1651, %1649
  %1657 = xor i64 %1649, %1651
  %1658 = xor i64 %1650, %1651
  %1659 = and i64 %1657, %1658
  %1660 = icmp slt i64 %1659, 0
  store i1 %1655, i1* %az
  store i1 %1656, i1* %cf
  store i1 %1660, i1* %of
  %1661 = icmp eq i64 %1651, 0
  store i1 %1661, i1* %zf
  %1662 = icmp slt i64 %1651, 0
  store i1 %1662, i1* %sf
  %1663 = trunc i64 %1651 to i8
  %1664 = call i8 @llvm.ctpop.i8(i8 %1663)
  %1665 = and i8 %1664, 1
  %1666 = icmp eq i8 %1665, 0
  store i1 %1666, i1* %pf
  store i64 %1651, i64* %rax
  store volatile i64 51161, i64* @assembly_address
  %1667 = load i32* %stack_var_-112
  %1668 = zext i32 %1667 to i64
  store i64 %1668, i64* %rdx
  store volatile i64 51164, i64* @assembly_address
  %1669 = load i64* %rdx
  %1670 = trunc i64 %1669 to i32
  %1671 = load i1* %of
  %1672 = shl i32 %1670, 3
  %1673 = icmp eq i32 %1672, 0
  store i1 %1673, i1* %zf
  %1674 = icmp slt i32 %1672, 0
  store i1 %1674, i1* %sf
  %1675 = trunc i32 %1672 to i8
  %1676 = call i8 @llvm.ctpop.i8(i8 %1675)
  %1677 = and i8 %1676, 1
  %1678 = icmp eq i8 %1677, 0
  store i1 %1678, i1* %pf
  %1679 = zext i32 %1672 to i64
  store i64 %1679, i64* %rdx
  %1680 = shl i32 %1670, 2
  %1681 = lshr i32 %1680, 31
  %1682 = trunc i32 %1681 to i1
  store i1 %1682, i1* %cf
  %1683 = lshr i32 %1672, 31
  %1684 = icmp ne i32 %1683, %1681
  %1685 = select i1 false, i1 %1684, i1 %1671
  store i1 %1685, i1* %of
  store volatile i64 51167, i64* @assembly_address
  %1686 = load i64* %rdx
  %1687 = trunc i64 %1686 to i32
  %1688 = sext i32 %1687 to i64
  store i64 %1688, i64* %rsi
  store volatile i64 51170, i64* @assembly_address
  %1689 = load i64* %rax
  %1690 = ashr i64 %1689, 63
  store i64 %1690, i64* %rdx
  store volatile i64 51172, i64* @assembly_address
  %1691 = load i64* %rsi
  %1692 = load i64* %rax
  %1693 = zext i64 %1692 to i128
  %1694 = load i64* %rdx
  %1695 = zext i64 %1694 to i128
  %1696 = shl i128 %1695, 64
  %1697 = or i128 %1696, %1693
  %1698 = zext i64 %1691 to i128
  %1699 = sdiv i128 %1697, %1698
  %1700 = trunc i128 %1699 to i64
  store i64 %1700, i64* %rax
  %1701 = srem i128 %1697, %1698
  %1702 = trunc i128 %1701 to i64
  store i64 %1702, i64* %rdx
  store volatile i64 51175, i64* @assembly_address
  %1703 = load i64* %rdx
  store i64 %1703, i64* %rax
  store volatile i64 51178, i64* @assembly_address
  %1704 = load i64* %rcx
  %1705 = load i64* %rax
  %1706 = sub i64 %1704, %1705
  %1707 = and i64 %1704, 15
  %1708 = and i64 %1705, 15
  %1709 = sub i64 %1707, %1708
  %1710 = icmp ugt i64 %1709, 15
  %1711 = icmp ult i64 %1704, %1705
  %1712 = xor i64 %1704, %1705
  %1713 = xor i64 %1704, %1706
  %1714 = and i64 %1712, %1713
  %1715 = icmp slt i64 %1714, 0
  store i1 %1710, i1* %az
  store i1 %1711, i1* %cf
  store i1 %1715, i1* %of
  %1716 = icmp eq i64 %1706, 0
  store i1 %1716, i1* %zf
  %1717 = icmp slt i64 %1706, 0
  store i1 %1717, i1* %sf
  %1718 = trunc i64 %1706 to i8
  %1719 = call i8 @llvm.ctpop.i8(i8 %1718)
  %1720 = and i8 %1719, 1
  %1721 = icmp eq i8 %1720, 0
  store i1 %1721, i1* %pf
  store i64 %1706, i64* %rcx
  store volatile i64 51181, i64* @assembly_address
  %1722 = load i64* %rcx
  store i64 %1722, i64* %rax
  store volatile i64 51184, i64* @assembly_address
  %1723 = load i64* %rax
  %1724 = load i64* %rdi
  %1725 = add i64 %1723, %1724
  %1726 = and i64 %1723, 15
  %1727 = and i64 %1724, 15
  %1728 = add i64 %1726, %1727
  %1729 = icmp ugt i64 %1728, 15
  %1730 = icmp ult i64 %1725, %1723
  %1731 = xor i64 %1723, %1725
  %1732 = xor i64 %1724, %1725
  %1733 = and i64 %1731, %1732
  %1734 = icmp slt i64 %1733, 0
  store i1 %1729, i1* %az
  store i1 %1730, i1* %cf
  store i1 %1734, i1* %of
  %1735 = icmp eq i64 %1725, 0
  store i1 %1735, i1* %zf
  %1736 = icmp slt i64 %1725, 0
  store i1 %1736, i1* %sf
  %1737 = trunc i64 %1725 to i8
  %1738 = call i8 @llvm.ctpop.i8(i8 %1737)
  %1739 = and i8 %1738, 1
  %1740 = icmp eq i8 %1739, 0
  store i1 %1740, i1* %pf
  store i64 %1725, i64* %rax
  store volatile i64 51187, i64* @assembly_address
  %1741 = load i64* %rax
  %1742 = trunc i64 %1741 to i32
  store i32 %1742, i32* %stack_var_-64
  store volatile i64 51191, i64* @assembly_address
  store i32 9, i32* %stack_var_-112
  store volatile i64 51198, i64* @assembly_address
  %1743 = load i32* %stack_var_-112
  %1744 = zext i32 %1743 to i64
  store i64 %1744, i64* %rax
  store volatile i64 51201, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 51206, i64* @assembly_address
  %1745 = load i64* %rax
  %1746 = trunc i64 %1745 to i32
  %1747 = zext i32 %1746 to i64
  store i64 %1747, i64* %rcx
  store volatile i64 51208, i64* @assembly_address
  %1748 = load i64* %rdx
  %1749 = load i64* %rcx
  %1750 = trunc i64 %1749 to i8
  %1751 = zext i8 %1750 to i64
  %1752 = and i64 %1751, 63
  %1753 = load i1* %of
  %1754 = icmp eq i64 %1752, 0
  br i1 %1754, label %1771, label %1755

; <label>:1755                                    ; preds = %block_c797
  %1756 = shl i64 %1748, %1752
  %1757 = icmp eq i64 %1756, 0
  store i1 %1757, i1* %zf
  %1758 = icmp slt i64 %1756, 0
  store i1 %1758, i1* %sf
  %1759 = trunc i64 %1756 to i8
  %1760 = call i8 @llvm.ctpop.i8(i8 %1759)
  %1761 = and i8 %1760, 1
  %1762 = icmp eq i8 %1761, 0
  store i1 %1762, i1* %pf
  store i64 %1756, i64* %rdx
  %1763 = sub i64 %1752, 1
  %1764 = shl i64 %1748, %1763
  %1765 = lshr i64 %1764, 63
  %1766 = trunc i64 %1765 to i1
  store i1 %1766, i1* %cf
  %1767 = lshr i64 %1756, 63
  %1768 = icmp ne i64 %1767, %1765
  %1769 = icmp eq i64 %1752, 1
  %1770 = select i1 %1769, i1 %1768, i1 %1753
  store i1 %1770, i1* %of
  br label %1771

; <label>:1771                                    ; preds = %block_c797, %1755
  store volatile i64 51211, i64* @assembly_address
  %1772 = load i64* %rdx
  store i64 %1772, i64* %rax
  store volatile i64 51214, i64* @assembly_address
  %1773 = load i64* %rax
  %1774 = sub i64 %1773, 1
  %1775 = and i64 %1773, 15
  %1776 = sub i64 %1775, 1
  %1777 = icmp ugt i64 %1776, 15
  %1778 = icmp ult i64 %1773, 1
  %1779 = xor i64 %1773, 1
  %1780 = xor i64 %1773, %1774
  %1781 = and i64 %1779, %1780
  %1782 = icmp slt i64 %1781, 0
  store i1 %1777, i1* %az
  store i1 %1778, i1* %cf
  store i1 %1782, i1* %of
  %1783 = icmp eq i64 %1774, 0
  store i1 %1783, i1* %zf
  %1784 = icmp slt i64 %1774, 0
  store i1 %1784, i1* %sf
  %1785 = trunc i64 %1774 to i8
  %1786 = call i8 @llvm.ctpop.i8(i8 %1785)
  %1787 = and i8 %1786, 1
  %1788 = icmp eq i8 %1787, 0
  store i1 %1788, i1* %pf
  store i64 %1774, i64* %rax
  store volatile i64 51218, i64* @assembly_address
  %1789 = load i64* %rax
  store i64 %1789, i64* %stack_var_-48
  store volatile i64 51222, i64* @assembly_address
  %1790 = load i32* %stack_var_-112
  %1791 = zext i32 %1790 to i64
  store i64 %1791, i64* %rax
  store volatile i64 51225, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 51230, i64* @assembly_address
  %1792 = load i64* %rax
  %1793 = trunc i64 %1792 to i32
  %1794 = zext i32 %1793 to i64
  store i64 %1794, i64* %rcx
  store volatile i64 51232, i64* @assembly_address
  %1795 = load i64* %rdx
  %1796 = trunc i64 %1795 to i32
  %1797 = load i64* %rcx
  %1798 = trunc i64 %1797 to i8
  %1799 = zext i8 %1798 to i32
  %1800 = and i32 %1799, 31
  %1801 = load i1* %of
  %1802 = icmp eq i32 %1800, 0
  br i1 %1802, label %1820, label %1803

; <label>:1803                                    ; preds = %1771
  %1804 = shl i32 %1796, %1800
  %1805 = icmp eq i32 %1804, 0
  store i1 %1805, i1* %zf
  %1806 = icmp slt i32 %1804, 0
  store i1 %1806, i1* %sf
  %1807 = trunc i32 %1804 to i8
  %1808 = call i8 @llvm.ctpop.i8(i8 %1807)
  %1809 = and i8 %1808, 1
  %1810 = icmp eq i8 %1809, 0
  store i1 %1810, i1* %pf
  %1811 = zext i32 %1804 to i64
  store i64 %1811, i64* %rdx
  %1812 = sub i32 %1800, 1
  %1813 = shl i32 %1796, %1812
  %1814 = lshr i32 %1813, 31
  %1815 = trunc i32 %1814 to i1
  store i1 %1815, i1* %cf
  %1816 = lshr i32 %1804, 31
  %1817 = icmp ne i32 %1816, %1814
  %1818 = icmp eq i32 %1800, 1
  %1819 = select i1 %1818, i1 %1817, i1 %1801
  store i1 %1819, i1* %of
  br label %1820

; <label>:1820                                    ; preds = %1771, %1803
  store volatile i64 51234, i64* @assembly_address
  %1821 = load i64* %rdx
  %1822 = trunc i64 %1821 to i32
  %1823 = zext i32 %1822 to i64
  store i64 %1823, i64* %rax
  store volatile i64 51236, i64* @assembly_address
  %1824 = load i64* %rax
  %1825 = trunc i64 %1824 to i32
  %1826 = sub i32 %1825, 1
  %1827 = and i32 %1825, 15
  %1828 = sub i32 %1827, 1
  %1829 = icmp ugt i32 %1828, 15
  %1830 = icmp ult i32 %1825, 1
  %1831 = xor i32 %1825, 1
  %1832 = xor i32 %1825, %1826
  %1833 = and i32 %1831, %1832
  %1834 = icmp slt i32 %1833, 0
  store i1 %1829, i1* %az
  store i1 %1830, i1* %cf
  store i1 %1834, i1* %of
  %1835 = icmp eq i32 %1826, 0
  store i1 %1835, i1* %zf
  %1836 = icmp slt i32 %1826, 0
  store i1 %1836, i1* %sf
  %1837 = trunc i32 %1826 to i8
  %1838 = call i8 @llvm.ctpop.i8(i8 %1837)
  %1839 = and i8 %1838, 1
  %1840 = icmp eq i8 %1839, 0
  store i1 %1840, i1* %pf
  %1841 = zext i32 %1826 to i64
  store i64 %1841, i64* %rax
  store volatile i64 51239, i64* @assembly_address
  %1842 = load i64* %rax
  %1843 = trunc i64 %1842 to i32
  store i32 %1843, i32* %stack_var_-116
  store volatile i64 51242, i64* @assembly_address
  br label %block_c4f0

block_c82f:                                       ; preds = %block_c789, %block_c77b
  store volatile i64 51247, i64* @assembly_address
  %1844 = load i32* %stack_var_-80
  %1845 = sext i32 %1844 to i64
  store i64 %1845, i64* %rax
  store volatile i64 51251, i64* @assembly_address
  %1846 = load i64* %rax
  %1847 = trunc i64 %1846 to i32
  store i32 %1847, i32* %stack_var_-16
  store volatile i64 51255, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a87e to i64), i64* %rax
  store volatile i64 51262, i64* @assembly_address
  %1848 = load i64* %rax
  store i64 %1848, i64* %stack_var_-88
  store volatile i64 51266, i64* @assembly_address
  %1849 = load i32* %stack_var_-80
  %1850 = sext i32 %1849 to i64
  store i64 %1850, i64* %rax
  store volatile i64 51270, i64* @assembly_address
  %1851 = load i64* %rax
  %1852 = load i32* %stack_var_-56
  %1853 = sext i32 %1852 to i64
  %1854 = trunc i64 %1851 to i32
  store i32 %1854, i32* %37
  %1855 = trunc i64 %1853 to i32
  store i32 %1855, i32* %35
  %1856 = sub i64 %1851, %1853
  %1857 = and i64 %1851, 15
  %1858 = and i64 %1853, 15
  %1859 = sub i64 %1857, %1858
  %1860 = icmp ugt i64 %1859, 15
  %1861 = icmp ult i64 %1851, %1853
  %1862 = xor i64 %1851, %1853
  %1863 = xor i64 %1851, %1856
  %1864 = and i64 %1862, %1863
  %1865 = icmp slt i64 %1864, 0
  store i1 %1860, i1* %az
  store i1 %1861, i1* %cf
  store i1 %1865, i1* %of
  %1866 = icmp eq i64 %1856, 0
  store i1 %1866, i1* %zf
  %1867 = icmp slt i64 %1856, 0
  store i1 %1867, i1* %sf
  %1868 = trunc i64 %1856 to i8
  %1869 = call i8 @llvm.ctpop.i8(i8 %1868)
  %1870 = and i8 %1869, 1
  %1871 = icmp eq i8 %1870, 0
  store i1 %1871, i1* %pf
  store volatile i64 51274, i64* @assembly_address
  %1872 = load i32* %37
  %1873 = sext i32 %1872 to i64
  %1874 = load i32* %35
  %1875 = sext i32 %1874 to i64
  %1876 = icmp slt i64 %1873, %1875
  br i1 %1876, label %block_c90a, label %block_c850

block_c850:                                       ; preds = %block_c82f
  store volatile i64 51280, i64* @assembly_address
  %1877 = load i32* %stack_var_-80
  %1878 = sext i32 %1877 to i64
  store i64 %1878, i64* %rax
  store volatile i64 51284, i64* @assembly_address
  %1879 = load i64* %rax
  %1880 = load i32* %stack_var_-56
  %1881 = sext i32 %1880 to i64
  %1882 = trunc i64 %1879 to i32
  store i32 %1882, i32* %33
  %1883 = trunc i64 %1881 to i32
  store i32 %1883, i32* %31
  %1884 = sub i64 %1879, %1881
  %1885 = and i64 %1879, 15
  %1886 = and i64 %1881, 15
  %1887 = sub i64 %1885, %1886
  %1888 = icmp ugt i64 %1887, 15
  %1889 = icmp ult i64 %1879, %1881
  %1890 = xor i64 %1879, %1881
  %1891 = xor i64 %1879, %1884
  %1892 = and i64 %1890, %1891
  %1893 = icmp slt i64 %1892, 0
  store i1 %1888, i1* %az
  store i1 %1889, i1* %cf
  store i1 %1893, i1* %of
  %1894 = icmp eq i64 %1884, 0
  store i1 %1894, i1* %zf
  %1895 = icmp slt i64 %1884, 0
  store i1 %1895, i1* %sf
  %1896 = trunc i64 %1884 to i8
  %1897 = call i8 @llvm.ctpop.i8(i8 %1896)
  %1898 = and i8 %1897, 1
  %1899 = icmp eq i8 %1898, 0
  store i1 %1899, i1* %pf
  store volatile i64 51288, i64* @assembly_address
  %1900 = load i32* %33
  %1901 = sext i32 %1900 to i64
  %1902 = load i32* %31
  %1903 = sext i32 %1902 to i64
  %1904 = icmp sle i64 %1901, %1903
  br i1 %1904, label %block_c8ba, label %block_c85a

block_c85a:                                       ; preds = %block_c850
  store volatile i64 51290, i64* @assembly_address
  %1905 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %1906 = zext i32 %1905 to i64
  store i64 %1906, i64* %rax
  store volatile i64 51296, i64* @assembly_address
  %1907 = load i64* %rax
  %1908 = trunc i64 %1907 to i32
  %1909 = load i64* %rax
  %1910 = trunc i64 %1909 to i32
  %1911 = and i32 %1908, %1910
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1912 = icmp eq i32 %1911, 0
  store i1 %1912, i1* %zf
  %1913 = icmp slt i32 %1911, 0
  store i1 %1913, i1* %sf
  %1914 = trunc i32 %1911 to i8
  %1915 = call i8 @llvm.ctpop.i8(i8 %1914)
  %1916 = and i8 %1915, 1
  %1917 = icmp eq i8 %1916, 0
  store i1 %1917, i1* %pf
  store volatile i64 51298, i64* @assembly_address
  %1918 = load i1* %zf
  %1919 = icmp eq i1 %1918, false
  br i1 %1919, label %block_c898, label %block_c864

block_c864:                                       ; preds = %block_c85a
  store volatile i64 51300, i64* @assembly_address
  %1920 = load i64** %stack_var_-120
  %1921 = ptrtoint i64* %1920 to i32
  %1922 = and i32 %1921, 15
  %1923 = icmp ugt i32 %1922, 15
  %1924 = icmp ult i32 %1921, 0
  %1925 = xor i32 %1921, 0
  %1926 = and i32 %1925, 0
  %1927 = icmp slt i32 %1926, 0
  store i1 %1923, i1* %az
  store i1 %1924, i1* %cf
  store i1 %1927, i1* %of
  store i64* %1920, i64** %29
  store i32 0, i32* %28
  %1928 = icmp eq i32 %1921, 0
  store i1 %1928, i1* %zf
  %1929 = icmp slt i32 %1921, 0
  store i1 %1929, i1* %sf
  %1930 = trunc i32 %1921 to i8
  %1931 = call i8 @llvm.ctpop.i8(i8 %1930)
  %1932 = and i8 %1931, 1
  %1933 = icmp eq i8 %1932, 0
  store i1 %1933, i1* %pf
  store volatile i64 51304, i64* @assembly_address
  %1934 = load i64** %29
  %1935 = ptrtoint i64* %1934 to i32
  %1936 = load i32* %28
  %1937 = icmp sle i32 %1935, %1936
  br i1 %1937, label %block_c898, label %block_c86a

block_c86a:                                       ; preds = %block_c864
  store volatile i64 51306, i64* @assembly_address
  %1938 = load i64** %stack_var_-120
  %1939 = ptrtoint i64* %1938 to i32
  %1940 = zext i32 %1939 to i64
  store i64 %1940, i64* %rdx
  store volatile i64 51309, i64* @assembly_address
  %1941 = load i32* %stack_var_-144
  %1942 = zext i32 %1941 to i64
  store i64 %1942, i64* %rax
  store volatile i64 51315, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rsi
  store volatile i64 51322, i64* @assembly_address
  %1943 = load i64* %rax
  %1944 = trunc i64 %1943 to i32
  %1945 = zext i32 %1944 to i64
  store i64 %1945, i64* %rdi
  store volatile i64 51324, i64* @assembly_address
  %1946 = load i64* %rdi
  %1947 = load i64* %rsi
  %1948 = inttoptr i64 %1947 to i8*
  %1949 = load i64* %rdx
  %1950 = trunc i64 %1946 to i32
  %1951 = call i64 @write_buf(i32 %1950, i8* %1948, i64 %1949)
  store i64 %1951, i64* %rax
  store i64 %1951, i64* %rax
  store volatile i64 51329, i64* @assembly_address
  %1952 = load i64** %stack_var_-120
  %1953 = ptrtoint i64* %1952 to i32
  %1954 = zext i32 %1953 to i64
  store i64 %1954, i64* %rax
  store volatile i64 51332, i64* @assembly_address
  %1955 = load i64* %rax
  %1956 = trunc i64 %1955 to i32
  %1957 = sext i32 %1956 to i64
  store i64 %1957, i64* %rdx
  store volatile i64 51335, i64* @assembly_address
  %1958 = load i64* @global_var_25f4c0
  store i64 %1958, i64* %rax
  store volatile i64 51342, i64* @assembly_address
  %1959 = load i64* %rax
  %1960 = load i64* %rdx
  %1961 = add i64 %1959, %1960
  %1962 = and i64 %1959, 15
  %1963 = and i64 %1960, 15
  %1964 = add i64 %1962, %1963
  %1965 = icmp ugt i64 %1964, 15
  %1966 = icmp ult i64 %1961, %1959
  %1967 = xor i64 %1959, %1961
  %1968 = xor i64 %1960, %1961
  %1969 = and i64 %1967, %1968
  %1970 = icmp slt i64 %1969, 0
  store i1 %1965, i1* %az
  store i1 %1966, i1* %cf
  store i1 %1970, i1* %of
  %1971 = icmp eq i64 %1961, 0
  store i1 %1971, i1* %zf
  %1972 = icmp slt i64 %1961, 0
  store i1 %1972, i1* %sf
  %1973 = trunc i64 %1961 to i8
  %1974 = call i8 @llvm.ctpop.i8(i8 %1973)
  %1975 = and i8 %1974, 1
  %1976 = icmp eq i8 %1975, 0
  store i1 %1976, i1* %pf
  store i64 %1961, i64* %rax
  store volatile i64 51345, i64* @assembly_address
  %1977 = load i64* %rax
  store i64 %1977, i64* @global_var_25f4c0
  br label %block_c898

block_c898:                                       ; preds = %block_c86a, %block_c864, %block_c85a
  store volatile i64 51352, i64* @assembly_address
  %1978 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1979 = zext i32 %1978 to i64
  store i64 %1979, i64* %rax
  store volatile i64 51358, i64* @assembly_address
  %1980 = load i64* %rax
  %1981 = trunc i64 %1980 to i32
  %1982 = load i64* %rax
  %1983 = trunc i64 %1982 to i32
  %1984 = and i32 %1981, %1983
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1985 = icmp eq i32 %1984, 0
  store i1 %1985, i1* %zf
  %1986 = icmp slt i32 %1984, 0
  store i1 %1986, i1* %sf
  %1987 = trunc i32 %1984 to i8
  %1988 = call i8 @llvm.ctpop.i8(i8 %1987)
  %1989 = and i8 %1988, 1
  %1990 = icmp eq i8 %1989, 0
  store i1 %1990, i1* %pf
  store volatile i64 51360, i64* @assembly_address
  %1991 = load i1* %zf
  br i1 %1991, label %block_c8ab, label %block_c8a2

block_c8a2:                                       ; preds = %block_c898
  store volatile i64 51362, i64* @assembly_address
  store i64 ptrtoint ([15 x i8]* @global_var_1212b to i64), i64* %rax
  store volatile i64 51369, i64* @assembly_address
  br label %block_c8b2

block_c8ab:                                       ; preds = %block_c898
  store volatile i64 51371, i64* @assembly_address
  store i64 ptrtoint ([46 x i8]* @global_var_12140 to i64), i64* %rax
  br label %block_c8b2

block_c8b2:                                       ; preds = %block_c8ab, %block_c8a2
  store volatile i64 51378, i64* @assembly_address
  %1992 = load i64* %rax
  store i64 %1992, i64* %rdi
  store volatile i64 51381, i64* @assembly_address
  %1993 = load i64* %rdi
  %1994 = inttoptr i64 %1993 to i8*
  %1995 = call i64 @gzip_error(i8* %1994)
  store i64 %1995, i64* %rax
  store i64 %1995, i64* %rax
  unreachable

block_c8ba:                                       ; preds = %block_c850
  store volatile i64 51386, i64* @assembly_address
  %1996 = load i64* %stack_var_-88
  %1997 = sub i64 %1996, 1
  %1998 = and i64 %1996, 15
  %1999 = sub i64 %1998, 1
  %2000 = icmp ugt i64 %1999, 15
  %2001 = icmp ult i64 %1996, 1
  %2002 = xor i64 %1996, 1
  %2003 = xor i64 %1996, %1997
  %2004 = and i64 %2002, %2003
  %2005 = icmp slt i64 %2004, 0
  store i1 %2000, i1* %az
  store i1 %2001, i1* %cf
  store i1 %2005, i1* %of
  %2006 = icmp eq i64 %1997, 0
  store i1 %2006, i1* %zf
  %2007 = icmp slt i64 %1997, 0
  store i1 %2007, i1* %sf
  %2008 = trunc i64 %1997 to i8
  %2009 = call i8 @llvm.ctpop.i8(i8 %2008)
  %2010 = and i8 %2009, 1
  %2011 = icmp eq i8 %2010, 0
  store i1 %2011, i1* %pf
  store i64 %1997, i64* %stack_var_-88
  store volatile i64 51391, i64* @assembly_address
  %2012 = load i32* %stack_var_-124
  %2013 = zext i32 %2012 to i64
  store i64 %2013, i64* %rax
  store volatile i64 51394, i64* @assembly_address
  %2014 = load i64* %rax
  %2015 = trunc i64 %2014 to i32
  %2016 = zext i32 %2015 to i64
  store i64 %2016, i64* %rdx
  store volatile i64 51396, i64* @assembly_address
  %2017 = load i64* %stack_var_-88
  store i64 %2017, i64* %rax
  store volatile i64 51400, i64* @assembly_address
  %2018 = load i64* %rdx
  %2019 = trunc i64 %2018 to i8
  %2020 = load i64* %rax
  %2021 = inttoptr i64 %2020 to i8*
  store i8 %2019, i8* %2021
  store volatile i64 51402, i64* @assembly_address
  %2022 = load i32* %stack_var_-72
  %2023 = sext i32 %2022 to i64
  store i64 %2023, i64* %rax
  store volatile i64 51406, i64* @assembly_address
  %2024 = load i64* %rax
  %2025 = trunc i64 %2024 to i32
  store i32 %2025, i32* %stack_var_-80
  store volatile i64 51410, i64* @assembly_address
  br label %block_c90a

block_c8d4:                                       ; preds = %block_c90a
  store volatile i64 51412, i64* @assembly_address
  %2026 = load i64* %stack_var_-88
  %2027 = sub i64 %2026, 1
  %2028 = and i64 %2026, 15
  %2029 = sub i64 %2028, 1
  %2030 = icmp ugt i64 %2029, 15
  %2031 = icmp ult i64 %2026, 1
  %2032 = xor i64 %2026, 1
  %2033 = xor i64 %2026, %2027
  %2034 = and i64 %2032, %2033
  %2035 = icmp slt i64 %2034, 0
  store i1 %2030, i1* %az
  store i1 %2031, i1* %cf
  store i1 %2035, i1* %of
  %2036 = icmp eq i64 %2027, 0
  store i1 %2036, i1* %zf
  %2037 = icmp slt i64 %2027, 0
  store i1 %2037, i1* %sf
  %2038 = trunc i64 %2027 to i8
  %2039 = call i8 @llvm.ctpop.i8(i8 %2038)
  %2040 = and i8 %2039, 1
  %2041 = icmp eq i8 %2040, 0
  store i1 %2041, i1* %pf
  store i64 %2027, i64* %stack_var_-88
  store volatile i64 51417, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdx
  store volatile i64 51424, i64* @assembly_address
  %2042 = load i32* %stack_var_-80
  %2043 = sext i32 %2042 to i64
  store i64 %2043, i64* %rax
  store volatile i64 51428, i64* @assembly_address
  %2044 = load i64* %rax
  %2045 = load i64* %rdx
  %2046 = add i64 %2044, %2045
  %2047 = and i64 %2044, 15
  %2048 = and i64 %2045, 15
  %2049 = add i64 %2047, %2048
  %2050 = icmp ugt i64 %2049, 15
  %2051 = icmp ult i64 %2046, %2044
  %2052 = xor i64 %2044, %2046
  %2053 = xor i64 %2045, %2046
  %2054 = and i64 %2052, %2053
  %2055 = icmp slt i64 %2054, 0
  store i1 %2050, i1* %az
  store i1 %2051, i1* %cf
  store i1 %2055, i1* %of
  %2056 = icmp eq i64 %2046, 0
  store i1 %2056, i1* %zf
  %2057 = icmp slt i64 %2046, 0
  store i1 %2057, i1* %sf
  %2058 = trunc i64 %2046 to i8
  %2059 = call i8 @llvm.ctpop.i8(i8 %2058)
  %2060 = and i8 %2059, 1
  %2061 = icmp eq i8 %2060, 0
  store i1 %2061, i1* %pf
  store i64 %2046, i64* %rax
  store volatile i64 51431, i64* @assembly_address
  %2062 = load i64* %rax
  %2063 = inttoptr i64 %2062 to i8*
  %2064 = load i8* %2063
  %2065 = zext i8 %2064 to i64
  store i64 %2065, i64* %rdx
  store volatile i64 51434, i64* @assembly_address
  %2066 = load i64* %stack_var_-88
  store i64 %2066, i64* %rax
  store volatile i64 51438, i64* @assembly_address
  %2067 = load i64* %rdx
  %2068 = trunc i64 %2067 to i8
  %2069 = load i64* %rax
  %2070 = inttoptr i64 %2069 to i8*
  store i8 %2068, i8* %2070
  store volatile i64 51440, i64* @assembly_address
  %2071 = load i32* %stack_var_-80
  %2072 = sext i32 %2071 to i64
  store i64 %2072, i64* %rax
  store volatile i64 51444, i64* @assembly_address
  %2073 = load i64* %rax
  %2074 = load i64* %rax
  %2075 = mul i64 %2074, 1
  %2076 = add i64 %2073, %2075
  store i64 %2076, i64* %rdx
  store volatile i64 51448, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 51455, i64* @assembly_address
  %2077 = load i64* %rdx
  %2078 = load i64* %rax
  %2079 = mul i64 %2078, 1
  %2080 = add i64 %2077, %2079
  %2081 = inttoptr i64 %2080 to i16*
  %2082 = load i16* %2081
  %2083 = zext i16 %2082 to i64
  store i64 %2083, i64* %rax
  store volatile i64 51459, i64* @assembly_address
  %2084 = load i64* %rax
  %2085 = trunc i64 %2084 to i16
  %2086 = zext i16 %2085 to i64
  store i64 %2086, i64* %rax
  store volatile i64 51462, i64* @assembly_address
  %2087 = load i64* %rax
  %2088 = trunc i64 %2087 to i32
  store i32 %2088, i32* %stack_var_-80
  br label %block_c90a

block_c90a:                                       ; preds = %block_c8d4, %block_c8ba, %block_c82f
  store volatile i64 51466, i64* @assembly_address
  %2089 = load i32* %stack_var_-80
  %2090 = sext i32 %2089 to i64
  store i64 %2090, i64* %rax
  store volatile i64 51470, i64* @assembly_address
  %2091 = load i64* %rax
  %2092 = sub i64 %2091, 255
  %2093 = and i64 %2091, 15
  %2094 = sub i64 %2093, 15
  %2095 = icmp ugt i64 %2094, 15
  %2096 = icmp ult i64 %2091, 255
  %2097 = xor i64 %2091, 255
  %2098 = xor i64 %2091, %2092
  %2099 = and i64 %2097, %2098
  %2100 = icmp slt i64 %2099, 0
  store i1 %2095, i1* %az
  store i1 %2096, i1* %cf
  store i1 %2100, i1* %of
  %2101 = icmp eq i64 %2092, 0
  store i1 %2101, i1* %zf
  %2102 = icmp slt i64 %2092, 0
  store i1 %2102, i1* %sf
  %2103 = trunc i64 %2092 to i8
  %2104 = call i8 @llvm.ctpop.i8(i8 %2103)
  %2105 = and i8 %2104, 1
  %2106 = icmp eq i8 %2105, 0
  store i1 %2106, i1* %pf
  store volatile i64 51476, i64* @assembly_address
  %2107 = load i1* %cf
  %2108 = load i1* %zf
  %2109 = or i1 %2107, %2108
  %2110 = icmp ne i1 %2109, true
  br i1 %2110, label %block_c8d4, label %block_c916

block_c916:                                       ; preds = %block_c90a
  store volatile i64 51478, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdx
  store volatile i64 51485, i64* @assembly_address
  %2111 = load i32* %stack_var_-80
  %2112 = sext i32 %2111 to i64
  store i64 %2112, i64* %rax
  store volatile i64 51489, i64* @assembly_address
  %2113 = load i64* %rax
  %2114 = load i64* %rdx
  %2115 = add i64 %2113, %2114
  %2116 = and i64 %2113, 15
  %2117 = and i64 %2114, 15
  %2118 = add i64 %2116, %2117
  %2119 = icmp ugt i64 %2118, 15
  %2120 = icmp ult i64 %2115, %2113
  %2121 = xor i64 %2113, %2115
  %2122 = xor i64 %2114, %2115
  %2123 = and i64 %2121, %2122
  %2124 = icmp slt i64 %2123, 0
  store i1 %2119, i1* %az
  store i1 %2120, i1* %cf
  store i1 %2124, i1* %of
  %2125 = icmp eq i64 %2115, 0
  store i1 %2125, i1* %zf
  %2126 = icmp slt i64 %2115, 0
  store i1 %2126, i1* %sf
  %2127 = trunc i64 %2115 to i8
  %2128 = call i8 @llvm.ctpop.i8(i8 %2127)
  %2129 = and i8 %2128, 1
  %2130 = icmp eq i8 %2129, 0
  store i1 %2130, i1* %pf
  store i64 %2115, i64* %rax
  store volatile i64 51492, i64* @assembly_address
  %2131 = load i64* %rax
  %2132 = inttoptr i64 %2131 to i8*
  %2133 = load i8* %2132
  %2134 = zext i8 %2133 to i64
  store i64 %2134, i64* %rax
  store volatile i64 51495, i64* @assembly_address
  %2135 = load i64* %rax
  %2136 = trunc i64 %2135 to i8
  %2137 = zext i8 %2136 to i64
  store i64 %2137, i64* %rax
  store volatile i64 51498, i64* @assembly_address
  %2138 = load i64* %rax
  %2139 = trunc i64 %2138 to i32
  store i32 %2139, i32* %stack_var_-124
  store volatile i64 51501, i64* @assembly_address
  %2140 = load i64* %stack_var_-88
  %2141 = sub i64 %2140, 1
  %2142 = and i64 %2140, 15
  %2143 = sub i64 %2142, 1
  %2144 = icmp ugt i64 %2143, 15
  %2145 = icmp ult i64 %2140, 1
  %2146 = xor i64 %2140, 1
  %2147 = xor i64 %2140, %2141
  %2148 = and i64 %2146, %2147
  %2149 = icmp slt i64 %2148, 0
  store i1 %2144, i1* %az
  store i1 %2145, i1* %cf
  store i1 %2149, i1* %of
  %2150 = icmp eq i64 %2141, 0
  store i1 %2150, i1* %zf
  %2151 = icmp slt i64 %2141, 0
  store i1 %2151, i1* %sf
  %2152 = trunc i64 %2141 to i8
  %2153 = call i8 @llvm.ctpop.i8(i8 %2152)
  %2154 = and i8 %2153, 1
  %2155 = icmp eq i8 %2154, 0
  store i1 %2155, i1* %pf
  store i64 %2141, i64* %stack_var_-88
  store volatile i64 51506, i64* @assembly_address
  %2156 = load i32* %stack_var_-124
  %2157 = zext i32 %2156 to i64
  store i64 %2157, i64* %rax
  store volatile i64 51509, i64* @assembly_address
  %2158 = load i64* %rax
  %2159 = trunc i64 %2158 to i32
  %2160 = zext i32 %2159 to i64
  store i64 %2160, i64* %rdx
  store volatile i64 51511, i64* @assembly_address
  %2161 = load i64* %stack_var_-88
  store i64 %2161, i64* %rax
  store volatile i64 51515, i64* @assembly_address
  %2162 = load i64* %rdx
  %2163 = trunc i64 %2162 to i8
  %2164 = load i64* %rax
  %2165 = inttoptr i64 %2164 to i8*
  store i8 %2163, i8* %2165
  store volatile i64 51517, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a87e to i64), i64* %rdx
  store volatile i64 51524, i64* @assembly_address
  %2166 = load i64* %stack_var_-88
  store i64 %2166, i64* %rax
  store volatile i64 51528, i64* @assembly_address
  %2167 = load i64* %rdx
  %2168 = load i64* %rax
  %2169 = sub i64 %2167, %2168
  %2170 = and i64 %2167, 15
  %2171 = and i64 %2168, 15
  %2172 = sub i64 %2170, %2171
  %2173 = icmp ugt i64 %2172, 15
  %2174 = icmp ult i64 %2167, %2168
  %2175 = xor i64 %2167, %2168
  %2176 = xor i64 %2167, %2169
  %2177 = and i64 %2175, %2176
  %2178 = icmp slt i64 %2177, 0
  store i1 %2173, i1* %az
  store i1 %2174, i1* %cf
  store i1 %2178, i1* %of
  %2179 = icmp eq i64 %2169, 0
  store i1 %2179, i1* %zf
  %2180 = icmp slt i64 %2169, 0
  store i1 %2180, i1* %sf
  %2181 = trunc i64 %2169 to i8
  %2182 = call i8 @llvm.ctpop.i8(i8 %2181)
  %2183 = and i8 %2182, 1
  %2184 = icmp eq i8 %2183, 0
  store i1 %2184, i1* %pf
  store i64 %2169, i64* %rdx
  store volatile i64 51531, i64* @assembly_address
  %2185 = load i64* %rdx
  store i64 %2185, i64* %rax
  store volatile i64 51534, i64* @assembly_address
  %2186 = load i64* %rax
  %2187 = trunc i64 %2186 to i32
  %2188 = inttoptr i32 %2187 to i64*
  store i64* %2188, i64** %stack_var_-100
  store volatile i64 51537, i64* @assembly_address
  %2189 = load i64** %stack_var_-100
  %2190 = ptrtoint i64* %2189 to i32
  %2191 = zext i32 %2190 to i64
  store i64 %2191, i64* %rdx
  store volatile i64 51540, i64* @assembly_address
  %2192 = load i64** %stack_var_-120
  %2193 = ptrtoint i64* %2192 to i32
  %2194 = zext i32 %2193 to i64
  store i64 %2194, i64* %rax
  store volatile i64 51543, i64* @assembly_address
  %2195 = load i64* %rax
  %2196 = trunc i64 %2195 to i32
  %2197 = load i64* %rdx
  %2198 = trunc i64 %2197 to i32
  %2199 = add i32 %2196, %2198
  %2200 = and i32 %2196, 15
  %2201 = and i32 %2198, 15
  %2202 = add i32 %2200, %2201
  %2203 = icmp ugt i32 %2202, 15
  %2204 = icmp ult i32 %2199, %2196
  %2205 = xor i32 %2196, %2199
  %2206 = xor i32 %2198, %2199
  %2207 = and i32 %2205, %2206
  %2208 = icmp slt i32 %2207, 0
  store i1 %2203, i1* %az
  store i1 %2204, i1* %cf
  store i1 %2208, i1* %of
  %2209 = icmp eq i32 %2199, 0
  store i1 %2209, i1* %zf
  %2210 = icmp slt i32 %2199, 0
  store i1 %2210, i1* %sf
  %2211 = trunc i32 %2199 to i8
  %2212 = call i8 @llvm.ctpop.i8(i8 %2211)
  %2213 = and i8 %2212, 1
  %2214 = icmp eq i8 %2213, 0
  store i1 %2214, i1* %pf
  %2215 = zext i32 %2199 to i64
  store i64 %2215, i64* %rax
  store volatile i64 51545, i64* @assembly_address
  %2216 = load i64* %rax
  %2217 = trunc i64 %2216 to i32
  store i64 %2216, i64* %27
  store i32 16383, i32* %26
  %2218 = sub i32 %2217, 16383
  %2219 = and i32 %2217, 15
  %2220 = sub i32 %2219, 15
  %2221 = icmp ugt i32 %2220, 15
  %2222 = icmp ult i32 %2217, 16383
  %2223 = xor i32 %2217, 16383
  %2224 = xor i32 %2217, %2218
  %2225 = and i32 %2223, %2224
  %2226 = icmp slt i32 %2225, 0
  store i1 %2221, i1* %az
  store i1 %2222, i1* %cf
  store i1 %2226, i1* %of
  %2227 = icmp eq i32 %2218, 0
  store i1 %2227, i1* %zf
  %2228 = icmp slt i32 %2218, 0
  store i1 %2228, i1* %sf
  %2229 = trunc i32 %2218 to i8
  %2230 = call i8 @llvm.ctpop.i8(i8 %2229)
  %2231 = and i8 %2230, 1
  %2232 = icmp eq i8 %2231, 0
  store i1 %2232, i1* %pf
  store volatile i64 51550, i64* @assembly_address
  %2233 = load i64* %27
  %2234 = load i32* %26
  %2235 = trunc i64 %2233 to i32
  %2236 = icmp sle i32 %2235, %2234
  br i1 %2236, label %block_ca1e, label %block_c964

block_c964:                                       ; preds = %block_c9f5, %block_c916
  store volatile i64 51556, i64* @assembly_address
  store i64 16384, i64* %rax
  store volatile i64 51561, i64* @assembly_address
  %2237 = load i64* %rax
  %2238 = trunc i64 %2237 to i32
  %2239 = load i64** %stack_var_-120
  %2240 = ptrtoint i64* %2239 to i32
  %2241 = sub i32 %2238, %2240
  %2242 = and i32 %2238, 15
  %2243 = and i32 %2240, 15
  %2244 = sub i32 %2242, %2243
  %2245 = icmp ugt i32 %2244, 15
  %2246 = icmp ult i32 %2238, %2240
  %2247 = xor i32 %2238, %2240
  %2248 = xor i32 %2238, %2241
  %2249 = and i32 %2247, %2248
  %2250 = icmp slt i32 %2249, 0
  store i1 %2245, i1* %az
  store i1 %2246, i1* %cf
  store i1 %2250, i1* %of
  %2251 = icmp eq i32 %2241, 0
  store i1 %2251, i1* %zf
  %2252 = icmp slt i32 %2241, 0
  store i1 %2252, i1* %sf
  %2253 = trunc i32 %2241 to i8
  %2254 = call i8 @llvm.ctpop.i8(i8 %2253)
  %2255 = and i8 %2254, 1
  %2256 = icmp eq i8 %2255, 0
  store i1 %2256, i1* %pf
  %2257 = zext i32 %2241 to i64
  store i64 %2257, i64* %rax
  store volatile i64 51564, i64* @assembly_address
  %2258 = load i64** %stack_var_-100
  %2259 = ptrtoint i64* %2258 to i32
  %2260 = load i64* %rax
  %2261 = trunc i64 %2260 to i32
  store i64* %2258, i64** %24
  store i64 %2260, i64* %23
  %2262 = sub i32 %2259, %2261
  %2263 = and i32 %2259, 15
  %2264 = and i32 %2261, 15
  %2265 = sub i32 %2263, %2264
  %2266 = icmp ugt i32 %2265, 15
  %2267 = icmp ult i32 %2259, %2261
  %2268 = xor i32 %2259, %2261
  %2269 = xor i32 %2259, %2262
  %2270 = and i32 %2268, %2269
  %2271 = icmp slt i32 %2270, 0
  store i1 %2266, i1* %az
  store i1 %2267, i1* %cf
  store i1 %2271, i1* %of
  %2272 = icmp eq i32 %2262, 0
  store i1 %2272, i1* %zf
  %2273 = icmp slt i32 %2262, 0
  store i1 %2273, i1* %sf
  %2274 = trunc i32 %2262 to i8
  %2275 = call i8 @llvm.ctpop.i8(i8 %2274)
  %2276 = and i8 %2275, 1
  %2277 = icmp eq i8 %2276, 0
  store i1 %2277, i1* %pf
  store volatile i64 51567, i64* @assembly_address
  %2278 = load i64** %24
  %2279 = ptrtoint i64* %2278 to i32
  %2280 = load i64* %23
  %2281 = sext i32 %2279 to i64
  %2282 = icmp sle i64 %2281, %2280
  br i1 %2282, label %block_c97c, label %block_c971

block_c971:                                       ; preds = %block_c964
  store volatile i64 51569, i64* @assembly_address
  store i64 16384, i64* %rax
  store volatile i64 51574, i64* @assembly_address
  %2283 = load i64* %rax
  %2284 = trunc i64 %2283 to i32
  %2285 = load i64** %stack_var_-120
  %2286 = ptrtoint i64* %2285 to i32
  %2287 = sub i32 %2284, %2286
  %2288 = and i32 %2284, 15
  %2289 = and i32 %2286, 15
  %2290 = sub i32 %2288, %2289
  %2291 = icmp ugt i32 %2290, 15
  %2292 = icmp ult i32 %2284, %2286
  %2293 = xor i32 %2284, %2286
  %2294 = xor i32 %2284, %2287
  %2295 = and i32 %2293, %2294
  %2296 = icmp slt i32 %2295, 0
  store i1 %2291, i1* %az
  store i1 %2292, i1* %cf
  store i1 %2296, i1* %of
  %2297 = icmp eq i32 %2287, 0
  store i1 %2297, i1* %zf
  %2298 = icmp slt i32 %2287, 0
  store i1 %2298, i1* %sf
  %2299 = trunc i32 %2287 to i8
  %2300 = call i8 @llvm.ctpop.i8(i8 %2299)
  %2301 = and i8 %2300, 1
  %2302 = icmp eq i8 %2301, 0
  store i1 %2302, i1* %pf
  %2303 = zext i32 %2287 to i64
  store i64 %2303, i64* %rax
  store volatile i64 51577, i64* @assembly_address
  %2304 = load i64* %rax
  %2305 = trunc i64 %2304 to i32
  %2306 = inttoptr i32 %2305 to i64*
  store i64* %2306, i64** %stack_var_-100
  br label %block_c97c

block_c97c:                                       ; preds = %block_c971, %block_c964
  store volatile i64 51580, i64* @assembly_address
  %2307 = load i64** %stack_var_-100
  %2308 = ptrtoint i64* %2307 to i32
  %2309 = and i32 %2308, 15
  %2310 = icmp ugt i32 %2309, 15
  %2311 = icmp ult i32 %2308, 0
  %2312 = xor i32 %2308, 0
  %2313 = and i32 %2312, 0
  %2314 = icmp slt i32 %2313, 0
  store i1 %2310, i1* %az
  store i1 %2311, i1* %cf
  store i1 %2314, i1* %of
  store i64* %2307, i64** %21
  store i32 0, i32* %20
  %2315 = icmp eq i32 %2308, 0
  store i1 %2315, i1* %zf
  %2316 = icmp slt i32 %2308, 0
  store i1 %2316, i1* %sf
  %2317 = trunc i32 %2308 to i8
  %2318 = call i8 @llvm.ctpop.i8(i8 %2317)
  %2319 = and i8 %2318, 1
  %2320 = icmp eq i8 %2319, 0
  store i1 %2320, i1* %pf
  store volatile i64 51584, i64* @assembly_address
  %2321 = load i64** %21
  %2322 = ptrtoint i64* %2321 to i32
  %2323 = load i32* %20
  %2324 = icmp sle i32 %2322, %2323
  br i1 %2324, label %block_c9ad, label %block_c982

block_c982:                                       ; preds = %block_c97c
  store volatile i64 51586, i64* @assembly_address
  %2325 = load i64** %stack_var_-100
  %2326 = ptrtoint i64* %2325 to i32
  %2327 = zext i32 %2326 to i64
  store i64 %2327, i64* %rax
  store volatile i64 51589, i64* @assembly_address
  %2328 = load i64* %rax
  %2329 = trunc i64 %2328 to i32
  %2330 = sext i32 %2329 to i64
  store i64 %2330, i64* %rdx
  store volatile i64 51592, i64* @assembly_address
  %2331 = load i64** %stack_var_-120
  %2332 = ptrtoint i64* %2331 to i32
  %2333 = zext i32 %2332 to i64
  store i64 %2333, i64* %rax
  store volatile i64 51595, i64* @assembly_address
  %2334 = load i64* %rax
  %2335 = trunc i64 %2334 to i32
  %2336 = sext i32 %2335 to i64
  store i64 %2336, i64* %rcx
  store volatile i64 51598, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 51605, i64* @assembly_address
  %2337 = load i64* %rcx
  %2338 = load i64* %rax
  %2339 = add i64 %2337, %2338
  %2340 = and i64 %2337, 15
  %2341 = and i64 %2338, 15
  %2342 = add i64 %2340, %2341
  %2343 = icmp ugt i64 %2342, 15
  %2344 = icmp ult i64 %2339, %2337
  %2345 = xor i64 %2337, %2339
  %2346 = xor i64 %2338, %2339
  %2347 = and i64 %2345, %2346
  %2348 = icmp slt i64 %2347, 0
  store i1 %2343, i1* %az
  store i1 %2344, i1* %cf
  store i1 %2348, i1* %of
  %2349 = icmp eq i64 %2339, 0
  store i1 %2349, i1* %zf
  %2350 = icmp slt i64 %2339, 0
  store i1 %2350, i1* %sf
  %2351 = trunc i64 %2339 to i8
  %2352 = call i8 @llvm.ctpop.i8(i8 %2351)
  %2353 = and i8 %2352, 1
  %2354 = icmp eq i8 %2353, 0
  store i1 %2354, i1* %pf
  store i64 %2339, i64* %rcx
  store volatile i64 51608, i64* @assembly_address
  %2355 = load i64* %stack_var_-88
  store i64 %2355, i64* %rax
  store volatile i64 51612, i64* @assembly_address
  %2356 = load i64* %rax
  store i64 %2356, i64* %rsi
  store volatile i64 51615, i64* @assembly_address
  %2357 = load i64* %rcx
  store i64 %2357, i64* %rdi
  store volatile i64 51618, i64* @assembly_address
  %2358 = load i64* %rdi
  %2359 = inttoptr i64 %2358 to i64*
  %2360 = load i64* %rsi
  %2361 = inttoptr i64 %2360 to i64*
  %2362 = load i64* %rdx
  %2363 = trunc i64 %2362 to i32
  %2364 = call i64* @memcpy(i64* %2359, i64* %2361, i32 %2363)
  %2365 = ptrtoint i64* %2364 to i64
  store i64 %2365, i64* %rax
  %2366 = ptrtoint i64* %2364 to i64
  store i64 %2366, i64* %rax
  store volatile i64 51623, i64* @assembly_address
  %2367 = load i64** %stack_var_-100
  %2368 = ptrtoint i64* %2367 to i32
  %2369 = zext i32 %2368 to i64
  store i64 %2369, i64* %rax
  store volatile i64 51626, i64* @assembly_address
  %2370 = load i64** %stack_var_-120
  %2371 = ptrtoint i64* %2370 to i32
  %2372 = load i64* %rax
  %2373 = trunc i64 %2372 to i32
  %2374 = add i32 %2371, %2373
  %2375 = and i32 %2371, 15
  %2376 = and i32 %2373, 15
  %2377 = add i32 %2375, %2376
  %2378 = icmp ugt i32 %2377, 15
  %2379 = icmp ult i32 %2374, %2371
  %2380 = xor i32 %2371, %2374
  %2381 = xor i32 %2373, %2374
  %2382 = and i32 %2380, %2381
  %2383 = icmp slt i32 %2382, 0
  store i1 %2378, i1* %az
  store i1 %2379, i1* %cf
  store i1 %2383, i1* %of
  %2384 = icmp eq i32 %2374, 0
  store i1 %2384, i1* %zf
  %2385 = icmp slt i32 %2374, 0
  store i1 %2385, i1* %sf
  %2386 = trunc i32 %2374 to i8
  %2387 = call i8 @llvm.ctpop.i8(i8 %2386)
  %2388 = and i8 %2387, 1
  %2389 = icmp eq i8 %2388, 0
  store i1 %2389, i1* %pf
  %2390 = inttoptr i32 %2374 to i64*
  store i64* %2390, i64** %stack_var_-120
  br label %block_c9ad

block_c9ad:                                       ; preds = %block_c982, %block_c97c
  store volatile i64 51629, i64* @assembly_address
  %2391 = load i64** %stack_var_-120
  %2392 = ptrtoint i64* %2391 to i32
  store i64* %2391, i64** %18
  store i32 16383, i32* %17
  %2393 = sub i32 %2392, 16383
  %2394 = and i32 %2392, 15
  %2395 = sub i32 %2394, 15
  %2396 = icmp ugt i32 %2395, 15
  %2397 = icmp ult i32 %2392, 16383
  %2398 = xor i32 %2392, 16383
  %2399 = xor i32 %2392, %2393
  %2400 = and i32 %2398, %2399
  %2401 = icmp slt i32 %2400, 0
  store i1 %2396, i1* %az
  store i1 %2397, i1* %cf
  store i1 %2401, i1* %of
  %2402 = icmp eq i32 %2393, 0
  store i1 %2402, i1* %zf
  %2403 = icmp slt i32 %2393, 0
  store i1 %2403, i1* %sf
  %2404 = trunc i32 %2393 to i8
  %2405 = call i8 @llvm.ctpop.i8(i8 %2404)
  %2406 = and i8 %2405, 1
  %2407 = icmp eq i8 %2406, 0
  store i1 %2407, i1* %pf
  store volatile i64 51636, i64* @assembly_address
  %2408 = load i64** %18
  %2409 = ptrtoint i64* %2408 to i32
  %2410 = load i32* %17
  %2411 = icmp sle i32 %2409, %2410
  br i1 %2411, label %block_c9f5, label %block_c9b6

block_c9b6:                                       ; preds = %block_c9ad
  store volatile i64 51638, i64* @assembly_address
  %2412 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %2413 = zext i32 %2412 to i64
  store i64 %2413, i64* %rax
  store volatile i64 51644, i64* @assembly_address
  %2414 = load i64* %rax
  %2415 = trunc i64 %2414 to i32
  %2416 = load i64* %rax
  %2417 = trunc i64 %2416 to i32
  %2418 = and i32 %2415, %2417
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2419 = icmp eq i32 %2418, 0
  store i1 %2419, i1* %zf
  %2420 = icmp slt i32 %2418, 0
  store i1 %2420, i1* %sf
  %2421 = trunc i32 %2418 to i8
  %2422 = call i8 @llvm.ctpop.i8(i8 %2421)
  %2423 = and i8 %2422, 1
  %2424 = icmp eq i8 %2423, 0
  store i1 %2424, i1* %pf
  store volatile i64 51646, i64* @assembly_address
  %2425 = load i1* %zf
  %2426 = icmp eq i1 %2425, false
  br i1 %2426, label %block_c9ee, label %block_c9c0

block_c9c0:                                       ; preds = %block_c9b6
  store volatile i64 51648, i64* @assembly_address
  %2427 = load i64** %stack_var_-120
  %2428 = ptrtoint i64* %2427 to i32
  %2429 = zext i32 %2428 to i64
  store i64 %2429, i64* %rdx
  store volatile i64 51651, i64* @assembly_address
  %2430 = load i32* %stack_var_-144
  %2431 = zext i32 %2430 to i64
  store i64 %2431, i64* %rax
  store volatile i64 51657, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rsi
  store volatile i64 51664, i64* @assembly_address
  %2432 = load i64* %rax
  %2433 = trunc i64 %2432 to i32
  %2434 = zext i32 %2433 to i64
  store i64 %2434, i64* %rdi
  store volatile i64 51666, i64* @assembly_address
  %2435 = load i64* %rdi
  %2436 = load i64* %rsi
  %2437 = inttoptr i64 %2436 to i8*
  %2438 = load i64* %rdx
  %2439 = trunc i64 %2435 to i32
  %2440 = call i64 @write_buf(i32 %2439, i8* %2437, i64 %2438)
  store i64 %2440, i64* %rax
  store i64 %2440, i64* %rax
  store volatile i64 51671, i64* @assembly_address
  %2441 = load i64** %stack_var_-120
  %2442 = ptrtoint i64* %2441 to i32
  %2443 = zext i32 %2442 to i64
  store i64 %2443, i64* %rax
  store volatile i64 51674, i64* @assembly_address
  %2444 = load i64* %rax
  %2445 = trunc i64 %2444 to i32
  %2446 = sext i32 %2445 to i64
  store i64 %2446, i64* %rdx
  store volatile i64 51677, i64* @assembly_address
  %2447 = load i64* @global_var_25f4c0
  store i64 %2447, i64* %rax
  store volatile i64 51684, i64* @assembly_address
  %2448 = load i64* %rax
  %2449 = load i64* %rdx
  %2450 = add i64 %2448, %2449
  %2451 = and i64 %2448, 15
  %2452 = and i64 %2449, 15
  %2453 = add i64 %2451, %2452
  %2454 = icmp ugt i64 %2453, 15
  %2455 = icmp ult i64 %2450, %2448
  %2456 = xor i64 %2448, %2450
  %2457 = xor i64 %2449, %2450
  %2458 = and i64 %2456, %2457
  %2459 = icmp slt i64 %2458, 0
  store i1 %2454, i1* %az
  store i1 %2455, i1* %cf
  store i1 %2459, i1* %of
  %2460 = icmp eq i64 %2450, 0
  store i1 %2460, i1* %zf
  %2461 = icmp slt i64 %2450, 0
  store i1 %2461, i1* %sf
  %2462 = trunc i64 %2450 to i8
  %2463 = call i8 @llvm.ctpop.i8(i8 %2462)
  %2464 = and i8 %2463, 1
  %2465 = icmp eq i8 %2464, 0
  store i1 %2465, i1* %pf
  store i64 %2450, i64* %rax
  store volatile i64 51687, i64* @assembly_address
  %2466 = load i64* %rax
  store i64 %2466, i64* @global_var_25f4c0
  br label %block_c9ee

block_c9ee:                                       ; preds = %block_c9c0, %block_c9b6
  store volatile i64 51694, i64* @assembly_address
  %2467 = inttoptr i32 0 to i64*
  store i64* %2467, i64** %stack_var_-120
  br label %block_c9f5

block_c9f5:                                       ; preds = %block_c9ee, %block_c9ad
  store volatile i64 51701, i64* @assembly_address
  %2468 = load i64** %stack_var_-100
  %2469 = ptrtoint i64* %2468 to i32
  %2470 = zext i32 %2469 to i64
  store i64 %2470, i64* %rax
  store volatile i64 51704, i64* @assembly_address
  %2471 = load i64* %rax
  %2472 = trunc i64 %2471 to i32
  %2473 = sext i32 %2472 to i64
  store i64 %2473, i64* %rax
  store volatile i64 51706, i64* @assembly_address
  %2474 = load i64* %stack_var_-88
  %2475 = load i64* %rax
  %2476 = add i64 %2474, %2475
  %2477 = and i64 %2474, 15
  %2478 = and i64 %2475, 15
  %2479 = add i64 %2477, %2478
  %2480 = icmp ugt i64 %2479, 15
  %2481 = icmp ult i64 %2476, %2474
  %2482 = xor i64 %2474, %2476
  %2483 = xor i64 %2475, %2476
  %2484 = and i64 %2482, %2483
  %2485 = icmp slt i64 %2484, 0
  store i1 %2480, i1* %az
  store i1 %2481, i1* %cf
  store i1 %2485, i1* %of
  %2486 = icmp eq i64 %2476, 0
  store i1 %2486, i1* %zf
  %2487 = icmp slt i64 %2476, 0
  store i1 %2487, i1* %sf
  %2488 = trunc i64 %2476 to i8
  %2489 = call i8 @llvm.ctpop.i8(i8 %2488)
  %2490 = and i8 %2489, 1
  %2491 = icmp eq i8 %2490, 0
  store i1 %2491, i1* %pf
  store i64 %2476, i64* %stack_var_-88
  store volatile i64 51710, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a87e to i64), i64* %rdx
  store volatile i64 51717, i64* @assembly_address
  %2492 = load i64* %stack_var_-88
  store i64 %2492, i64* %rax
  store volatile i64 51721, i64* @assembly_address
  %2493 = load i64* %rdx
  %2494 = load i64* %rax
  %2495 = sub i64 %2493, %2494
  %2496 = and i64 %2493, 15
  %2497 = and i64 %2494, 15
  %2498 = sub i64 %2496, %2497
  %2499 = icmp ugt i64 %2498, 15
  %2500 = icmp ult i64 %2493, %2494
  %2501 = xor i64 %2493, %2494
  %2502 = xor i64 %2493, %2495
  %2503 = and i64 %2501, %2502
  %2504 = icmp slt i64 %2503, 0
  store i1 %2499, i1* %az
  store i1 %2500, i1* %cf
  store i1 %2504, i1* %of
  %2505 = icmp eq i64 %2495, 0
  store i1 %2505, i1* %zf
  %2506 = icmp slt i64 %2495, 0
  store i1 %2506, i1* %sf
  %2507 = trunc i64 %2495 to i8
  %2508 = call i8 @llvm.ctpop.i8(i8 %2507)
  %2509 = and i8 %2508, 1
  %2510 = icmp eq i8 %2509, 0
  store i1 %2510, i1* %pf
  store i64 %2495, i64* %rdx
  store volatile i64 51724, i64* @assembly_address
  %2511 = load i64* %rdx
  store i64 %2511, i64* %rax
  store volatile i64 51727, i64* @assembly_address
  %2512 = load i64* %rax
  %2513 = trunc i64 %2512 to i32
  %2514 = inttoptr i32 %2513 to i64*
  store i64* %2514, i64** %stack_var_-100
  store volatile i64 51730, i64* @assembly_address
  %2515 = load i64** %stack_var_-100
  %2516 = ptrtoint i64* %2515 to i32
  %2517 = and i32 %2516, 15
  %2518 = icmp ugt i32 %2517, 15
  %2519 = icmp ult i32 %2516, 0
  %2520 = xor i32 %2516, 0
  %2521 = and i32 %2520, 0
  %2522 = icmp slt i32 %2521, 0
  store i1 %2518, i1* %az
  store i1 %2519, i1* %cf
  store i1 %2522, i1* %of
  %2523 = icmp eq i32 %2516, 0
  store i1 %2523, i1* %zf
  %2524 = icmp slt i32 %2516, 0
  store i1 %2524, i1* %sf
  %2525 = trunc i32 %2516 to i8
  %2526 = call i8 @llvm.ctpop.i8(i8 %2525)
  %2527 = and i8 %2526, 1
  %2528 = icmp eq i8 %2527, 0
  store i1 %2528, i1* %pf
  store volatile i64 51734, i64* @assembly_address
  %2529 = load i1* %zf
  %2530 = load i1* %sf
  %2531 = load i1* %of
  %2532 = icmp eq i1 %2530, %2531
  %2533 = icmp eq i1 %2529, false
  %2534 = icmp eq i1 %2532, %2533
  br i1 %2534, label %block_c964, label %block_ca1c

block_ca1c:                                       ; preds = %block_c9f5
  store volatile i64 51740, i64* @assembly_address
  br label %block_ca49

block_ca1e:                                       ; preds = %block_c916
  store volatile i64 51742, i64* @assembly_address
  %2535 = load i64** %stack_var_-100
  %2536 = ptrtoint i64* %2535 to i32
  %2537 = zext i32 %2536 to i64
  store i64 %2537, i64* %rax
  store volatile i64 51745, i64* @assembly_address
  %2538 = load i64* %rax
  %2539 = trunc i64 %2538 to i32
  %2540 = sext i32 %2539 to i64
  store i64 %2540, i64* %rdx
  store volatile i64 51748, i64* @assembly_address
  %2541 = load i64** %stack_var_-120
  %2542 = ptrtoint i64* %2541 to i32
  %2543 = zext i32 %2542 to i64
  store i64 %2543, i64* %rax
  store volatile i64 51751, i64* @assembly_address
  %2544 = load i64* %rax
  %2545 = trunc i64 %2544 to i32
  %2546 = sext i32 %2545 to i64
  store i64 %2546, i64* %rcx
  store volatile i64 51754, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 51761, i64* @assembly_address
  %2547 = load i64* %rcx
  %2548 = load i64* %rax
  %2549 = add i64 %2547, %2548
  %2550 = and i64 %2547, 15
  %2551 = and i64 %2548, 15
  %2552 = add i64 %2550, %2551
  %2553 = icmp ugt i64 %2552, 15
  %2554 = icmp ult i64 %2549, %2547
  %2555 = xor i64 %2547, %2549
  %2556 = xor i64 %2548, %2549
  %2557 = and i64 %2555, %2556
  %2558 = icmp slt i64 %2557, 0
  store i1 %2553, i1* %az
  store i1 %2554, i1* %cf
  store i1 %2558, i1* %of
  %2559 = icmp eq i64 %2549, 0
  store i1 %2559, i1* %zf
  %2560 = icmp slt i64 %2549, 0
  store i1 %2560, i1* %sf
  %2561 = trunc i64 %2549 to i8
  %2562 = call i8 @llvm.ctpop.i8(i8 %2561)
  %2563 = and i8 %2562, 1
  %2564 = icmp eq i8 %2563, 0
  store i1 %2564, i1* %pf
  store i64 %2549, i64* %rcx
  store volatile i64 51764, i64* @assembly_address
  %2565 = load i64* %stack_var_-88
  store i64 %2565, i64* %rax
  store volatile i64 51768, i64* @assembly_address
  %2566 = load i64* %rax
  store i64 %2566, i64* %rsi
  store volatile i64 51771, i64* @assembly_address
  %2567 = load i64* %rcx
  store i64 %2567, i64* %rdi
  store volatile i64 51774, i64* @assembly_address
  %2568 = load i64* %rdi
  %2569 = inttoptr i64 %2568 to i64*
  %2570 = load i64* %rsi
  %2571 = inttoptr i64 %2570 to i64*
  %2572 = load i64* %rdx
  %2573 = trunc i64 %2572 to i32
  %2574 = call i64* @memcpy(i64* %2569, i64* %2571, i32 %2573)
  %2575 = ptrtoint i64* %2574 to i64
  store i64 %2575, i64* %rax
  %2576 = ptrtoint i64* %2574 to i64
  store i64 %2576, i64* %rax
  store volatile i64 51779, i64* @assembly_address
  %2577 = load i64** %stack_var_-100
  %2578 = ptrtoint i64* %2577 to i32
  %2579 = zext i32 %2578 to i64
  store i64 %2579, i64* %rax
  store volatile i64 51782, i64* @assembly_address
  %2580 = load i64** %stack_var_-120
  %2581 = ptrtoint i64* %2580 to i32
  %2582 = load i64* %rax
  %2583 = trunc i64 %2582 to i32
  %2584 = add i32 %2581, %2583
  %2585 = and i32 %2581, 15
  %2586 = and i32 %2583, 15
  %2587 = add i32 %2585, %2586
  %2588 = icmp ugt i32 %2587, 15
  %2589 = icmp ult i32 %2584, %2581
  %2590 = xor i32 %2581, %2584
  %2591 = xor i32 %2583, %2584
  %2592 = and i32 %2590, %2591
  %2593 = icmp slt i32 %2592, 0
  store i1 %2588, i1* %az
  store i1 %2589, i1* %cf
  store i1 %2593, i1* %of
  %2594 = icmp eq i32 %2584, 0
  store i1 %2594, i1* %zf
  %2595 = icmp slt i32 %2584, 0
  store i1 %2595, i1* %sf
  %2596 = trunc i32 %2584 to i8
  %2597 = call i8 @llvm.ctpop.i8(i8 %2596)
  %2598 = and i8 %2597, 1
  %2599 = icmp eq i8 %2598, 0
  store i1 %2599, i1* %pf
  %2600 = inttoptr i32 %2584 to i64*
  store i64* %2600, i64** %stack_var_-120
  br label %block_ca49

block_ca49:                                       ; preds = %block_ca1e, %block_ca1c
  store volatile i64 51785, i64* @assembly_address
  %2601 = load i32* %stack_var_-56
  %2602 = sext i32 %2601 to i64
  store i64 %2602, i64* %rax
  store volatile i64 51789, i64* @assembly_address
  %2603 = load i64* %rax
  %2604 = trunc i64 %2603 to i32
  store i32 %2604, i32* %stack_var_-80
  store volatile i64 51793, i64* @assembly_address
  %2605 = load i32* %stack_var_-80
  %2606 = sext i32 %2605 to i64
  store i64 %2606, i64* %rax
  store volatile i64 51797, i64* @assembly_address
  %2607 = load i64* %rax
  %2608 = load i64* %stack_var_-40
  %2609 = trunc i64 %2607 to i32
  store i32 %2609, i32* %15
  %2610 = trunc i64 %2608 to i32
  store i32 %2610, i32* %13
  %2611 = sub i64 %2607, %2608
  %2612 = and i64 %2607, 15
  %2613 = and i64 %2608, 15
  %2614 = sub i64 %2612, %2613
  %2615 = icmp ugt i64 %2614, 15
  %2616 = icmp ult i64 %2607, %2608
  %2617 = xor i64 %2607, %2608
  %2618 = xor i64 %2607, %2611
  %2619 = and i64 %2617, %2618
  %2620 = icmp slt i64 %2619, 0
  store i1 %2615, i1* %az
  store i1 %2616, i1* %cf
  store i1 %2620, i1* %of
  %2621 = icmp eq i64 %2611, 0
  store i1 %2621, i1* %zf
  %2622 = icmp slt i64 %2611, 0
  store i1 %2622, i1* %sf
  %2623 = trunc i64 %2611 to i8
  %2624 = call i8 @llvm.ctpop.i8(i8 %2623)
  %2625 = and i8 %2624, 1
  %2626 = icmp eq i8 %2625, 0
  store i1 %2626, i1* %pf
  store volatile i64 51801, i64* @assembly_address
  %2627 = load i32* %15
  %2628 = sext i32 %2627 to i64
  %2629 = load i32* %13
  %2630 = sext i32 %2629 to i64
  %2631 = icmp sge i64 %2628, %2630
  br i1 %2631, label %block_ca95, label %block_ca5b

block_ca5b:                                       ; preds = %block_ca49
  store volatile i64 51803, i64* @assembly_address
  %2632 = load i32* %stack_var_-72
  %2633 = sext i32 %2632 to i64
  store i64 %2633, i64* %rax
  store volatile i64 51807, i64* @assembly_address
  %2634 = load i64* %rax
  %2635 = trunc i64 %2634 to i32
  %2636 = zext i32 %2635 to i64
  store i64 %2636, i64* %rcx
  store volatile i64 51809, i64* @assembly_address
  %2637 = load i32* %stack_var_-80
  %2638 = sext i32 %2637 to i64
  store i64 %2638, i64* %rax
  store volatile i64 51813, i64* @assembly_address
  %2639 = load i64* %rax
  %2640 = load i64* %rax
  %2641 = mul i64 %2640, 1
  %2642 = add i64 %2639, %2641
  store i64 %2642, i64* %rdx
  store volatile i64 51817, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a880 to i64), i64* %rax
  store volatile i64 51824, i64* @assembly_address
  %2643 = load i64* %rcx
  %2644 = trunc i64 %2643 to i16
  %2645 = load i64* %rdx
  %2646 = load i64* %rax
  %2647 = mul i64 %2646, 1
  %2648 = add i64 %2645, %2647
  %2649 = inttoptr i64 %2648 to i16*
  store i16 %2644, i16* %2649
  store volatile i64 51828, i64* @assembly_address
  %2650 = load i32* %stack_var_-124
  %2651 = zext i32 %2650 to i64
  store i64 %2651, i64* %rax
  store volatile i64 51831, i64* @assembly_address
  %2652 = load i64* %rax
  %2653 = trunc i64 %2652 to i32
  %2654 = zext i32 %2653 to i64
  store i64 %2654, i64* %rcx
  store volatile i64 51833, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdx
  store volatile i64 51840, i64* @assembly_address
  %2655 = load i32* %stack_var_-80
  %2656 = sext i32 %2655 to i64
  store i64 %2656, i64* %rax
  store volatile i64 51844, i64* @assembly_address
  %2657 = load i64* %rax
  %2658 = load i64* %rdx
  %2659 = add i64 %2657, %2658
  %2660 = and i64 %2657, 15
  %2661 = and i64 %2658, 15
  %2662 = add i64 %2660, %2661
  %2663 = icmp ugt i64 %2662, 15
  %2664 = icmp ult i64 %2659, %2657
  %2665 = xor i64 %2657, %2659
  %2666 = xor i64 %2658, %2659
  %2667 = and i64 %2665, %2666
  %2668 = icmp slt i64 %2667, 0
  store i1 %2663, i1* %az
  store i1 %2664, i1* %cf
  store i1 %2668, i1* %of
  %2669 = icmp eq i64 %2659, 0
  store i1 %2669, i1* %zf
  %2670 = icmp slt i64 %2659, 0
  store i1 %2670, i1* %sf
  %2671 = trunc i64 %2659 to i8
  %2672 = call i8 @llvm.ctpop.i8(i8 %2671)
  %2673 = and i8 %2672, 1
  %2674 = icmp eq i8 %2673, 0
  store i1 %2674, i1* %pf
  store i64 %2659, i64* %rax
  store volatile i64 51847, i64* @assembly_address
  %2675 = load i64* %rcx
  %2676 = trunc i64 %2675 to i8
  %2677 = load i64* %rax
  %2678 = inttoptr i64 %2677 to i8*
  store i8 %2676, i8* %2678
  store volatile i64 51849, i64* @assembly_address
  %2679 = load i32* %stack_var_-80
  %2680 = sext i32 %2679 to i64
  store i64 %2680, i64* %rax
  store volatile i64 51853, i64* @assembly_address
  %2681 = load i64* %rax
  %2682 = add i64 %2681, 1
  %2683 = and i64 %2681, 15
  %2684 = add i64 %2683, 1
  %2685 = icmp ugt i64 %2684, 15
  %2686 = icmp ult i64 %2682, %2681
  %2687 = xor i64 %2681, %2682
  %2688 = xor i64 1, %2682
  %2689 = and i64 %2687, %2688
  %2690 = icmp slt i64 %2689, 0
  store i1 %2685, i1* %az
  store i1 %2686, i1* %cf
  store i1 %2690, i1* %of
  %2691 = icmp eq i64 %2682, 0
  store i1 %2691, i1* %zf
  %2692 = icmp slt i64 %2682, 0
  store i1 %2692, i1* %sf
  %2693 = trunc i64 %2682 to i8
  %2694 = call i8 @llvm.ctpop.i8(i8 %2693)
  %2695 = and i8 %2694, 1
  %2696 = icmp eq i8 %2695, 0
  store i1 %2696, i1* %pf
  store i64 %2682, i64* %rax
  store volatile i64 51857, i64* @assembly_address
  %2697 = load i64* %rax
  %2698 = trunc i64 %2697 to i32
  store i32 %2698, i32* %stack_var_-56
  br label %block_ca95

block_ca95:                                       ; preds = %block_ca5b, %block_ca49
  store volatile i64 51861, i64* @assembly_address
  %2699 = load i32* %stack_var_-16
  %2700 = sext i32 %2699 to i64
  store i64 %2700, i64* %rax
  store volatile i64 51865, i64* @assembly_address
  %2701 = load i64* %rax
  %2702 = trunc i64 %2701 to i32
  store i32 %2702, i32* %stack_var_-72
  br label %block_ca9d

block_ca9d:                                       ; preds = %block_ca95, %block_c74c, %block_c620
  store volatile i64 51869, i64* @assembly_address
  %2703 = load i64* %stack_var_-32
  store i64 %2703, i64* %rax
  store volatile i64 51873, i64* @assembly_address
  %2704 = load i64* %rax
  %2705 = load i32* %stack_var_-64
  %2706 = sext i32 %2705 to i64
  store i64 %2704, i64* %12
  store i64 %2706, i64* %11
  %2707 = sub i64 %2704, %2706
  %2708 = and i64 %2704, 15
  %2709 = and i64 %2706, 15
  %2710 = sub i64 %2708, %2709
  %2711 = icmp ugt i64 %2710, 15
  %2712 = icmp ult i64 %2704, %2706
  %2713 = xor i64 %2704, %2706
  %2714 = xor i64 %2704, %2707
  %2715 = and i64 %2713, %2714
  %2716 = icmp slt i64 %2715, 0
  store i1 %2711, i1* %az
  store i1 %2712, i1* %cf
  store i1 %2716, i1* %of
  %2717 = icmp eq i64 %2707, 0
  store i1 %2717, i1* %zf
  %2718 = icmp slt i64 %2707, 0
  store i1 %2718, i1* %sf
  %2719 = trunc i64 %2707 to i8
  %2720 = call i8 @llvm.ctpop.i8(i8 %2719)
  %2721 = and i8 %2720, 1
  %2722 = icmp eq i8 %2721, 0
  store i1 %2722, i1* %pf
  store volatile i64 51877, i64* @assembly_address
  %2723 = load i64* %12
  %2724 = load i64* %11
  %2725 = icmp sgt i64 %2723, %2724
  br i1 %2725, label %block_c629, label %block_caab

block_caab:                                       ; preds = %block_ca9d
  store volatile i64 51883, i64* @assembly_address
  %2726 = load i32* %stack_var_-108
  %2727 = and i32 %2726, 15
  %2728 = icmp ugt i32 %2727, 15
  %2729 = icmp ult i32 %2726, 0
  %2730 = xor i32 %2726, 0
  %2731 = and i32 %2730, 0
  %2732 = icmp slt i32 %2731, 0
  store i1 %2728, i1* %az
  store i1 %2729, i1* %cf
  store i1 %2732, i1* %of
  %2733 = icmp eq i32 %2726, 0
  store i1 %2733, i1* %zf
  %2734 = icmp slt i32 %2726, 0
  store i1 %2734, i1* %sf
  %2735 = trunc i32 %2726 to i8
  %2736 = call i8 @llvm.ctpop.i8(i8 %2735)
  %2737 = and i8 %2736, 1
  %2738 = icmp eq i8 %2737, 0
  store i1 %2738, i1* %pf
  store volatile i64 51887, i64* @assembly_address
  %2739 = load i1* %zf
  %2740 = icmp eq i1 %2739, false
  br i1 %2740, label %block_c4ef, label %block_cab5

block_cab5:                                       ; preds = %block_caab
  store volatile i64 51893, i64* @assembly_address
  %2741 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %2742 = zext i32 %2741 to i64
  store i64 %2742, i64* %rax
  store volatile i64 51899, i64* @assembly_address
  %2743 = load i64* %rax
  %2744 = trunc i64 %2743 to i32
  %2745 = load i64* %rax
  %2746 = trunc i64 %2745 to i32
  %2747 = and i32 %2744, %2746
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2748 = icmp eq i32 %2747, 0
  store i1 %2748, i1* %zf
  %2749 = icmp slt i32 %2747, 0
  store i1 %2749, i1* %sf
  %2750 = trunc i32 %2747 to i8
  %2751 = call i8 @llvm.ctpop.i8(i8 %2750)
  %2752 = and i8 %2751, 1
  %2753 = icmp eq i8 %2752, 0
  store i1 %2753, i1* %pf
  store volatile i64 51901, i64* @assembly_address
  %2754 = load i1* %zf
  %2755 = icmp eq i1 %2754, false
  br i1 %2755, label %block_caf3, label %block_cabf

block_cabf:                                       ; preds = %block_cab5
  store volatile i64 51903, i64* @assembly_address
  %2756 = load i64** %stack_var_-120
  %2757 = ptrtoint i64* %2756 to i32
  %2758 = and i32 %2757, 15
  %2759 = icmp ugt i32 %2758, 15
  %2760 = icmp ult i32 %2757, 0
  %2761 = xor i32 %2757, 0
  %2762 = and i32 %2761, 0
  %2763 = icmp slt i32 %2762, 0
  store i1 %2759, i1* %az
  store i1 %2760, i1* %cf
  store i1 %2763, i1* %of
  store i64* %2756, i64** %9
  store i32 0, i32* %8
  %2764 = icmp eq i32 %2757, 0
  store i1 %2764, i1* %zf
  %2765 = icmp slt i32 %2757, 0
  store i1 %2765, i1* %sf
  %2766 = trunc i32 %2757 to i8
  %2767 = call i8 @llvm.ctpop.i8(i8 %2766)
  %2768 = and i8 %2767, 1
  %2769 = icmp eq i8 %2768, 0
  store i1 %2769, i1* %pf
  store volatile i64 51907, i64* @assembly_address
  %2770 = load i64** %9
  %2771 = ptrtoint i64* %2770 to i32
  %2772 = load i32* %8
  %2773 = icmp sle i32 %2771, %2772
  br i1 %2773, label %block_caf3, label %block_cac5

block_cac5:                                       ; preds = %block_cabf
  store volatile i64 51909, i64* @assembly_address
  %2774 = load i64** %stack_var_-120
  %2775 = ptrtoint i64* %2774 to i32
  %2776 = zext i32 %2775 to i64
  store i64 %2776, i64* %rdx
  store volatile i64 51912, i64* @assembly_address
  %2777 = load i32* %stack_var_-144
  %2778 = zext i32 %2777 to i64
  store i64 %2778, i64* %rax
  store volatile i64 51918, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rsi
  store volatile i64 51925, i64* @assembly_address
  %2779 = load i64* %rax
  %2780 = trunc i64 %2779 to i32
  %2781 = zext i32 %2780 to i64
  store i64 %2781, i64* %rdi
  store volatile i64 51927, i64* @assembly_address
  %2782 = load i64* %rdi
  %2783 = load i64* %rsi
  %2784 = inttoptr i64 %2783 to i8*
  %2785 = load i64* %rdx
  %2786 = trunc i64 %2782 to i32
  %2787 = call i64 @write_buf(i32 %2786, i8* %2784, i64 %2785)
  store i64 %2787, i64* %rax
  store i64 %2787, i64* %rax
  store volatile i64 51932, i64* @assembly_address
  %2788 = load i64** %stack_var_-120
  %2789 = ptrtoint i64* %2788 to i32
  %2790 = zext i32 %2789 to i64
  store i64 %2790, i64* %rax
  store volatile i64 51935, i64* @assembly_address
  %2791 = load i64* %rax
  %2792 = trunc i64 %2791 to i32
  %2793 = sext i32 %2792 to i64
  store i64 %2793, i64* %rdx
  store volatile i64 51938, i64* @assembly_address
  %2794 = load i64* @global_var_25f4c0
  store i64 %2794, i64* %rax
  store volatile i64 51945, i64* @assembly_address
  %2795 = load i64* %rax
  %2796 = load i64* %rdx
  %2797 = add i64 %2795, %2796
  %2798 = and i64 %2795, 15
  %2799 = and i64 %2796, 15
  %2800 = add i64 %2798, %2799
  %2801 = icmp ugt i64 %2800, 15
  %2802 = icmp ult i64 %2797, %2795
  %2803 = xor i64 %2795, %2797
  %2804 = xor i64 %2796, %2797
  %2805 = and i64 %2803, %2804
  %2806 = icmp slt i64 %2805, 0
  store i1 %2801, i1* %az
  store i1 %2802, i1* %cf
  store i1 %2806, i1* %of
  %2807 = icmp eq i64 %2797, 0
  store i1 %2807, i1* %zf
  %2808 = icmp slt i64 %2797, 0
  store i1 %2808, i1* %sf
  %2809 = trunc i64 %2797 to i8
  %2810 = call i8 @llvm.ctpop.i8(i8 %2809)
  %2811 = and i8 %2810, 1
  %2812 = icmp eq i8 %2811, 0
  store i1 %2812, i1* %pf
  store i64 %2797, i64* %rax
  store volatile i64 51948, i64* @assembly_address
  %2813 = load i64* %rax
  store i64 %2813, i64* @global_var_25f4c0
  br label %block_caf3

block_caf3:                                       ; preds = %block_cac5, %block_cabf, %block_cab5
  store volatile i64 51955, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_caf8

block_caf8:                                       ; preds = %block_caf3, %block_c3e6
  store volatile i64 51960, i64* @assembly_address
  %2814 = load i64* %stack_var_-8
  store i64 %2814, i64* %rbp
  %2815 = ptrtoint i64* %stack_var_0 to i64
  store i64 %2815, i64* %rsp
  store volatile i64 51961, i64* @assembly_address
  %2816 = load i64* %rax
  ret i64 %2816
}

declare i64 @233(i64, i32)

declare i64 @234(i64, i64)

define i64 @read_byte() {
block_cafa:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 51962, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 51963, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 51966, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 16
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 16
  %8 = xor i64 %3, 16
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %18, i64* %rsp
  store volatile i64 51970, i64* @assembly_address
  %19 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %20 = zext i32 %19 to i64
  store i64 %20, i64* %rdx
  store volatile i64 51976, i64* @assembly_address
  %21 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %22 = zext i32 %21 to i64
  store i64 %22, i64* %rax
  store volatile i64 51982, i64* @assembly_address
  %23 = load i64* %rdx
  %24 = trunc i64 %23 to i32
  %25 = load i64* %rax
  %26 = trunc i64 %25 to i32
  %27 = sub i32 %24, %26
  %28 = and i32 %24, 15
  %29 = and i32 %26, 15
  %30 = sub i32 %28, %29
  %31 = icmp ugt i32 %30, 15
  %32 = icmp ult i32 %24, %26
  %33 = xor i32 %24, %26
  %34 = xor i32 %24, %27
  %35 = and i32 %33, %34
  %36 = icmp slt i32 %35, 0
  store i1 %31, i1* %az
  store i1 %32, i1* %cf
  store i1 %36, i1* %of
  %37 = icmp eq i32 %27, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i32 %27, 0
  store i1 %38, i1* %sf
  %39 = trunc i32 %27 to i8
  %40 = call i8 @llvm.ctpop.i8(i8 %39)
  %41 = and i8 %40, 1
  %42 = icmp eq i8 %41, 0
  store i1 %42, i1* %pf
  store volatile i64 51984, i64* @assembly_address
  %43 = load i1* %cf
  %44 = icmp eq i1 %43, false
  br i1 %44, label %block_cb33, label %block_cb12

block_cb12:                                       ; preds = %block_cafa
  store volatile i64 51986, i64* @assembly_address
  %45 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %46 = zext i32 %45 to i64
  store i64 %46, i64* %rax
  store volatile i64 51992, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 51995, i64* @assembly_address
  %47 = load i64* %rdx
  %48 = trunc i64 %47 to i32
  store i32 %48, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 52001, i64* @assembly_address
  %49 = load i64* %rax
  %50 = trunc i64 %49 to i32
  %51 = zext i32 %50 to i64
  store i64 %51, i64* %rdx
  store volatile i64 52003, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 52010, i64* @assembly_address
  %52 = load i64* %rdx
  %53 = load i64* %rax
  %54 = mul i64 %53, 1
  %55 = add i64 %52, %54
  %56 = inttoptr i64 %55 to i8*
  %57 = load i8* %56
  %58 = zext i8 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 52014, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i8
  %61 = zext i8 %60 to i64
  store i64 %61, i64* %rax
  store volatile i64 52017, i64* @assembly_address
  br label %block_cb3d

block_cb33:                                       ; preds = %block_cafa
  store volatile i64 52019, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 52024, i64* @assembly_address
  %62 = load i64* %rdi
  %63 = trunc i64 %62 to i32
  %64 = call i64 @fill_inbuf(i32 %63)
  store i64 %64, i64* %rax
  store i64 %64, i64* %rax
  br label %block_cb3d

block_cb3d:                                       ; preds = %block_cb33, %block_cb12
  store volatile i64 52029, i64* @assembly_address
  %65 = load i64* %rax
  %66 = trunc i64 %65 to i32
  store i32 %66, i32* %stack_var_-12
  store volatile i64 52032, i64* @assembly_address
  %67 = load i32* %stack_var_-12
  %68 = and i32 %67, 15
  %69 = icmp ugt i32 %68, 15
  %70 = icmp ult i32 %67, 0
  %71 = xor i32 %67, 0
  %72 = and i32 %71, 0
  %73 = icmp slt i32 %72, 0
  store i1 %69, i1* %az
  store i1 %70, i1* %cf
  store i1 %73, i1* %of
  %74 = icmp eq i32 %67, 0
  store i1 %74, i1* %zf
  %75 = icmp slt i32 %67, 0
  store i1 %75, i1* %sf
  %76 = trunc i32 %67 to i8
  %77 = call i8 @llvm.ctpop.i8(i8 %76)
  %78 = and i8 %77, 1
  %79 = icmp eq i8 %78, 0
  store i1 %79, i1* %pf
  store volatile i64 52036, i64* @assembly_address
  %80 = load i1* %sf
  %81 = icmp eq i1 %80, false
  br i1 %81, label %block_cb52, label %block_cb46

block_cb46:                                       ; preds = %block_cb3d
  store volatile i64 52038, i64* @assembly_address
  store i64 ptrtoint ([50 x i8]* @global_var_12170 to i64), i64* %rdi
  store volatile i64 52045, i64* @assembly_address
  %82 = load i64* %rdi
  %83 = inttoptr i64 %82 to i8*
  %84 = call i64 @gzip_error(i8* %83)
  store i64 %84, i64* %rax
  store i64 %84, i64* %rax
  unreachable

block_cb52:                                       ; preds = %block_cb3d
  store volatile i64 52050, i64* @assembly_address
  %85 = load i32* %stack_var_-12
  %86 = zext i32 %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 52053, i64* @assembly_address
  %87 = load i64* %stack_var_-8
  store i64 %87, i64* %rbp
  %88 = ptrtoint i64* %stack_var_0 to i64
  store i64 %88, i64* %rsp
  store volatile i64 52054, i64* @assembly_address
  %89 = load i64* %rax
  %90 = load i64* %rax
  ret i64 %90
}

define i64 @read_tree(i64 %arg1, i64 %arg2) {
block_cb57:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-36 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i32
  %0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i64
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i64
  %8 = alloca i32
  %9 = alloca i64
  %10 = alloca i32
  %11 = alloca i64
  %12 = alloca i32
  %13 = alloca i32
  %14 = alloca i64
  %15 = alloca i32
  %16 = alloca i32
  store volatile i64 52055, i64* @assembly_address
  %17 = load i64* %rbp
  store i64 %17, i64* %stack_var_-8
  %18 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %18, i64* %rsp
  store volatile i64 52056, i64* @assembly_address
  %19 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %19, i64* %rbp
  store volatile i64 52059, i64* @assembly_address
  %20 = load i64* %rbx
  store i64 %20, i64* %stack_var_-16
  %21 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %21, i64* %rsp
  store volatile i64 52060, i64* @assembly_address
  %22 = load i64* %rsp
  %23 = sub i64 %22, 24
  %24 = and i64 %22, 15
  %25 = sub i64 %24, 8
  %26 = icmp ugt i64 %25, 15
  %27 = icmp ult i64 %22, 24
  %28 = xor i64 %22, 24
  %29 = xor i64 %22, %23
  %30 = and i64 %28, %29
  %31 = icmp slt i64 %30, 0
  store i1 %26, i1* %az
  store i1 %27, i1* %cf
  store i1 %31, i1* %of
  %32 = icmp eq i64 %23, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i64 %23, 0
  store i1 %33, i1* %sf
  %34 = trunc i64 %23 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  %38 = ptrtoint i32* %stack_var_-40 to i64
  store i64 %38, i64* %rsp
  store volatile i64 52064, i64* @assembly_address
  store i32 1, i32* %stack_var_-28
  store volatile i64 52071, i64* @assembly_address
  store i64 0, i64* @global_var_21a160
  store volatile i64 52082, i64* @assembly_address
  store i32 1, i32* %stack_var_-32
  store volatile i64 52089, i64* @assembly_address
  br label %block_cb9f

block_cb7b:                                       ; preds = %block_cb9f
  store volatile i64 52091, i64* @assembly_address
  %39 = load i64* @global_var_21a160
  store i64 %39, i64* %rax
  store volatile i64 52098, i64* @assembly_address
  %40 = load i64* %rax
  %41 = load i1* %of
  %42 = shl i64 %40, 8
  %43 = icmp eq i64 %42, 0
  store i1 %43, i1* %zf
  %44 = icmp slt i64 %42, 0
  store i1 %44, i1* %sf
  %45 = trunc i64 %42 to i8
  %46 = call i8 @llvm.ctpop.i8(i8 %45)
  %47 = and i8 %46, 1
  %48 = icmp eq i8 %47, 0
  store i1 %48, i1* %pf
  store i64 %42, i64* %rax
  %49 = shl i64 %40, 7
  %50 = lshr i64 %49, 63
  %51 = trunc i64 %50 to i1
  store i1 %51, i1* %cf
  %52 = lshr i64 %42, 63
  %53 = icmp ne i64 %52, %50
  %54 = select i1 false, i1 %53, i1 %41
  store i1 %54, i1* %of
  store volatile i64 52102, i64* @assembly_address
  %55 = load i64* %rax
  store i64 %55, i64* %rbx
  store volatile i64 52105, i64* @assembly_address
  %56 = call i64 @read_byte()
  store i64 %56, i64* %rax
  store i64 %56, i64* %rax
  store i64 %56, i64* %rax
  store volatile i64 52110, i64* @assembly_address
  %57 = load i64* %rax
  %58 = trunc i64 %57 to i8
  %59 = zext i8 %58 to i64
  store i64 %59, i64* %rax
  store volatile i64 52113, i64* @assembly_address
  %60 = load i64* %rax
  %61 = load i64* %rbx
  %62 = or i64 %60, %61
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %63 = icmp eq i64 %62, 0
  store i1 %63, i1* %zf
  %64 = icmp slt i64 %62, 0
  store i1 %64, i1* %sf
  %65 = trunc i64 %62 to i8
  %66 = call i8 @llvm.ctpop.i8(i8 %65)
  %67 = and i8 %66, 1
  %68 = icmp eq i8 %67, 0
  store i1 %68, i1* %pf
  store i64 %62, i64* %rax
  store volatile i64 52116, i64* @assembly_address
  %69 = load i64* %rax
  store i64 %69, i64* @global_var_21a160
  store volatile i64 52123, i64* @assembly_address
  %70 = load i32* %stack_var_-32
  %71 = add i32 %70, 1
  %72 = and i32 %70, 15
  %73 = add i32 %72, 1
  %74 = icmp ugt i32 %73, 15
  %75 = icmp ult i32 %71, %70
  %76 = xor i32 %70, %71
  %77 = xor i32 1, %71
  %78 = and i32 %76, %77
  %79 = icmp slt i32 %78, 0
  store i1 %74, i1* %az
  store i1 %75, i1* %cf
  store i1 %79, i1* %of
  %80 = icmp eq i32 %71, 0
  store i1 %80, i1* %zf
  %81 = icmp slt i32 %71, 0
  store i1 %81, i1* %sf
  %82 = trunc i32 %71 to i8
  %83 = call i8 @llvm.ctpop.i8(i8 %82)
  %84 = and i8 %83, 1
  %85 = icmp eq i8 %84, 0
  store i1 %85, i1* %pf
  store i32 %71, i32* %stack_var_-32
  br label %block_cb9f

block_cb9f:                                       ; preds = %block_cb7b, %block_cb57
  store volatile i64 52127, i64* @assembly_address
  %86 = load i32* %stack_var_-32
  store i32 %86, i32* %16
  store i32 4, i32* %15
  %87 = sub i32 %86, 4
  %88 = and i32 %86, 15
  %89 = sub i32 %88, 4
  %90 = icmp ugt i32 %89, 15
  %91 = icmp ult i32 %86, 4
  %92 = xor i32 %86, 4
  %93 = xor i32 %86, %87
  %94 = and i32 %92, %93
  %95 = icmp slt i32 %94, 0
  store i1 %90, i1* %az
  store i1 %91, i1* %cf
  store i1 %95, i1* %of
  %96 = icmp eq i32 %87, 0
  store i1 %96, i1* %zf
  %97 = icmp slt i32 %87, 0
  store i1 %97, i1* %sf
  %98 = trunc i32 %87 to i8
  %99 = call i8 @llvm.ctpop.i8(i8 %98)
  %100 = and i8 %99, 1
  %101 = icmp eq i8 %100, 0
  store i1 %101, i1* %pf
  store volatile i64 52131, i64* @assembly_address
  %102 = load i32* %16
  %103 = load i32* %15
  %104 = icmp sle i32 %102, %103
  br i1 %104, label %block_cb7b, label %block_cba5

block_cba5:                                       ; preds = %block_cb9f
  store volatile i64 52133, i64* @assembly_address
  %105 = call i64 @read_byte()
  store i64 %105, i64* %rax
  store i64 %105, i64* %rax
  store i64 %105, i64* %rax
  store volatile i64 52138, i64* @assembly_address
  %106 = load i64* %rax
  %107 = trunc i64 %106 to i8
  %108 = zext i8 %107 to i64
  store i64 %108, i64* %rax
  store volatile i64 52141, i64* @assembly_address
  %109 = load i64* %rax
  %110 = trunc i64 %109 to i32
  store i32 %110, i32* bitcast (i64* @global_var_21a168 to i32*)
  store volatile i64 52147, i64* @assembly_address
  %111 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %112 = zext i32 %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 52153, i64* @assembly_address
  %113 = load i64* %rax
  %114 = trunc i64 %113 to i32
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i32
  %117 = and i32 %114, %116
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %118 = icmp eq i32 %117, 0
  store i1 %118, i1* %zf
  %119 = icmp slt i32 %117, 0
  store i1 %119, i1* %sf
  %120 = trunc i32 %117 to i8
  %121 = call i8 @llvm.ctpop.i8(i8 %120)
  %122 = and i8 %121, 1
  %123 = icmp eq i8 %122, 0
  store i1 %123, i1* %pf
  store volatile i64 52155, i64* @assembly_address
  %124 = load i1* %zf
  %125 = load i1* %sf
  %126 = load i1* %of
  %127 = icmp ne i1 %125, %126
  %128 = or i1 %124, %127
  br i1 %128, label %block_cbc8, label %block_cbbd

block_cbbd:                                       ; preds = %block_cba5
  store volatile i64 52157, i64* @assembly_address
  %129 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %130 = zext i32 %129 to i64
  store i64 %130, i64* %rax
  store volatile i64 52163, i64* @assembly_address
  %131 = load i64* %rax
  %132 = trunc i64 %131 to i32
  %133 = trunc i64 %131 to i32
  store i32 %133, i32* %13
  store i32 25, i32* %12
  %134 = sub i32 %132, 25
  %135 = and i32 %132, 15
  %136 = sub i32 %135, 9
  %137 = icmp ugt i32 %136, 15
  %138 = icmp ult i32 %132, 25
  %139 = xor i32 %132, 25
  %140 = xor i32 %132, %134
  %141 = and i32 %139, %140
  %142 = icmp slt i32 %141, 0
  store i1 %137, i1* %az
  store i1 %138, i1* %cf
  store i1 %142, i1* %of
  %143 = icmp eq i32 %134, 0
  store i1 %143, i1* %zf
  %144 = icmp slt i32 %134, 0
  store i1 %144, i1* %sf
  %145 = trunc i32 %134 to i8
  %146 = call i8 @llvm.ctpop.i8(i8 %145)
  %147 = and i8 %146, 1
  %148 = icmp eq i8 %147, 0
  store i1 %148, i1* %pf
  store volatile i64 52166, i64* @assembly_address
  %149 = load i32* %13
  %150 = sext i32 %149 to i64
  %151 = load i32* %12
  %152 = trunc i64 %150 to i32
  %153 = icmp sle i32 %152, %151
  br i1 %153, label %block_cbd4, label %block_cbc8

block_cbc8:                                       ; preds = %block_cbbd, %block_cba5
  store volatile i64 52168, i64* @assembly_address
  store i64 ptrtoint ([64 x i8]* @global_var_121a8 to i64), i64* %rdi
  store volatile i64 52175, i64* @assembly_address
  %154 = load i64* %rdi
  %155 = inttoptr i64 %154 to i8*
  %156 = call i64 @gzip_error(i8* %155)
  store i64 %156, i64* %rax
  store i64 %156, i64* %rax
  unreachable

block_cbd4:                                       ; preds = %block_cbbd
  store volatile i64 52180, i64* @assembly_address
  store i32 0, i32* %stack_var_-32
  store volatile i64 52187, i64* @assembly_address
  %157 = sext i32 1 to i64
  %158 = trunc i64 %157 to i32
  store i32 %158, i32* %stack_var_-40
  store volatile i64 52194, i64* @assembly_address
  br label %block_cc8a

block_cbe7:                                       ; preds = %block_cc8a
  store volatile i64 52199, i64* @assembly_address
  %159 = call i64 @read_byte()
  store i64 %159, i64* %rax
  store i64 %159, i64* %rax
  store i64 %159, i64* %rax
  store volatile i64 52204, i64* @assembly_address
  %160 = load i64* %rax
  %161 = trunc i64 %160 to i8
  %162 = zext i8 %161 to i64
  store i64 %162, i64* %rdx
  store volatile i64 52207, i64* @assembly_address
  %163 = load i32* %stack_var_-40
  %164 = sext i32 %163 to i64
  %165 = trunc i64 %164 to i32
  %166 = zext i32 %165 to i64
  store i64 %166, i64* %rax
  store volatile i64 52210, i64* @assembly_address
  %167 = load i64* %rax
  %168 = trunc i64 %167 to i32
  %169 = sext i32 %168 to i64
  store i64 %169, i64* %rax
  store volatile i64 52212, i64* @assembly_address
  %170 = load i64* %rax
  %171 = mul i64 %170, 4
  store i64 %171, i64* %rcx
  store volatile i64 52220, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52227, i64* @assembly_address
  %172 = load i64* %rdx
  %173 = trunc i64 %172 to i32
  %174 = load i64* %rcx
  %175 = load i64* %rax
  %176 = mul i64 %175, 1
  %177 = add i64 %174, %176
  %178 = inttoptr i64 %177 to i32*
  store i32 %173, i32* %178
  store volatile i64 52230, i64* @assembly_address
  %179 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %180 = zext i32 %179 to i64
  store i64 %180, i64* %rax
  store volatile i64 52236, i64* @assembly_address
  %181 = load i32* %stack_var_-40
  %182 = sext i32 %181 to i64
  %183 = trunc i64 %182 to i32
  %184 = load i64* %rax
  %185 = trunc i64 %184 to i32
  %186 = sub i32 %183, %185
  %187 = and i32 %183, 15
  %188 = and i32 %185, 15
  %189 = sub i32 %187, %188
  %190 = icmp ugt i32 %189, 15
  %191 = icmp ult i32 %183, %185
  %192 = xor i32 %183, %185
  %193 = xor i32 %183, %186
  %194 = and i32 %192, %193
  %195 = icmp slt i32 %194, 0
  store i1 %190, i1* %az
  store i1 %191, i1* %cf
  store i1 %195, i1* %of
  %196 = icmp eq i32 %186, 0
  store i1 %196, i1* %zf
  %197 = icmp slt i32 %186, 0
  store i1 %197, i1* %sf
  %198 = trunc i32 %186 to i8
  %199 = call i8 @llvm.ctpop.i8(i8 %198)
  %200 = and i8 %199, 1
  %201 = icmp eq i8 %200, 0
  store i1 %201, i1* %pf
  store volatile i64 52239, i64* @assembly_address
  %202 = load i1* %zf
  %203 = zext i1 %202 to i8
  %204 = zext i8 %203 to i64
  %205 = load i64* %rax
  %206 = and i64 %205, -256
  %207 = or i64 %206, %204
  store i64 %207, i64* %rax
  store volatile i64 52242, i64* @assembly_address
  %208 = load i64* %rax
  %209 = trunc i64 %208 to i8
  %210 = zext i8 %209 to i64
  store i64 %210, i64* %rax
  store volatile i64 52245, i64* @assembly_address
  %211 = load i32* %stack_var_-28
  %212 = zext i32 %211 to i64
  store i64 %212, i64* %rdx
  store volatile i64 52248, i64* @assembly_address
  %213 = load i64* %rdx
  %214 = trunc i64 %213 to i32
  %215 = zext i32 %214 to i64
  store i64 %215, i64* %rcx
  store volatile i64 52250, i64* @assembly_address
  %216 = load i64* %rcx
  %217 = trunc i64 %216 to i32
  %218 = load i64* %rax
  %219 = trunc i64 %218 to i32
  %220 = sub i32 %217, %219
  %221 = and i32 %217, 15
  %222 = and i32 %219, 15
  %223 = sub i32 %221, %222
  %224 = icmp ugt i32 %223, 15
  %225 = icmp ult i32 %217, %219
  %226 = xor i32 %217, %219
  %227 = xor i32 %217, %220
  %228 = and i32 %226, %227
  %229 = icmp slt i32 %228, 0
  store i1 %224, i1* %az
  store i1 %225, i1* %cf
  store i1 %229, i1* %of
  %230 = icmp eq i32 %220, 0
  store i1 %230, i1* %zf
  %231 = icmp slt i32 %220, 0
  store i1 %231, i1* %sf
  %232 = trunc i32 %220 to i8
  %233 = call i8 @llvm.ctpop.i8(i8 %232)
  %234 = and i8 %233, 1
  %235 = icmp eq i8 %234, 0
  store i1 %235, i1* %pf
  %236 = zext i32 %220 to i64
  store i64 %236, i64* %rcx
  store volatile i64 52252, i64* @assembly_address
  %237 = load i32* %stack_var_-40
  %238 = sext i32 %237 to i64
  %239 = trunc i64 %238 to i32
  %240 = zext i32 %239 to i64
  store i64 %240, i64* %rax
  store volatile i64 52255, i64* @assembly_address
  %241 = load i64* %rax
  %242 = trunc i64 %241 to i32
  %243 = sext i32 %242 to i64
  store i64 %243, i64* %rax
  store volatile i64 52257, i64* @assembly_address
  %244 = load i64* %rax
  %245 = mul i64 %244, 4
  store i64 %245, i64* %rdx
  store volatile i64 52265, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52272, i64* @assembly_address
  %246 = load i64* %rdx
  %247 = load i64* %rax
  %248 = mul i64 %247, 1
  %249 = add i64 %246, %248
  %250 = inttoptr i64 %249 to i32*
  %251 = load i32* %250
  %252 = zext i32 %251 to i64
  store i64 %252, i64* %rax
  store volatile i64 52275, i64* @assembly_address
  %253 = load i64* %rcx
  %254 = trunc i64 %253 to i32
  %255 = load i64* %rax
  %256 = trunc i64 %255 to i32
  %257 = trunc i64 %253 to i32
  store i32 %257, i32* %10
  store i64 %255, i64* %9
  %258 = sub i32 %254, %256
  %259 = and i32 %254, 15
  %260 = and i32 %256, 15
  %261 = sub i32 %259, %260
  %262 = icmp ugt i32 %261, 15
  %263 = icmp ult i32 %254, %256
  %264 = xor i32 %254, %256
  %265 = xor i32 %254, %258
  %266 = and i32 %264, %265
  %267 = icmp slt i32 %266, 0
  store i1 %262, i1* %az
  store i1 %263, i1* %cf
  store i1 %267, i1* %of
  %268 = icmp eq i32 %258, 0
  store i1 %268, i1* %zf
  %269 = icmp slt i32 %258, 0
  store i1 %269, i1* %sf
  %270 = trunc i32 %258 to i8
  %271 = call i8 @llvm.ctpop.i8(i8 %270)
  %272 = and i8 %271, 1
  %273 = icmp eq i8 %272, 0
  store i1 %273, i1* %pf
  store volatile i64 52277, i64* @assembly_address
  %274 = load i32* %10
  %275 = sext i32 %274 to i64
  %276 = load i64* %9
  %277 = icmp sge i64 %275, %276
  br i1 %277, label %block_cc43, label %block_cc37

block_cc37:                                       ; preds = %block_cbe7
  store volatile i64 52279, i64* @assembly_address
  store i64 ptrtoint ([32 x i8]* @global_var_121e8 to i64), i64* %rdi
  store volatile i64 52286, i64* @assembly_address
  %278 = load i64* %rdi
  %279 = inttoptr i64 %278 to i8*
  %280 = call i64 @gzip_error(i8* %279)
  store i64 %280, i64* %rax
  store i64 %280, i64* %rax
  unreachable

block_cc43:                                       ; preds = %block_cbe7
  store volatile i64 52291, i64* @assembly_address
  %281 = load i32* %stack_var_-40
  %282 = sext i32 %281 to i64
  %283 = trunc i64 %282 to i32
  %284 = zext i32 %283 to i64
  store i64 %284, i64* %rax
  store volatile i64 52294, i64* @assembly_address
  %285 = load i64* %rax
  %286 = trunc i64 %285 to i32
  %287 = sext i32 %286 to i64
  store i64 %287, i64* %rax
  store volatile i64 52296, i64* @assembly_address
  %288 = load i64* %rax
  %289 = mul i64 %288, 4
  store i64 %289, i64* %rdx
  store volatile i64 52304, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52311, i64* @assembly_address
  %290 = load i64* %rdx
  %291 = load i64* %rax
  %292 = mul i64 %291, 1
  %293 = add i64 %290, %292
  %294 = inttoptr i64 %293 to i32*
  %295 = load i32* %294
  %296 = zext i32 %295 to i64
  store i64 %296, i64* %rax
  store volatile i64 52314, i64* @assembly_address
  %297 = load i32* %stack_var_-28
  %298 = zext i32 %297 to i64
  store i64 %298, i64* %rdx
  store volatile i64 52317, i64* @assembly_address
  %299 = load i64* %rdx
  %300 = trunc i64 %299 to i32
  %301 = load i64* %rax
  %302 = trunc i64 %301 to i32
  %303 = sub i32 %300, %302
  %304 = and i32 %300, 15
  %305 = and i32 %302, 15
  %306 = sub i32 %304, %305
  %307 = icmp ugt i32 %306, 15
  %308 = icmp ult i32 %300, %302
  %309 = xor i32 %300, %302
  %310 = xor i32 %300, %303
  %311 = and i32 %309, %310
  %312 = icmp slt i32 %311, 0
  store i1 %307, i1* %az
  store i1 %308, i1* %cf
  store i1 %312, i1* %of
  %313 = icmp eq i32 %303, 0
  store i1 %313, i1* %zf
  %314 = icmp slt i32 %303, 0
  store i1 %314, i1* %sf
  %315 = trunc i32 %303 to i8
  %316 = call i8 @llvm.ctpop.i8(i8 %315)
  %317 = and i8 %316, 1
  %318 = icmp eq i8 %317, 0
  store i1 %318, i1* %pf
  %319 = zext i32 %303 to i64
  store i64 %319, i64* %rdx
  store volatile i64 52319, i64* @assembly_address
  %320 = load i64* %rdx
  %321 = trunc i64 %320 to i32
  %322 = zext i32 %321 to i64
  store i64 %322, i64* %rax
  store volatile i64 52321, i64* @assembly_address
  %323 = load i64* %rax
  %324 = trunc i64 %323 to i32
  %325 = add i32 %324, 1
  %326 = and i32 %324, 15
  %327 = add i32 %326, 1
  %328 = icmp ugt i32 %327, 15
  %329 = icmp ult i32 %325, %324
  %330 = xor i32 %324, %325
  %331 = xor i32 1, %325
  %332 = and i32 %330, %331
  %333 = icmp slt i32 %332, 0
  store i1 %328, i1* %az
  store i1 %329, i1* %cf
  store i1 %333, i1* %of
  %334 = icmp eq i32 %325, 0
  store i1 %334, i1* %zf
  %335 = icmp slt i32 %325, 0
  store i1 %335, i1* %sf
  %336 = trunc i32 %325 to i8
  %337 = call i8 @llvm.ctpop.i8(i8 %336)
  %338 = and i8 %337, 1
  %339 = icmp eq i8 %338, 0
  store i1 %339, i1* %pf
  %340 = zext i32 %325 to i64
  store i64 %340, i64* %rax
  store volatile i64 52324, i64* @assembly_address
  %341 = load i64* %rax
  %342 = trunc i64 %341 to i32
  %343 = load i64* %rax
  %344 = trunc i64 %343 to i32
  %345 = add i32 %342, %344
  %346 = and i32 %342, 15
  %347 = and i32 %344, 15
  %348 = add i32 %346, %347
  %349 = icmp ugt i32 %348, 15
  %350 = icmp ult i32 %345, %342
  %351 = xor i32 %342, %345
  %352 = xor i32 %344, %345
  %353 = and i32 %351, %352
  %354 = icmp slt i32 %353, 0
  store i1 %349, i1* %az
  store i1 %350, i1* %cf
  store i1 %354, i1* %of
  %355 = icmp eq i32 %345, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i32 %345, 0
  store i1 %356, i1* %sf
  %357 = trunc i32 %345 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  %361 = zext i32 %345 to i64
  store i64 %361, i64* %rax
  store volatile i64 52326, i64* @assembly_address
  %362 = load i64* %rax
  %363 = trunc i64 %362 to i32
  %364 = sub i32 %363, 1
  %365 = and i32 %363, 15
  %366 = sub i32 %365, 1
  %367 = icmp ugt i32 %366, 15
  %368 = icmp ult i32 %363, 1
  %369 = xor i32 %363, 1
  %370 = xor i32 %363, %364
  %371 = and i32 %369, %370
  %372 = icmp slt i32 %371, 0
  store i1 %367, i1* %az
  store i1 %368, i1* %cf
  store i1 %372, i1* %of
  %373 = icmp eq i32 %364, 0
  store i1 %373, i1* %zf
  %374 = icmp slt i32 %364, 0
  store i1 %374, i1* %sf
  %375 = trunc i32 %364 to i8
  %376 = call i8 @llvm.ctpop.i8(i8 %375)
  %377 = and i8 %376, 1
  %378 = icmp eq i8 %377, 0
  store i1 %378, i1* %pf
  %379 = zext i32 %364 to i64
  store i64 %379, i64* %rax
  store volatile i64 52329, i64* @assembly_address
  %380 = load i64* %rax
  %381 = trunc i64 %380 to i32
  store i32 %381, i32* %stack_var_-28
  store volatile i64 52332, i64* @assembly_address
  %382 = load i32* %stack_var_-40
  %383 = sext i32 %382 to i64
  %384 = trunc i64 %383 to i32
  %385 = zext i32 %384 to i64
  store i64 %385, i64* %rax
  store volatile i64 52335, i64* @assembly_address
  %386 = load i64* %rax
  %387 = trunc i64 %386 to i32
  %388 = sext i32 %387 to i64
  store i64 %388, i64* %rax
  store volatile i64 52337, i64* @assembly_address
  %389 = load i64* %rax
  %390 = mul i64 %389, 4
  store i64 %390, i64* %rdx
  store volatile i64 52345, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52352, i64* @assembly_address
  %391 = load i64* %rdx
  %392 = load i64* %rax
  %393 = mul i64 %392, 1
  %394 = add i64 %391, %393
  %395 = inttoptr i64 %394 to i32*
  %396 = load i32* %395
  %397 = zext i32 %396 to i64
  store i64 %397, i64* %rax
  store volatile i64 52355, i64* @assembly_address
  %398 = load i32* %stack_var_-32
  %399 = load i64* %rax
  %400 = trunc i64 %399 to i32
  %401 = add i32 %398, %400
  %402 = and i32 %398, 15
  %403 = and i32 %400, 15
  %404 = add i32 %402, %403
  %405 = icmp ugt i32 %404, 15
  %406 = icmp ult i32 %401, %398
  %407 = xor i32 %398, %401
  %408 = xor i32 %400, %401
  %409 = and i32 %407, %408
  %410 = icmp slt i32 %409, 0
  store i1 %405, i1* %az
  store i1 %406, i1* %cf
  store i1 %410, i1* %of
  %411 = icmp eq i32 %401, 0
  store i1 %411, i1* %zf
  %412 = icmp slt i32 %401, 0
  store i1 %412, i1* %sf
  %413 = trunc i32 %401 to i8
  %414 = call i8 @llvm.ctpop.i8(i8 %413)
  %415 = and i8 %414, 1
  %416 = icmp eq i8 %415, 0
  store i1 %416, i1* %pf
  store i32 %401, i32* %stack_var_-32
  store volatile i64 52358, i64* @assembly_address
  %417 = load i32* %stack_var_-40
  %418 = sext i32 %417 to i64
  %419 = trunc i64 %418 to i32
  %420 = add i32 %419, 1
  %421 = and i32 %419, 15
  %422 = add i32 %421, 1
  %423 = icmp ugt i32 %422, 15
  %424 = icmp ult i32 %420, %419
  %425 = xor i32 %419, %420
  %426 = xor i32 1, %420
  %427 = and i32 %425, %426
  %428 = icmp slt i32 %427, 0
  store i1 %423, i1* %az
  store i1 %424, i1* %cf
  store i1 %428, i1* %of
  %429 = icmp eq i32 %420, 0
  store i1 %429, i1* %zf
  %430 = icmp slt i32 %420, 0
  store i1 %430, i1* %sf
  %431 = trunc i32 %420 to i8
  %432 = call i8 @llvm.ctpop.i8(i8 %431)
  %433 = and i8 %432, 1
  %434 = icmp eq i8 %433, 0
  store i1 %434, i1* %pf
  %435 = sext i32 %420 to i64
  %436 = trunc i64 %435 to i32
  store i32 %436, i32* %stack_var_-40
  br label %block_cc8a

block_cc8a:                                       ; preds = %block_cc43, %block_cbd4
  store volatile i64 52362, i64* @assembly_address
  %437 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %438 = zext i32 %437 to i64
  store i64 %438, i64* %rax
  store volatile i64 52368, i64* @assembly_address
  %439 = load i32* %stack_var_-40
  %440 = sext i32 %439 to i64
  %441 = trunc i64 %440 to i32
  %442 = load i64* %rax
  %443 = trunc i64 %442 to i32
  store i32 %441, i32* %8
  %444 = trunc i64 %442 to i32
  store i32 %444, i32* %6
  %445 = sub i32 %441, %443
  %446 = and i32 %441, 15
  %447 = and i32 %443, 15
  %448 = sub i32 %446, %447
  %449 = icmp ugt i32 %448, 15
  %450 = icmp ult i32 %441, %443
  %451 = xor i32 %441, %443
  %452 = xor i32 %441, %445
  %453 = and i32 %451, %452
  %454 = icmp slt i32 %453, 0
  store i1 %449, i1* %az
  store i1 %450, i1* %cf
  store i1 %454, i1* %of
  %455 = icmp eq i32 %445, 0
  store i1 %455, i1* %zf
  %456 = icmp slt i32 %445, 0
  store i1 %456, i1* %sf
  %457 = trunc i32 %445 to i8
  %458 = call i8 @llvm.ctpop.i8(i8 %457)
  %459 = and i8 %458, 1
  %460 = icmp eq i8 %459, 0
  store i1 %460, i1* %pf
  store volatile i64 52371, i64* @assembly_address
  %461 = load i32* %8
  %462 = load i32* %6
  %463 = sext i32 %462 to i64
  %464 = sext i32 %461 to i64
  %465 = icmp sle i64 %464, %463
  br i1 %465, label %block_cbe7, label %block_cc99

block_cc99:                                       ; preds = %block_cc8a
  store volatile i64 52377, i64* @assembly_address
  %466 = load i32* %stack_var_-32
  store i32 %466, i32* %5
  store i32 255, i32* %4
  %467 = sub i32 %466, 255
  %468 = and i32 %466, 15
  %469 = sub i32 %468, 15
  %470 = icmp ugt i32 %469, 15
  %471 = icmp ult i32 %466, 255
  %472 = xor i32 %466, 255
  %473 = xor i32 %466, %467
  %474 = and i32 %472, %473
  %475 = icmp slt i32 %474, 0
  store i1 %470, i1* %az
  store i1 %471, i1* %cf
  store i1 %475, i1* %of
  %476 = icmp eq i32 %467, 0
  store i1 %476, i1* %zf
  %477 = icmp slt i32 %467, 0
  store i1 %477, i1* %sf
  %478 = trunc i32 %467 to i8
  %479 = call i8 @llvm.ctpop.i8(i8 %478)
  %480 = and i8 %479, 1
  %481 = icmp eq i8 %480, 0
  store i1 %481, i1* %pf
  store volatile i64 52384, i64* @assembly_address
  %482 = load i32* %5
  %483 = load i32* %4
  %484 = icmp sle i32 %482, %483
  br i1 %484, label %block_ccae, label %block_cca2

block_cca2:                                       ; preds = %block_cc99
  store volatile i64 52386, i64* @assembly_address
  store i64 ptrtoint ([32 x i8]* @global_var_121e8 to i64), i64* %rdi
  store volatile i64 52393, i64* @assembly_address
  %485 = load i64* %rdi
  %486 = inttoptr i64 %485 to i8*
  %487 = call i64 @gzip_error(i8* %486)
  store i64 %487, i64* %rax
  store i64 %487, i64* %rax
  unreachable

block_ccae:                                       ; preds = %block_cc99
  store volatile i64 52398, i64* @assembly_address
  %488 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %489 = zext i32 %488 to i64
  store i64 %489, i64* %rax
  store volatile i64 52404, i64* @assembly_address
  %490 = load i64* %rax
  %491 = trunc i64 %490 to i32
  %492 = sext i32 %491 to i64
  store i64 %492, i64* %rdx
  store volatile i64 52407, i64* @assembly_address
  %493 = load i64* %rdx
  %494 = mul i64 %493, 4
  store i64 %494, i64* %rcx
  store volatile i64 52415, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rdx
  store volatile i64 52422, i64* @assembly_address
  %495 = load i64* %rcx
  %496 = load i64* %rdx
  %497 = mul i64 %496, 1
  %498 = add i64 %495, %497
  %499 = inttoptr i64 %498 to i32*
  %500 = load i32* %499
  %501 = zext i32 %500 to i64
  store i64 %501, i64* %rdx
  store volatile i64 52425, i64* @assembly_address
  %502 = load i64* %rdx
  %503 = add i64 %502, 1
  %504 = trunc i64 %503 to i32
  %505 = zext i32 %504 to i64
  store i64 %505, i64* %rcx
  store volatile i64 52428, i64* @assembly_address
  %506 = load i64* %rax
  %507 = trunc i64 %506 to i32
  %508 = sext i32 %507 to i64
  store i64 %508, i64* %rax
  store volatile i64 52430, i64* @assembly_address
  %509 = load i64* %rax
  %510 = mul i64 %509, 4
  store i64 %510, i64* %rdx
  store volatile i64 52438, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52445, i64* @assembly_address
  %511 = load i64* %rcx
  %512 = trunc i64 %511 to i32
  %513 = load i64* %rdx
  %514 = load i64* %rax
  %515 = mul i64 %514, 1
  %516 = add i64 %513, %515
  %517 = inttoptr i64 %516 to i32*
  store i32 %512, i32* %517
  store volatile i64 52448, i64* @assembly_address
  store i32 0, i32* %stack_var_-36
  store volatile i64 52455, i64* @assembly_address
  %518 = sext i32 1 to i64
  %519 = trunc i64 %518 to i32
  store i32 %519, i32* %stack_var_-40
  store volatile i64 52462, i64* @assembly_address
  br label %block_cd51

block_ccf0:                                       ; preds = %block_cd51
  store volatile i64 52464, i64* @assembly_address
  %520 = load i32* %stack_var_-40
  %521 = sext i32 %520 to i64
  %522 = trunc i64 %521 to i32
  %523 = zext i32 %522 to i64
  store i64 %523, i64* %rax
  store volatile i64 52467, i64* @assembly_address
  %524 = load i64* %rax
  %525 = trunc i64 %524 to i32
  %526 = sext i32 %525 to i64
  store i64 %526, i64* %rax
  store volatile i64 52469, i64* @assembly_address
  %527 = load i64* %rax
  %528 = mul i64 %527, 4
  store i64 %528, i64* %rcx
  store volatile i64 52477, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a280 to i64), i64* %rax
  store volatile i64 52484, i64* @assembly_address
  %529 = load i32* %stack_var_-36
  %530 = zext i32 %529 to i64
  store i64 %530, i64* %rdx
  store volatile i64 52487, i64* @assembly_address
  %531 = load i64* %rdx
  %532 = trunc i64 %531 to i32
  %533 = load i64* %rcx
  %534 = load i64* %rax
  %535 = mul i64 %534, 1
  %536 = add i64 %533, %535
  %537 = inttoptr i64 %536 to i32*
  store i32 %532, i32* %537
  store volatile i64 52490, i64* @assembly_address
  %538 = load i32* %stack_var_-40
  %539 = sext i32 %538 to i64
  %540 = trunc i64 %539 to i32
  %541 = zext i32 %540 to i64
  store i64 %541, i64* %rax
  store volatile i64 52493, i64* @assembly_address
  %542 = load i64* %rax
  %543 = trunc i64 %542 to i32
  %544 = sext i32 %543 to i64
  store i64 %544, i64* %rax
  store volatile i64 52495, i64* @assembly_address
  %545 = load i64* %rax
  %546 = mul i64 %545, 4
  store i64 %546, i64* %rdx
  store volatile i64 52503, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52510, i64* @assembly_address
  %547 = load i64* %rdx
  %548 = load i64* %rax
  %549 = mul i64 %548, 1
  %550 = add i64 %547, %549
  %551 = inttoptr i64 %550 to i32*
  %552 = load i32* %551
  %553 = zext i32 %552 to i64
  store i64 %553, i64* %rax
  store volatile i64 52513, i64* @assembly_address
  %554 = load i64* %rax
  %555 = trunc i64 %554 to i32
  store i32 %555, i32* %stack_var_-32
  store volatile i64 52516, i64* @assembly_address
  br label %block_cd47

block_cd26:                                       ; preds = %block_cd47
  store volatile i64 52518, i64* @assembly_address
  %556 = load i32* %stack_var_-36
  %557 = zext i32 %556 to i64
  store i64 %557, i64* %rbx
  store volatile i64 52521, i64* @assembly_address
  %558 = load i64* %rbx
  %559 = add i64 %558, 1
  %560 = trunc i64 %559 to i32
  %561 = zext i32 %560 to i64
  store i64 %561, i64* %rax
  store volatile i64 52524, i64* @assembly_address
  %562 = load i64* %rax
  %563 = trunc i64 %562 to i32
  store i32 %563, i32* %stack_var_-36
  store volatile i64 52527, i64* @assembly_address
  %564 = call i64 @read_byte()
  store i64 %564, i64* %rax
  store i64 %564, i64* %rax
  store i64 %564, i64* %rax
  store volatile i64 52532, i64* @assembly_address
  %565 = load i64* %rax
  %566 = trunc i64 %565 to i32
  %567 = zext i32 %566 to i64
  store i64 %567, i64* %rcx
  store volatile i64 52534, i64* @assembly_address
  %568 = load i64* %rbx
  %569 = trunc i64 %568 to i32
  %570 = sext i32 %569 to i64
  store i64 %570, i64* %rdx
  store volatile i64 52537, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a180 to i64), i64* %rax
  store volatile i64 52544, i64* @assembly_address
  %571 = load i64* %rcx
  %572 = trunc i64 %571 to i8
  %573 = load i64* %rdx
  %574 = load i64* %rax
  %575 = mul i64 %574, 1
  %576 = add i64 %573, %575
  %577 = inttoptr i64 %576 to i8*
  store i8 %572, i8* %577
  store volatile i64 52547, i64* @assembly_address
  %578 = load i32* %stack_var_-32
  %579 = sub i32 %578, 1
  %580 = and i32 %578, 15
  %581 = sub i32 %580, 1
  %582 = icmp ugt i32 %581, 15
  %583 = icmp ult i32 %578, 1
  %584 = xor i32 %578, 1
  %585 = xor i32 %578, %579
  %586 = and i32 %584, %585
  %587 = icmp slt i32 %586, 0
  store i1 %582, i1* %az
  store i1 %583, i1* %cf
  store i1 %587, i1* %of
  %588 = icmp eq i32 %579, 0
  store i1 %588, i1* %zf
  %589 = icmp slt i32 %579, 0
  store i1 %589, i1* %sf
  %590 = trunc i32 %579 to i8
  %591 = call i8 @llvm.ctpop.i8(i8 %590)
  %592 = and i8 %591, 1
  %593 = icmp eq i8 %592, 0
  store i1 %593, i1* %pf
  store i32 %579, i32* %stack_var_-32
  br label %block_cd47

block_cd47:                                       ; preds = %block_cd26, %block_ccf0
  store volatile i64 52551, i64* @assembly_address
  %594 = load i32* %stack_var_-32
  %595 = and i32 %594, 15
  %596 = icmp ugt i32 %595, 15
  %597 = icmp ult i32 %594, 0
  %598 = xor i32 %594, 0
  %599 = and i32 %598, 0
  %600 = icmp slt i32 %599, 0
  store i1 %596, i1* %az
  store i1 %597, i1* %cf
  store i1 %600, i1* %of
  %601 = icmp eq i32 %594, 0
  store i1 %601, i1* %zf
  %602 = icmp slt i32 %594, 0
  store i1 %602, i1* %sf
  %603 = trunc i32 %594 to i8
  %604 = call i8 @llvm.ctpop.i8(i8 %603)
  %605 = and i8 %604, 1
  %606 = icmp eq i8 %605, 0
  store i1 %606, i1* %pf
  store volatile i64 52555, i64* @assembly_address
  %607 = load i1* %zf
  %608 = load i1* %sf
  %609 = load i1* %of
  %610 = icmp eq i1 %608, %609
  %611 = icmp eq i1 %607, false
  %612 = icmp eq i1 %610, %611
  br i1 %612, label %block_cd26, label %block_cd4d

block_cd4d:                                       ; preds = %block_cd47
  store volatile i64 52557, i64* @assembly_address
  %613 = load i32* %stack_var_-40
  %614 = sext i32 %613 to i64
  %615 = trunc i64 %614 to i32
  %616 = add i32 %615, 1
  %617 = and i32 %615, 15
  %618 = add i32 %617, 1
  %619 = icmp ugt i32 %618, 15
  %620 = icmp ult i32 %616, %615
  %621 = xor i32 %615, %616
  %622 = xor i32 1, %616
  %623 = and i32 %621, %622
  %624 = icmp slt i32 %623, 0
  store i1 %619, i1* %az
  store i1 %620, i1* %cf
  store i1 %624, i1* %of
  %625 = icmp eq i32 %616, 0
  store i1 %625, i1* %zf
  %626 = icmp slt i32 %616, 0
  store i1 %626, i1* %sf
  %627 = trunc i32 %616 to i8
  %628 = call i8 @llvm.ctpop.i8(i8 %627)
  %629 = and i8 %628, 1
  %630 = icmp eq i8 %629, 0
  store i1 %630, i1* %pf
  %631 = sext i32 %616 to i64
  %632 = trunc i64 %631 to i32
  store i32 %632, i32* %stack_var_-40
  br label %block_cd51

block_cd51:                                       ; preds = %block_cd4d, %block_ccae
  store volatile i64 52561, i64* @assembly_address
  %633 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %634 = zext i32 %633 to i64
  store i64 %634, i64* %rax
  store volatile i64 52567, i64* @assembly_address
  %635 = load i32* %stack_var_-40
  %636 = sext i32 %635 to i64
  %637 = trunc i64 %636 to i32
  %638 = load i64* %rax
  %639 = trunc i64 %638 to i32
  store i32 %637, i32* %3
  %640 = trunc i64 %638 to i32
  store i32 %640, i32* %1
  %641 = sub i32 %637, %639
  %642 = and i32 %637, 15
  %643 = and i32 %639, 15
  %644 = sub i32 %642, %643
  %645 = icmp ugt i32 %644, 15
  %646 = icmp ult i32 %637, %639
  %647 = xor i32 %637, %639
  %648 = xor i32 %637, %641
  %649 = and i32 %647, %648
  %650 = icmp slt i32 %649, 0
  store i1 %645, i1* %az
  store i1 %646, i1* %cf
  store i1 %650, i1* %of
  %651 = icmp eq i32 %641, 0
  store i1 %651, i1* %zf
  %652 = icmp slt i32 %641, 0
  store i1 %652, i1* %sf
  %653 = trunc i32 %641 to i8
  %654 = call i8 @llvm.ctpop.i8(i8 %653)
  %655 = and i8 %654, 1
  %656 = icmp eq i8 %655, 0
  store i1 %656, i1* %pf
  store volatile i64 52570, i64* @assembly_address
  %657 = load i32* %3
  %658 = load i32* %1
  %659 = sext i32 %658 to i64
  %660 = sext i32 %657 to i64
  %661 = icmp sle i64 %660, %659
  br i1 %661, label %block_ccf0, label %block_cd5c

block_cd5c:                                       ; preds = %block_cd51
  store volatile i64 52572, i64* @assembly_address
  %662 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %663 = zext i32 %662 to i64
  store i64 %663, i64* %rax
  store volatile i64 52578, i64* @assembly_address
  %664 = load i64* %rax
  %665 = trunc i64 %664 to i32
  %666 = sext i32 %665 to i64
  store i64 %666, i64* %rdx
  store volatile i64 52581, i64* @assembly_address
  %667 = load i64* %rdx
  %668 = mul i64 %667, 4
  store i64 %668, i64* %rcx
  store volatile i64 52589, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rdx
  store volatile i64 52596, i64* @assembly_address
  %669 = load i64* %rcx
  %670 = load i64* %rdx
  %671 = mul i64 %670, 1
  %672 = add i64 %669, %671
  %673 = inttoptr i64 %672 to i32*
  %674 = load i32* %673
  %675 = zext i32 %674 to i64
  store i64 %675, i64* %rdx
  store volatile i64 52599, i64* @assembly_address
  %676 = load i64* %rdx
  %677 = add i64 %676, 1
  %678 = trunc i64 %677 to i32
  %679 = zext i32 %678 to i64
  store i64 %679, i64* %rcx
  store volatile i64 52602, i64* @assembly_address
  %680 = load i64* %rax
  %681 = trunc i64 %680 to i32
  %682 = sext i32 %681 to i64
  store i64 %682, i64* %rax
  store volatile i64 52604, i64* @assembly_address
  %683 = load i64* %rax
  %684 = mul i64 %683, 4
  store i64 %684, i64* %rdx
  store volatile i64 52612, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52619, i64* @assembly_address
  %685 = load i64* %rcx
  %686 = trunc i64 %685 to i32
  %687 = load i64* %rdx
  %688 = load i64* %rax
  %689 = mul i64 %688, 1
  %690 = add i64 %687, %689
  %691 = inttoptr i64 %690 to i32*
  store i32 %686, i32* %691
  store volatile i64 52622, i64* @assembly_address
  store volatile i64 52623, i64* @assembly_address
  %692 = load i64* %rsp
  %693 = add i64 %692, 24
  %694 = and i64 %692, 15
  %695 = add i64 %694, 8
  %696 = icmp ugt i64 %695, 15
  %697 = icmp ult i64 %693, %692
  %698 = xor i64 %692, %693
  %699 = xor i64 24, %693
  %700 = and i64 %698, %699
  %701 = icmp slt i64 %700, 0
  store i1 %696, i1* %az
  store i1 %697, i1* %cf
  store i1 %701, i1* %of
  %702 = icmp eq i64 %693, 0
  store i1 %702, i1* %zf
  %703 = icmp slt i64 %693, 0
  store i1 %703, i1* %sf
  %704 = trunc i64 %693 to i8
  %705 = call i8 @llvm.ctpop.i8(i8 %704)
  %706 = and i8 %705, 1
  %707 = icmp eq i8 %706, 0
  store i1 %707, i1* %pf
  %708 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %708, i64* %rsp
  store volatile i64 52627, i64* @assembly_address
  %709 = load i64* %stack_var_-16
  store i64 %709, i64* %rbx
  %710 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %710, i64* %rsp
  store volatile i64 52628, i64* @assembly_address
  %711 = load i64* %stack_var_-8
  store i64 %711, i64* %rbp
  %712 = ptrtoint i64* %stack_var_0 to i64
  store i64 %712, i64* %rsp
  store volatile i64 52629, i64* @assembly_address
  %713 = load i64* %rax
  %714 = load i64* %rax
  ret i64 %714
}

define i64 @build_tree1() {
block_cd96:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-20 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  %0 = alloca i32
  %1 = alloca i64
  %2 = alloca i32
  store volatile i64 52630, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 52631, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 52634, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 52638, i64* @assembly_address
  store i32 0, i32* %stack_var_-28
  store volatile i64 52645, i64* @assembly_address
  %22 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %23 = zext i32 %22 to i64
  store i64 %23, i64* %rax
  store volatile i64 52651, i64* @assembly_address
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-24
  store volatile i64 52654, i64* @assembly_address
  br label %block_ce1e

block_cdb0:                                       ; preds = %block_ce1e
  store volatile i64 52656, i64* @assembly_address
  %26 = load i32* %stack_var_-28
  %27 = load i1* %of
  %28 = ashr i32 %26, 1
  %29 = icmp eq i32 %28, 0
  store i1 %29, i1* %zf
  %30 = icmp slt i32 %28, 0
  store i1 %30, i1* %sf
  %31 = trunc i32 %28 to i8
  %32 = call i8 @llvm.ctpop.i8(i8 %31)
  %33 = and i8 %32, 1
  %34 = icmp eq i8 %33, 0
  store i1 %34, i1* %pf
  store i32 %28, i32* %stack_var_-28
  %35 = and i32 1, %26
  %36 = icmp ne i32 %35, 0
  store i1 %36, i1* %cf
  %37 = select i1 true, i1 false, i1 %27
  store i1 %37, i1* %of
  store volatile i64 52659, i64* @assembly_address
  %38 = load i32* %stack_var_-24
  %39 = zext i32 %38 to i64
  store i64 %39, i64* %rax
  store volatile i64 52662, i64* @assembly_address
  %40 = load i64* %rax
  %41 = trunc i64 %40 to i32
  %42 = sext i32 %41 to i64
  store i64 %42, i64* %rax
  store volatile i64 52664, i64* @assembly_address
  %43 = load i64* %rax
  %44 = mul i64 %43, 4
  store i64 %44, i64* %rcx
  store volatile i64 52672, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a380 to i64), i64* %rax
  store volatile i64 52679, i64* @assembly_address
  %45 = load i32* %stack_var_-28
  %46 = zext i32 %45 to i64
  store i64 %46, i64* %rdx
  store volatile i64 52682, i64* @assembly_address
  %47 = load i64* %rdx
  %48 = trunc i64 %47 to i32
  %49 = load i64* %rcx
  %50 = load i64* %rax
  %51 = mul i64 %50, 1
  %52 = add i64 %49, %51
  %53 = inttoptr i64 %52 to i32*
  store i32 %48, i32* %53
  store volatile i64 52685, i64* @assembly_address
  %54 = load i32* %stack_var_-24
  %55 = zext i32 %54 to i64
  store i64 %55, i64* %rax
  store volatile i64 52688, i64* @assembly_address
  %56 = load i64* %rax
  %57 = trunc i64 %56 to i32
  %58 = sext i32 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 52690, i64* @assembly_address
  %59 = load i64* %rax
  %60 = mul i64 %59, 4
  store i64 %60, i64* %rdx
  store volatile i64 52698, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a280 to i64), i64* %rax
  store volatile i64 52705, i64* @assembly_address
  %61 = load i64* %rdx
  %62 = load i64* %rax
  %63 = mul i64 %62, 1
  %64 = add i64 %61, %63
  %65 = inttoptr i64 %64 to i32*
  %66 = load i32* %65
  %67 = zext i32 %66 to i64
  store i64 %67, i64* %rax
  store volatile i64 52708, i64* @assembly_address
  %68 = load i64* %rax
  %69 = trunc i64 %68 to i32
  %70 = load i32* %stack_var_-28
  %71 = sub i32 %69, %70
  %72 = and i32 %69, 15
  %73 = and i32 %70, 15
  %74 = sub i32 %72, %73
  %75 = icmp ugt i32 %74, 15
  %76 = icmp ult i32 %69, %70
  %77 = xor i32 %69, %70
  %78 = xor i32 %69, %71
  %79 = and i32 %77, %78
  %80 = icmp slt i32 %79, 0
  store i1 %75, i1* %az
  store i1 %76, i1* %cf
  store i1 %80, i1* %of
  %81 = icmp eq i32 %71, 0
  store i1 %81, i1* %zf
  %82 = icmp slt i32 %71, 0
  store i1 %82, i1* %sf
  %83 = trunc i32 %71 to i8
  %84 = call i8 @llvm.ctpop.i8(i8 %83)
  %85 = and i8 %84, 1
  %86 = icmp eq i8 %85, 0
  store i1 %86, i1* %pf
  %87 = zext i32 %71 to i64
  store i64 %87, i64* %rax
  store volatile i64 52711, i64* @assembly_address
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  %90 = zext i32 %89 to i64
  store i64 %90, i64* %rdx
  store volatile i64 52713, i64* @assembly_address
  %91 = load i32* %stack_var_-24
  %92 = zext i32 %91 to i64
  store i64 %92, i64* %rax
  store volatile i64 52716, i64* @assembly_address
  %93 = load i64* %rax
  %94 = trunc i64 %93 to i32
  %95 = sext i32 %94 to i64
  store i64 %95, i64* %rax
  store volatile i64 52718, i64* @assembly_address
  %96 = load i64* %rax
  %97 = mul i64 %96, 4
  store i64 %97, i64* %rcx
  store volatile i64 52726, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a280 to i64), i64* %rax
  store volatile i64 52733, i64* @assembly_address
  %98 = load i64* %rdx
  %99 = trunc i64 %98 to i32
  %100 = load i64* %rcx
  %101 = load i64* %rax
  %102 = mul i64 %101, 1
  %103 = add i64 %100, %102
  %104 = inttoptr i64 %103 to i32*
  store i32 %99, i32* %104
  store volatile i64 52736, i64* @assembly_address
  %105 = load i32* %stack_var_-24
  %106 = zext i32 %105 to i64
  store i64 %106, i64* %rax
  store volatile i64 52739, i64* @assembly_address
  %107 = load i64* %rax
  %108 = trunc i64 %107 to i32
  %109 = sext i32 %108 to i64
  store i64 %109, i64* %rax
  store volatile i64 52741, i64* @assembly_address
  %110 = load i64* %rax
  %111 = mul i64 %110, 4
  store i64 %111, i64* %rdx
  store volatile i64 52749, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52756, i64* @assembly_address
  %112 = load i64* %rdx
  %113 = load i64* %rax
  %114 = mul i64 %113, 1
  %115 = add i64 %112, %114
  %116 = inttoptr i64 %115 to i32*
  %117 = load i32* %116
  %118 = zext i32 %117 to i64
  store i64 %118, i64* %rax
  store volatile i64 52759, i64* @assembly_address
  %119 = load i32* %stack_var_-28
  %120 = load i64* %rax
  %121 = trunc i64 %120 to i32
  %122 = add i32 %119, %121
  %123 = and i32 %119, 15
  %124 = and i32 %121, 15
  %125 = add i32 %123, %124
  %126 = icmp ugt i32 %125, 15
  %127 = icmp ult i32 %122, %119
  %128 = xor i32 %119, %122
  %129 = xor i32 %121, %122
  %130 = and i32 %128, %129
  %131 = icmp slt i32 %130, 0
  store i1 %126, i1* %az
  store i1 %127, i1* %cf
  store i1 %131, i1* %of
  %132 = icmp eq i32 %122, 0
  store i1 %132, i1* %zf
  %133 = icmp slt i32 %122, 0
  store i1 %133, i1* %sf
  %134 = trunc i32 %122 to i8
  %135 = call i8 @llvm.ctpop.i8(i8 %134)
  %136 = and i8 %135, 1
  %137 = icmp eq i8 %136, 0
  store i1 %137, i1* %pf
  store i32 %122, i32* %stack_var_-28
  store volatile i64 52762, i64* @assembly_address
  %138 = load i32* %stack_var_-24
  %139 = sub i32 %138, 1
  %140 = and i32 %138, 15
  %141 = sub i32 %140, 1
  %142 = icmp ugt i32 %141, 15
  %143 = icmp ult i32 %138, 1
  %144 = xor i32 %138, 1
  %145 = xor i32 %138, %139
  %146 = and i32 %144, %145
  %147 = icmp slt i32 %146, 0
  store i1 %142, i1* %az
  store i1 %143, i1* %cf
  store i1 %147, i1* %of
  %148 = icmp eq i32 %139, 0
  store i1 %148, i1* %zf
  %149 = icmp slt i32 %139, 0
  store i1 %149, i1* %sf
  %150 = trunc i32 %139 to i8
  %151 = call i8 @llvm.ctpop.i8(i8 %150)
  %152 = and i8 %151, 1
  %153 = icmp eq i8 %152, 0
  store i1 %153, i1* %pf
  store i32 %139, i32* %stack_var_-24
  br label %block_ce1e

block_ce1e:                                       ; preds = %block_cdb0, %block_cd96
  store volatile i64 52766, i64* @assembly_address
  %154 = load i32* %stack_var_-24
  %155 = and i32 %154, 15
  %156 = icmp ugt i32 %155, 15
  %157 = icmp ult i32 %154, 0
  %158 = xor i32 %154, 0
  %159 = and i32 %158, 0
  %160 = icmp slt i32 %159, 0
  store i1 %156, i1* %az
  store i1 %157, i1* %cf
  store i1 %160, i1* %of
  %161 = icmp eq i32 %154, 0
  store i1 %161, i1* %zf
  %162 = icmp slt i32 %154, 0
  store i1 %162, i1* %sf
  %163 = trunc i32 %154 to i8
  %164 = call i8 @llvm.ctpop.i8(i8 %163)
  %165 = and i8 %164, 1
  %166 = icmp eq i8 %165, 0
  store i1 %166, i1* %pf
  store volatile i64 52770, i64* @assembly_address
  %167 = load i1* %zf
  %168 = load i1* %sf
  %169 = load i1* %of
  %170 = icmp eq i1 %168, %169
  %171 = icmp eq i1 %167, false
  %172 = icmp eq i1 %170, %171
  br i1 %172, label %block_cdb0, label %block_ce24

block_ce24:                                       ; preds = %block_ce1e
  store volatile i64 52772, i64* @assembly_address
  %173 = load i32* %stack_var_-28
  %174 = zext i32 %173 to i64
  store i64 %174, i64* %rax
  store volatile i64 52775, i64* @assembly_address
  %175 = load i64* %rax
  %176 = trunc i64 %175 to i32
  %177 = load i1* %of
  %178 = ashr i32 %176, 1
  %179 = icmp eq i32 %178, 0
  store i1 %179, i1* %zf
  %180 = icmp slt i32 %178, 0
  store i1 %180, i1* %sf
  %181 = trunc i32 %178 to i8
  %182 = call i8 @llvm.ctpop.i8(i8 %181)
  %183 = and i8 %182, 1
  %184 = icmp eq i8 %183, 0
  store i1 %184, i1* %pf
  %185 = zext i32 %178 to i64
  store i64 %185, i64* %rax
  %186 = and i32 1, %176
  %187 = icmp ne i32 %186, 0
  store i1 %187, i1* %cf
  %188 = select i1 true, i1 false, i1 %177
  store i1 %188, i1* %of
  store volatile i64 52777, i64* @assembly_address
  %189 = load i64* %rax
  %190 = trunc i64 %189 to i32
  %191 = sub i32 %190, 1
  %192 = and i32 %190, 15
  %193 = sub i32 %192, 1
  %194 = icmp ugt i32 %193, 15
  %195 = icmp ult i32 %190, 1
  %196 = xor i32 %190, 1
  %197 = xor i32 %190, %191
  %198 = and i32 %196, %197
  %199 = icmp slt i32 %198, 0
  store i1 %194, i1* %az
  store i1 %195, i1* %cf
  store i1 %199, i1* %of
  %200 = icmp eq i32 %191, 0
  store i1 %200, i1* %zf
  %201 = icmp slt i32 %191, 0
  store i1 %201, i1* %sf
  %202 = trunc i32 %191 to i8
  %203 = call i8 @llvm.ctpop.i8(i8 %202)
  %204 = and i8 %203, 1
  %205 = icmp eq i8 %204, 0
  store i1 %205, i1* %pf
  store volatile i64 52780, i64* @assembly_address
  %206 = load i1* %zf
  br i1 %206, label %block_ce3a, label %block_ce2e

block_ce2e:                                       ; preds = %block_ce24
  store volatile i64 52782, i64* @assembly_address
  store i64 ptrtoint ([31 x i8]* @global_var_12208 to i64), i64* %rdi
  store volatile i64 52789, i64* @assembly_address
  %207 = load i64* %rdi
  %208 = inttoptr i64 %207 to i8*
  %209 = call i64 @gzip_error(i8* %208)
  store i64 %209, i64* %rax
  store i64 %209, i64* %rax
  unreachable

block_ce3a:                                       ; preds = %block_ce24
  store volatile i64 52794, i64* @assembly_address
  %210 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %211 = zext i32 %210 to i64
  store i64 %211, i64* %rax
  store volatile i64 52800, i64* @assembly_address
  store i64 12, i64* %rdx
  store volatile i64 52805, i64* @assembly_address
  %212 = load i64* %rax
  %213 = trunc i64 %212 to i32
  %214 = sub i32 %213, 12
  %215 = and i32 %213, 15
  %216 = sub i32 %215, 12
  %217 = icmp ugt i32 %216, 15
  %218 = icmp ult i32 %213, 12
  %219 = xor i32 %213, 12
  %220 = xor i32 %213, %214
  %221 = and i32 %219, %220
  %222 = icmp slt i32 %221, 0
  store i1 %217, i1* %az
  store i1 %218, i1* %cf
  store i1 %222, i1* %of
  %223 = icmp eq i32 %214, 0
  store i1 %223, i1* %zf
  %224 = icmp slt i32 %214, 0
  store i1 %224, i1* %sf
  %225 = trunc i32 %214 to i8
  %226 = call i8 @llvm.ctpop.i8(i8 %225)
  %227 = and i8 %226, 1
  %228 = icmp eq i8 %227, 0
  store i1 %228, i1* %pf
  store volatile i64 52808, i64* @assembly_address
  %229 = load i1* %zf
  %230 = load i1* %sf
  %231 = load i1* %of
  %232 = icmp eq i1 %230, %231
  %233 = icmp eq i1 %229, false
  %234 = icmp eq i1 %232, %233
  %235 = load i64* %rax
  %236 = trunc i64 %235 to i32
  %237 = load i64* %rdx
  %238 = trunc i64 %237 to i32
  %239 = select i1 %234, i32 %238, i32 %236
  %240 = zext i32 %239 to i64
  store i64 %240, i64* %rax
  store volatile i64 52811, i64* @assembly_address
  %241 = load i64* %rax
  %242 = trunc i64 %241 to i32
  store i32 %242, i32* bitcast (i64* @global_var_21a3e8 to i32*)
  store volatile i64 52817, i64* @assembly_address
  %243 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %244 = zext i32 %243 to i64
  store i64 %244, i64* %rax
  store volatile i64 52823, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 52828, i64* @assembly_address
  %245 = load i64* %rax
  %246 = trunc i64 %245 to i32
  %247 = zext i32 %246 to i64
  store i64 %247, i64* %rcx
  store volatile i64 52830, i64* @assembly_address
  %248 = load i64* %rdx
  %249 = trunc i64 %248 to i32
  %250 = load i64* %rcx
  %251 = trunc i64 %250 to i8
  %252 = zext i8 %251 to i32
  %253 = and i32 %252, 31
  %254 = load i1* %of
  %255 = icmp eq i32 %253, 0
  br i1 %255, label %273, label %256

; <label>:256                                     ; preds = %block_ce3a
  %257 = shl i32 %249, %253
  %258 = icmp eq i32 %257, 0
  store i1 %258, i1* %zf
  %259 = icmp slt i32 %257, 0
  store i1 %259, i1* %sf
  %260 = trunc i32 %257 to i8
  %261 = call i8 @llvm.ctpop.i8(i8 %260)
  %262 = and i8 %261, 1
  %263 = icmp eq i8 %262, 0
  store i1 %263, i1* %pf
  %264 = zext i32 %257 to i64
  store i64 %264, i64* %rdx
  %265 = sub i32 %253, 1
  %266 = shl i32 %249, %265
  %267 = lshr i32 %266, 31
  %268 = trunc i32 %267 to i1
  store i1 %268, i1* %cf
  %269 = lshr i32 %257, 31
  %270 = icmp ne i32 %269, %267
  %271 = icmp eq i32 %253, 1
  %272 = select i1 %271, i1 %270, i1 %254
  store i1 %272, i1* %of
  br label %273

; <label>:273                                     ; preds = %block_ce3a, %256
  store volatile i64 52832, i64* @assembly_address
  %274 = load i64* %rdx
  %275 = trunc i64 %274 to i32
  %276 = zext i32 %275 to i64
  store i64 %276, i64* %rax
  store volatile i64 52834, i64* @assembly_address
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i32
  %279 = sext i32 %278 to i64
  store i64 %279, i64* %rdx
  store volatile i64 52837, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 52844, i64* @assembly_address
  %280 = load i64* %rax
  %281 = load i64* %rdx
  %282 = add i64 %280, %281
  %283 = and i64 %280, 15
  %284 = and i64 %281, 15
  %285 = add i64 %283, %284
  %286 = icmp ugt i64 %285, 15
  %287 = icmp ult i64 %282, %280
  %288 = xor i64 %280, %282
  %289 = xor i64 %281, %282
  %290 = and i64 %288, %289
  %291 = icmp slt i64 %290, 0
  store i1 %286, i1* %az
  store i1 %287, i1* %cf
  store i1 %291, i1* %of
  %292 = icmp eq i64 %282, 0
  store i1 %292, i1* %zf
  %293 = icmp slt i64 %282, 0
  store i1 %293, i1* %sf
  %294 = trunc i64 %282 to i8
  %295 = call i8 @llvm.ctpop.i8(i8 %294)
  %296 = and i8 %295, 1
  %297 = icmp eq i8 %296, 0
  store i1 %297, i1* %pf
  store i64 %282, i64* %rax
  store volatile i64 52847, i64* @assembly_address
  %298 = load i64* %rax
  store i64 %298, i64* %stack_var_-16
  store volatile i64 52851, i64* @assembly_address
  store i32 1, i32* %stack_var_-24
  store volatile i64 52858, i64* @assembly_address
  br label %block_cec8

block_ce7c:                                       ; preds = %block_cec8
  store volatile i64 52860, i64* @assembly_address
  %299 = load i32* %stack_var_-24
  %300 = zext i32 %299 to i64
  store i64 %300, i64* %rax
  store volatile i64 52863, i64* @assembly_address
  %301 = load i64* %rax
  %302 = trunc i64 %301 to i32
  %303 = sext i32 %302 to i64
  store i64 %303, i64* %rax
  store volatile i64 52865, i64* @assembly_address
  %304 = load i64* %rax
  %305 = mul i64 %304, 4
  store i64 %305, i64* %rdx
  store volatile i64 52873, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 52880, i64* @assembly_address
  %306 = load i64* %rdx
  %307 = load i64* %rax
  %308 = mul i64 %307, 1
  %309 = add i64 %306, %308
  %310 = inttoptr i64 %309 to i32*
  %311 = load i32* %310
  %312 = zext i32 %311 to i64
  store i64 %312, i64* %rdx
  store volatile i64 52883, i64* @assembly_address
  %313 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %314 = zext i32 %313 to i64
  store i64 %314, i64* %rax
  store volatile i64 52889, i64* @assembly_address
  %315 = load i64* %rax
  %316 = trunc i64 %315 to i32
  %317 = load i32* %stack_var_-24
  %318 = sub i32 %316, %317
  %319 = and i32 %316, 15
  %320 = and i32 %317, 15
  %321 = sub i32 %319, %320
  %322 = icmp ugt i32 %321, 15
  %323 = icmp ult i32 %316, %317
  %324 = xor i32 %316, %317
  %325 = xor i32 %316, %318
  %326 = and i32 %324, %325
  %327 = icmp slt i32 %326, 0
  store i1 %322, i1* %az
  store i1 %323, i1* %cf
  store i1 %327, i1* %of
  %328 = icmp eq i32 %318, 0
  store i1 %328, i1* %zf
  %329 = icmp slt i32 %318, 0
  store i1 %329, i1* %sf
  %330 = trunc i32 %318 to i8
  %331 = call i8 @llvm.ctpop.i8(i8 %330)
  %332 = and i8 %331, 1
  %333 = icmp eq i8 %332, 0
  store i1 %333, i1* %pf
  %334 = zext i32 %318 to i64
  store i64 %334, i64* %rax
  store volatile i64 52892, i64* @assembly_address
  %335 = load i64* %rax
  %336 = trunc i64 %335 to i32
  %337 = zext i32 %336 to i64
  store i64 %337, i64* %rcx
  store volatile i64 52894, i64* @assembly_address
  %338 = load i64* %rdx
  %339 = trunc i64 %338 to i32
  %340 = load i64* %rcx
  %341 = trunc i64 %340 to i8
  %342 = zext i8 %341 to i32
  %343 = and i32 %342, 31
  %344 = load i1* %of
  %345 = icmp eq i32 %343, 0
  br i1 %345, label %363, label %346

; <label>:346                                     ; preds = %block_ce7c
  %347 = shl i32 %339, %343
  %348 = icmp eq i32 %347, 0
  store i1 %348, i1* %zf
  %349 = icmp slt i32 %347, 0
  store i1 %349, i1* %sf
  %350 = trunc i32 %347 to i8
  %351 = call i8 @llvm.ctpop.i8(i8 %350)
  %352 = and i8 %351, 1
  %353 = icmp eq i8 %352, 0
  store i1 %353, i1* %pf
  %354 = zext i32 %347 to i64
  store i64 %354, i64* %rdx
  %355 = sub i32 %343, 1
  %356 = shl i32 %339, %355
  %357 = lshr i32 %356, 31
  %358 = trunc i32 %357 to i1
  store i1 %358, i1* %cf
  %359 = lshr i32 %347, 31
  %360 = icmp ne i32 %359, %357
  %361 = icmp eq i32 %343, 1
  %362 = select i1 %361, i1 %360, i1 %344
  store i1 %362, i1* %of
  br label %363

; <label>:363                                     ; preds = %block_ce7c, %346
  store volatile i64 52896, i64* @assembly_address
  %364 = load i64* %rdx
  %365 = trunc i64 %364 to i32
  %366 = zext i32 %365 to i64
  store i64 %366, i64* %rax
  store volatile i64 52898, i64* @assembly_address
  %367 = load i64* %rax
  %368 = trunc i64 %367 to i32
  store i32 %368, i32* %stack_var_-20
  store volatile i64 52901, i64* @assembly_address
  br label %block_ceb7

block_cea7:                                       ; preds = %block_ceb7
  store volatile i64 52903, i64* @assembly_address
  %369 = load i64* %stack_var_-16
  %370 = sub i64 %369, 1
  %371 = and i64 %369, 15
  %372 = sub i64 %371, 1
  %373 = icmp ugt i64 %372, 15
  %374 = icmp ult i64 %369, 1
  %375 = xor i64 %369, 1
  %376 = xor i64 %369, %370
  %377 = and i64 %375, %376
  %378 = icmp slt i64 %377, 0
  store i1 %373, i1* %az
  store i1 %374, i1* %cf
  store i1 %378, i1* %of
  %379 = icmp eq i64 %370, 0
  store i1 %379, i1* %zf
  %380 = icmp slt i64 %370, 0
  store i1 %380, i1* %sf
  %381 = trunc i64 %370 to i8
  %382 = call i8 @llvm.ctpop.i8(i8 %381)
  %383 = and i8 %382, 1
  %384 = icmp eq i8 %383, 0
  store i1 %384, i1* %pf
  store i64 %370, i64* %stack_var_-16
  store volatile i64 52908, i64* @assembly_address
  %385 = load i32* %stack_var_-24
  %386 = zext i32 %385 to i64
  store i64 %386, i64* %rax
  store volatile i64 52911, i64* @assembly_address
  %387 = load i64* %rax
  %388 = trunc i64 %387 to i32
  %389 = zext i32 %388 to i64
  store i64 %389, i64* %rdx
  store volatile i64 52913, i64* @assembly_address
  %390 = load i64* %stack_var_-16
  store i64 %390, i64* %rax
  store volatile i64 52917, i64* @assembly_address
  %391 = load i64* %rdx
  %392 = trunc i64 %391 to i8
  %393 = load i64* %rax
  %394 = inttoptr i64 %393 to i8*
  store i8 %392, i8* %394
  br label %block_ceb7

block_ceb7:                                       ; preds = %block_cea7, %363
  store volatile i64 52919, i64* @assembly_address
  %395 = load i32* %stack_var_-20
  %396 = zext i32 %395 to i64
  store i64 %396, i64* %rax
  store volatile i64 52922, i64* @assembly_address
  %397 = load i64* %rax
  %398 = add i64 %397, -1
  %399 = trunc i64 %398 to i32
  %400 = zext i32 %399 to i64
  store i64 %400, i64* %rdx
  store volatile i64 52925, i64* @assembly_address
  %401 = load i64* %rdx
  %402 = trunc i64 %401 to i32
  store i32 %402, i32* %stack_var_-20
  store volatile i64 52928, i64* @assembly_address
  %403 = load i64* %rax
  %404 = trunc i64 %403 to i32
  %405 = load i64* %rax
  %406 = trunc i64 %405 to i32
  %407 = and i32 %404, %406
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %408 = icmp eq i32 %407, 0
  store i1 %408, i1* %zf
  %409 = icmp slt i32 %407, 0
  store i1 %409, i1* %sf
  %410 = trunc i32 %407 to i8
  %411 = call i8 @llvm.ctpop.i8(i8 %410)
  %412 = and i8 %411, 1
  %413 = icmp eq i8 %412, 0
  store i1 %413, i1* %pf
  store volatile i64 52930, i64* @assembly_address
  %414 = load i1* %zf
  %415 = icmp eq i1 %414, false
  br i1 %415, label %block_cea7, label %block_cec4

block_cec4:                                       ; preds = %block_ceb7
  store volatile i64 52932, i64* @assembly_address
  %416 = load i32* %stack_var_-24
  %417 = add i32 %416, 1
  %418 = and i32 %416, 15
  %419 = add i32 %418, 1
  %420 = icmp ugt i32 %419, 15
  %421 = icmp ult i32 %417, %416
  %422 = xor i32 %416, %417
  %423 = xor i32 1, %417
  %424 = and i32 %422, %423
  %425 = icmp slt i32 %424, 0
  store i1 %420, i1* %az
  store i1 %421, i1* %cf
  store i1 %425, i1* %of
  %426 = icmp eq i32 %417, 0
  store i1 %426, i1* %zf
  %427 = icmp slt i32 %417, 0
  store i1 %427, i1* %sf
  %428 = trunc i32 %417 to i8
  %429 = call i8 @llvm.ctpop.i8(i8 %428)
  %430 = and i8 %429, 1
  %431 = icmp eq i8 %430, 0
  store i1 %431, i1* %pf
  store i32 %417, i32* %stack_var_-24
  br label %block_cec8

block_cec8:                                       ; preds = %block_cec4, %273
  store volatile i64 52936, i64* @assembly_address
  %432 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %433 = zext i32 %432 to i64
  store i64 %433, i64* %rax
  store volatile i64 52942, i64* @assembly_address
  %434 = load i32* %stack_var_-24
  %435 = load i64* %rax
  %436 = trunc i64 %435 to i32
  store i32 %434, i32* %2
  %437 = trunc i64 %435 to i32
  store i32 %437, i32* %0
  %438 = sub i32 %434, %436
  %439 = and i32 %434, 15
  %440 = and i32 %436, 15
  %441 = sub i32 %439, %440
  %442 = icmp ugt i32 %441, 15
  %443 = icmp ult i32 %434, %436
  %444 = xor i32 %434, %436
  %445 = xor i32 %434, %438
  %446 = and i32 %444, %445
  %447 = icmp slt i32 %446, 0
  store i1 %442, i1* %az
  store i1 %443, i1* %cf
  store i1 %447, i1* %of
  %448 = icmp eq i32 %438, 0
  store i1 %448, i1* %zf
  %449 = icmp slt i32 %438, 0
  store i1 %449, i1* %sf
  %450 = trunc i32 %438 to i8
  %451 = call i8 @llvm.ctpop.i8(i8 %450)
  %452 = and i8 %451, 1
  %453 = icmp eq i8 %452, 0
  store i1 %453, i1* %pf
  store volatile i64 52945, i64* @assembly_address
  %454 = load i32* %2
  %455 = load i32* %0
  %456 = sext i32 %455 to i64
  %457 = sext i32 %454 to i64
  %458 = icmp sle i64 %457, %456
  br i1 %458, label %block_ce7c, label %block_ced3

block_ced3:                                       ; preds = %block_cec8
  store volatile i64 52947, i64* @assembly_address
  br label %block_cee1

block_ced5:                                       ; preds = %block_cee1
  store volatile i64 52949, i64* @assembly_address
  %459 = load i64* %stack_var_-16
  %460 = sub i64 %459, 1
  %461 = and i64 %459, 15
  %462 = sub i64 %461, 1
  %463 = icmp ugt i64 %462, 15
  %464 = icmp ult i64 %459, 1
  %465 = xor i64 %459, 1
  %466 = xor i64 %459, %460
  %467 = and i64 %465, %466
  %468 = icmp slt i64 %467, 0
  store i1 %463, i1* %az
  store i1 %464, i1* %cf
  store i1 %468, i1* %of
  %469 = icmp eq i64 %460, 0
  store i1 %469, i1* %zf
  %470 = icmp slt i64 %460, 0
  store i1 %470, i1* %sf
  %471 = trunc i64 %460 to i8
  %472 = call i8 @llvm.ctpop.i8(i8 %471)
  %473 = and i8 %472, 1
  %474 = icmp eq i8 %473, 0
  store i1 %474, i1* %pf
  store i64 %460, i64* %stack_var_-16
  store volatile i64 52954, i64* @assembly_address
  %475 = load i64* %stack_var_-16
  store i64 %475, i64* %rax
  store volatile i64 52958, i64* @assembly_address
  %476 = load i64* %rax
  %477 = inttoptr i64 %476 to i8*
  store i8 0, i8* %477
  br label %block_cee1

block_cee1:                                       ; preds = %block_ced5, %block_ced3
  store volatile i64 52961, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 52968, i64* @assembly_address
  %478 = load i64* %stack_var_-16
  %479 = load i64* %rax
  %480 = sub i64 %478, %479
  %481 = and i64 %478, 15
  %482 = and i64 %479, 15
  %483 = sub i64 %481, %482
  %484 = icmp ugt i64 %483, 15
  %485 = icmp ult i64 %478, %479
  %486 = xor i64 %478, %479
  %487 = xor i64 %478, %480
  %488 = and i64 %486, %487
  %489 = icmp slt i64 %488, 0
  store i1 %484, i1* %az
  store i1 %485, i1* %cf
  store i1 %489, i1* %of
  %490 = icmp eq i64 %480, 0
  store i1 %490, i1* %zf
  %491 = icmp slt i64 %480, 0
  store i1 %491, i1* %sf
  %492 = trunc i64 %480 to i8
  %493 = call i8 @llvm.ctpop.i8(i8 %492)
  %494 = and i8 %493, 1
  %495 = icmp eq i8 %494, 0
  store i1 %495, i1* %pf
  store volatile i64 52972, i64* @assembly_address
  %496 = load i1* %cf
  %497 = load i1* %zf
  %498 = or i1 %496, %497
  %499 = icmp ne i1 %498, true
  br i1 %499, label %block_ced5, label %block_ceee

block_ceee:                                       ; preds = %block_cee1
  store volatile i64 52974, i64* @assembly_address
  store volatile i64 52975, i64* @assembly_address
  %500 = load i64* %stack_var_-8
  store i64 %500, i64* %rbp
  %501 = ptrtoint i64* %stack_var_0 to i64
  store i64 %501, i64* %rsp
  store volatile i64 52976, i64* @assembly_address
  %502 = load i64* %rax
  %503 = load i64* %rax
  ret i64 %503
}

define i64 @unpack(i32 %arg1, i64 %arg2) {
block_cef1:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-44 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-40 = alloca i32
  %stack_var_-64 = alloca i32
  %stack_var_-60 = alloca i32
  %stack_var_-72 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i64
  %3 = alloca i32
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i64
  %8 = alloca i32
  %9 = alloca i64
  store volatile i64 52977, i64* @assembly_address
  %10 = load i64* %rbp
  store i64 %10, i64* %stack_var_-8
  %11 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %11, i64* %rsp
  store volatile i64 52978, i64* @assembly_address
  %12 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %12, i64* %rbp
  store volatile i64 52981, i64* @assembly_address
  %13 = load i64* %rbx
  store i64 %13, i64* %stack_var_-16
  %14 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %14, i64* %rsp
  store volatile i64 52982, i64* @assembly_address
  %15 = load i64* %rsp
  %16 = sub i64 %15, 56
  %17 = and i64 %15, 15
  %18 = sub i64 %17, 8
  %19 = icmp ugt i64 %18, 15
  %20 = icmp ult i64 %15, 56
  %21 = xor i64 %15, 56
  %22 = xor i64 %15, %16
  %23 = and i64 %21, %22
  %24 = icmp slt i64 %23, 0
  store i1 %19, i1* %az
  store i1 %20, i1* %cf
  store i1 %24, i1* %of
  %25 = icmp eq i64 %16, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i64 %16, 0
  store i1 %26, i1* %sf
  %27 = trunc i64 %16 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  %31 = ptrtoint i64* %stack_var_-72 to i64
  store i64 %31, i64* %rsp
  store volatile i64 52986, i64* @assembly_address
  %32 = load i64* %rdi
  %33 = trunc i64 %32 to i32
  store i32 %33, i32* %stack_var_-60
  store volatile i64 52989, i64* @assembly_address
  %34 = load i64* %rsi
  %35 = trunc i64 %34 to i32
  store i32 %35, i32* %stack_var_-64
  store volatile i64 52992, i64* @assembly_address
  %36 = load i32* %stack_var_-60
  %37 = zext i32 %36 to i64
  store i64 %37, i64* %rax
  store volatile i64 52995, i64* @assembly_address
  %38 = load i64* %rax
  %39 = trunc i64 %38 to i32
  store i32 %39, i32* bitcast (i64* @global_var_24f0a0 to i32*)
  store volatile i64 53001, i64* @assembly_address
  %40 = load i32* %stack_var_-64
  %41 = zext i32 %40 to i64
  store i64 %41, i64* %rax
  store volatile i64 53004, i64* @assembly_address
  %42 = load i64* %rax
  %43 = trunc i64 %42 to i32
  store i32 %43, i32* bitcast (i64* @global_var_24a880 to i32*)
  store volatile i64 53010, i64* @assembly_address
  %44 = load i64* %rdi
  %45 = load i64* %rsi
  %46 = call i64 @read_tree(i64 %44, i64 %45)
  store i64 %46, i64* %rax
  store i64 %46, i64* %rax
  store i64 %46, i64* %rax
  store volatile i64 53015, i64* @assembly_address
  %47 = call i64 @build_tree1()
  store i64 %47, i64* %rax
  store i64 %47, i64* %rax
  store i64 %47, i64* %rax
  store volatile i64 53020, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a3f8 to i32*)
  store volatile i64 53030, i64* @assembly_address
  store i64 0, i64* @global_var_21a3f0
  store volatile i64 53041, i64* @assembly_address
  %48 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %49 = zext i32 %48 to i64
  store i64 %49, i64* %rax
  store volatile i64 53047, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 53052, i64* @assembly_address
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  %52 = zext i32 %51 to i64
  store i64 %52, i64* %rcx
  store volatile i64 53054, i64* @assembly_address
  %53 = load i64* %rdx
  %54 = trunc i64 %53 to i32
  %55 = load i64* %rcx
  %56 = trunc i64 %55 to i8
  %57 = zext i8 %56 to i32
  %58 = and i32 %57, 31
  %59 = load i1* %of
  %60 = icmp eq i32 %58, 0
  br i1 %60, label %78, label %61

; <label>:61                                      ; preds = %block_cef1
  %62 = shl i32 %54, %58
  %63 = icmp eq i32 %62, 0
  store i1 %63, i1* %zf
  %64 = icmp slt i32 %62, 0
  store i1 %64, i1* %sf
  %65 = trunc i32 %62 to i8
  %66 = call i8 @llvm.ctpop.i8(i8 %65)
  %67 = and i8 %66, 1
  %68 = icmp eq i8 %67, 0
  store i1 %68, i1* %pf
  %69 = zext i32 %62 to i64
  store i64 %69, i64* %rdx
  %70 = sub i32 %58, 1
  %71 = shl i32 %54, %70
  %72 = lshr i32 %71, 31
  %73 = trunc i32 %72 to i1
  store i1 %73, i1* %cf
  %74 = lshr i32 %62, 31
  %75 = icmp ne i32 %74, %72
  %76 = icmp eq i32 %58, 1
  %77 = select i1 %76, i1 %75, i1 %59
  store i1 %77, i1* %of
  br label %78

; <label>:78                                      ; preds = %block_cef1, %61
  store volatile i64 53056, i64* @assembly_address
  %79 = load i64* %rdx
  %80 = trunc i64 %79 to i32
  %81 = zext i32 %80 to i64
  store i64 %81, i64* %rax
  store volatile i64 53058, i64* @assembly_address
  %82 = load i64* %rax
  %83 = trunc i64 %82 to i32
  %84 = sub i32 %83, 1
  %85 = and i32 %83, 15
  %86 = sub i32 %85, 1
  %87 = icmp ugt i32 %86, 15
  %88 = icmp ult i32 %83, 1
  %89 = xor i32 %83, 1
  %90 = xor i32 %83, %84
  %91 = and i32 %89, %90
  %92 = icmp slt i32 %91, 0
  store i1 %87, i1* %az
  store i1 %88, i1* %cf
  store i1 %92, i1* %of
  %93 = icmp eq i32 %84, 0
  store i1 %93, i1* %zf
  %94 = icmp slt i32 %84, 0
  store i1 %94, i1* %sf
  %95 = trunc i32 %84 to i8
  %96 = call i8 @llvm.ctpop.i8(i8 %95)
  %97 = and i8 %96, 1
  %98 = icmp eq i8 %97, 0
  store i1 %98, i1* %pf
  %99 = zext i32 %84 to i64
  store i64 %99, i64* %rax
  store volatile i64 53061, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i32
  store i32 %101, i32* %stack_var_-40
  store volatile i64 53064, i64* @assembly_address
  %102 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %103 = zext i32 %102 to i64
  store i64 %103, i64* %rax
  store volatile i64 53070, i64* @assembly_address
  %104 = load i64* %rax
  %105 = trunc i64 %104 to i32
  %106 = sext i32 %105 to i64
  store i64 %106, i64* %rax
  store volatile i64 53072, i64* @assembly_address
  %107 = load i64* %rax
  %108 = mul i64 %107, 4
  store i64 %108, i64* %rdx
  store volatile i64 53080, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a300 to i64), i64* %rax
  store volatile i64 53087, i64* @assembly_address
  %109 = load i64* %rdx
  %110 = load i64* %rax
  %111 = mul i64 %110, 1
  %112 = add i64 %109, %111
  %113 = inttoptr i64 %112 to i32*
  %114 = load i32* %113
  %115 = zext i32 %114 to i64
  store i64 %115, i64* %rax
  store volatile i64 53090, i64* @assembly_address
  %116 = load i64* %rax
  %117 = trunc i64 %116 to i32
  %118 = sub i32 %117, 1
  %119 = and i32 %117, 15
  %120 = sub i32 %119, 1
  %121 = icmp ugt i32 %120, 15
  %122 = icmp ult i32 %117, 1
  %123 = xor i32 %117, 1
  %124 = xor i32 %117, %118
  %125 = and i32 %123, %124
  %126 = icmp slt i32 %125, 0
  store i1 %121, i1* %az
  store i1 %122, i1* %cf
  store i1 %126, i1* %of
  %127 = icmp eq i32 %118, 0
  store i1 %127, i1* %zf
  %128 = icmp slt i32 %118, 0
  store i1 %128, i1* %sf
  %129 = trunc i32 %118 to i8
  %130 = call i8 @llvm.ctpop.i8(i8 %129)
  %131 = and i8 %130, 1
  %132 = icmp eq i8 %131, 0
  store i1 %132, i1* %pf
  %133 = zext i32 %118 to i64
  store i64 %133, i64* %rax
  store volatile i64 53093, i64* @assembly_address
  %134 = load i64* %rax
  %135 = trunc i64 %134 to i32
  store i32 %135, i32* %stack_var_-36
  store volatile i64 53096, i64* @assembly_address
  br label %block_cf99

block_cf6a:                                       ; preds = %block_cf99
  store volatile i64 53098, i64* @assembly_address
  %136 = load i64* @global_var_21a3f0
  store i64 %136, i64* %rax
  store volatile i64 53105, i64* @assembly_address
  %137 = load i64* %rax
  %138 = load i1* %of
  %139 = shl i64 %137, 8
  %140 = icmp eq i64 %139, 0
  store i1 %140, i1* %zf
  %141 = icmp slt i64 %139, 0
  store i1 %141, i1* %sf
  %142 = trunc i64 %139 to i8
  %143 = call i8 @llvm.ctpop.i8(i8 %142)
  %144 = and i8 %143, 1
  %145 = icmp eq i8 %144, 0
  store i1 %145, i1* %pf
  store i64 %139, i64* %rax
  %146 = shl i64 %137, 7
  %147 = lshr i64 %146, 63
  %148 = trunc i64 %147 to i1
  store i1 %148, i1* %cf
  %149 = lshr i64 %139, 63
  %150 = icmp ne i64 %149, %147
  %151 = select i1 false, i1 %150, i1 %138
  store i1 %151, i1* %of
  store volatile i64 53109, i64* @assembly_address
  %152 = load i64* %rax
  store i64 %152, i64* %rbx
  store volatile i64 53112, i64* @assembly_address
  %153 = call i64 @read_byte()
  store i64 %153, i64* %rax
  store i64 %153, i64* %rax
  store i64 %153, i64* %rax
  store volatile i64 53117, i64* @assembly_address
  %154 = load i64* %rax
  %155 = trunc i64 %154 to i8
  %156 = zext i8 %155 to i64
  store i64 %156, i64* %rax
  store volatile i64 53120, i64* @assembly_address
  %157 = load i64* %rax
  %158 = load i64* %rbx
  %159 = or i64 %157, %158
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %160 = icmp eq i64 %159, 0
  store i1 %160, i1* %zf
  %161 = icmp slt i64 %159, 0
  store i1 %161, i1* %sf
  %162 = trunc i64 %159 to i8
  %163 = call i8 @llvm.ctpop.i8(i8 %162)
  %164 = and i8 %163, 1
  %165 = icmp eq i8 %164, 0
  store i1 %165, i1* %pf
  store i64 %159, i64* %rax
  store volatile i64 53123, i64* @assembly_address
  %166 = load i64* %rax
  store i64 %166, i64* @global_var_21a3f0
  store volatile i64 53130, i64* @assembly_address
  %167 = load i32* bitcast (i64* @global_var_21a3f8 to i32*)
  %168 = zext i32 %167 to i64
  store i64 %168, i64* %rax
  store volatile i64 53136, i64* @assembly_address
  %169 = load i64* %rax
  %170 = trunc i64 %169 to i32
  %171 = add i32 %170, 8
  %172 = and i32 %170, 15
  %173 = add i32 %172, 8
  %174 = icmp ugt i32 %173, 15
  %175 = icmp ult i32 %171, %170
  %176 = xor i32 %170, %171
  %177 = xor i32 8, %171
  %178 = and i32 %176, %177
  %179 = icmp slt i32 %178, 0
  store i1 %174, i1* %az
  store i1 %175, i1* %cf
  store i1 %179, i1* %of
  %180 = icmp eq i32 %171, 0
  store i1 %180, i1* %zf
  %181 = icmp slt i32 %171, 0
  store i1 %181, i1* %sf
  %182 = trunc i32 %171 to i8
  %183 = call i8 @llvm.ctpop.i8(i8 %182)
  %184 = and i8 %183, 1
  %185 = icmp eq i8 %184, 0
  store i1 %185, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a400 to i64), i64* %rax
  store volatile i64 53139, i64* @assembly_address
  %186 = load i64* %rax
  %187 = trunc i64 %186 to i32
  store i32 %187, i32* bitcast (i64* @global_var_21a3f8 to i32*)
  br label %block_cf99

block_cf99:                                       ; preds = %block_d0fe, %block_cf6a, %78
  store volatile i64 53145, i64* @assembly_address
  %188 = load i32* bitcast (i64* @global_var_21a3f8 to i32*)
  %189 = zext i32 %188 to i64
  store i64 %189, i64* %rdx
  store volatile i64 53151, i64* @assembly_address
  %190 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %191 = zext i32 %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 53157, i64* @assembly_address
  %192 = load i64* %rdx
  %193 = trunc i64 %192 to i32
  %194 = load i64* %rax
  %195 = trunc i64 %194 to i32
  %196 = trunc i64 %192 to i32
  store i32 %196, i32* %8
  %197 = trunc i64 %194 to i32
  store i32 %197, i32* %6
  %198 = sub i32 %193, %195
  %199 = and i32 %193, 15
  %200 = and i32 %195, 15
  %201 = sub i32 %199, %200
  %202 = icmp ugt i32 %201, 15
  %203 = icmp ult i32 %193, %195
  %204 = xor i32 %193, %195
  %205 = xor i32 %193, %198
  %206 = and i32 %204, %205
  %207 = icmp slt i32 %206, 0
  store i1 %202, i1* %az
  store i1 %203, i1* %cf
  store i1 %207, i1* %of
  %208 = icmp eq i32 %198, 0
  store i1 %208, i1* %zf
  %209 = icmp slt i32 %198, 0
  store i1 %209, i1* %sf
  %210 = trunc i32 %198 to i8
  %211 = call i8 @llvm.ctpop.i8(i8 %210)
  %212 = and i8 %211, 1
  %213 = icmp eq i8 %212, 0
  store i1 %213, i1* %pf
  store volatile i64 53159, i64* @assembly_address
  %214 = load i32* %8
  %215 = sext i32 %214 to i64
  %216 = load i32* %6
  %217 = sext i32 %216 to i64
  %218 = icmp slt i64 %215, %217
  br i1 %218, label %block_cf6a, label %block_cfa9

block_cfa9:                                       ; preds = %block_cf99
  store volatile i64 53161, i64* @assembly_address
  %219 = load i64* @global_var_21a3f0
  store i64 %219, i64* %rdx
  store volatile i64 53168, i64* @assembly_address
  %220 = load i32* bitcast (i64* @global_var_21a3f8 to i32*)
  %221 = zext i32 %220 to i64
  store i64 %221, i64* %rcx
  store volatile i64 53174, i64* @assembly_address
  %222 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %223 = zext i32 %222 to i64
  store i64 %223, i64* %rax
  store volatile i64 53180, i64* @assembly_address
  %224 = load i64* %rcx
  %225 = trunc i64 %224 to i32
  %226 = load i64* %rax
  %227 = trunc i64 %226 to i32
  %228 = sub i32 %225, %227
  %229 = and i32 %225, 15
  %230 = and i32 %227, 15
  %231 = sub i32 %229, %230
  %232 = icmp ugt i32 %231, 15
  %233 = icmp ult i32 %225, %227
  %234 = xor i32 %225, %227
  %235 = xor i32 %225, %228
  %236 = and i32 %234, %235
  %237 = icmp slt i32 %236, 0
  store i1 %232, i1* %az
  store i1 %233, i1* %cf
  store i1 %237, i1* %of
  %238 = icmp eq i32 %228, 0
  store i1 %238, i1* %zf
  %239 = icmp slt i32 %228, 0
  store i1 %239, i1* %sf
  %240 = trunc i32 %228 to i8
  %241 = call i8 @llvm.ctpop.i8(i8 %240)
  %242 = and i8 %241, 1
  %243 = icmp eq i8 %242, 0
  store i1 %243, i1* %pf
  %244 = zext i32 %228 to i64
  store i64 %244, i64* %rcx
  store volatile i64 53182, i64* @assembly_address
  %245 = load i64* %rcx
  %246 = trunc i64 %245 to i32
  %247 = zext i32 %246 to i64
  store i64 %247, i64* %rax
  store volatile i64 53184, i64* @assembly_address
  %248 = load i64* %rax
  %249 = trunc i64 %248 to i32
  %250 = zext i32 %249 to i64
  store i64 %250, i64* %rcx
  store volatile i64 53186, i64* @assembly_address
  %251 = load i64* %rdx
  %252 = load i64* %rcx
  %253 = trunc i64 %252 to i8
  %254 = zext i8 %253 to i64
  %255 = and i64 %254, 63
  %256 = load i1* %of
  %257 = icmp eq i64 %255, 0
  br i1 %257, label %273, label %258

; <label>:258                                     ; preds = %block_cfa9
  %259 = lshr i64 %251, %255
  %260 = icmp eq i64 %259, 0
  store i1 %260, i1* %zf
  %261 = icmp slt i64 %259, 0
  store i1 %261, i1* %sf
  %262 = trunc i64 %259 to i8
  %263 = call i8 @llvm.ctpop.i8(i8 %262)
  %264 = and i8 %263, 1
  %265 = icmp eq i8 %264, 0
  store i1 %265, i1* %pf
  store i64 %259, i64* %rdx
  %266 = sub i64 %255, 1
  %267 = shl i64 1, %266
  %268 = and i64 %267, %251
  %269 = icmp ne i64 %268, 0
  store i1 %269, i1* %cf
  %270 = icmp eq i64 %255, 1
  %271 = icmp slt i64 %251, 0
  %272 = select i1 %270, i1 %271, i1 %256
  store i1 %272, i1* %of
  br label %273

; <label>:273                                     ; preds = %block_cfa9, %258
  store volatile i64 53189, i64* @assembly_address
  %274 = load i64* %rdx
  store i64 %274, i64* %rax
  store volatile i64 53192, i64* @assembly_address
  %275 = load i64* %rax
  %276 = trunc i64 %275 to i32
  %277 = load i32* %stack_var_-40
  %278 = and i32 %276, %277
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %279 = icmp eq i32 %278, 0
  store i1 %279, i1* %zf
  %280 = icmp slt i32 %278, 0
  store i1 %280, i1* %sf
  %281 = trunc i32 %278 to i8
  %282 = call i8 @llvm.ctpop.i8(i8 %281)
  %283 = and i8 %282, 1
  %284 = icmp eq i8 %283, 0
  store i1 %284, i1* %pf
  %285 = zext i32 %278 to i64
  store i64 %285, i64* %rax
  store volatile i64 53195, i64* @assembly_address
  %286 = load i64* %rax
  %287 = trunc i64 %286 to i32
  %288 = zext i32 %287 to i64
  store i64 %288, i64* %rbx
  store volatile i64 53197, i64* @assembly_address
  %289 = load i64* %rbx
  %290 = trunc i64 %289 to i32
  %291 = zext i32 %290 to i64
  store i64 %291, i64* %rdx
  store volatile i64 53199, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 53206, i64* @assembly_address
  %292 = load i64* %rdx
  %293 = load i64* %rax
  %294 = mul i64 %293, 1
  %295 = add i64 %292, %294
  %296 = inttoptr i64 %295 to i8*
  %297 = load i8* %296
  %298 = zext i8 %297 to i64
  store i64 %298, i64* %rax
  store volatile i64 53210, i64* @assembly_address
  %299 = load i64* %rax
  %300 = trunc i64 %299 to i8
  %301 = zext i8 %300 to i64
  store i64 %301, i64* %rax
  store volatile i64 53213, i64* @assembly_address
  %302 = load i64* %rax
  %303 = trunc i64 %302 to i32
  store i32 %303, i32* %stack_var_-44
  store volatile i64 53216, i64* @assembly_address
  %304 = load i32* %stack_var_-44
  %305 = and i32 %304, 15
  %306 = icmp ugt i32 %305, 15
  %307 = icmp ult i32 %304, 0
  %308 = xor i32 %304, 0
  %309 = and i32 %308, 0
  %310 = icmp slt i32 %309, 0
  store i1 %306, i1* %az
  store i1 %307, i1* %cf
  store i1 %310, i1* %of
  store i32 %304, i32* %5
  store i32 0, i32* %4
  %311 = icmp eq i32 %304, 0
  store i1 %311, i1* %zf
  %312 = icmp slt i32 %304, 0
  store i1 %312, i1* %sf
  %313 = trunc i32 %304 to i8
  %314 = call i8 @llvm.ctpop.i8(i8 %313)
  %315 = and i8 %314, 1
  %316 = icmp eq i8 %315, 0
  store i1 %316, i1* %pf
  store volatile i64 53220, i64* @assembly_address
  %317 = load i32* %5
  %318 = load i32* %4
  %319 = icmp sle i32 %317, %318
  br i1 %319, label %block_cff8, label %block_cfe6

block_cfe6:                                       ; preds = %273
  store volatile i64 53222, i64* @assembly_address
  %320 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %321 = zext i32 %320 to i64
  store i64 %321, i64* %rax
  store volatile i64 53228, i64* @assembly_address
  %322 = load i64* %rax
  %323 = trunc i64 %322 to i32
  %324 = load i32* %stack_var_-44
  %325 = sub i32 %323, %324
  %326 = and i32 %323, 15
  %327 = and i32 %324, 15
  %328 = sub i32 %326, %327
  %329 = icmp ugt i32 %328, 15
  %330 = icmp ult i32 %323, %324
  %331 = xor i32 %323, %324
  %332 = xor i32 %323, %325
  %333 = and i32 %331, %332
  %334 = icmp slt i32 %333, 0
  store i1 %329, i1* %az
  store i1 %330, i1* %cf
  store i1 %334, i1* %of
  %335 = icmp eq i32 %325, 0
  store i1 %335, i1* %zf
  %336 = icmp slt i32 %325, 0
  store i1 %336, i1* %sf
  %337 = trunc i32 %325 to i8
  %338 = call i8 @llvm.ctpop.i8(i8 %337)
  %339 = and i8 %338, 1
  %340 = icmp eq i8 %339, 0
  store i1 %340, i1* %pf
  %341 = zext i32 %325 to i64
  store i64 %341, i64* %rax
  store volatile i64 53231, i64* @assembly_address
  %342 = load i64* %rax
  %343 = trunc i64 %342 to i32
  %344 = zext i32 %343 to i64
  store i64 %344, i64* %rcx
  store volatile i64 53233, i64* @assembly_address
  %345 = load i64* %rbx
  %346 = trunc i64 %345 to i32
  %347 = load i64* %rcx
  %348 = trunc i64 %347 to i8
  %349 = zext i8 %348 to i32
  %350 = and i32 %349, 31
  %351 = load i1* %of
  %352 = icmp eq i32 %350, 0
  br i1 %352, label %369, label %353

; <label>:353                                     ; preds = %block_cfe6
  %354 = lshr i32 %346, %350
  %355 = icmp eq i32 %354, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i32 %354, 0
  store i1 %356, i1* %sf
  %357 = trunc i32 %354 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  %361 = zext i32 %354 to i64
  store i64 %361, i64* %rbx
  %362 = sub i32 %350, 1
  %363 = shl i32 1, %362
  %364 = and i32 %363, %346
  %365 = icmp ne i32 %364, 0
  store i1 %365, i1* %cf
  %366 = icmp eq i32 %350, 1
  %367 = icmp slt i32 %346, 0
  %368 = select i1 %366, i1 %367, i1 %351
  store i1 %368, i1* %of
  br label %369

; <label>:369                                     ; preds = %block_cfe6, %353
  store volatile i64 53235, i64* @assembly_address
  br label %block_d09a

block_cff8:                                       ; preds = %273
  store volatile i64 53240, i64* @assembly_address
  %370 = load i32* %stack_var_-40
  %371 = zext i32 %370 to i64
  store i64 %371, i64* %rax
  store volatile i64 53243, i64* @assembly_address
  %372 = load i64* %rax
  store i64 %372, i64* %stack_var_-32
  store volatile i64 53247, i64* @assembly_address
  %373 = load i32* bitcast (i64* @global_var_21a3e8 to i32*)
  %374 = zext i32 %373 to i64
  store i64 %374, i64* %rax
  store volatile i64 53253, i64* @assembly_address
  %375 = load i64* %rax
  %376 = trunc i64 %375 to i32
  store i32 %376, i32* %stack_var_-44
  store volatile i64 53256, i64* @assembly_address
  br label %block_d07b

block_d00a:                                       ; preds = %block_d07b
  store volatile i64 53258, i64* @assembly_address
  %377 = load i32* %stack_var_-44
  %378 = add i32 %377, 1
  %379 = and i32 %377, 15
  %380 = add i32 %379, 1
  %381 = icmp ugt i32 %380, 15
  %382 = icmp ult i32 %378, %377
  %383 = xor i32 %377, %378
  %384 = xor i32 1, %378
  %385 = and i32 %383, %384
  %386 = icmp slt i32 %385, 0
  store i1 %381, i1* %az
  store i1 %382, i1* %cf
  store i1 %386, i1* %of
  %387 = icmp eq i32 %378, 0
  store i1 %387, i1* %zf
  %388 = icmp slt i32 %378, 0
  store i1 %388, i1* %sf
  %389 = trunc i32 %378 to i8
  %390 = call i8 @llvm.ctpop.i8(i8 %389)
  %391 = and i8 %390, 1
  %392 = icmp eq i8 %391, 0
  store i1 %392, i1* %pf
  store i32 %378, i32* %stack_var_-44
  store volatile i64 53262, i64* @assembly_address
  %393 = load i64* %stack_var_-32
  store i64 %393, i64* %rax
  store volatile i64 53266, i64* @assembly_address
  %394 = load i64* %rax
  %395 = load i64* %rax
  %396 = add i64 %394, %395
  %397 = and i64 %394, 15
  %398 = and i64 %395, 15
  %399 = add i64 %397, %398
  %400 = icmp ugt i64 %399, 15
  %401 = icmp ult i64 %396, %394
  %402 = xor i64 %394, %396
  %403 = xor i64 %395, %396
  %404 = and i64 %402, %403
  %405 = icmp slt i64 %404, 0
  store i1 %400, i1* %az
  store i1 %401, i1* %cf
  store i1 %405, i1* %of
  %406 = icmp eq i64 %396, 0
  store i1 %406, i1* %zf
  %407 = icmp slt i64 %396, 0
  store i1 %407, i1* %sf
  %408 = trunc i64 %396 to i8
  %409 = call i8 @llvm.ctpop.i8(i8 %408)
  %410 = and i8 %409, 1
  %411 = icmp eq i8 %410, 0
  store i1 %411, i1* %pf
  store i64 %396, i64* %rax
  store volatile i64 53269, i64* @assembly_address
  %412 = load i64* %rax
  %413 = add i64 %412, 1
  %414 = and i64 %412, 15
  %415 = add i64 %414, 1
  %416 = icmp ugt i64 %415, 15
  %417 = icmp ult i64 %413, %412
  %418 = xor i64 %412, %413
  %419 = xor i64 1, %413
  %420 = and i64 %418, %419
  %421 = icmp slt i64 %420, 0
  store i1 %416, i1* %az
  store i1 %417, i1* %cf
  store i1 %421, i1* %of
  %422 = icmp eq i64 %413, 0
  store i1 %422, i1* %zf
  %423 = icmp slt i64 %413, 0
  store i1 %423, i1* %sf
  %424 = trunc i64 %413 to i8
  %425 = call i8 @llvm.ctpop.i8(i8 %424)
  %426 = and i8 %425, 1
  %427 = icmp eq i8 %426, 0
  store i1 %427, i1* %pf
  store i64 %413, i64* %rax
  store volatile i64 53273, i64* @assembly_address
  %428 = load i64* %rax
  store i64 %428, i64* %stack_var_-32
  store volatile i64 53277, i64* @assembly_address
  br label %block_d04e

block_d01f:                                       ; preds = %block_d04e
  store volatile i64 53279, i64* @assembly_address
  %429 = load i64* @global_var_21a3f0
  store i64 %429, i64* %rax
  store volatile i64 53286, i64* @assembly_address
  %430 = load i64* %rax
  %431 = load i1* %of
  %432 = shl i64 %430, 8
  %433 = icmp eq i64 %432, 0
  store i1 %433, i1* %zf
  %434 = icmp slt i64 %432, 0
  store i1 %434, i1* %sf
  %435 = trunc i64 %432 to i8
  %436 = call i8 @llvm.ctpop.i8(i8 %435)
  %437 = and i8 %436, 1
  %438 = icmp eq i8 %437, 0
  store i1 %438, i1* %pf
  store i64 %432, i64* %rax
  %439 = shl i64 %430, 7
  %440 = lshr i64 %439, 63
  %441 = trunc i64 %440 to i1
  store i1 %441, i1* %cf
  %442 = lshr i64 %432, 63
  %443 = icmp ne i64 %442, %440
  %444 = select i1 false, i1 %443, i1 %431
  store i1 %444, i1* %of
  store volatile i64 53290, i64* @assembly_address
  %445 = load i64* %rax
  store i64 %445, i64* %rbx
  store volatile i64 53293, i64* @assembly_address
  %446 = call i64 @read_byte()
  store i64 %446, i64* %rax
  store i64 %446, i64* %rax
  store i64 %446, i64* %rax
  store volatile i64 53298, i64* @assembly_address
  %447 = load i64* %rax
  %448 = trunc i64 %447 to i8
  %449 = zext i8 %448 to i64
  store i64 %449, i64* %rax
  store volatile i64 53301, i64* @assembly_address
  %450 = load i64* %rax
  %451 = load i64* %rbx
  %452 = or i64 %450, %451
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %453 = icmp eq i64 %452, 0
  store i1 %453, i1* %zf
  %454 = icmp slt i64 %452, 0
  store i1 %454, i1* %sf
  %455 = trunc i64 %452 to i8
  %456 = call i8 @llvm.ctpop.i8(i8 %455)
  %457 = and i8 %456, 1
  %458 = icmp eq i8 %457, 0
  store i1 %458, i1* %pf
  store i64 %452, i64* %rax
  store volatile i64 53304, i64* @assembly_address
  %459 = load i64* %rax
  store i64 %459, i64* @global_var_21a3f0
  store volatile i64 53311, i64* @assembly_address
  %460 = load i32* bitcast (i64* @global_var_21a3f8 to i32*)
  %461 = zext i32 %460 to i64
  store i64 %461, i64* %rax
  store volatile i64 53317, i64* @assembly_address
  %462 = load i64* %rax
  %463 = trunc i64 %462 to i32
  %464 = add i32 %463, 8
  %465 = and i32 %463, 15
  %466 = add i32 %465, 8
  %467 = icmp ugt i32 %466, 15
  %468 = icmp ult i32 %464, %463
  %469 = xor i32 %463, %464
  %470 = xor i32 8, %464
  %471 = and i32 %469, %470
  %472 = icmp slt i32 %471, 0
  store i1 %467, i1* %az
  store i1 %468, i1* %cf
  store i1 %472, i1* %of
  %473 = icmp eq i32 %464, 0
  store i1 %473, i1* %zf
  %474 = icmp slt i32 %464, 0
  store i1 %474, i1* %sf
  %475 = trunc i32 %464 to i8
  %476 = call i8 @llvm.ctpop.i8(i8 %475)
  %477 = and i8 %476, 1
  %478 = icmp eq i8 %477, 0
  store i1 %478, i1* %pf
  store i64 ptrtoint (i64* @global_var_21a400 to i64), i64* %rax
  store volatile i64 53320, i64* @assembly_address
  %479 = load i64* %rax
  %480 = trunc i64 %479 to i32
  store i32 %480, i32* bitcast (i64* @global_var_21a3f8 to i32*)
  br label %block_d04e

block_d04e:                                       ; preds = %block_d01f, %block_d00a
  store volatile i64 53326, i64* @assembly_address
  %481 = load i32* bitcast (i64* @global_var_21a3f8 to i32*)
  %482 = zext i32 %481 to i64
  store i64 %482, i64* %rax
  store volatile i64 53332, i64* @assembly_address
  %483 = load i32* %stack_var_-44
  %484 = load i64* %rax
  %485 = trunc i64 %484 to i32
  store i32 %483, i32* %3
  %486 = trunc i64 %484 to i32
  store i32 %486, i32* %1
  %487 = sub i32 %483, %485
  %488 = and i32 %483, 15
  %489 = and i32 %485, 15
  %490 = sub i32 %488, %489
  %491 = icmp ugt i32 %490, 15
  %492 = icmp ult i32 %483, %485
  %493 = xor i32 %483, %485
  %494 = xor i32 %483, %487
  %495 = and i32 %493, %494
  %496 = icmp slt i32 %495, 0
  store i1 %491, i1* %az
  store i1 %492, i1* %cf
  store i1 %496, i1* %of
  %497 = icmp eq i32 %487, 0
  store i1 %497, i1* %zf
  %498 = icmp slt i32 %487, 0
  store i1 %498, i1* %sf
  %499 = trunc i32 %487 to i8
  %500 = call i8 @llvm.ctpop.i8(i8 %499)
  %501 = and i8 %500, 1
  %502 = icmp eq i8 %501, 0
  store i1 %502, i1* %pf
  store volatile i64 53335, i64* @assembly_address
  %503 = load i32* %3
  %504 = load i32* %1
  %505 = sext i32 %504 to i64
  %506 = sext i32 %503 to i64
  %507 = icmp sgt i64 %506, %505
  br i1 %507, label %block_d01f, label %block_d059

block_d059:                                       ; preds = %block_d04e
  store volatile i64 53337, i64* @assembly_address
  %508 = load i64* @global_var_21a3f0
  store i64 %508, i64* %rdx
  store volatile i64 53344, i64* @assembly_address
  %509 = load i32* bitcast (i64* @global_var_21a3f8 to i32*)
  %510 = zext i32 %509 to i64
  store i64 %510, i64* %rax
  store volatile i64 53350, i64* @assembly_address
  %511 = load i64* %rax
  %512 = trunc i64 %511 to i32
  %513 = load i32* %stack_var_-44
  %514 = sub i32 %512, %513
  %515 = and i32 %512, 15
  %516 = and i32 %513, 15
  %517 = sub i32 %515, %516
  %518 = icmp ugt i32 %517, 15
  %519 = icmp ult i32 %512, %513
  %520 = xor i32 %512, %513
  %521 = xor i32 %512, %514
  %522 = and i32 %520, %521
  %523 = icmp slt i32 %522, 0
  store i1 %518, i1* %az
  store i1 %519, i1* %cf
  store i1 %523, i1* %of
  %524 = icmp eq i32 %514, 0
  store i1 %524, i1* %zf
  %525 = icmp slt i32 %514, 0
  store i1 %525, i1* %sf
  %526 = trunc i32 %514 to i8
  %527 = call i8 @llvm.ctpop.i8(i8 %526)
  %528 = and i8 %527, 1
  %529 = icmp eq i8 %528, 0
  store i1 %529, i1* %pf
  %530 = zext i32 %514 to i64
  store i64 %530, i64* %rax
  store volatile i64 53353, i64* @assembly_address
  %531 = load i64* %rax
  %532 = trunc i64 %531 to i32
  %533 = zext i32 %532 to i64
  store i64 %533, i64* %rcx
  store volatile i64 53355, i64* @assembly_address
  %534 = load i64* %rdx
  %535 = load i64* %rcx
  %536 = trunc i64 %535 to i8
  %537 = zext i8 %536 to i64
  %538 = and i64 %537, 63
  %539 = load i1* %of
  %540 = icmp eq i64 %538, 0
  br i1 %540, label %556, label %541

; <label>:541                                     ; preds = %block_d059
  %542 = lshr i64 %534, %538
  %543 = icmp eq i64 %542, 0
  store i1 %543, i1* %zf
  %544 = icmp slt i64 %542, 0
  store i1 %544, i1* %sf
  %545 = trunc i64 %542 to i8
  %546 = call i8 @llvm.ctpop.i8(i8 %545)
  %547 = and i8 %546, 1
  %548 = icmp eq i8 %547, 0
  store i1 %548, i1* %pf
  store i64 %542, i64* %rdx
  %549 = sub i64 %538, 1
  %550 = shl i64 1, %549
  %551 = and i64 %550, %534
  %552 = icmp ne i64 %551, 0
  store i1 %552, i1* %cf
  %553 = icmp eq i64 %538, 1
  %554 = icmp slt i64 %534, 0
  %555 = select i1 %553, i1 %554, i1 %539
  store i1 %555, i1* %of
  br label %556

; <label>:556                                     ; preds = %block_d059, %541
  store volatile i64 53358, i64* @assembly_address
  %557 = load i64* %rdx
  store i64 %557, i64* %rax
  store volatile i64 53361, i64* @assembly_address
  %558 = load i64* %rax
  %559 = trunc i64 %558 to i32
  %560 = zext i32 %559 to i64
  store i64 %560, i64* %rdx
  store volatile i64 53363, i64* @assembly_address
  %561 = load i64* %stack_var_-32
  store i64 %561, i64* %rax
  store volatile i64 53367, i64* @assembly_address
  %562 = load i64* %rdx
  %563 = trunc i64 %562 to i32
  %564 = zext i32 %563 to i64
  store i64 %564, i64* %rbx
  store volatile i64 53369, i64* @assembly_address
  %565 = load i64* %rbx
  %566 = trunc i64 %565 to i32
  %567 = load i64* %rax
  %568 = trunc i64 %567 to i32
  %569 = and i32 %566, %568
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %570 = icmp eq i32 %569, 0
  store i1 %570, i1* %zf
  %571 = icmp slt i32 %569, 0
  store i1 %571, i1* %sf
  %572 = trunc i32 %569 to i8
  %573 = call i8 @llvm.ctpop.i8(i8 %572)
  %574 = and i8 %573, 1
  %575 = icmp eq i8 %574, 0
  store i1 %575, i1* %pf
  %576 = zext i32 %569 to i64
  store i64 %576, i64* %rbx
  br label %block_d07b

block_d07b:                                       ; preds = %556, %block_cff8
  store volatile i64 53371, i64* @assembly_address
  %577 = load i32* %stack_var_-44
  %578 = zext i32 %577 to i64
  store i64 %578, i64* %rax
  store volatile i64 53374, i64* @assembly_address
  %579 = load i64* %rax
  %580 = trunc i64 %579 to i32
  %581 = sext i32 %580 to i64
  store i64 %581, i64* %rax
  store volatile i64 53376, i64* @assembly_address
  %582 = load i64* %rax
  %583 = mul i64 %582, 4
  store i64 %583, i64* %rdx
  store volatile i64 53384, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a380 to i64), i64* %rax
  store volatile i64 53391, i64* @assembly_address
  %584 = load i64* %rdx
  %585 = load i64* %rax
  %586 = mul i64 %585, 1
  %587 = add i64 %584, %586
  %588 = inttoptr i64 %587 to i32*
  %589 = load i32* %588
  %590 = zext i32 %589 to i64
  store i64 %590, i64* %rax
  store volatile i64 53394, i64* @assembly_address
  %591 = load i64* %rbx
  %592 = trunc i64 %591 to i32
  %593 = load i64* %rax
  %594 = trunc i64 %593 to i32
  %595 = sub i32 %592, %594
  %596 = and i32 %592, 15
  %597 = and i32 %594, 15
  %598 = sub i32 %596, %597
  %599 = icmp ugt i32 %598, 15
  %600 = icmp ult i32 %592, %594
  %601 = xor i32 %592, %594
  %602 = xor i32 %592, %595
  %603 = and i32 %601, %602
  %604 = icmp slt i32 %603, 0
  store i1 %599, i1* %az
  store i1 %600, i1* %cf
  store i1 %604, i1* %of
  %605 = icmp eq i32 %595, 0
  store i1 %605, i1* %zf
  %606 = icmp slt i32 %595, 0
  store i1 %606, i1* %sf
  %607 = trunc i32 %595 to i8
  %608 = call i8 @llvm.ctpop.i8(i8 %607)
  %609 = and i8 %608, 1
  %610 = icmp eq i8 %609, 0
  store i1 %610, i1* %pf
  store volatile i64 53396, i64* @assembly_address
  %611 = load i1* %cf
  br i1 %611, label %block_d00a, label %block_d09a

block_d09a:                                       ; preds = %block_d07b, %369
  store volatile i64 53402, i64* @assembly_address
  %612 = load i64* %rbx
  %613 = trunc i64 %612 to i32
  %614 = load i32* %stack_var_-36
  %615 = sub i32 %613, %614
  %616 = and i32 %613, 15
  %617 = and i32 %614, 15
  %618 = sub i32 %616, %617
  %619 = icmp ugt i32 %618, 15
  %620 = icmp ult i32 %613, %614
  %621 = xor i32 %613, %614
  %622 = xor i32 %613, %615
  %623 = and i32 %621, %622
  %624 = icmp slt i32 %623, 0
  store i1 %619, i1* %az
  store i1 %620, i1* %cf
  store i1 %624, i1* %of
  %625 = icmp eq i32 %615, 0
  store i1 %625, i1* %zf
  %626 = icmp slt i32 %615, 0
  store i1 %626, i1* %sf
  %627 = trunc i32 %615 to i8
  %628 = call i8 @llvm.ctpop.i8(i8 %627)
  %629 = and i8 %628, 1
  %630 = icmp eq i8 %629, 0
  store i1 %630, i1* %pf
  store volatile i64 53405, i64* @assembly_address
  %631 = load i1* %zf
  %632 = icmp eq i1 %631, false
  br i1 %632, label %block_d0aa, label %block_d09f

block_d09f:                                       ; preds = %block_d09a
  store volatile i64 53407, i64* @assembly_address
  %633 = load i32* bitcast (i64* @global_var_21a168 to i32*)
  %634 = zext i32 %633 to i64
  store i64 %634, i64* %rax
  store volatile i64 53413, i64* @assembly_address
  %635 = load i32* %stack_var_-44
  %636 = load i64* %rax
  %637 = trunc i64 %636 to i32
  %638 = sub i32 %635, %637
  %639 = and i32 %635, 15
  %640 = and i32 %637, 15
  %641 = sub i32 %639, %640
  %642 = icmp ugt i32 %641, 15
  %643 = icmp ult i32 %635, %637
  %644 = xor i32 %635, %637
  %645 = xor i32 %635, %638
  %646 = and i32 %644, %645
  %647 = icmp slt i32 %646, 0
  store i1 %642, i1* %az
  store i1 %643, i1* %cf
  store i1 %647, i1* %of
  %648 = icmp eq i32 %638, 0
  store i1 %648, i1* %zf
  %649 = icmp slt i32 %638, 0
  store i1 %649, i1* %sf
  %650 = trunc i32 %638 to i8
  %651 = call i8 @llvm.ctpop.i8(i8 %650)
  %652 = and i8 %651, 1
  %653 = icmp eq i8 %652, 0
  store i1 %653, i1* %pf
  store volatile i64 53416, i64* @assembly_address
  %654 = load i1* %zf
  br i1 %654, label %block_d112, label %block_d0aa

block_d0aa:                                       ; preds = %block_d09f, %block_d09a
  store volatile i64 53418, i64* @assembly_address
  %655 = load i32* %stack_var_-44
  %656 = zext i32 %655 to i64
  store i64 %656, i64* %rax
  store volatile i64 53421, i64* @assembly_address
  %657 = load i64* %rax
  %658 = trunc i64 %657 to i32
  %659 = sext i32 %658 to i64
  store i64 %659, i64* %rax
  store volatile i64 53423, i64* @assembly_address
  %660 = load i64* %rax
  %661 = mul i64 %660, 4
  store i64 %661, i64* %rdx
  store volatile i64 53431, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a280 to i64), i64* %rax
  store volatile i64 53438, i64* @assembly_address
  %662 = load i64* %rdx
  %663 = load i64* %rax
  %664 = mul i64 %663, 1
  %665 = add i64 %662, %664
  %666 = inttoptr i64 %665 to i32*
  %667 = load i32* %666
  %668 = zext i32 %667 to i64
  store i64 %668, i64* %rax
  store volatile i64 53441, i64* @assembly_address
  %669 = load i64* %rbx
  %670 = load i64* %rax
  %671 = mul i64 %670, 1
  %672 = add i64 %669, %671
  %673 = trunc i64 %672 to i32
  %674 = zext i32 %673 to i64
  store i64 %674, i64* %rcx
  store volatile i64 53444, i64* @assembly_address
  %675 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %676 = zext i32 %675 to i64
  store i64 %676, i64* %rax
  store volatile i64 53450, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 53453, i64* @assembly_address
  %677 = load i64* %rdx
  %678 = trunc i64 %677 to i32
  store i32 %678, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 53459, i64* @assembly_address
  %679 = load i64* %rcx
  %680 = trunc i64 %679 to i32
  %681 = zext i32 %680 to i64
  store i64 %681, i64* %rcx
  store volatile i64 53461, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a180 to i64), i64* %rdx
  store volatile i64 53468, i64* @assembly_address
  %682 = load i64* %rcx
  %683 = load i64* %rdx
  %684 = mul i64 %683, 1
  %685 = add i64 %682, %684
  %686 = inttoptr i64 %685 to i8*
  %687 = load i8* %686
  %688 = zext i8 %687 to i64
  store i64 %688, i64* %rdx
  store volatile i64 53472, i64* @assembly_address
  %689 = load i64* %rax
  %690 = trunc i64 %689 to i32
  %691 = zext i32 %690 to i64
  store i64 %691, i64* %rcx
  store volatile i64 53474, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 53481, i64* @assembly_address
  %692 = load i64* %rdx
  %693 = trunc i64 %692 to i8
  %694 = load i64* %rcx
  %695 = load i64* %rax
  %696 = mul i64 %695, 1
  %697 = add i64 %694, %696
  %698 = inttoptr i64 %697 to i8*
  store i8 %693, i8* %698
  store volatile i64 53484, i64* @assembly_address
  %699 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %700 = zext i32 %699 to i64
  store i64 %700, i64* %rax
  store volatile i64 53490, i64* @assembly_address
  %701 = load i64* %rax
  %702 = trunc i64 %701 to i32
  %703 = sub i32 %702, 32768
  %704 = and i32 %702, 15
  %705 = icmp ugt i32 %704, 15
  %706 = icmp ult i32 %702, 32768
  %707 = xor i32 %702, 32768
  %708 = xor i32 %702, %703
  %709 = and i32 %707, %708
  %710 = icmp slt i32 %709, 0
  store i1 %705, i1* %az
  store i1 %706, i1* %cf
  store i1 %710, i1* %of
  %711 = icmp eq i32 %703, 0
  store i1 %711, i1* %zf
  %712 = icmp slt i32 %703, 0
  store i1 %712, i1* %sf
  %713 = trunc i32 %703 to i8
  %714 = call i8 @llvm.ctpop.i8(i8 %713)
  %715 = and i8 %714, 1
  %716 = icmp eq i8 %715, 0
  store i1 %716, i1* %pf
  store volatile i64 53495, i64* @assembly_address
  %717 = load i1* %zf
  %718 = icmp eq i1 %717, false
  br i1 %718, label %block_d0fe, label %block_d0f9

block_d0f9:                                       ; preds = %block_d0aa
  store volatile i64 53497, i64* @assembly_address
  %719 = call i64 @flush_window()
  store i64 %719, i64* %rax
  store i64 %719, i64* %rax
  store i64 %719, i64* %rax
  br label %block_d0fe

block_d0fe:                                       ; preds = %block_d0f9, %block_d0aa
  store volatile i64 53502, i64* @assembly_address
  %720 = load i32* bitcast (i64* @global_var_21a3f8 to i32*)
  %721 = zext i32 %720 to i64
  store i64 %721, i64* %rax
  store volatile i64 53508, i64* @assembly_address
  %722 = load i64* %rax
  %723 = trunc i64 %722 to i32
  %724 = load i32* %stack_var_-44
  %725 = sub i32 %723, %724
  %726 = and i32 %723, 15
  %727 = and i32 %724, 15
  %728 = sub i32 %726, %727
  %729 = icmp ugt i32 %728, 15
  %730 = icmp ult i32 %723, %724
  %731 = xor i32 %723, %724
  %732 = xor i32 %723, %725
  %733 = and i32 %731, %732
  %734 = icmp slt i32 %733, 0
  store i1 %729, i1* %az
  store i1 %730, i1* %cf
  store i1 %734, i1* %of
  %735 = icmp eq i32 %725, 0
  store i1 %735, i1* %zf
  %736 = icmp slt i32 %725, 0
  store i1 %736, i1* %sf
  %737 = trunc i32 %725 to i8
  %738 = call i8 @llvm.ctpop.i8(i8 %737)
  %739 = and i8 %738, 1
  %740 = icmp eq i8 %739, 0
  store i1 %740, i1* %pf
  %741 = zext i32 %725 to i64
  store i64 %741, i64* %rax
  store volatile i64 53511, i64* @assembly_address
  %742 = load i64* %rax
  %743 = trunc i64 %742 to i32
  store i32 %743, i32* bitcast (i64* @global_var_21a3f8 to i32*)
  store volatile i64 53517, i64* @assembly_address
  br label %block_cf99

block_d112:                                       ; preds = %block_d09f
  store volatile i64 53522, i64* @assembly_address
  store volatile i64 53523, i64* @assembly_address
  %744 = call i64 @flush_window()
  store i64 %744, i64* %rax
  store i64 %744, i64* %rax
  store i64 %744, i64* %rax
  store volatile i64 53528, i64* @assembly_address
  %745 = load i64* @global_var_25f4c0
  store i64 %745, i64* %rax
  store volatile i64 53535, i64* @assembly_address
  %746 = load i64* %rax
  %747 = trunc i64 %746 to i32
  %748 = zext i32 %747 to i64
  store i64 %748, i64* %rdx
  store volatile i64 53537, i64* @assembly_address
  %749 = load i64* @global_var_21a160
  store i64 %749, i64* %rax
  store volatile i64 53544, i64* @assembly_address
  %750 = load i64* %rdx
  %751 = load i64* %rax
  %752 = sub i64 %750, %751
  %753 = and i64 %750, 15
  %754 = and i64 %751, 15
  %755 = sub i64 %753, %754
  %756 = icmp ugt i64 %755, 15
  %757 = icmp ult i64 %750, %751
  %758 = xor i64 %750, %751
  %759 = xor i64 %750, %752
  %760 = and i64 %758, %759
  %761 = icmp slt i64 %760, 0
  store i1 %756, i1* %az
  store i1 %757, i1* %cf
  store i1 %761, i1* %of
  %762 = icmp eq i64 %752, 0
  store i1 %762, i1* %zf
  %763 = icmp slt i64 %752, 0
  store i1 %763, i1* %sf
  %764 = trunc i64 %752 to i8
  %765 = call i8 @llvm.ctpop.i8(i8 %764)
  %766 = and i8 %765, 1
  %767 = icmp eq i8 %766, 0
  store i1 %767, i1* %pf
  store volatile i64 53547, i64* @assembly_address
  %768 = load i1* %zf
  br i1 %768, label %block_d139, label %block_d12d

block_d12d:                                       ; preds = %block_d112
  store volatile i64 53549, i64* @assembly_address
  store i64 ptrtoint ([38 x i8]* @global_var_12228 to i64), i64* %rdi
  store volatile i64 53556, i64* @assembly_address
  %769 = load i64* %rdi
  %770 = inttoptr i64 %769 to i8*
  %771 = call i64 @gzip_error(i8* %770)
  store i64 %771, i64* %rax
  store i64 %771, i64* %rax
  unreachable

block_d139:                                       ; preds = %block_d112
  store volatile i64 53561, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 53566, i64* @assembly_address
  %772 = load i64* %rsp
  %773 = add i64 %772, 56
  %774 = and i64 %772, 15
  %775 = add i64 %774, 8
  %776 = icmp ugt i64 %775, 15
  %777 = icmp ult i64 %773, %772
  %778 = xor i64 %772, %773
  %779 = xor i64 56, %773
  %780 = and i64 %778, %779
  %781 = icmp slt i64 %780, 0
  store i1 %776, i1* %az
  store i1 %777, i1* %cf
  store i1 %781, i1* %of
  %782 = icmp eq i64 %773, 0
  store i1 %782, i1* %zf
  %783 = icmp slt i64 %773, 0
  store i1 %783, i1* %sf
  %784 = trunc i64 %773 to i8
  %785 = call i8 @llvm.ctpop.i8(i8 %784)
  %786 = and i8 %785, 1
  %787 = icmp eq i8 %786, 0
  store i1 %787, i1* %pf
  %788 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %788, i64* %rsp
  store volatile i64 53570, i64* @assembly_address
  %789 = load i64* %stack_var_-16
  store i64 %789, i64* %rbx
  %790 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %790, i64* %rsp
  store volatile i64 53571, i64* @assembly_address
  %791 = load i64* %stack_var_-8
  store i64 %791, i64* %rbp
  %792 = ptrtoint i64* %stack_var_0 to i64
  store i64 %792, i64* %rsp
  store volatile i64 53572, i64* @assembly_address
  %793 = load i64* %rax
  ret i64 %793
}

declare i64 @235(i64, i32)

declare i64 @236(i64, i64)

define i64 @check_zipfile(i32 %arg1) {
block_d145:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %1 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 53573, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 53574, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 53577, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 53581, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = trunc i64 %21 to i32
  store i32 %22, i32* %stack_var_-28
  store volatile i64 53584, i64* @assembly_address
  %23 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %24 = zext i32 %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 53590, i64* @assembly_address
  %25 = load i64* %rax
  %26 = trunc i64 %25 to i32
  %27 = zext i32 %26 to i64
  store i64 %27, i64* %rdx
  store volatile i64 53592, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 53599, i64* @assembly_address
  %28 = load i64* %rax
  %29 = load i64* %rdx
  %30 = add i64 %28, %29
  %31 = and i64 %28, 15
  %32 = and i64 %29, 15
  %33 = add i64 %31, %32
  %34 = icmp ugt i64 %33, 15
  %35 = icmp ult i64 %30, %28
  %36 = xor i64 %28, %30
  %37 = xor i64 %29, %30
  %38 = and i64 %36, %37
  %39 = icmp slt i64 %38, 0
  store i1 %34, i1* %az
  store i1 %35, i1* %cf
  store i1 %39, i1* %of
  %40 = icmp eq i64 %30, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i64 %30, 0
  store i1 %41, i1* %sf
  %42 = trunc i64 %30 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  store i64 %30, i64* %rax
  store volatile i64 53602, i64* @assembly_address
  %46 = load i64* %rax
  %47 = inttoptr i64 %46 to i8*
  store i8* %47, i8** %stack_var_-16
  store volatile i64 53606, i64* @assembly_address
  %48 = load i32* %stack_var_-28
  %49 = zext i32 %48 to i64
  store i64 %49, i64* %rax
  store volatile i64 53609, i64* @assembly_address
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  store i32 %51, i32* bitcast (i64* @global_var_24f0a0 to i32*)
  store volatile i64 53615, i64* @assembly_address
  %52 = load i8** %stack_var_-16
  %53 = ptrtoint i8* %52 to i64
  store i64 %53, i64* %rax
  store volatile i64 53619, i64* @assembly_address
  %54 = load i64* %rax
  %55 = add i64 %54, 26
  %56 = and i64 %54, 15
  %57 = add i64 %56, 10
  %58 = icmp ugt i64 %57, 15
  %59 = icmp ult i64 %55, %54
  %60 = xor i64 %54, %55
  %61 = xor i64 26, %55
  %62 = and i64 %60, %61
  %63 = icmp slt i64 %62, 0
  store i1 %58, i1* %az
  store i1 %59, i1* %cf
  store i1 %63, i1* %of
  %64 = icmp eq i64 %55, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i64 %55, 0
  store i1 %65, i1* %sf
  %66 = trunc i64 %55 to i8
  %67 = call i8 @llvm.ctpop.i8(i8 %66)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  store i64 %55, i64* %rax
  store volatile i64 53623, i64* @assembly_address
  %70 = load i64* %rax
  %71 = inttoptr i64 %70 to i8*
  %72 = load i8* %71
  %73 = zext i8 %72 to i64
  store i64 %73, i64* %rax
  store volatile i64 53626, i64* @assembly_address
  %74 = load i64* %rax
  %75 = trunc i64 %74 to i8
  %76 = zext i8 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 53629, i64* @assembly_address
  %77 = load i8** %stack_var_-16
  %78 = ptrtoint i8* %77 to i64
  store i64 %78, i64* %rdx
  store volatile i64 53633, i64* @assembly_address
  %79 = load i64* %rdx
  %80 = add i64 %79, 27
  %81 = and i64 %79, 15
  %82 = add i64 %81, 11
  %83 = icmp ugt i64 %82, 15
  %84 = icmp ult i64 %80, %79
  %85 = xor i64 %79, %80
  %86 = xor i64 27, %80
  %87 = and i64 %85, %86
  %88 = icmp slt i64 %87, 0
  store i1 %83, i1* %az
  store i1 %84, i1* %cf
  store i1 %88, i1* %of
  %89 = icmp eq i64 %80, 0
  store i1 %89, i1* %zf
  %90 = icmp slt i64 %80, 0
  store i1 %90, i1* %sf
  %91 = trunc i64 %80 to i8
  %92 = call i8 @llvm.ctpop.i8(i8 %91)
  %93 = and i8 %92, 1
  %94 = icmp eq i8 %93, 0
  store i1 %94, i1* %pf
  store i64 %80, i64* %rdx
  store volatile i64 53637, i64* @assembly_address
  %95 = load i64* %rdx
  %96 = inttoptr i64 %95 to i8*
  %97 = load i8* %96
  %98 = zext i8 %97 to i64
  store i64 %98, i64* %rdx
  store volatile i64 53640, i64* @assembly_address
  %99 = load i64* %rdx
  %100 = trunc i64 %99 to i8
  %101 = zext i8 %100 to i64
  store i64 %101, i64* %rdx
  store volatile i64 53643, i64* @assembly_address
  %102 = load i64* %rdx
  %103 = trunc i64 %102 to i32
  %104 = load i1* %of
  %105 = shl i32 %103, 8
  %106 = icmp eq i32 %105, 0
  store i1 %106, i1* %zf
  %107 = icmp slt i32 %105, 0
  store i1 %107, i1* %sf
  %108 = trunc i32 %105 to i8
  %109 = call i8 @llvm.ctpop.i8(i8 %108)
  %110 = and i8 %109, 1
  %111 = icmp eq i8 %110, 0
  store i1 %111, i1* %pf
  %112 = zext i32 %105 to i64
  store i64 %112, i64* %rdx
  %113 = shl i32 %103, 7
  %114 = lshr i32 %113, 31
  %115 = trunc i32 %114 to i1
  store i1 %115, i1* %cf
  %116 = lshr i32 %105, 31
  %117 = icmp ne i32 %116, %114
  %118 = select i1 false, i1 %117, i1 %104
  store i1 %118, i1* %of
  store volatile i64 53646, i64* @assembly_address
  %119 = load i64* %rax
  %120 = trunc i64 %119 to i32
  %121 = load i64* %rdx
  %122 = trunc i64 %121 to i32
  %123 = or i32 %120, %122
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %124 = icmp eq i32 %123, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i32 %123, 0
  store i1 %125, i1* %sf
  %126 = trunc i32 %123 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  %130 = zext i32 %123 to i64
  store i64 %130, i64* %rax
  store volatile i64 53648, i64* @assembly_address
  %131 = load i64* %rax
  %132 = add i64 %131, 30
  %133 = trunc i64 %132 to i32
  %134 = zext i32 %133 to i64
  store i64 %134, i64* %rcx
  store volatile i64 53651, i64* @assembly_address
  %135 = load i8** %stack_var_-16
  %136 = ptrtoint i8* %135 to i64
  store i64 %136, i64* %rax
  store volatile i64 53655, i64* @assembly_address
  %137 = load i64* %rax
  %138 = add i64 %137, 28
  %139 = and i64 %137, 15
  %140 = add i64 %139, 12
  %141 = icmp ugt i64 %140, 15
  %142 = icmp ult i64 %138, %137
  %143 = xor i64 %137, %138
  %144 = xor i64 28, %138
  %145 = and i64 %143, %144
  %146 = icmp slt i64 %145, 0
  store i1 %141, i1* %az
  store i1 %142, i1* %cf
  store i1 %146, i1* %of
  %147 = icmp eq i64 %138, 0
  store i1 %147, i1* %zf
  %148 = icmp slt i64 %138, 0
  store i1 %148, i1* %sf
  %149 = trunc i64 %138 to i8
  %150 = call i8 @llvm.ctpop.i8(i8 %149)
  %151 = and i8 %150, 1
  %152 = icmp eq i8 %151, 0
  store i1 %152, i1* %pf
  store i64 %138, i64* %rax
  store volatile i64 53659, i64* @assembly_address
  %153 = load i64* %rax
  %154 = inttoptr i64 %153 to i8*
  %155 = load i8* %154
  %156 = zext i8 %155 to i64
  store i64 %156, i64* %rax
  store volatile i64 53662, i64* @assembly_address
  %157 = load i64* %rax
  %158 = trunc i64 %157 to i8
  %159 = zext i8 %158 to i64
  store i64 %159, i64* %rax
  store volatile i64 53665, i64* @assembly_address
  %160 = load i8** %stack_var_-16
  %161 = ptrtoint i8* %160 to i64
  store i64 %161, i64* %rdx
  store volatile i64 53669, i64* @assembly_address
  %162 = load i64* %rdx
  %163 = add i64 %162, 29
  %164 = and i64 %162, 15
  %165 = add i64 %164, 13
  %166 = icmp ugt i64 %165, 15
  %167 = icmp ult i64 %163, %162
  %168 = xor i64 %162, %163
  %169 = xor i64 29, %163
  %170 = and i64 %168, %169
  %171 = icmp slt i64 %170, 0
  store i1 %166, i1* %az
  store i1 %167, i1* %cf
  store i1 %171, i1* %of
  %172 = icmp eq i64 %163, 0
  store i1 %172, i1* %zf
  %173 = icmp slt i64 %163, 0
  store i1 %173, i1* %sf
  %174 = trunc i64 %163 to i8
  %175 = call i8 @llvm.ctpop.i8(i8 %174)
  %176 = and i8 %175, 1
  %177 = icmp eq i8 %176, 0
  store i1 %177, i1* %pf
  store i64 %163, i64* %rdx
  store volatile i64 53673, i64* @assembly_address
  %178 = load i64* %rdx
  %179 = inttoptr i64 %178 to i8*
  %180 = load i8* %179
  %181 = zext i8 %180 to i64
  store i64 %181, i64* %rdx
  store volatile i64 53676, i64* @assembly_address
  %182 = load i64* %rdx
  %183 = trunc i64 %182 to i8
  %184 = zext i8 %183 to i64
  store i64 %184, i64* %rdx
  store volatile i64 53679, i64* @assembly_address
  %185 = load i64* %rdx
  %186 = trunc i64 %185 to i32
  %187 = load i1* %of
  %188 = shl i32 %186, 8
  %189 = icmp eq i32 %188, 0
  store i1 %189, i1* %zf
  %190 = icmp slt i32 %188, 0
  store i1 %190, i1* %sf
  %191 = trunc i32 %188 to i8
  %192 = call i8 @llvm.ctpop.i8(i8 %191)
  %193 = and i8 %192, 1
  %194 = icmp eq i8 %193, 0
  store i1 %194, i1* %pf
  %195 = zext i32 %188 to i64
  store i64 %195, i64* %rdx
  %196 = shl i32 %186, 7
  %197 = lshr i32 %196, 31
  %198 = trunc i32 %197 to i1
  store i1 %198, i1* %cf
  %199 = lshr i32 %188, 31
  %200 = icmp ne i32 %199, %197
  %201 = select i1 false, i1 %200, i1 %187
  store i1 %201, i1* %of
  store volatile i64 53682, i64* @assembly_address
  %202 = load i64* %rax
  %203 = trunc i64 %202 to i32
  %204 = load i64* %rdx
  %205 = trunc i64 %204 to i32
  %206 = or i32 %203, %205
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %207 = icmp eq i32 %206, 0
  store i1 %207, i1* %zf
  %208 = icmp slt i32 %206, 0
  store i1 %208, i1* %sf
  %209 = trunc i32 %206 to i8
  %210 = call i8 @llvm.ctpop.i8(i8 %209)
  %211 = and i8 %210, 1
  %212 = icmp eq i8 %211, 0
  store i1 %212, i1* %pf
  %213 = zext i32 %206 to i64
  store i64 %213, i64* %rax
  store volatile i64 53684, i64* @assembly_address
  %214 = load i64* %rax
  %215 = trunc i64 %214 to i32
  %216 = load i64* %rcx
  %217 = trunc i64 %216 to i32
  %218 = add i32 %215, %217
  %219 = and i32 %215, 15
  %220 = and i32 %217, 15
  %221 = add i32 %219, %220
  %222 = icmp ugt i32 %221, 15
  %223 = icmp ult i32 %218, %215
  %224 = xor i32 %215, %218
  %225 = xor i32 %217, %218
  %226 = and i32 %224, %225
  %227 = icmp slt i32 %226, 0
  store i1 %222, i1* %az
  store i1 %223, i1* %cf
  store i1 %227, i1* %of
  %228 = icmp eq i32 %218, 0
  store i1 %228, i1* %zf
  %229 = icmp slt i32 %218, 0
  store i1 %229, i1* %sf
  %230 = trunc i32 %218 to i8
  %231 = call i8 @llvm.ctpop.i8(i8 %230)
  %232 = and i8 %231, 1
  %233 = icmp eq i8 %232, 0
  store i1 %233, i1* %pf
  %234 = zext i32 %218 to i64
  store i64 %234, i64* %rax
  store volatile i64 53686, i64* @assembly_address
  %235 = load i64* %rax
  %236 = trunc i64 %235 to i32
  %237 = zext i32 %236 to i64
  store i64 %237, i64* %rdx
  store volatile i64 53688, i64* @assembly_address
  %238 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %239 = zext i32 %238 to i64
  store i64 %239, i64* %rax
  store volatile i64 53694, i64* @assembly_address
  %240 = load i64* %rax
  %241 = trunc i64 %240 to i32
  %242 = load i64* %rdx
  %243 = trunc i64 %242 to i32
  %244 = add i32 %241, %243
  %245 = and i32 %241, 15
  %246 = and i32 %243, 15
  %247 = add i32 %245, %246
  %248 = icmp ugt i32 %247, 15
  %249 = icmp ult i32 %244, %241
  %250 = xor i32 %241, %244
  %251 = xor i32 %243, %244
  %252 = and i32 %250, %251
  %253 = icmp slt i32 %252, 0
  store i1 %248, i1* %az
  store i1 %249, i1* %cf
  store i1 %253, i1* %of
  %254 = icmp eq i32 %244, 0
  store i1 %254, i1* %zf
  %255 = icmp slt i32 %244, 0
  store i1 %255, i1* %sf
  %256 = trunc i32 %244 to i8
  %257 = call i8 @llvm.ctpop.i8(i8 %256)
  %258 = and i8 %257, 1
  %259 = icmp eq i8 %258, 0
  store i1 %259, i1* %pf
  %260 = zext i32 %244 to i64
  store i64 %260, i64* %rax
  store volatile i64 53696, i64* @assembly_address
  %261 = load i64* %rax
  %262 = trunc i64 %261 to i32
  store i32 %262, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 53702, i64* @assembly_address
  %263 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %264 = zext i32 %263 to i64
  store i64 %264, i64* %rdx
  store volatile i64 53708, i64* @assembly_address
  %265 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %266 = zext i32 %265 to i64
  store i64 %266, i64* %rax
  store volatile i64 53714, i64* @assembly_address
  %267 = load i64* %rdx
  %268 = trunc i64 %267 to i32
  %269 = load i64* %rax
  %270 = trunc i64 %269 to i32
  %271 = sub i32 %268, %270
  %272 = and i32 %268, 15
  %273 = and i32 %270, 15
  %274 = sub i32 %272, %273
  %275 = icmp ugt i32 %274, 15
  %276 = icmp ult i32 %268, %270
  %277 = xor i32 %268, %270
  %278 = xor i32 %268, %271
  %279 = and i32 %277, %278
  %280 = icmp slt i32 %279, 0
  store i1 %275, i1* %az
  store i1 %276, i1* %cf
  store i1 %280, i1* %of
  %281 = icmp eq i32 %271, 0
  store i1 %281, i1* %zf
  %282 = icmp slt i32 %271, 0
  store i1 %282, i1* %sf
  %283 = trunc i32 %271 to i8
  %284 = call i8 @llvm.ctpop.i8(i8 %283)
  %285 = and i8 %284, 1
  %286 = icmp eq i8 %285, 0
  store i1 %286, i1* %pf
  store volatile i64 53716, i64* @assembly_address
  %287 = load i1* %cf
  %288 = load i1* %zf
  %289 = or i1 %287, %288
  %290 = icmp ne i1 %289, true
  br i1 %290, label %block_d228, label %block_d1d6

block_d1d6:                                       ; preds = %block_d145
  store volatile i64 53718, i64* @assembly_address
  %291 = load i8** %stack_var_-16
  %292 = ptrtoint i8* %291 to i64
  store i64 %292, i64* %rax
  store volatile i64 53722, i64* @assembly_address
  %293 = load i64* %rax
  %294 = inttoptr i64 %293 to i8*
  %295 = load i8* %294
  %296 = zext i8 %295 to i64
  store i64 %296, i64* %rax
  store volatile i64 53725, i64* @assembly_address
  %297 = load i64* %rax
  %298 = trunc i64 %297 to i8
  %299 = zext i8 %298 to i64
  store i64 %299, i64* %rax
  store volatile i64 53728, i64* @assembly_address
  %300 = load i8** %stack_var_-16
  %301 = ptrtoint i8* %300 to i64
  store i64 %301, i64* %rdx
  store volatile i64 53732, i64* @assembly_address
  %302 = load i64* %rdx
  %303 = add i64 %302, 1
  %304 = and i64 %302, 15
  %305 = add i64 %304, 1
  %306 = icmp ugt i64 %305, 15
  %307 = icmp ult i64 %303, %302
  %308 = xor i64 %302, %303
  %309 = xor i64 1, %303
  %310 = and i64 %308, %309
  %311 = icmp slt i64 %310, 0
  store i1 %306, i1* %az
  store i1 %307, i1* %cf
  store i1 %311, i1* %of
  %312 = icmp eq i64 %303, 0
  store i1 %312, i1* %zf
  %313 = icmp slt i64 %303, 0
  store i1 %313, i1* %sf
  %314 = trunc i64 %303 to i8
  %315 = call i8 @llvm.ctpop.i8(i8 %314)
  %316 = and i8 %315, 1
  %317 = icmp eq i8 %316, 0
  store i1 %317, i1* %pf
  store i64 %303, i64* %rdx
  store volatile i64 53736, i64* @assembly_address
  %318 = load i64* %rdx
  %319 = inttoptr i64 %318 to i8*
  %320 = load i8* %319
  %321 = zext i8 %320 to i64
  store i64 %321, i64* %rdx
  store volatile i64 53739, i64* @assembly_address
  %322 = load i64* %rdx
  %323 = trunc i64 %322 to i8
  %324 = zext i8 %323 to i64
  store i64 %324, i64* %rdx
  store volatile i64 53742, i64* @assembly_address
  %325 = load i64* %rdx
  %326 = trunc i64 %325 to i32
  %327 = load i1* %of
  %328 = shl i32 %326, 8
  %329 = icmp eq i32 %328, 0
  store i1 %329, i1* %zf
  %330 = icmp slt i32 %328, 0
  store i1 %330, i1* %sf
  %331 = trunc i32 %328 to i8
  %332 = call i8 @llvm.ctpop.i8(i8 %331)
  %333 = and i8 %332, 1
  %334 = icmp eq i8 %333, 0
  store i1 %334, i1* %pf
  %335 = zext i32 %328 to i64
  store i64 %335, i64* %rdx
  %336 = shl i32 %326, 7
  %337 = lshr i32 %336, 31
  %338 = trunc i32 %337 to i1
  store i1 %338, i1* %cf
  %339 = lshr i32 %328, 31
  %340 = icmp ne i32 %339, %337
  %341 = select i1 false, i1 %340, i1 %327
  store i1 %341, i1* %of
  store volatile i64 53745, i64* @assembly_address
  %342 = load i64* %rax
  %343 = trunc i64 %342 to i32
  %344 = load i64* %rdx
  %345 = trunc i64 %344 to i32
  %346 = or i32 %343, %345
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %347 = icmp eq i32 %346, 0
  store i1 %347, i1* %zf
  %348 = icmp slt i32 %346, 0
  store i1 %348, i1* %sf
  %349 = trunc i32 %346 to i8
  %350 = call i8 @llvm.ctpop.i8(i8 %349)
  %351 = and i8 %350, 1
  %352 = icmp eq i8 %351, 0
  store i1 %352, i1* %pf
  %353 = zext i32 %346 to i64
  store i64 %353, i64* %rax
  store volatile i64 53747, i64* @assembly_address
  %354 = load i64* %rax
  %355 = trunc i64 %354 to i32
  %356 = sext i32 %355 to i64
  store i64 %356, i64* %rax
  store volatile i64 53749, i64* @assembly_address
  %357 = load i8** %stack_var_-16
  %358 = ptrtoint i8* %357 to i64
  store i64 %358, i64* %rdx
  store volatile i64 53753, i64* @assembly_address
  %359 = load i64* %rdx
  %360 = add i64 %359, 2
  %361 = and i64 %359, 15
  %362 = add i64 %361, 2
  %363 = icmp ugt i64 %362, 15
  %364 = icmp ult i64 %360, %359
  %365 = xor i64 %359, %360
  %366 = xor i64 2, %360
  %367 = and i64 %365, %366
  %368 = icmp slt i64 %367, 0
  store i1 %363, i1* %az
  store i1 %364, i1* %cf
  store i1 %368, i1* %of
  %369 = icmp eq i64 %360, 0
  store i1 %369, i1* %zf
  %370 = icmp slt i64 %360, 0
  store i1 %370, i1* %sf
  %371 = trunc i64 %360 to i8
  %372 = call i8 @llvm.ctpop.i8(i8 %371)
  %373 = and i8 %372, 1
  %374 = icmp eq i8 %373, 0
  store i1 %374, i1* %pf
  store i64 %360, i64* %rdx
  store volatile i64 53757, i64* @assembly_address
  %375 = load i64* %rdx
  %376 = inttoptr i64 %375 to i8*
  %377 = load i8* %376
  %378 = zext i8 %377 to i64
  store i64 %378, i64* %rdx
  store volatile i64 53760, i64* @assembly_address
  %379 = load i64* %rdx
  %380 = trunc i64 %379 to i8
  %381 = zext i8 %380 to i64
  store i64 %381, i64* %rdx
  store volatile i64 53763, i64* @assembly_address
  %382 = load i8** %stack_var_-16
  %383 = ptrtoint i8* %382 to i64
  store i64 %383, i64* %rcx
  store volatile i64 53767, i64* @assembly_address
  %384 = load i64* %rcx
  %385 = add i64 %384, 3
  %386 = and i64 %384, 15
  %387 = add i64 %386, 3
  %388 = icmp ugt i64 %387, 15
  %389 = icmp ult i64 %385, %384
  %390 = xor i64 %384, %385
  %391 = xor i64 3, %385
  %392 = and i64 %390, %391
  %393 = icmp slt i64 %392, 0
  store i1 %388, i1* %az
  store i1 %389, i1* %cf
  store i1 %393, i1* %of
  %394 = icmp eq i64 %385, 0
  store i1 %394, i1* %zf
  %395 = icmp slt i64 %385, 0
  store i1 %395, i1* %sf
  %396 = trunc i64 %385 to i8
  %397 = call i8 @llvm.ctpop.i8(i8 %396)
  %398 = and i8 %397, 1
  %399 = icmp eq i8 %398, 0
  store i1 %399, i1* %pf
  store i64 %385, i64* %rcx
  store volatile i64 53771, i64* @assembly_address
  %400 = load i64* %rcx
  %401 = inttoptr i64 %400 to i8*
  %402 = load i8* %401
  %403 = zext i8 %402 to i64
  store i64 %403, i64* %rcx
  store volatile i64 53774, i64* @assembly_address
  %404 = load i64* %rcx
  %405 = trunc i64 %404 to i8
  %406 = zext i8 %405 to i64
  store i64 %406, i64* %rcx
  store volatile i64 53777, i64* @assembly_address
  %407 = load i64* %rcx
  %408 = trunc i64 %407 to i32
  %409 = load i1* %of
  %410 = shl i32 %408, 8
  %411 = icmp eq i32 %410, 0
  store i1 %411, i1* %zf
  %412 = icmp slt i32 %410, 0
  store i1 %412, i1* %sf
  %413 = trunc i32 %410 to i8
  %414 = call i8 @llvm.ctpop.i8(i8 %413)
  %415 = and i8 %414, 1
  %416 = icmp eq i8 %415, 0
  store i1 %416, i1* %pf
  %417 = zext i32 %410 to i64
  store i64 %417, i64* %rcx
  %418 = shl i32 %408, 7
  %419 = lshr i32 %418, 31
  %420 = trunc i32 %419 to i1
  store i1 %420, i1* %cf
  %421 = lshr i32 %410, 31
  %422 = icmp ne i32 %421, %419
  %423 = select i1 false, i1 %422, i1 %409
  store i1 %423, i1* %of
  store volatile i64 53780, i64* @assembly_address
  %424 = load i64* %rdx
  %425 = trunc i64 %424 to i32
  %426 = load i64* %rcx
  %427 = trunc i64 %426 to i32
  %428 = or i32 %425, %427
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %429 = icmp eq i32 %428, 0
  store i1 %429, i1* %zf
  %430 = icmp slt i32 %428, 0
  store i1 %430, i1* %sf
  %431 = trunc i32 %428 to i8
  %432 = call i8 @llvm.ctpop.i8(i8 %431)
  %433 = and i8 %432, 1
  %434 = icmp eq i8 %433, 0
  store i1 %434, i1* %pf
  %435 = zext i32 %428 to i64
  store i64 %435, i64* %rdx
  store volatile i64 53782, i64* @assembly_address
  %436 = load i64* %rdx
  %437 = trunc i64 %436 to i32
  %438 = sext i32 %437 to i64
  store i64 %438, i64* %rdx
  store volatile i64 53785, i64* @assembly_address
  %439 = load i64* %rdx
  %440 = load i1* %of
  %441 = shl i64 %439, 16
  %442 = icmp eq i64 %441, 0
  store i1 %442, i1* %zf
  %443 = icmp slt i64 %441, 0
  store i1 %443, i1* %sf
  %444 = trunc i64 %441 to i8
  %445 = call i8 @llvm.ctpop.i8(i8 %444)
  %446 = and i8 %445, 1
  %447 = icmp eq i8 %446, 0
  store i1 %447, i1* %pf
  store i64 %441, i64* %rdx
  %448 = shl i64 %439, 15
  %449 = lshr i64 %448, 63
  %450 = trunc i64 %449 to i1
  store i1 %450, i1* %cf
  %451 = lshr i64 %441, 63
  %452 = icmp ne i64 %451, %449
  %453 = select i1 false, i1 %452, i1 %440
  store i1 %453, i1* %of
  store volatile i64 53789, i64* @assembly_address
  %454 = load i64* %rax
  %455 = load i64* %rdx
  %456 = or i64 %454, %455
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %457 = icmp eq i64 %456, 0
  store i1 %457, i1* %zf
  %458 = icmp slt i64 %456, 0
  store i1 %458, i1* %sf
  %459 = trunc i64 %456 to i8
  %460 = call i8 @llvm.ctpop.i8(i8 %459)
  %461 = and i8 %460, 1
  %462 = icmp eq i8 %461, 0
  store i1 %462, i1* %pf
  store i64 %456, i64* %rax
  store volatile i64 53792, i64* @assembly_address
  %463 = load i64* %rax
  %464 = sub i64 %463, 67324752
  %465 = and i64 %463, 15
  %466 = icmp ugt i64 %465, 15
  %467 = icmp ult i64 %463, 67324752
  %468 = xor i64 %463, 67324752
  %469 = xor i64 %463, %464
  %470 = and i64 %468, %469
  %471 = icmp slt i64 %470, 0
  store i1 %466, i1* %az
  store i1 %467, i1* %cf
  store i1 %471, i1* %of
  %472 = icmp eq i64 %464, 0
  store i1 %472, i1* %zf
  %473 = icmp slt i64 %464, 0
  store i1 %473, i1* %sf
  %474 = trunc i64 %464 to i8
  %475 = call i8 @llvm.ctpop.i8(i8 %474)
  %476 = and i8 %475, 1
  %477 = icmp eq i8 %476, 0
  store i1 %477, i1* %pf
  store volatile i64 53798, i64* @assembly_address
  %478 = load i1* %zf
  br i1 %478, label %block_d265, label %block_d228

block_d228:                                       ; preds = %block_d1d6, %block_d145
  store volatile i64 53800, i64* @assembly_address
  %479 = load i64* @global_var_25f4c8
  store i64 %479, i64* %rdx
  store volatile i64 53807, i64* @assembly_address
  %480 = load i64* @global_var_216580
  store i64 %480, i64* %rax
  store volatile i64 53814, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 53821, i64* @assembly_address
  store i64 ptrtoint ([31 x i8]* @global_var_12250 to i64), i64* %rsi
  store volatile i64 53828, i64* @assembly_address
  %481 = load i64* %rax
  store i64 %481, i64* %rdi
  store volatile i64 53831, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 53836, i64* @assembly_address
  %482 = load i64* %rdi
  %483 = inttoptr i64 %482 to %_IO_FILE*
  %484 = load i64* %rsi
  %485 = inttoptr i64 %484 to i8*
  %486 = load i64* %rdx
  %487 = inttoptr i64 %486 to i8*
  %488 = load i64* %rcx
  %489 = inttoptr i64 %488 to i8*
  %490 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %483, i8* %485, i8* %487, i8* %489)
  %491 = sext i32 %490 to i64
  store i64 %491, i64* %rax
  %492 = sext i32 %490 to i64
  store i64 %492, i64* %rax
  store volatile i64 53841, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 53851, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 53856, i64* @assembly_address
  br label %block_d354

block_d265:                                       ; preds = %block_d1d6
  store volatile i64 53861, i64* @assembly_address
  %493 = load i8** %stack_var_-16
  %494 = ptrtoint i8* %493 to i64
  store i64 %494, i64* %rax
  store volatile i64 53865, i64* @assembly_address
  %495 = load i64* %rax
  %496 = add i64 %495, 8
  %497 = and i64 %495, 15
  %498 = add i64 %497, 8
  %499 = icmp ugt i64 %498, 15
  %500 = icmp ult i64 %496, %495
  %501 = xor i64 %495, %496
  %502 = xor i64 8, %496
  %503 = and i64 %501, %502
  %504 = icmp slt i64 %503, 0
  store i1 %499, i1* %az
  store i1 %500, i1* %cf
  store i1 %504, i1* %of
  %505 = icmp eq i64 %496, 0
  store i1 %505, i1* %zf
  %506 = icmp slt i64 %496, 0
  store i1 %506, i1* %sf
  %507 = trunc i64 %496 to i8
  %508 = call i8 @llvm.ctpop.i8(i8 %507)
  %509 = and i8 %508, 1
  %510 = icmp eq i8 %509, 0
  store i1 %510, i1* %pf
  store i64 %496, i64* %rax
  store volatile i64 53869, i64* @assembly_address
  %511 = load i64* %rax
  %512 = inttoptr i64 %511 to i8*
  %513 = load i8* %512
  %514 = zext i8 %513 to i64
  store i64 %514, i64* %rax
  store volatile i64 53872, i64* @assembly_address
  %515 = load i64* %rax
  %516 = trunc i64 %515 to i8
  %517 = zext i8 %516 to i64
  store i64 %517, i64* %rax
  store volatile i64 53875, i64* @assembly_address
  %518 = load i64* %rax
  %519 = trunc i64 %518 to i32
  store i32 %519, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 53881, i64* @assembly_address
  %520 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %521 = zext i32 %520 to i64
  store i64 %521, i64* %rax
  store volatile i64 53887, i64* @assembly_address
  %522 = load i64* %rax
  %523 = trunc i64 %522 to i32
  %524 = load i64* %rax
  %525 = trunc i64 %524 to i32
  %526 = and i32 %523, %525
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %527 = icmp eq i32 %526, 0
  store i1 %527, i1* %zf
  %528 = icmp slt i32 %526, 0
  store i1 %528, i1* %sf
  %529 = trunc i32 %526 to i8
  %530 = call i8 @llvm.ctpop.i8(i8 %529)
  %531 = and i8 %530, 1
  %532 = icmp eq i8 %531, 0
  store i1 %532, i1* %pf
  store volatile i64 53889, i64* @assembly_address
  %533 = load i1* %zf
  br i1 %533, label %block_d2cb, label %block_d283

block_d283:                                       ; preds = %block_d265
  store volatile i64 53891, i64* @assembly_address
  %534 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %535 = zext i32 %534 to i64
  store i64 %535, i64* %rax
  store volatile i64 53897, i64* @assembly_address
  %536 = load i64* %rax
  %537 = trunc i64 %536 to i32
  %538 = sub i32 %537, 8
  %539 = and i32 %537, 15
  %540 = sub i32 %539, 8
  %541 = icmp ugt i32 %540, 15
  %542 = icmp ult i32 %537, 8
  %543 = xor i32 %537, 8
  %544 = xor i32 %537, %538
  %545 = and i32 %543, %544
  %546 = icmp slt i32 %545, 0
  store i1 %541, i1* %az
  store i1 %542, i1* %cf
  store i1 %546, i1* %of
  %547 = icmp eq i32 %538, 0
  store i1 %547, i1* %zf
  %548 = icmp slt i32 %538, 0
  store i1 %548, i1* %sf
  %549 = trunc i32 %538 to i8
  %550 = call i8 @llvm.ctpop.i8(i8 %549)
  %551 = and i8 %550, 1
  %552 = icmp eq i8 %551, 0
  store i1 %552, i1* %pf
  store volatile i64 53900, i64* @assembly_address
  %553 = load i1* %zf
  br i1 %553, label %block_d2cb, label %block_d28e

block_d28e:                                       ; preds = %block_d283
  store volatile i64 53902, i64* @assembly_address
  %554 = load i64* @global_var_25f4c8
  store i64 %554, i64* %rdx
  store volatile i64 53909, i64* @assembly_address
  %555 = load i64* @global_var_216580
  store i64 %555, i64* %rax
  store volatile i64 53916, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 53923, i64* @assembly_address
  store i64 ptrtoint ([58 x i8]* @global_var_12270 to i64), i64* %rsi
  store volatile i64 53930, i64* @assembly_address
  %556 = load i64* %rax
  store i64 %556, i64* %rdi
  store volatile i64 53933, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 53938, i64* @assembly_address
  %557 = load i64* %rdi
  %558 = inttoptr i64 %557 to %_IO_FILE*
  %559 = load i64* %rsi
  %560 = inttoptr i64 %559 to i8*
  %561 = load i64* %rdx
  %562 = inttoptr i64 %561 to i8*
  %563 = load i64* %rcx
  %564 = inttoptr i64 %563 to i8*
  %565 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %558, i8* %560, i8* %562, i8* %564)
  %566 = sext i32 %565 to i64
  store i64 %566, i64* %rax
  %567 = sext i32 %565 to i64
  store i64 %567, i64* %rax
  store volatile i64 53943, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 53953, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 53958, i64* @assembly_address
  br label %block_d354

block_d2cb:                                       ; preds = %block_d283, %block_d265
  store volatile i64 53963, i64* @assembly_address
  %568 = load i8** %stack_var_-16
  %569 = ptrtoint i8* %568 to i64
  store i64 %569, i64* %rax
  store volatile i64 53967, i64* @assembly_address
  %570 = load i64* %rax
  %571 = add i64 %570, 6
  %572 = and i64 %570, 15
  %573 = add i64 %572, 6
  %574 = icmp ugt i64 %573, 15
  %575 = icmp ult i64 %571, %570
  %576 = xor i64 %570, %571
  %577 = xor i64 6, %571
  %578 = and i64 %576, %577
  %579 = icmp slt i64 %578, 0
  store i1 %574, i1* %az
  store i1 %575, i1* %cf
  store i1 %579, i1* %of
  %580 = icmp eq i64 %571, 0
  store i1 %580, i1* %zf
  %581 = icmp slt i64 %571, 0
  store i1 %581, i1* %sf
  %582 = trunc i64 %571 to i8
  %583 = call i8 @llvm.ctpop.i8(i8 %582)
  %584 = and i8 %583, 1
  %585 = icmp eq i8 %584, 0
  store i1 %585, i1* %pf
  store i64 %571, i64* %rax
  store volatile i64 53971, i64* @assembly_address
  %586 = load i64* %rax
  %587 = inttoptr i64 %586 to i8*
  %588 = load i8* %587
  %589 = zext i8 %588 to i64
  store i64 %589, i64* %rax
  store volatile i64 53974, i64* @assembly_address
  %590 = load i64* %rax
  %591 = trunc i64 %590 to i8
  %592 = zext i8 %591 to i64
  store i64 %592, i64* %rax
  store volatile i64 53977, i64* @assembly_address
  %593 = load i64* %rax
  %594 = trunc i64 %593 to i32
  %595 = and i32 %594, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %596 = icmp eq i32 %595, 0
  store i1 %596, i1* %zf
  %597 = icmp slt i32 %595, 0
  store i1 %597, i1* %sf
  %598 = trunc i32 %595 to i8
  %599 = call i8 @llvm.ctpop.i8(i8 %598)
  %600 = and i8 %599, 1
  %601 = icmp eq i8 %600, 0
  store i1 %601, i1* %pf
  %602 = zext i32 %595 to i64
  store i64 %602, i64* %rax
  store volatile i64 53980, i64* @assembly_address
  %603 = load i64* %rax
  %604 = trunc i64 %603 to i32
  store i32 %604, i32* bitcast (i64* @global_var_21a3fc to i32*)
  store volatile i64 53986, i64* @assembly_address
  %605 = load i32* bitcast (i64* @global_var_21a3fc to i32*)
  %606 = zext i32 %605 to i64
  store i64 %606, i64* %rax
  store volatile i64 53992, i64* @assembly_address
  %607 = load i64* %rax
  %608 = trunc i64 %607 to i32
  %609 = load i64* %rax
  %610 = trunc i64 %609 to i32
  %611 = and i32 %608, %610
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %612 = icmp eq i32 %611, 0
  store i1 %612, i1* %zf
  %613 = icmp slt i32 %611, 0
  store i1 %613, i1* %sf
  %614 = trunc i32 %611 to i8
  %615 = call i8 @llvm.ctpop.i8(i8 %614)
  %616 = and i8 %615, 1
  %617 = icmp eq i8 %616, 0
  store i1 %617, i1* %pf
  store volatile i64 53994, i64* @assembly_address
  %618 = load i1* %zf
  br i1 %618, label %block_d326, label %block_d2ec

block_d2ec:                                       ; preds = %block_d2cb
  store volatile i64 53996, i64* @assembly_address
  %619 = load i64* @global_var_25f4c8
  store i64 %619, i64* %rdx
  store volatile i64 54003, i64* @assembly_address
  %620 = load i64* @global_var_216580
  store i64 %620, i64* %rax
  store volatile i64 54010, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 54017, i64* @assembly_address
  store i64 ptrtoint ([38 x i8]* @global_var_122b0 to i64), i64* %rsi
  store volatile i64 54024, i64* @assembly_address
  %621 = load i64* %rax
  store i64 %621, i64* %rdi
  store volatile i64 54027, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 54032, i64* @assembly_address
  %622 = load i64* %rdi
  %623 = inttoptr i64 %622 to %_IO_FILE*
  %624 = load i64* %rsi
  %625 = inttoptr i64 %624 to i8*
  %626 = load i64* %rdx
  %627 = inttoptr i64 %626 to i8*
  %628 = load i64* %rcx
  %629 = inttoptr i64 %628 to i8*
  %630 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %623, i8* %625, i8* %627, i8* %629)
  %631 = sext i32 %630 to i64
  store i64 %631, i64* %rax
  %632 = sext i32 %630 to i64
  store i64 %632, i64* %rax
  store volatile i64 54037, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 54047, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 54052, i64* @assembly_address
  br label %block_d354

block_d326:                                       ; preds = %block_d2cb
  store volatile i64 54054, i64* @assembly_address
  %633 = load i8** %stack_var_-16
  %634 = ptrtoint i8* %633 to i64
  store i64 %634, i64* %rax
  store volatile i64 54058, i64* @assembly_address
  %635 = load i64* %rax
  %636 = add i64 %635, 6
  %637 = and i64 %635, 15
  %638 = add i64 %637, 6
  %639 = icmp ugt i64 %638, 15
  %640 = icmp ult i64 %636, %635
  %641 = xor i64 %635, %636
  %642 = xor i64 6, %636
  %643 = and i64 %641, %642
  %644 = icmp slt i64 %643, 0
  store i1 %639, i1* %az
  store i1 %640, i1* %cf
  store i1 %644, i1* %of
  %645 = icmp eq i64 %636, 0
  store i1 %645, i1* %zf
  %646 = icmp slt i64 %636, 0
  store i1 %646, i1* %sf
  %647 = trunc i64 %636 to i8
  %648 = call i8 @llvm.ctpop.i8(i8 %647)
  %649 = and i8 %648, 1
  %650 = icmp eq i8 %649, 0
  store i1 %650, i1* %pf
  store i64 %636, i64* %rax
  store volatile i64 54062, i64* @assembly_address
  %651 = load i64* %rax
  %652 = inttoptr i64 %651 to i8*
  %653 = load i8* %652
  %654 = zext i8 %653 to i64
  store i64 %654, i64* %rax
  store volatile i64 54065, i64* @assembly_address
  %655 = load i64* %rax
  %656 = trunc i64 %655 to i8
  %657 = zext i8 %656 to i64
  store i64 %657, i64* %rax
  store volatile i64 54068, i64* @assembly_address
  %658 = load i64* %rax
  %659 = trunc i64 %658 to i32
  %660 = and i32 %659, 8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %661 = icmp eq i32 %660, 0
  store i1 %661, i1* %zf
  %662 = icmp slt i32 %660, 0
  store i1 %662, i1* %sf
  %663 = trunc i32 %660 to i8
  %664 = call i8 @llvm.ctpop.i8(i8 %663)
  %665 = and i8 %664, 1
  %666 = icmp eq i8 %665, 0
  store i1 %666, i1* %pf
  %667 = zext i32 %660 to i64
  store i64 %667, i64* %rax
  store volatile i64 54071, i64* @assembly_address
  %668 = load i64* %rax
  %669 = trunc i64 %668 to i32
  %670 = load i64* %rax
  %671 = trunc i64 %670 to i32
  %672 = and i32 %669, %671
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %673 = icmp eq i32 %672, 0
  store i1 %673, i1* %zf
  %674 = icmp slt i32 %672, 0
  store i1 %674, i1* %sf
  %675 = trunc i32 %672 to i8
  %676 = call i8 @llvm.ctpop.i8(i8 %675)
  %677 = and i8 %676, 1
  %678 = icmp eq i8 %677, 0
  store i1 %678, i1* %pf
  store volatile i64 54073, i64* @assembly_address
  %679 = load i1* %zf
  %680 = icmp eq i1 %679, false
  %681 = zext i1 %680 to i8
  %682 = zext i8 %681 to i64
  %683 = load i64* %rax
  %684 = and i64 %683, -256
  %685 = or i64 %684, %682
  store i64 %685, i64* %rax
  store volatile i64 54076, i64* @assembly_address
  %686 = load i64* %rax
  %687 = trunc i64 %686 to i8
  %688 = zext i8 %687 to i64
  store i64 %688, i64* %rax
  store volatile i64 54079, i64* @assembly_address
  %689 = load i64* %rax
  %690 = trunc i64 %689 to i32
  store i32 %690, i32* bitcast (i64* @global_var_21a404 to i32*)
  store volatile i64 54085, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21a400 to i32*)
  store volatile i64 54095, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_d354

block_d354:                                       ; preds = %block_d326, %block_d2ec, %block_d28e, %block_d228
  store volatile i64 54100, i64* @assembly_address
  %691 = load i64* %stack_var_-8
  store i64 %691, i64* %rbp
  %692 = ptrtoint i64* %stack_var_0 to i64
  store i64 %692, i64* %rsp
  store volatile i64 54101, i64* @assembly_address
  %693 = load i64* %rax
  ret i64 %693
}

declare i64 @237(i64)

define i64 @unzip(i32 %arg1, i64 %arg2) {
block_d356:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-41 = alloca i32
  %1 = alloca i8
  %stack_var_-42 = alloca i32
  %2 = alloca i8
  %stack_var_-43 = alloca i32
  %3 = alloca i8
  %stack_var_-44 = alloca i32
  %4 = alloca i8
  %stack_var_-49 = alloca i32
  %5 = alloca i8
  %stack_var_-50 = alloca i32
  %6 = alloca i8
  %stack_var_-51 = alloca i32
  %7 = alloca i8
  %stack_var_-52 = alloca i32
  %8 = alloca i8
  %stack_var_-53 = alloca i32
  %9 = alloca i8
  %stack_var_-54 = alloca i32
  %10 = alloca i8
  %stack_var_-55 = alloca i32
  %11 = alloca i8
  %stack_var_-56 = alloca i32
  %12 = alloca i8
  %stack_var_-84 = alloca i32
  %stack_var_-85 = alloca i8
  %stack_var_-76 = alloca i32
  %stack_var_-80 = alloca i32
  %stack_var_-64 = alloca i64
  %stack_var_-72 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-96 = alloca i32
  %stack_var_-92 = alloca i32
  %stack_var_-104 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %13 = alloca i32
  %14 = alloca i32
  %15 = alloca i32
  %16 = alloca i32
  store volatile i64 54102, i64* @assembly_address
  %17 = load i64* %rbp
  store i64 %17, i64* %stack_var_-8
  %18 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %18, i64* %rsp
  store volatile i64 54103, i64* @assembly_address
  %19 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %19, i64* %rbp
  store volatile i64 54106, i64* @assembly_address
  %20 = load i64* %rbx
  store i64 %20, i64* %stack_var_-16
  %21 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %21, i64* %rsp
  store volatile i64 54107, i64* @assembly_address
  %22 = load i64* %rsp
  %23 = sub i64 %22, 88
  %24 = and i64 %22, 15
  %25 = sub i64 %24, 8
  %26 = icmp ugt i64 %25, 15
  %27 = icmp ult i64 %22, 88
  %28 = xor i64 %22, 88
  %29 = xor i64 %22, %23
  %30 = and i64 %28, %29
  %31 = icmp slt i64 %30, 0
  store i1 %26, i1* %az
  store i1 %27, i1* %cf
  store i1 %31, i1* %of
  %32 = icmp eq i64 %23, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i64 %23, 0
  store i1 %33, i1* %sf
  %34 = trunc i64 %23 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  %38 = ptrtoint i64* %stack_var_-104 to i64
  store i64 %38, i64* %rsp
  store volatile i64 54111, i64* @assembly_address
  %39 = load i64* %rdi
  %40 = trunc i64 %39 to i32
  store i32 %40, i32* %stack_var_-92
  store volatile i64 54114, i64* @assembly_address
  %41 = load i64* %rsi
  %42 = trunc i64 %41 to i32
  store i32 %42, i32* %stack_var_-96
  store volatile i64 54117, i64* @assembly_address
  %43 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %43, i64* %rax
  store volatile i64 54126, i64* @assembly_address
  %44 = load i64* %rax
  store i64 %44, i64* %stack_var_-32
  store volatile i64 54130, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %45 = icmp eq i32 0, 0
  store i1 %45, i1* %zf
  %46 = icmp slt i32 0, 0
  store i1 %46, i1* %sf
  %47 = trunc i32 0 to i8
  %48 = call i8 @llvm.ctpop.i8(i8 %47)
  %49 = and i8 %48, 1
  %50 = icmp eq i8 %49, 0
  store i1 %50, i1* %pf
  %51 = zext i32 0 to i64
  store i64 %51, i64* %rax
  store volatile i64 54132, i64* @assembly_address
  store i64 0, i64* %stack_var_-72
  store volatile i64 54140, i64* @assembly_address
  store i64 0, i64* %stack_var_-64
  store volatile i64 54148, i64* @assembly_address
  store i32 0, i32* %stack_var_-80
  store volatile i64 54155, i64* @assembly_address
  %52 = load i32* %stack_var_-92
  %53 = zext i32 %52 to i64
  store i64 %53, i64* %rax
  store volatile i64 54158, i64* @assembly_address
  %54 = load i64* %rax
  %55 = trunc i64 %54 to i32
  store i32 %55, i32* bitcast (i64* @global_var_24f0a0 to i32*)
  store volatile i64 54164, i64* @assembly_address
  %56 = load i32* %stack_var_-96
  %57 = zext i32 %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 54167, i64* @assembly_address
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i32
  store i32 %59, i32* bitcast (i64* @global_var_24a880 to i32*)
  store volatile i64 54173, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 54178, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 54183, i64* @assembly_address
  %60 = load i64* %rdi
  %61 = inttoptr i64 %60 to i8*
  %62 = load i64* %rsi
  %63 = trunc i64 %62 to i32
  %64 = call i64 @updcrc(i8* %61, i32 %63)
  store i64 %64, i64* %rax
  store i64 %64, i64* %rax
  store volatile i64 54188, i64* @assembly_address
  %65 = load i32* bitcast (i64* @global_var_21a400 to i32*)
  %66 = zext i32 %65 to i64
  store i64 %66, i64* %rax
  store volatile i64 54194, i64* @assembly_address
  %67 = load i64* %rax
  %68 = trunc i64 %67 to i32
  %69 = load i64* %rax
  %70 = trunc i64 %69 to i32
  %71 = and i32 %68, %70
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %72 = icmp eq i32 %71, 0
  store i1 %72, i1* %zf
  %73 = icmp slt i32 %71, 0
  store i1 %73, i1* %sf
  %74 = trunc i32 %71 to i8
  %75 = call i8 @llvm.ctpop.i8(i8 %74)
  %76 = and i8 %75, 1
  %77 = icmp eq i8 %76, 0
  store i1 %77, i1* %pf
  store volatile i64 54196, i64* @assembly_address
  %78 = load i1* %zf
  br i1 %78, label %block_d44c, label %block_d3ba

block_d3ba:                                       ; preds = %block_d356
  store volatile i64 54202, i64* @assembly_address
  %79 = load i32* bitcast (i64* @global_var_21a404 to i32*)
  %80 = zext i32 %79 to i64
  store i64 %80, i64* %rax
  store volatile i64 54208, i64* @assembly_address
  %81 = load i64* %rax
  %82 = trunc i64 %81 to i32
  %83 = load i64* %rax
  %84 = trunc i64 %83 to i32
  %85 = and i32 %82, %84
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %86 = icmp eq i32 %85, 0
  store i1 %86, i1* %zf
  %87 = icmp slt i32 %85, 0
  store i1 %87, i1* %sf
  %88 = trunc i32 %85 to i8
  %89 = call i8 @llvm.ctpop.i8(i8 %88)
  %90 = and i8 %89, 1
  %91 = icmp eq i8 %90, 0
  store i1 %91, i1* %pf
  store volatile i64 54210, i64* @assembly_address
  %92 = load i1* %zf
  %93 = icmp eq i1 %92, false
  br i1 %93, label %block_d44c, label %block_d3c8

block_d3c8:                                       ; preds = %block_d3ba
  store volatile i64 54216, i64* @assembly_address
  %94 = load i8* bitcast (i64* @global_var_25f50e to i8*)
  %95 = zext i8 %94 to i64
  store i64 %95, i64* %rax
  store volatile i64 54223, i64* @assembly_address
  %96 = load i64* %rax
  %97 = trunc i64 %96 to i8
  %98 = zext i8 %97 to i64
  store i64 %98, i64* %rax
  store volatile i64 54226, i64* @assembly_address
  %99 = load i8* bitcast (i64* @global_var_25f50f to i8*)
  %100 = zext i8 %99 to i64
  store i64 %100, i64* %rdx
  store volatile i64 54233, i64* @assembly_address
  %101 = load i64* %rdx
  %102 = trunc i64 %101 to i8
  %103 = zext i8 %102 to i64
  store i64 %103, i64* %rdx
  store volatile i64 54236, i64* @assembly_address
  %104 = load i64* %rdx
  %105 = trunc i64 %104 to i32
  %106 = load i1* %of
  %107 = shl i32 %105, 8
  %108 = icmp eq i32 %107, 0
  store i1 %108, i1* %zf
  %109 = icmp slt i32 %107, 0
  store i1 %109, i1* %sf
  %110 = trunc i32 %107 to i8
  %111 = call i8 @llvm.ctpop.i8(i8 %110)
  %112 = and i8 %111, 1
  %113 = icmp eq i8 %112, 0
  store i1 %113, i1* %pf
  %114 = zext i32 %107 to i64
  store i64 %114, i64* %rdx
  %115 = shl i32 %105, 7
  %116 = lshr i32 %115, 31
  %117 = trunc i32 %116 to i1
  store i1 %117, i1* %cf
  %118 = lshr i32 %107, 31
  %119 = icmp ne i32 %118, %116
  %120 = select i1 false, i1 %119, i1 %106
  store i1 %120, i1* %of
  store volatile i64 54239, i64* @assembly_address
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i32
  %123 = load i64* %rdx
  %124 = trunc i64 %123 to i32
  %125 = or i32 %122, %124
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %126 = icmp eq i32 %125, 0
  store i1 %126, i1* %zf
  %127 = icmp slt i32 %125, 0
  store i1 %127, i1* %sf
  %128 = trunc i32 %125 to i8
  %129 = call i8 @llvm.ctpop.i8(i8 %128)
  %130 = and i8 %129, 1
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %pf
  %132 = zext i32 %125 to i64
  store i64 %132, i64* %rax
  store volatile i64 54241, i64* @assembly_address
  %133 = load i64* %rax
  %134 = trunc i64 %133 to i32
  %135 = sext i32 %134 to i64
  store i64 %135, i64* %rax
  store volatile i64 54243, i64* @assembly_address
  %136 = load i8* bitcast (i64* @global_var_25f510 to i8*)
  %137 = zext i8 %136 to i64
  store i64 %137, i64* %rdx
  store volatile i64 54250, i64* @assembly_address
  %138 = load i64* %rdx
  %139 = trunc i64 %138 to i8
  %140 = zext i8 %139 to i64
  store i64 %140, i64* %rdx
  store volatile i64 54253, i64* @assembly_address
  %141 = load i8* bitcast (i64* @global_var_25f511 to i8*)
  %142 = zext i8 %141 to i64
  store i64 %142, i64* %rcx
  store volatile i64 54260, i64* @assembly_address
  %143 = load i64* %rcx
  %144 = trunc i64 %143 to i8
  %145 = zext i8 %144 to i64
  store i64 %145, i64* %rcx
  store volatile i64 54263, i64* @assembly_address
  %146 = load i64* %rcx
  %147 = trunc i64 %146 to i32
  %148 = load i1* %of
  %149 = shl i32 %147, 8
  %150 = icmp eq i32 %149, 0
  store i1 %150, i1* %zf
  %151 = icmp slt i32 %149, 0
  store i1 %151, i1* %sf
  %152 = trunc i32 %149 to i8
  %153 = call i8 @llvm.ctpop.i8(i8 %152)
  %154 = and i8 %153, 1
  %155 = icmp eq i8 %154, 0
  store i1 %155, i1* %pf
  %156 = zext i32 %149 to i64
  store i64 %156, i64* %rcx
  %157 = shl i32 %147, 7
  %158 = lshr i32 %157, 31
  %159 = trunc i32 %158 to i1
  store i1 %159, i1* %cf
  %160 = lshr i32 %149, 31
  %161 = icmp ne i32 %160, %158
  %162 = select i1 false, i1 %161, i1 %148
  store i1 %162, i1* %of
  store volatile i64 54266, i64* @assembly_address
  %163 = load i64* %rdx
  %164 = trunc i64 %163 to i32
  %165 = load i64* %rcx
  %166 = trunc i64 %165 to i32
  %167 = or i32 %164, %166
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %168 = icmp eq i32 %167, 0
  store i1 %168, i1* %zf
  %169 = icmp slt i32 %167, 0
  store i1 %169, i1* %sf
  %170 = trunc i32 %167 to i8
  %171 = call i8 @llvm.ctpop.i8(i8 %170)
  %172 = and i8 %171, 1
  %173 = icmp eq i8 %172, 0
  store i1 %173, i1* %pf
  %174 = zext i32 %167 to i64
  store i64 %174, i64* %rdx
  store volatile i64 54268, i64* @assembly_address
  %175 = load i64* %rdx
  %176 = trunc i64 %175 to i32
  %177 = sext i32 %176 to i64
  store i64 %177, i64* %rdx
  store volatile i64 54271, i64* @assembly_address
  %178 = load i64* %rdx
  %179 = load i1* %of
  %180 = shl i64 %178, 16
  %181 = icmp eq i64 %180, 0
  store i1 %181, i1* %zf
  %182 = icmp slt i64 %180, 0
  store i1 %182, i1* %sf
  %183 = trunc i64 %180 to i8
  %184 = call i8 @llvm.ctpop.i8(i8 %183)
  %185 = and i8 %184, 1
  %186 = icmp eq i8 %185, 0
  store i1 %186, i1* %pf
  store i64 %180, i64* %rdx
  %187 = shl i64 %178, 15
  %188 = lshr i64 %187, 63
  %189 = trunc i64 %188 to i1
  store i1 %189, i1* %cf
  %190 = lshr i64 %180, 63
  %191 = icmp ne i64 %190, %188
  %192 = select i1 false, i1 %191, i1 %179
  store i1 %192, i1* %of
  store volatile i64 54275, i64* @assembly_address
  %193 = load i64* %rax
  %194 = load i64* %rdx
  %195 = or i64 %193, %194
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %196 = icmp eq i64 %195, 0
  store i1 %196, i1* %zf
  %197 = icmp slt i64 %195, 0
  store i1 %197, i1* %sf
  %198 = trunc i64 %195 to i8
  %199 = call i8 @llvm.ctpop.i8(i8 %198)
  %200 = and i8 %199, 1
  %201 = icmp eq i8 %200, 0
  store i1 %201, i1* %pf
  store i64 %195, i64* %rax
  store volatile i64 54278, i64* @assembly_address
  %202 = load i64* %rax
  store i64 %202, i64* %stack_var_-72
  store volatile i64 54282, i64* @assembly_address
  %203 = load i8* bitcast (i64* @global_var_25f516 to i8*)
  %204 = zext i8 %203 to i64
  store i64 %204, i64* %rax
  store volatile i64 54289, i64* @assembly_address
  %205 = load i64* %rax
  %206 = trunc i64 %205 to i8
  %207 = zext i8 %206 to i64
  store i64 %207, i64* %rax
  store volatile i64 54292, i64* @assembly_address
  %208 = load i8* bitcast (i64* @global_var_25f517 to i8*)
  %209 = zext i8 %208 to i64
  store i64 %209, i64* %rdx
  store volatile i64 54299, i64* @assembly_address
  %210 = load i64* %rdx
  %211 = trunc i64 %210 to i8
  %212 = zext i8 %211 to i64
  store i64 %212, i64* %rdx
  store volatile i64 54302, i64* @assembly_address
  %213 = load i64* %rdx
  %214 = trunc i64 %213 to i32
  %215 = load i1* %of
  %216 = shl i32 %214, 8
  %217 = icmp eq i32 %216, 0
  store i1 %217, i1* %zf
  %218 = icmp slt i32 %216, 0
  store i1 %218, i1* %sf
  %219 = trunc i32 %216 to i8
  %220 = call i8 @llvm.ctpop.i8(i8 %219)
  %221 = and i8 %220, 1
  %222 = icmp eq i8 %221, 0
  store i1 %222, i1* %pf
  %223 = zext i32 %216 to i64
  store i64 %223, i64* %rdx
  %224 = shl i32 %214, 7
  %225 = lshr i32 %224, 31
  %226 = trunc i32 %225 to i1
  store i1 %226, i1* %cf
  %227 = lshr i32 %216, 31
  %228 = icmp ne i32 %227, %225
  %229 = select i1 false, i1 %228, i1 %215
  store i1 %229, i1* %of
  store volatile i64 54305, i64* @assembly_address
  %230 = load i64* %rax
  %231 = trunc i64 %230 to i32
  %232 = load i64* %rdx
  %233 = trunc i64 %232 to i32
  %234 = or i32 %231, %233
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %235 = icmp eq i32 %234, 0
  store i1 %235, i1* %zf
  %236 = icmp slt i32 %234, 0
  store i1 %236, i1* %sf
  %237 = trunc i32 %234 to i8
  %238 = call i8 @llvm.ctpop.i8(i8 %237)
  %239 = and i8 %238, 1
  %240 = icmp eq i8 %239, 0
  store i1 %240, i1* %pf
  %241 = zext i32 %234 to i64
  store i64 %241, i64* %rax
  store volatile i64 54307, i64* @assembly_address
  %242 = load i64* %rax
  %243 = trunc i64 %242 to i32
  %244 = sext i32 %243 to i64
  store i64 %244, i64* %rax
  store volatile i64 54309, i64* @assembly_address
  %245 = load i8* bitcast (i64* @global_var_25f518 to i8*)
  %246 = zext i8 %245 to i64
  store i64 %246, i64* %rdx
  store volatile i64 54316, i64* @assembly_address
  %247 = load i64* %rdx
  %248 = trunc i64 %247 to i8
  %249 = zext i8 %248 to i64
  store i64 %249, i64* %rdx
  store volatile i64 54319, i64* @assembly_address
  %250 = load i8* bitcast (i64* @global_var_25f519 to i8*)
  %251 = zext i8 %250 to i64
  store i64 %251, i64* %rcx
  store volatile i64 54326, i64* @assembly_address
  %252 = load i64* %rcx
  %253 = trunc i64 %252 to i8
  %254 = zext i8 %253 to i64
  store i64 %254, i64* %rcx
  store volatile i64 54329, i64* @assembly_address
  %255 = load i64* %rcx
  %256 = trunc i64 %255 to i32
  %257 = load i1* %of
  %258 = shl i32 %256, 8
  %259 = icmp eq i32 %258, 0
  store i1 %259, i1* %zf
  %260 = icmp slt i32 %258, 0
  store i1 %260, i1* %sf
  %261 = trunc i32 %258 to i8
  %262 = call i8 @llvm.ctpop.i8(i8 %261)
  %263 = and i8 %262, 1
  %264 = icmp eq i8 %263, 0
  store i1 %264, i1* %pf
  %265 = zext i32 %258 to i64
  store i64 %265, i64* %rcx
  %266 = shl i32 %256, 7
  %267 = lshr i32 %266, 31
  %268 = trunc i32 %267 to i1
  store i1 %268, i1* %cf
  %269 = lshr i32 %258, 31
  %270 = icmp ne i32 %269, %267
  %271 = select i1 false, i1 %270, i1 %257
  store i1 %271, i1* %of
  store volatile i64 54332, i64* @assembly_address
  %272 = load i64* %rdx
  %273 = trunc i64 %272 to i32
  %274 = load i64* %rcx
  %275 = trunc i64 %274 to i32
  %276 = or i32 %273, %275
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %277 = icmp eq i32 %276, 0
  store i1 %277, i1* %zf
  %278 = icmp slt i32 %276, 0
  store i1 %278, i1* %sf
  %279 = trunc i32 %276 to i8
  %280 = call i8 @llvm.ctpop.i8(i8 %279)
  %281 = and i8 %280, 1
  %282 = icmp eq i8 %281, 0
  store i1 %282, i1* %pf
  %283 = zext i32 %276 to i64
  store i64 %283, i64* %rdx
  store volatile i64 54334, i64* @assembly_address
  %284 = load i64* %rdx
  %285 = trunc i64 %284 to i32
  %286 = sext i32 %285 to i64
  store i64 %286, i64* %rdx
  store volatile i64 54337, i64* @assembly_address
  %287 = load i64* %rdx
  %288 = load i1* %of
  %289 = shl i64 %287, 16
  %290 = icmp eq i64 %289, 0
  store i1 %290, i1* %zf
  %291 = icmp slt i64 %289, 0
  store i1 %291, i1* %sf
  %292 = trunc i64 %289 to i8
  %293 = call i8 @llvm.ctpop.i8(i8 %292)
  %294 = and i8 %293, 1
  %295 = icmp eq i8 %294, 0
  store i1 %295, i1* %pf
  store i64 %289, i64* %rdx
  %296 = shl i64 %287, 15
  %297 = lshr i64 %296, 63
  %298 = trunc i64 %297 to i1
  store i1 %298, i1* %cf
  %299 = lshr i64 %289, 63
  %300 = icmp ne i64 %299, %297
  %301 = select i1 false, i1 %300, i1 %288
  store i1 %301, i1* %of
  store volatile i64 54341, i64* @assembly_address
  %302 = load i64* %rax
  %303 = load i64* %rdx
  %304 = or i64 %302, %303
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %305 = icmp eq i64 %304, 0
  store i1 %305, i1* %zf
  %306 = icmp slt i64 %304, 0
  store i1 %306, i1* %sf
  %307 = trunc i64 %304 to i8
  %308 = call i8 @llvm.ctpop.i8(i8 %307)
  %309 = and i8 %308, 1
  %310 = icmp eq i8 %309, 0
  store i1 %310, i1* %pf
  store i64 %304, i64* %rax
  store volatile i64 54344, i64* @assembly_address
  %311 = load i64* %rax
  store i64 %311, i64* %stack_var_-64
  br label %block_d44c

block_d44c:                                       ; preds = %block_d3c8, %block_d3ba, %block_d356
  store volatile i64 54348, i64* @assembly_address
  %312 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %313 = zext i32 %312 to i64
  store i64 %313, i64* %rax
  store volatile i64 54354, i64* @assembly_address
  %314 = load i64* %rax
  %315 = trunc i64 %314 to i32
  %316 = sub i32 %315, 8
  %317 = and i32 %315, 15
  %318 = sub i32 %317, 8
  %319 = icmp ugt i32 %318, 15
  %320 = icmp ult i32 %315, 8
  %321 = xor i32 %315, 8
  %322 = xor i32 %315, %316
  %323 = and i32 %321, %322
  %324 = icmp slt i32 %323, 0
  store i1 %319, i1* %az
  store i1 %320, i1* %cf
  store i1 %324, i1* %of
  %325 = icmp eq i32 %316, 0
  store i1 %325, i1* %zf
  %326 = icmp slt i32 %316, 0
  store i1 %326, i1* %sf
  %327 = trunc i32 %316 to i8
  %328 = call i8 @llvm.ctpop.i8(i8 %327)
  %329 = and i8 %328, 1
  %330 = icmp eq i8 %329, 0
  store i1 %330, i1* %pf
  store volatile i64 54357, i64* @assembly_address
  %331 = load i1* %zf
  %332 = icmp eq i1 %331, false
  br i1 %332, label %block_d480, label %block_d457

block_d457:                                       ; preds = %block_d44c
  store volatile i64 54359, i64* @assembly_address
  %333 = load i64* %rdi
  %334 = inttoptr i64 %333 to %z_stream_s*
  %335 = load i64* %rsi
  %336 = trunc i64 %335 to i32
  %337 = call i32 @inflate(%z_stream_s* %334, i32 %336)
  %338 = sext i32 %337 to i64
  store i64 %338, i64* %rax
  %339 = sext i32 %337 to i64
  store i64 %339, i64* %rax
  store volatile i64 54364, i64* @assembly_address
  %340 = load i64* %rax
  %341 = trunc i64 %340 to i32
  store i32 %341, i32* %stack_var_-76
  store volatile i64 54367, i64* @assembly_address
  %342 = load i32* %stack_var_-76
  %343 = sub i32 %342, 3
  %344 = and i32 %342, 15
  %345 = sub i32 %344, 3
  %346 = icmp ugt i32 %345, 15
  %347 = icmp ult i32 %342, 3
  %348 = xor i32 %342, 3
  %349 = xor i32 %342, %343
  %350 = and i32 %348, %349
  %351 = icmp slt i32 %350, 0
  store i1 %346, i1* %az
  store i1 %347, i1* %cf
  store i1 %351, i1* %of
  %352 = icmp eq i32 %343, 0
  store i1 %352, i1* %zf
  %353 = icmp slt i32 %343, 0
  store i1 %353, i1* %sf
  %354 = trunc i32 %343 to i8
  %355 = call i8 @llvm.ctpop.i8(i8 %354)
  %356 = and i8 %355, 1
  %357 = icmp eq i8 %356, 0
  store i1 %357, i1* %pf
  store volatile i64 54371, i64* @assembly_address
  %358 = load i1* %zf
  %359 = icmp eq i1 %358, false
  br i1 %359, label %block_d46a, label %block_d465

block_d465:                                       ; preds = %block_d457
  store volatile i64 54373, i64* @assembly_address
  %360 = call i64 @xalloc_die()
  store i64 %360, i64* %rax
  store i64 %360, i64* %rax
  store i64 %360, i64* %rax
  unreachable

block_d46a:                                       ; preds = %block_d457
  store volatile i64 54378, i64* @assembly_address
  %361 = load i32* %stack_var_-76
  %362 = and i32 %361, 15
  %363 = icmp ugt i32 %362, 15
  %364 = icmp ult i32 %361, 0
  %365 = xor i32 %361, 0
  %366 = and i32 %365, 0
  %367 = icmp slt i32 %366, 0
  store i1 %363, i1* %az
  store i1 %364, i1* %cf
  store i1 %367, i1* %of
  %368 = icmp eq i32 %361, 0
  store i1 %368, i1* %zf
  %369 = icmp slt i32 %361, 0
  store i1 %369, i1* %sf
  %370 = trunc i32 %361 to i8
  %371 = call i8 @llvm.ctpop.i8(i8 %370)
  %372 = and i8 %371, 1
  %373 = icmp eq i8 %372, 0
  store i1 %373, i1* %pf
  store volatile i64 54382, i64* @assembly_address
  %374 = load i1* %zf
  br i1 %374, label %block_d636, label %block_d474

block_d474:                                       ; preds = %block_d46a
  store volatile i64 54388, i64* @assembly_address
  store i64 ptrtoint ([41 x i8]* @global_var_122d8 to i64), i64* %rdi
  store volatile i64 54395, i64* @assembly_address
  %375 = load i64* %rdi
  %376 = inttoptr i64 %375 to i8*
  %377 = call i64 @gzip_error(i8* %376)
  store i64 %377, i64* %rax
  store i64 %377, i64* %rax
  unreachable

block_d480:                                       ; preds = %block_d44c
  store volatile i64 54400, i64* @assembly_address
  %378 = load i32* bitcast (i64* @global_var_21a400 to i32*)
  %379 = zext i32 %378 to i64
  store i64 %379, i64* %rax
  store volatile i64 54406, i64* @assembly_address
  %380 = load i64* %rax
  %381 = trunc i64 %380 to i32
  %382 = load i64* %rax
  %383 = trunc i64 %382 to i32
  %384 = and i32 %381, %383
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %385 = icmp eq i32 %384, 0
  store i1 %385, i1* %zf
  %386 = icmp slt i32 %384, 0
  store i1 %386, i1* %sf
  %387 = trunc i32 %384 to i8
  %388 = call i8 @llvm.ctpop.i8(i8 %387)
  %389 = and i8 %388, 1
  %390 = icmp eq i8 %389, 0
  store i1 %390, i1* %pf
  store volatile i64 54408, i64* @assembly_address
  %391 = load i1* %zf
  br i1 %391, label %block_d62a, label %block_d48e

block_d48e:                                       ; preds = %block_d480
  store volatile i64 54414, i64* @assembly_address
  %392 = load i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  %393 = zext i32 %392 to i64
  store i64 %393, i64* %rax
  store volatile i64 54420, i64* @assembly_address
  %394 = load i64* %rax
  %395 = trunc i64 %394 to i32
  %396 = load i64* %rax
  %397 = trunc i64 %396 to i32
  %398 = and i32 %395, %397
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %399 = icmp eq i32 %398, 0
  store i1 %399, i1* %zf
  %400 = icmp slt i32 %398, 0
  store i1 %400, i1* %sf
  %401 = trunc i32 %398 to i8
  %402 = call i8 @llvm.ctpop.i8(i8 %401)
  %403 = and i8 %402, 1
  %404 = icmp eq i8 %403, 0
  store i1 %404, i1* %pf
  store volatile i64 54422, i64* @assembly_address
  %405 = load i1* %zf
  %406 = icmp eq i1 %405, false
  br i1 %406, label %block_d62a, label %block_d49c

block_d49c:                                       ; preds = %block_d48e
  store volatile i64 54428, i64* @assembly_address
  %407 = load i8* bitcast (i64* @global_var_25f516 to i8*)
  %408 = zext i8 %407 to i64
  store i64 %408, i64* %rax
  store volatile i64 54435, i64* @assembly_address
  %409 = load i64* %rax
  %410 = trunc i64 %409 to i8
  %411 = zext i8 %410 to i64
  store i64 %411, i64* %rax
  store volatile i64 54438, i64* @assembly_address
  %412 = load i8* bitcast (i64* @global_var_25f517 to i8*)
  %413 = zext i8 %412 to i64
  store i64 %413, i64* %rdx
  store volatile i64 54445, i64* @assembly_address
  %414 = load i64* %rdx
  %415 = trunc i64 %414 to i8
  %416 = zext i8 %415 to i64
  store i64 %416, i64* %rdx
  store volatile i64 54448, i64* @assembly_address
  %417 = load i64* %rdx
  %418 = trunc i64 %417 to i32
  %419 = load i1* %of
  %420 = shl i32 %418, 8
  %421 = icmp eq i32 %420, 0
  store i1 %421, i1* %zf
  %422 = icmp slt i32 %420, 0
  store i1 %422, i1* %sf
  %423 = trunc i32 %420 to i8
  %424 = call i8 @llvm.ctpop.i8(i8 %423)
  %425 = and i8 %424, 1
  %426 = icmp eq i8 %425, 0
  store i1 %426, i1* %pf
  %427 = zext i32 %420 to i64
  store i64 %427, i64* %rdx
  %428 = shl i32 %418, 7
  %429 = lshr i32 %428, 31
  %430 = trunc i32 %429 to i1
  store i1 %430, i1* %cf
  %431 = lshr i32 %420, 31
  %432 = icmp ne i32 %431, %429
  %433 = select i1 false, i1 %432, i1 %419
  store i1 %433, i1* %of
  store volatile i64 54451, i64* @assembly_address
  %434 = load i64* %rax
  %435 = trunc i64 %434 to i32
  %436 = load i64* %rdx
  %437 = trunc i64 %436 to i32
  %438 = or i32 %435, %437
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %439 = icmp eq i32 %438, 0
  store i1 %439, i1* %zf
  %440 = icmp slt i32 %438, 0
  store i1 %440, i1* %sf
  %441 = trunc i32 %438 to i8
  %442 = call i8 @llvm.ctpop.i8(i8 %441)
  %443 = and i8 %442, 1
  %444 = icmp eq i8 %443, 0
  store i1 %444, i1* %pf
  %445 = zext i32 %438 to i64
  store i64 %445, i64* %rax
  store volatile i64 54453, i64* @assembly_address
  %446 = load i64* %rax
  %447 = trunc i64 %446 to i32
  %448 = sext i32 %447 to i64
  store i64 %448, i64* %rax
  store volatile i64 54455, i64* @assembly_address
  %449 = load i8* bitcast (i64* @global_var_25f518 to i8*)
  %450 = zext i8 %449 to i64
  store i64 %450, i64* %rdx
  store volatile i64 54462, i64* @assembly_address
  %451 = load i64* %rdx
  %452 = trunc i64 %451 to i8
  %453 = zext i8 %452 to i64
  store i64 %453, i64* %rdx
  store volatile i64 54465, i64* @assembly_address
  %454 = load i8* bitcast (i64* @global_var_25f519 to i8*)
  %455 = zext i8 %454 to i64
  store i64 %455, i64* %rcx
  store volatile i64 54472, i64* @assembly_address
  %456 = load i64* %rcx
  %457 = trunc i64 %456 to i8
  %458 = zext i8 %457 to i64
  store i64 %458, i64* %rcx
  store volatile i64 54475, i64* @assembly_address
  %459 = load i64* %rcx
  %460 = trunc i64 %459 to i32
  %461 = load i1* %of
  %462 = shl i32 %460, 8
  %463 = icmp eq i32 %462, 0
  store i1 %463, i1* %zf
  %464 = icmp slt i32 %462, 0
  store i1 %464, i1* %sf
  %465 = trunc i32 %462 to i8
  %466 = call i8 @llvm.ctpop.i8(i8 %465)
  %467 = and i8 %466, 1
  %468 = icmp eq i8 %467, 0
  store i1 %468, i1* %pf
  %469 = zext i32 %462 to i64
  store i64 %469, i64* %rcx
  %470 = shl i32 %460, 7
  %471 = lshr i32 %470, 31
  %472 = trunc i32 %471 to i1
  store i1 %472, i1* %cf
  %473 = lshr i32 %462, 31
  %474 = icmp ne i32 %473, %471
  %475 = select i1 false, i1 %474, i1 %461
  store i1 %475, i1* %of
  store volatile i64 54478, i64* @assembly_address
  %476 = load i64* %rdx
  %477 = trunc i64 %476 to i32
  %478 = load i64* %rcx
  %479 = trunc i64 %478 to i32
  %480 = or i32 %477, %479
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %481 = icmp eq i32 %480, 0
  store i1 %481, i1* %zf
  %482 = icmp slt i32 %480, 0
  store i1 %482, i1* %sf
  %483 = trunc i32 %480 to i8
  %484 = call i8 @llvm.ctpop.i8(i8 %483)
  %485 = and i8 %484, 1
  %486 = icmp eq i8 %485, 0
  store i1 %486, i1* %pf
  %487 = zext i32 %480 to i64
  store i64 %487, i64* %rdx
  store volatile i64 54480, i64* @assembly_address
  %488 = load i64* %rdx
  %489 = trunc i64 %488 to i32
  %490 = sext i32 %489 to i64
  store i64 %490, i64* %rdx
  store volatile i64 54483, i64* @assembly_address
  %491 = load i64* %rdx
  %492 = load i1* %of
  %493 = shl i64 %491, 16
  %494 = icmp eq i64 %493, 0
  store i1 %494, i1* %zf
  %495 = icmp slt i64 %493, 0
  store i1 %495, i1* %sf
  %496 = trunc i64 %493 to i8
  %497 = call i8 @llvm.ctpop.i8(i8 %496)
  %498 = and i8 %497, 1
  %499 = icmp eq i8 %498, 0
  store i1 %499, i1* %pf
  store i64 %493, i64* %rdx
  %500 = shl i64 %491, 15
  %501 = lshr i64 %500, 63
  %502 = trunc i64 %501 to i1
  store i1 %502, i1* %cf
  %503 = lshr i64 %493, 63
  %504 = icmp ne i64 %503, %501
  %505 = select i1 false, i1 %504, i1 %492
  store i1 %505, i1* %of
  store volatile i64 54487, i64* @assembly_address
  %506 = load i64* %rax
  %507 = load i64* %rdx
  %508 = or i64 %506, %507
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %509 = icmp eq i64 %508, 0
  store i1 %509, i1* %zf
  %510 = icmp slt i64 %508, 0
  store i1 %510, i1* %sf
  %511 = trunc i64 %508 to i8
  %512 = call i8 @llvm.ctpop.i8(i8 %511)
  %513 = and i8 %512, 1
  %514 = icmp eq i8 %513, 0
  store i1 %514, i1* %pf
  store i64 %508, i64* %rax
  store volatile i64 54490, i64* @assembly_address
  %515 = load i64* %rax
  store i64 %515, i64* %rbx
  store volatile i64 54493, i64* @assembly_address
  %516 = load i8* bitcast (i64* @global_var_25f512 to i8*)
  %517 = zext i8 %516 to i64
  store i64 %517, i64* %rax
  store volatile i64 54500, i64* @assembly_address
  %518 = load i64* %rax
  %519 = trunc i64 %518 to i8
  %520 = zext i8 %519 to i64
  store i64 %520, i64* %rax
  store volatile i64 54503, i64* @assembly_address
  %521 = load i8* bitcast (i64* @global_var_25f513 to i8*)
  %522 = zext i8 %521 to i64
  store i64 %522, i64* %rdx
  store volatile i64 54510, i64* @assembly_address
  %523 = load i64* %rdx
  %524 = trunc i64 %523 to i8
  %525 = zext i8 %524 to i64
  store i64 %525, i64* %rdx
  store volatile i64 54513, i64* @assembly_address
  %526 = load i64* %rdx
  %527 = trunc i64 %526 to i32
  %528 = load i1* %of
  %529 = shl i32 %527, 8
  %530 = icmp eq i32 %529, 0
  store i1 %530, i1* %zf
  %531 = icmp slt i32 %529, 0
  store i1 %531, i1* %sf
  %532 = trunc i32 %529 to i8
  %533 = call i8 @llvm.ctpop.i8(i8 %532)
  %534 = and i8 %533, 1
  %535 = icmp eq i8 %534, 0
  store i1 %535, i1* %pf
  %536 = zext i32 %529 to i64
  store i64 %536, i64* %rdx
  %537 = shl i32 %527, 7
  %538 = lshr i32 %537, 31
  %539 = trunc i32 %538 to i1
  store i1 %539, i1* %cf
  %540 = lshr i32 %529, 31
  %541 = icmp ne i32 %540, %538
  %542 = select i1 false, i1 %541, i1 %528
  store i1 %542, i1* %of
  store volatile i64 54516, i64* @assembly_address
  %543 = load i64* %rax
  %544 = trunc i64 %543 to i32
  %545 = load i64* %rdx
  %546 = trunc i64 %545 to i32
  %547 = or i32 %544, %546
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %548 = icmp eq i32 %547, 0
  store i1 %548, i1* %zf
  %549 = icmp slt i32 %547, 0
  store i1 %549, i1* %sf
  %550 = trunc i32 %547 to i8
  %551 = call i8 @llvm.ctpop.i8(i8 %550)
  %552 = and i8 %551, 1
  %553 = icmp eq i8 %552, 0
  store i1 %553, i1* %pf
  %554 = zext i32 %547 to i64
  store i64 %554, i64* %rax
  store volatile i64 54518, i64* @assembly_address
  %555 = load i64* %rax
  %556 = trunc i64 %555 to i32
  %557 = sext i32 %556 to i64
  store i64 %557, i64* %rax
  store volatile i64 54520, i64* @assembly_address
  %558 = load i8* bitcast (i64* @global_var_25f514 to i8*)
  %559 = zext i8 %558 to i64
  store i64 %559, i64* %rdx
  store volatile i64 54527, i64* @assembly_address
  %560 = load i64* %rdx
  %561 = trunc i64 %560 to i8
  %562 = zext i8 %561 to i64
  store i64 %562, i64* %rdx
  store volatile i64 54530, i64* @assembly_address
  %563 = load i8* bitcast (i64* @global_var_25f515 to i8*)
  %564 = zext i8 %563 to i64
  store i64 %564, i64* %rcx
  store volatile i64 54537, i64* @assembly_address
  %565 = load i64* %rcx
  %566 = trunc i64 %565 to i8
  %567 = zext i8 %566 to i64
  store i64 %567, i64* %rcx
  store volatile i64 54540, i64* @assembly_address
  %568 = load i64* %rcx
  %569 = trunc i64 %568 to i32
  %570 = load i1* %of
  %571 = shl i32 %569, 8
  %572 = icmp eq i32 %571, 0
  store i1 %572, i1* %zf
  %573 = icmp slt i32 %571, 0
  store i1 %573, i1* %sf
  %574 = trunc i32 %571 to i8
  %575 = call i8 @llvm.ctpop.i8(i8 %574)
  %576 = and i8 %575, 1
  %577 = icmp eq i8 %576, 0
  store i1 %577, i1* %pf
  %578 = zext i32 %571 to i64
  store i64 %578, i64* %rcx
  %579 = shl i32 %569, 7
  %580 = lshr i32 %579, 31
  %581 = trunc i32 %580 to i1
  store i1 %581, i1* %cf
  %582 = lshr i32 %571, 31
  %583 = icmp ne i32 %582, %580
  %584 = select i1 false, i1 %583, i1 %570
  store i1 %584, i1* %of
  store volatile i64 54543, i64* @assembly_address
  %585 = load i64* %rdx
  %586 = trunc i64 %585 to i32
  %587 = load i64* %rcx
  %588 = trunc i64 %587 to i32
  %589 = or i32 %586, %588
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %590 = icmp eq i32 %589, 0
  store i1 %590, i1* %zf
  %591 = icmp slt i32 %589, 0
  store i1 %591, i1* %sf
  %592 = trunc i32 %589 to i8
  %593 = call i8 @llvm.ctpop.i8(i8 %592)
  %594 = and i8 %593, 1
  %595 = icmp eq i8 %594, 0
  store i1 %595, i1* %pf
  %596 = zext i32 %589 to i64
  store i64 %596, i64* %rdx
  store volatile i64 54545, i64* @assembly_address
  %597 = load i64* %rdx
  %598 = trunc i64 %597 to i32
  %599 = sext i32 %598 to i64
  store i64 %599, i64* %rdx
  store volatile i64 54548, i64* @assembly_address
  %600 = load i64* %rdx
  %601 = load i1* %of
  %602 = shl i64 %600, 16
  %603 = icmp eq i64 %602, 0
  store i1 %603, i1* %zf
  %604 = icmp slt i64 %602, 0
  store i1 %604, i1* %sf
  %605 = trunc i64 %602 to i8
  %606 = call i8 @llvm.ctpop.i8(i8 %605)
  %607 = and i8 %606, 1
  %608 = icmp eq i8 %607, 0
  store i1 %608, i1* %pf
  store i64 %602, i64* %rdx
  %609 = shl i64 %600, 15
  %610 = lshr i64 %609, 63
  %611 = trunc i64 %610 to i1
  store i1 %611, i1* %cf
  %612 = lshr i64 %602, 63
  %613 = icmp ne i64 %612, %610
  %614 = select i1 false, i1 %613, i1 %601
  store i1 %614, i1* %of
  store volatile i64 54552, i64* @assembly_address
  %615 = load i64* %rdx
  %616 = load i64* %rax
  %617 = or i64 %615, %616
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %618 = icmp eq i64 %617, 0
  store i1 %618, i1* %zf
  %619 = icmp slt i64 %617, 0
  store i1 %619, i1* %sf
  %620 = trunc i64 %617 to i8
  %621 = call i8 @llvm.ctpop.i8(i8 %620)
  %622 = and i8 %621, 1
  %623 = icmp eq i8 %622, 0
  store i1 %623, i1* %pf
  store i64 %617, i64* %rdx
  store volatile i64 54555, i64* @assembly_address
  %624 = load i32* bitcast (i64* @global_var_21a3fc to i32*)
  %625 = zext i32 %624 to i64
  store i64 %625, i64* %rax
  store volatile i64 54561, i64* @assembly_address
  %626 = load i64* %rax
  %627 = trunc i64 %626 to i32
  %628 = load i64* %rax
  %629 = trunc i64 %628 to i32
  %630 = and i32 %627, %629
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %631 = icmp eq i32 %630, 0
  store i1 %631, i1* %zf
  %632 = icmp slt i32 %630, 0
  store i1 %632, i1* %sf
  %633 = trunc i32 %630 to i8
  %634 = call i8 @llvm.ctpop.i8(i8 %633)
  %635 = and i8 %634, 1
  %636 = icmp eq i8 %635, 0
  store i1 %636, i1* %pf
  store volatile i64 54563, i64* @assembly_address
  %637 = load i1* %zf
  br i1 %637, label %block_d52c, label %block_d525

block_d525:                                       ; preds = %block_d49c
  store volatile i64 54565, i64* @assembly_address
  store i64 12, i64* %rax
  store volatile i64 54570, i64* @assembly_address
  br label %block_d531

block_d52c:                                       ; preds = %block_d49c
  store volatile i64 54572, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_d531

block_d531:                                       ; preds = %block_d52c, %block_d525
  store volatile i64 54577, i64* @assembly_address
  %638 = load i64* %rdx
  %639 = load i64* %rax
  %640 = sub i64 %638, %639
  %641 = and i64 %638, 15
  %642 = and i64 %639, 15
  %643 = sub i64 %641, %642
  %644 = icmp ugt i64 %643, 15
  %645 = icmp ult i64 %638, %639
  %646 = xor i64 %638, %639
  %647 = xor i64 %638, %640
  %648 = and i64 %646, %647
  %649 = icmp slt i64 %648, 0
  store i1 %644, i1* %az
  store i1 %645, i1* %cf
  store i1 %649, i1* %of
  %650 = icmp eq i64 %640, 0
  store i1 %650, i1* %zf
  %651 = icmp slt i64 %640, 0
  store i1 %651, i1* %sf
  %652 = trunc i64 %640 to i8
  %653 = call i8 @llvm.ctpop.i8(i8 %652)
  %654 = and i8 %653, 1
  %655 = icmp eq i8 %654, 0
  store i1 %655, i1* %pf
  store i64 %640, i64* %rdx
  store volatile i64 54580, i64* @assembly_address
  %656 = load i64* %rdx
  store i64 %656, i64* %rax
  store volatile i64 54583, i64* @assembly_address
  %657 = load i64* %rbx
  %658 = load i64* %rax
  %659 = sub i64 %657, %658
  %660 = and i64 %657, 15
  %661 = and i64 %658, 15
  %662 = sub i64 %660, %661
  %663 = icmp ugt i64 %662, 15
  %664 = icmp ult i64 %657, %658
  %665 = xor i64 %657, %658
  %666 = xor i64 %657, %659
  %667 = and i64 %665, %666
  %668 = icmp slt i64 %667, 0
  store i1 %663, i1* %az
  store i1 %664, i1* %cf
  store i1 %668, i1* %of
  %669 = icmp eq i64 %659, 0
  store i1 %669, i1* %zf
  %670 = icmp slt i64 %659, 0
  store i1 %670, i1* %sf
  %671 = trunc i64 %659 to i8
  %672 = call i8 @llvm.ctpop.i8(i8 %671)
  %673 = and i8 %672, 1
  %674 = icmp eq i8 %673, 0
  store i1 %674, i1* %pf
  store volatile i64 54586, i64* @assembly_address
  %675 = load i1* %zf
  br i1 %675, label %block_d617, label %block_d540

block_d540:                                       ; preds = %block_d531
  store volatile i64 54592, i64* @assembly_address
  %676 = load i8* bitcast (i64* @global_var_25f512 to i8*)
  %677 = zext i8 %676 to i64
  store i64 %677, i64* %rax
  store volatile i64 54599, i64* @assembly_address
  %678 = load i64* %rax
  %679 = trunc i64 %678 to i8
  %680 = zext i8 %679 to i64
  store i64 %680, i64* %rax
  store volatile i64 54602, i64* @assembly_address
  %681 = load i8* bitcast (i64* @global_var_25f513 to i8*)
  %682 = zext i8 %681 to i64
  store i64 %682, i64* %rdx
  store volatile i64 54609, i64* @assembly_address
  %683 = load i64* %rdx
  %684 = trunc i64 %683 to i8
  %685 = zext i8 %684 to i64
  store i64 %685, i64* %rdx
  store volatile i64 54612, i64* @assembly_address
  %686 = load i64* %rdx
  %687 = trunc i64 %686 to i32
  %688 = load i1* %of
  %689 = shl i32 %687, 8
  %690 = icmp eq i32 %689, 0
  store i1 %690, i1* %zf
  %691 = icmp slt i32 %689, 0
  store i1 %691, i1* %sf
  %692 = trunc i32 %689 to i8
  %693 = call i8 @llvm.ctpop.i8(i8 %692)
  %694 = and i8 %693, 1
  %695 = icmp eq i8 %694, 0
  store i1 %695, i1* %pf
  %696 = zext i32 %689 to i64
  store i64 %696, i64* %rdx
  %697 = shl i32 %687, 7
  %698 = lshr i32 %697, 31
  %699 = trunc i32 %698 to i1
  store i1 %699, i1* %cf
  %700 = lshr i32 %689, 31
  %701 = icmp ne i32 %700, %698
  %702 = select i1 false, i1 %701, i1 %688
  store i1 %702, i1* %of
  store volatile i64 54615, i64* @assembly_address
  %703 = load i64* %rax
  %704 = trunc i64 %703 to i32
  %705 = load i64* %rdx
  %706 = trunc i64 %705 to i32
  %707 = or i32 %704, %706
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %708 = icmp eq i32 %707, 0
  store i1 %708, i1* %zf
  %709 = icmp slt i32 %707, 0
  store i1 %709, i1* %sf
  %710 = trunc i32 %707 to i8
  %711 = call i8 @llvm.ctpop.i8(i8 %710)
  %712 = and i8 %711, 1
  %713 = icmp eq i8 %712, 0
  store i1 %713, i1* %pf
  %714 = zext i32 %707 to i64
  store i64 %714, i64* %rax
  store volatile i64 54617, i64* @assembly_address
  %715 = load i64* %rax
  %716 = trunc i64 %715 to i32
  %717 = sext i32 %716 to i64
  store i64 %717, i64* %rax
  store volatile i64 54619, i64* @assembly_address
  %718 = load i8* bitcast (i64* @global_var_25f514 to i8*)
  %719 = zext i8 %718 to i64
  store i64 %719, i64* %rdx
  store volatile i64 54626, i64* @assembly_address
  %720 = load i64* %rdx
  %721 = trunc i64 %720 to i8
  %722 = zext i8 %721 to i64
  store i64 %722, i64* %rdx
  store volatile i64 54629, i64* @assembly_address
  %723 = load i8* bitcast (i64* @global_var_25f515 to i8*)
  %724 = zext i8 %723 to i64
  store i64 %724, i64* %rcx
  store volatile i64 54636, i64* @assembly_address
  %725 = load i64* %rcx
  %726 = trunc i64 %725 to i8
  %727 = zext i8 %726 to i64
  store i64 %727, i64* %rcx
  store volatile i64 54639, i64* @assembly_address
  %728 = load i64* %rcx
  %729 = trunc i64 %728 to i32
  %730 = load i1* %of
  %731 = shl i32 %729, 8
  %732 = icmp eq i32 %731, 0
  store i1 %732, i1* %zf
  %733 = icmp slt i32 %731, 0
  store i1 %733, i1* %sf
  %734 = trunc i32 %731 to i8
  %735 = call i8 @llvm.ctpop.i8(i8 %734)
  %736 = and i8 %735, 1
  %737 = icmp eq i8 %736, 0
  store i1 %737, i1* %pf
  %738 = zext i32 %731 to i64
  store i64 %738, i64* %rcx
  %739 = shl i32 %729, 7
  %740 = lshr i32 %739, 31
  %741 = trunc i32 %740 to i1
  store i1 %741, i1* %cf
  %742 = lshr i32 %731, 31
  %743 = icmp ne i32 %742, %740
  %744 = select i1 false, i1 %743, i1 %730
  store i1 %744, i1* %of
  store volatile i64 54642, i64* @assembly_address
  %745 = load i64* %rdx
  %746 = trunc i64 %745 to i32
  %747 = load i64* %rcx
  %748 = trunc i64 %747 to i32
  %749 = or i32 %746, %748
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %750 = icmp eq i32 %749, 0
  store i1 %750, i1* %zf
  %751 = icmp slt i32 %749, 0
  store i1 %751, i1* %sf
  %752 = trunc i32 %749 to i8
  %753 = call i8 @llvm.ctpop.i8(i8 %752)
  %754 = and i8 %753, 1
  %755 = icmp eq i8 %754, 0
  store i1 %755, i1* %pf
  %756 = zext i32 %749 to i64
  store i64 %756, i64* %rdx
  store volatile i64 54644, i64* @assembly_address
  %757 = load i64* %rdx
  %758 = trunc i64 %757 to i32
  %759 = sext i32 %758 to i64
  store i64 %759, i64* %rdx
  store volatile i64 54647, i64* @assembly_address
  %760 = load i64* %rdx
  %761 = load i1* %of
  %762 = shl i64 %760, 16
  %763 = icmp eq i64 %762, 0
  store i1 %763, i1* %zf
  %764 = icmp slt i64 %762, 0
  store i1 %764, i1* %sf
  %765 = trunc i64 %762 to i8
  %766 = call i8 @llvm.ctpop.i8(i8 %765)
  %767 = and i8 %766, 1
  %768 = icmp eq i8 %767, 0
  store i1 %768, i1* %pf
  store i64 %762, i64* %rdx
  %769 = shl i64 %760, 15
  %770 = lshr i64 %769, 63
  %771 = trunc i64 %770 to i1
  store i1 %771, i1* %cf
  %772 = lshr i64 %762, 63
  %773 = icmp ne i64 %772, %770
  %774 = select i1 false, i1 %773, i1 %761
  store i1 %774, i1* %of
  store volatile i64 54651, i64* @assembly_address
  %775 = load i64* %rdx
  %776 = load i64* %rax
  %777 = or i64 %775, %776
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %778 = icmp eq i64 %777, 0
  store i1 %778, i1* %zf
  %779 = icmp slt i64 %777, 0
  store i1 %779, i1* %sf
  %780 = trunc i64 %777 to i8
  %781 = call i8 @llvm.ctpop.i8(i8 %780)
  %782 = and i8 %781, 1
  %783 = icmp eq i8 %782, 0
  store i1 %783, i1* %pf
  store i64 %777, i64* %rdx
  store volatile i64 54654, i64* @assembly_address
  %784 = load i64* @global_var_216580
  store i64 %784, i64* %rax
  store volatile i64 54661, i64* @assembly_address
  %785 = load i64* %rdx
  store i64 %785, i64* %rcx
  store volatile i64 54664, i64* @assembly_address
  %786 = load i64* %rbx
  store i64 %786, i64* %rdx
  store volatile i64 54667, i64* @assembly_address
  store i64 ptrtoint ([18 x i8]* @global_var_12301 to i64), i64* %rsi
  store volatile i64 54674, i64* @assembly_address
  %787 = load i64* %rax
  store i64 %787, i64* %rdi
  store volatile i64 54677, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 54682, i64* @assembly_address
  %788 = load i64* %rdi
  %789 = inttoptr i64 %788 to %_IO_FILE*
  %790 = load i64* %rsi
  %791 = inttoptr i64 %790 to i8*
  %792 = load i64* %rdx
  %793 = trunc i64 %792 to i32
  %794 = load i64* %rcx
  %795 = trunc i64 %794 to i32
  %796 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %789, i8* %791, i32 %793, i32 %795)
  %797 = sext i32 %796 to i64
  store i64 %797, i64* %rax
  %798 = sext i32 %796 to i64
  store i64 %798, i64* %rax
  store volatile i64 54687, i64* @assembly_address
  store i64 ptrtoint ([41 x i8]* @global_var_12318 to i64), i64* %rdi
  store volatile i64 54694, i64* @assembly_address
  %799 = load i64* %rdi
  %800 = inttoptr i64 %799 to i8*
  %801 = call i64 @gzip_error(i8* %800)
  store i64 %801, i64* %rax
  store i64 %801, i64* %rax
  unreachable

block_d5ab:                                       ; preds = %block_d617
  store volatile i64 54699, i64* @assembly_address
  %802 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %803 = zext i32 %802 to i64
  store i64 %803, i64* %rdx
  store volatile i64 54705, i64* @assembly_address
  %804 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %805 = zext i32 %804 to i64
  store i64 %805, i64* %rax
  store volatile i64 54711, i64* @assembly_address
  %806 = load i64* %rdx
  %807 = trunc i64 %806 to i32
  %808 = load i64* %rax
  %809 = trunc i64 %808 to i32
  %810 = sub i32 %807, %809
  %811 = and i32 %807, 15
  %812 = and i32 %809, 15
  %813 = sub i32 %811, %812
  %814 = icmp ugt i32 %813, 15
  %815 = icmp ult i32 %807, %809
  %816 = xor i32 %807, %809
  %817 = xor i32 %807, %810
  %818 = and i32 %816, %817
  %819 = icmp slt i32 %818, 0
  store i1 %814, i1* %az
  store i1 %815, i1* %cf
  store i1 %819, i1* %of
  %820 = icmp eq i32 %810, 0
  store i1 %820, i1* %zf
  %821 = icmp slt i32 %810, 0
  store i1 %821, i1* %sf
  %822 = trunc i32 %810 to i8
  %823 = call i8 @llvm.ctpop.i8(i8 %822)
  %824 = and i8 %823, 1
  %825 = icmp eq i8 %824, 0
  store i1 %825, i1* %pf
  store volatile i64 54713, i64* @assembly_address
  %826 = load i1* %cf
  %827 = icmp eq i1 %826, false
  br i1 %827, label %block_d5d9, label %block_d5bb

block_d5bb:                                       ; preds = %block_d5ab
  store volatile i64 54715, i64* @assembly_address
  %828 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %829 = zext i32 %828 to i64
  store i64 %829, i64* %rax
  store volatile i64 54721, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 54724, i64* @assembly_address
  %830 = load i64* %rdx
  %831 = trunc i64 %830 to i32
  store i32 %831, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 54730, i64* @assembly_address
  %832 = load i64* %rax
  %833 = trunc i64 %832 to i32
  %834 = zext i32 %833 to i64
  store i64 %834, i64* %rdx
  store volatile i64 54732, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 54739, i64* @assembly_address
  %835 = load i64* %rdx
  %836 = load i64* %rax
  %837 = mul i64 %836, 1
  %838 = add i64 %835, %837
  %839 = inttoptr i64 %838 to i8*
  %840 = load i8* %839
  %841 = zext i8 %840 to i64
  store i64 %841, i64* %rax
  store volatile i64 54743, i64* @assembly_address
  br label %block_d5e3

block_d5d9:                                       ; preds = %block_d5ab
  store volatile i64 54745, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 54750, i64* @assembly_address
  %842 = load i64* %rdi
  %843 = trunc i64 %842 to i32
  %844 = call i64 @fill_inbuf(i32 %843)
  store i64 %844, i64* %rax
  store i64 %844, i64* %rax
  br label %block_d5e3

block_d5e3:                                       ; preds = %block_d5d9, %block_d5bb
  store volatile i64 54755, i64* @assembly_address
  %845 = load i64* %rax
  %846 = trunc i64 %845 to i8
  store i8 %846, i8* %stack_var_-85
  store volatile i64 54758, i64* @assembly_address
  %847 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %848 = zext i32 %847 to i64
  store i64 %848, i64* %rax
  store volatile i64 54764, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 54767, i64* @assembly_address
  %849 = load i64* %rdx
  %850 = trunc i64 %849 to i32
  store i32 %850, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 54773, i64* @assembly_address
  %851 = load i64* %rax
  %852 = trunc i64 %851 to i32
  %853 = zext i32 %852 to i64
  store i64 %853, i64* %rcx
  store volatile i64 54775, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rax
  store volatile i64 54782, i64* @assembly_address
  %854 = load i8* %stack_var_-85
  %855 = zext i8 %854 to i64
  store i64 %855, i64* %rdx
  store volatile i64 54786, i64* @assembly_address
  %856 = load i64* %rdx
  %857 = trunc i64 %856 to i8
  %858 = load i64* %rcx
  %859 = load i64* %rax
  %860 = mul i64 %859, 1
  %861 = add i64 %858, %860
  %862 = inttoptr i64 %861 to i8*
  store i8 %857, i8* %862
  store volatile i64 54789, i64* @assembly_address
  %863 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %864 = zext i32 %863 to i64
  store i64 %864, i64* %rax
  store volatile i64 54795, i64* @assembly_address
  %865 = load i64* %rax
  %866 = trunc i64 %865 to i32
  %867 = sub i32 %866, 32768
  %868 = and i32 %866, 15
  %869 = icmp ugt i32 %868, 15
  %870 = icmp ult i32 %866, 32768
  %871 = xor i32 %866, 32768
  %872 = xor i32 %866, %867
  %873 = and i32 %871, %872
  %874 = icmp slt i32 %873, 0
  store i1 %869, i1* %az
  store i1 %870, i1* %cf
  store i1 %874, i1* %of
  %875 = icmp eq i32 %867, 0
  store i1 %875, i1* %zf
  %876 = icmp slt i32 %867, 0
  store i1 %876, i1* %sf
  %877 = trunc i32 %867 to i8
  %878 = call i8 @llvm.ctpop.i8(i8 %877)
  %879 = and i8 %878, 1
  %880 = icmp eq i8 %879, 0
  store i1 %880, i1* %pf
  store volatile i64 54800, i64* @assembly_address
  %881 = load i1* %zf
  %882 = icmp eq i1 %881, false
  br i1 %882, label %block_d617, label %block_d612

block_d612:                                       ; preds = %block_d5e3
  store volatile i64 54802, i64* @assembly_address
  %883 = call i64 @flush_window()
  store i64 %883, i64* %rax
  store i64 %883, i64* %rax
  store i64 %883, i64* %rax
  br label %block_d617

block_d617:                                       ; preds = %block_d612, %block_d5e3, %block_d531
  store volatile i64 54807, i64* @assembly_address
  %884 = load i64* %rbx
  store i64 %884, i64* %rax
  store volatile i64 54810, i64* @assembly_address
  %885 = load i64* %rax
  %886 = add i64 %885, -1
  store i64 %886, i64* %rbx
  store volatile i64 54814, i64* @assembly_address
  %887 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %888 = icmp eq i64 %887, 0
  store i1 %888, i1* %zf
  %889 = icmp slt i64 %887, 0
  store i1 %889, i1* %sf
  %890 = trunc i64 %887 to i8
  %891 = call i8 @llvm.ctpop.i8(i8 %890)
  %892 = and i8 %891, 1
  %893 = icmp eq i8 %892, 0
  store i1 %893, i1* %pf
  store volatile i64 54817, i64* @assembly_address
  %894 = load i1* %zf
  %895 = icmp eq i1 %894, false
  br i1 %895, label %block_d5ab, label %block_d623

block_d623:                                       ; preds = %block_d617
  store volatile i64 54819, i64* @assembly_address
  %896 = call i64 @flush_window()
  store i64 %896, i64* %rax
  store i64 %896, i64* %rax
  store i64 %896, i64* %rax
  store volatile i64 54824, i64* @assembly_address
  br label %block_d636

block_d62a:                                       ; preds = %block_d48e, %block_d480
  store volatile i64 54826, i64* @assembly_address
  store i64 ptrtoint ([31 x i8]* @global_var_12348 to i64), i64* %rdi
  store volatile i64 54833, i64* @assembly_address
  %897 = load i64* %rdi
  %898 = inttoptr i64 %897 to i8*
  %899 = call i64 @gzip_error(i8* %898)
  store i64 %899, i64* %rax
  store i64 %899, i64* %rax
  unreachable

block_d636:                                       ; preds = %block_d623, %block_d46a
  store volatile i64 54838, i64* @assembly_address
  %900 = load i32* bitcast (i64* @global_var_21a400 to i32*)
  %901 = zext i32 %900 to i64
  store i64 %901, i64* %rax
  store volatile i64 54844, i64* @assembly_address
  %902 = load i64* %rax
  %903 = trunc i64 %902 to i32
  %904 = load i64* %rax
  %905 = trunc i64 %904 to i32
  %906 = and i32 %903, %905
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %907 = icmp eq i32 %906, 0
  store i1 %907, i1* %zf
  %908 = icmp slt i32 %906, 0
  store i1 %908, i1* %sf
  %909 = trunc i32 %906 to i8
  %910 = call i8 @llvm.ctpop.i8(i8 %909)
  %911 = and i8 %910, 1
  %912 = icmp eq i8 %911, 0
  store i1 %912, i1* %pf
  store volatile i64 54846, i64* @assembly_address
  %913 = load i1* %zf
  %914 = icmp eq i1 %913, false
  br i1 %914, label %block_d70a, label %block_d644

block_d644:                                       ; preds = %block_d636
  store volatile i64 54852, i64* @assembly_address
  store i32 0, i32* %stack_var_-84
  store volatile i64 54859, i64* @assembly_address
  br label %block_d693

block_d64d:                                       ; preds = %block_d693
  store volatile i64 54861, i64* @assembly_address
  %915 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %916 = zext i32 %915 to i64
  store i64 %916, i64* %rdx
  store volatile i64 54867, i64* @assembly_address
  %917 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %918 = zext i32 %917 to i64
  store i64 %918, i64* %rax
  store volatile i64 54873, i64* @assembly_address
  %919 = load i64* %rdx
  %920 = trunc i64 %919 to i32
  %921 = load i64* %rax
  %922 = trunc i64 %921 to i32
  %923 = sub i32 %920, %922
  %924 = and i32 %920, 15
  %925 = and i32 %922, 15
  %926 = sub i32 %924, %925
  %927 = icmp ugt i32 %926, 15
  %928 = icmp ult i32 %920, %922
  %929 = xor i32 %920, %922
  %930 = xor i32 %920, %923
  %931 = and i32 %929, %930
  %932 = icmp slt i32 %931, 0
  store i1 %927, i1* %az
  store i1 %928, i1* %cf
  store i1 %932, i1* %of
  %933 = icmp eq i32 %923, 0
  store i1 %933, i1* %zf
  %934 = icmp slt i32 %923, 0
  store i1 %934, i1* %sf
  %935 = trunc i32 %923 to i8
  %936 = call i8 @llvm.ctpop.i8(i8 %935)
  %937 = and i8 %936, 1
  %938 = icmp eq i8 %937, 0
  store i1 %938, i1* %pf
  store volatile i64 54875, i64* @assembly_address
  %939 = load i1* %cf
  %940 = icmp eq i1 %939, false
  br i1 %940, label %block_d67b, label %block_d65d

block_d65d:                                       ; preds = %block_d64d
  store volatile i64 54877, i64* @assembly_address
  %941 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %942 = zext i32 %941 to i64
  store i64 %942, i64* %rax
  store volatile i64 54883, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 54886, i64* @assembly_address
  %943 = load i64* %rdx
  %944 = trunc i64 %943 to i32
  store i32 %944, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 54892, i64* @assembly_address
  %945 = load i64* %rax
  %946 = trunc i64 %945 to i32
  %947 = zext i32 %946 to i64
  store i64 %947, i64* %rdx
  store volatile i64 54894, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 54901, i64* @assembly_address
  %948 = load i64* %rdx
  %949 = load i64* %rax
  %950 = mul i64 %949, 1
  %951 = add i64 %948, %950
  %952 = inttoptr i64 %951 to i8*
  %953 = load i8* %952
  %954 = zext i8 %953 to i64
  store i64 %954, i64* %rax
  store volatile i64 54905, i64* @assembly_address
  br label %block_d685

block_d67b:                                       ; preds = %block_d64d
  store volatile i64 54907, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 54912, i64* @assembly_address
  %955 = load i64* %rdi
  %956 = trunc i64 %955 to i32
  %957 = call i64 @fill_inbuf(i32 %956)
  store i64 %957, i64* %rax
  store i64 %957, i64* %rax
  br label %block_d685

block_d685:                                       ; preds = %block_d67b, %block_d65d
  store volatile i64 54917, i64* @assembly_address
  %958 = load i32* %stack_var_-84
  %959 = zext i32 %958 to i64
  store i64 %959, i64* %rdx
  store volatile i64 54920, i64* @assembly_address
  %960 = load i64* %rdx
  %961 = trunc i64 %960 to i32
  %962 = sext i32 %961 to i64
  store i64 %962, i64* %rdx
  store volatile i64 54923, i64* @assembly_address
  %963 = load i64* %rax
  %964 = trunc i64 %963 to i8
  %965 = load i64* %rbp
  %966 = load i64* %rdx
  %967 = mul i64 %966, 1
  %968 = add i64 %965, -48
  %969 = add i64 %968, %967
  %970 = inttoptr i64 %969 to i8*
  store i8 %964, i8* %970
  store volatile i64 54927, i64* @assembly_address
  %971 = load i32* %stack_var_-84
  %972 = add i32 %971, 1
  %973 = and i32 %971, 15
  %974 = add i32 %973, 1
  %975 = icmp ugt i32 %974, 15
  %976 = icmp ult i32 %972, %971
  %977 = xor i32 %971, %972
  %978 = xor i32 1, %972
  %979 = and i32 %977, %978
  %980 = icmp slt i32 %979, 0
  store i1 %975, i1* %az
  store i1 %976, i1* %cf
  store i1 %980, i1* %of
  %981 = icmp eq i32 %972, 0
  store i1 %981, i1* %zf
  %982 = icmp slt i32 %972, 0
  store i1 %982, i1* %sf
  %983 = trunc i32 %972 to i8
  %984 = call i8 @llvm.ctpop.i8(i8 %983)
  %985 = and i8 %984, 1
  %986 = icmp eq i8 %985, 0
  store i1 %986, i1* %pf
  store i32 %972, i32* %stack_var_-84
  br label %block_d693

block_d693:                                       ; preds = %block_d685, %block_d644
  store volatile i64 54931, i64* @assembly_address
  %987 = load i32* %stack_var_-84
  store i32 %987, i32* %16
  store i32 7, i32* %15
  %988 = sub i32 %987, 7
  %989 = and i32 %987, 15
  %990 = sub i32 %989, 7
  %991 = icmp ugt i32 %990, 15
  %992 = icmp ult i32 %987, 7
  %993 = xor i32 %987, 7
  %994 = xor i32 %987, %988
  %995 = and i32 %993, %994
  %996 = icmp slt i32 %995, 0
  store i1 %991, i1* %az
  store i1 %992, i1* %cf
  store i1 %996, i1* %of
  %997 = icmp eq i32 %988, 0
  store i1 %997, i1* %zf
  %998 = icmp slt i32 %988, 0
  store i1 %998, i1* %sf
  %999 = trunc i32 %988 to i8
  %1000 = call i8 @llvm.ctpop.i8(i8 %999)
  %1001 = and i8 %1000, 1
  %1002 = icmp eq i8 %1001, 0
  store i1 %1002, i1* %pf
  store volatile i64 54935, i64* @assembly_address
  %1003 = load i32* %16
  %1004 = load i32* %15
  %1005 = icmp sle i32 %1003, %1004
  br i1 %1005, label %block_d64d, label %block_d699

block_d699:                                       ; preds = %block_d693
  store volatile i64 54937, i64* @assembly_address
  %1006 = load i32* %stack_var_-56
  %1007 = trunc i32 %1006 to i8
  %1008 = zext i8 %1007 to i64
  store i64 %1008, i64* %rax
  store volatile i64 54941, i64* @assembly_address
  %1009 = load i64* %rax
  %1010 = trunc i64 %1009 to i8
  %1011 = zext i8 %1010 to i64
  store i64 %1011, i64* %rax
  store volatile i64 54944, i64* @assembly_address
  %1012 = load i32* %stack_var_-55
  %1013 = trunc i32 %1012 to i8
  %1014 = zext i8 %1013 to i64
  store i64 %1014, i64* %rdx
  store volatile i64 54948, i64* @assembly_address
  %1015 = load i64* %rdx
  %1016 = trunc i64 %1015 to i8
  %1017 = zext i8 %1016 to i64
  store i64 %1017, i64* %rdx
  store volatile i64 54951, i64* @assembly_address
  %1018 = load i64* %rdx
  %1019 = trunc i64 %1018 to i32
  %1020 = load i1* %of
  %1021 = shl i32 %1019, 8
  %1022 = icmp eq i32 %1021, 0
  store i1 %1022, i1* %zf
  %1023 = icmp slt i32 %1021, 0
  store i1 %1023, i1* %sf
  %1024 = trunc i32 %1021 to i8
  %1025 = call i8 @llvm.ctpop.i8(i8 %1024)
  %1026 = and i8 %1025, 1
  %1027 = icmp eq i8 %1026, 0
  store i1 %1027, i1* %pf
  %1028 = zext i32 %1021 to i64
  store i64 %1028, i64* %rdx
  %1029 = shl i32 %1019, 7
  %1030 = lshr i32 %1029, 31
  %1031 = trunc i32 %1030 to i1
  store i1 %1031, i1* %cf
  %1032 = lshr i32 %1021, 31
  %1033 = icmp ne i32 %1032, %1030
  %1034 = select i1 false, i1 %1033, i1 %1020
  store i1 %1034, i1* %of
  store volatile i64 54954, i64* @assembly_address
  %1035 = load i64* %rax
  %1036 = trunc i64 %1035 to i32
  %1037 = load i64* %rdx
  %1038 = trunc i64 %1037 to i32
  %1039 = or i32 %1036, %1038
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1040 = icmp eq i32 %1039, 0
  store i1 %1040, i1* %zf
  %1041 = icmp slt i32 %1039, 0
  store i1 %1041, i1* %sf
  %1042 = trunc i32 %1039 to i8
  %1043 = call i8 @llvm.ctpop.i8(i8 %1042)
  %1044 = and i8 %1043, 1
  %1045 = icmp eq i8 %1044, 0
  store i1 %1045, i1* %pf
  %1046 = zext i32 %1039 to i64
  store i64 %1046, i64* %rax
  store volatile i64 54956, i64* @assembly_address
  %1047 = load i64* %rax
  %1048 = trunc i64 %1047 to i32
  %1049 = sext i32 %1048 to i64
  store i64 %1049, i64* %rax
  store volatile i64 54958, i64* @assembly_address
  %1050 = load i32* %stack_var_-54
  %1051 = trunc i32 %1050 to i8
  %1052 = zext i8 %1051 to i64
  store i64 %1052, i64* %rdx
  store volatile i64 54962, i64* @assembly_address
  %1053 = load i64* %rdx
  %1054 = trunc i64 %1053 to i8
  %1055 = zext i8 %1054 to i64
  store i64 %1055, i64* %rdx
  store volatile i64 54965, i64* @assembly_address
  %1056 = load i32* %stack_var_-53
  %1057 = trunc i32 %1056 to i8
  %1058 = zext i8 %1057 to i64
  store i64 %1058, i64* %rcx
  store volatile i64 54969, i64* @assembly_address
  %1059 = load i64* %rcx
  %1060 = trunc i64 %1059 to i8
  %1061 = zext i8 %1060 to i64
  store i64 %1061, i64* %rcx
  store volatile i64 54972, i64* @assembly_address
  %1062 = load i64* %rcx
  %1063 = trunc i64 %1062 to i32
  %1064 = load i1* %of
  %1065 = shl i32 %1063, 8
  %1066 = icmp eq i32 %1065, 0
  store i1 %1066, i1* %zf
  %1067 = icmp slt i32 %1065, 0
  store i1 %1067, i1* %sf
  %1068 = trunc i32 %1065 to i8
  %1069 = call i8 @llvm.ctpop.i8(i8 %1068)
  %1070 = and i8 %1069, 1
  %1071 = icmp eq i8 %1070, 0
  store i1 %1071, i1* %pf
  %1072 = zext i32 %1065 to i64
  store i64 %1072, i64* %rcx
  %1073 = shl i32 %1063, 7
  %1074 = lshr i32 %1073, 31
  %1075 = trunc i32 %1074 to i1
  store i1 %1075, i1* %cf
  %1076 = lshr i32 %1065, 31
  %1077 = icmp ne i32 %1076, %1074
  %1078 = select i1 false, i1 %1077, i1 %1064
  store i1 %1078, i1* %of
  store volatile i64 54975, i64* @assembly_address
  %1079 = load i64* %rdx
  %1080 = trunc i64 %1079 to i32
  %1081 = load i64* %rcx
  %1082 = trunc i64 %1081 to i32
  %1083 = or i32 %1080, %1082
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1084 = icmp eq i32 %1083, 0
  store i1 %1084, i1* %zf
  %1085 = icmp slt i32 %1083, 0
  store i1 %1085, i1* %sf
  %1086 = trunc i32 %1083 to i8
  %1087 = call i8 @llvm.ctpop.i8(i8 %1086)
  %1088 = and i8 %1087, 1
  %1089 = icmp eq i8 %1088, 0
  store i1 %1089, i1* %pf
  %1090 = zext i32 %1083 to i64
  store i64 %1090, i64* %rdx
  store volatile i64 54977, i64* @assembly_address
  %1091 = load i64* %rdx
  %1092 = trunc i64 %1091 to i32
  %1093 = sext i32 %1092 to i64
  store i64 %1093, i64* %rdx
  store volatile i64 54980, i64* @assembly_address
  %1094 = load i64* %rdx
  %1095 = load i1* %of
  %1096 = shl i64 %1094, 16
  %1097 = icmp eq i64 %1096, 0
  store i1 %1097, i1* %zf
  %1098 = icmp slt i64 %1096, 0
  store i1 %1098, i1* %sf
  %1099 = trunc i64 %1096 to i8
  %1100 = call i8 @llvm.ctpop.i8(i8 %1099)
  %1101 = and i8 %1100, 1
  %1102 = icmp eq i8 %1101, 0
  store i1 %1102, i1* %pf
  store i64 %1096, i64* %rdx
  %1103 = shl i64 %1094, 15
  %1104 = lshr i64 %1103, 63
  %1105 = trunc i64 %1104 to i1
  store i1 %1105, i1* %cf
  %1106 = lshr i64 %1096, 63
  %1107 = icmp ne i64 %1106, %1104
  %1108 = select i1 false, i1 %1107, i1 %1095
  store i1 %1108, i1* %of
  store volatile i64 54984, i64* @assembly_address
  %1109 = load i64* %rax
  %1110 = load i64* %rdx
  %1111 = or i64 %1109, %1110
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1112 = icmp eq i64 %1111, 0
  store i1 %1112, i1* %zf
  %1113 = icmp slt i64 %1111, 0
  store i1 %1113, i1* %sf
  %1114 = trunc i64 %1111 to i8
  %1115 = call i8 @llvm.ctpop.i8(i8 %1114)
  %1116 = and i8 %1115, 1
  %1117 = icmp eq i8 %1116, 0
  store i1 %1117, i1* %pf
  store i64 %1111, i64* %rax
  store volatile i64 54987, i64* @assembly_address
  %1118 = load i64* %rax
  store i64 %1118, i64* %stack_var_-72
  store volatile i64 54991, i64* @assembly_address
  %1119 = load i32* %stack_var_-52
  %1120 = trunc i32 %1119 to i8
  %1121 = zext i8 %1120 to i64
  store i64 %1121, i64* %rax
  store volatile i64 54995, i64* @assembly_address
  %1122 = load i64* %rax
  %1123 = trunc i64 %1122 to i8
  %1124 = zext i8 %1123 to i64
  store i64 %1124, i64* %rax
  store volatile i64 54998, i64* @assembly_address
  %1125 = load i32* %stack_var_-51
  %1126 = trunc i32 %1125 to i8
  %1127 = zext i8 %1126 to i64
  store i64 %1127, i64* %rdx
  store volatile i64 55002, i64* @assembly_address
  %1128 = load i64* %rdx
  %1129 = trunc i64 %1128 to i8
  %1130 = zext i8 %1129 to i64
  store i64 %1130, i64* %rdx
  store volatile i64 55005, i64* @assembly_address
  %1131 = load i64* %rdx
  %1132 = trunc i64 %1131 to i32
  %1133 = load i1* %of
  %1134 = shl i32 %1132, 8
  %1135 = icmp eq i32 %1134, 0
  store i1 %1135, i1* %zf
  %1136 = icmp slt i32 %1134, 0
  store i1 %1136, i1* %sf
  %1137 = trunc i32 %1134 to i8
  %1138 = call i8 @llvm.ctpop.i8(i8 %1137)
  %1139 = and i8 %1138, 1
  %1140 = icmp eq i8 %1139, 0
  store i1 %1140, i1* %pf
  %1141 = zext i32 %1134 to i64
  store i64 %1141, i64* %rdx
  %1142 = shl i32 %1132, 7
  %1143 = lshr i32 %1142, 31
  %1144 = trunc i32 %1143 to i1
  store i1 %1144, i1* %cf
  %1145 = lshr i32 %1134, 31
  %1146 = icmp ne i32 %1145, %1143
  %1147 = select i1 false, i1 %1146, i1 %1133
  store i1 %1147, i1* %of
  store volatile i64 55008, i64* @assembly_address
  %1148 = load i64* %rax
  %1149 = trunc i64 %1148 to i32
  %1150 = load i64* %rdx
  %1151 = trunc i64 %1150 to i32
  %1152 = or i32 %1149, %1151
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1153 = icmp eq i32 %1152, 0
  store i1 %1153, i1* %zf
  %1154 = icmp slt i32 %1152, 0
  store i1 %1154, i1* %sf
  %1155 = trunc i32 %1152 to i8
  %1156 = call i8 @llvm.ctpop.i8(i8 %1155)
  %1157 = and i8 %1156, 1
  %1158 = icmp eq i8 %1157, 0
  store i1 %1158, i1* %pf
  %1159 = zext i32 %1152 to i64
  store i64 %1159, i64* %rax
  store volatile i64 55010, i64* @assembly_address
  %1160 = load i64* %rax
  %1161 = trunc i64 %1160 to i32
  %1162 = sext i32 %1161 to i64
  store i64 %1162, i64* %rax
  store volatile i64 55012, i64* @assembly_address
  %1163 = load i32* %stack_var_-50
  %1164 = trunc i32 %1163 to i8
  %1165 = zext i8 %1164 to i64
  store i64 %1165, i64* %rdx
  store volatile i64 55016, i64* @assembly_address
  %1166 = load i64* %rdx
  %1167 = trunc i64 %1166 to i8
  %1168 = zext i8 %1167 to i64
  store i64 %1168, i64* %rdx
  store volatile i64 55019, i64* @assembly_address
  %1169 = load i32* %stack_var_-49
  %1170 = trunc i32 %1169 to i8
  %1171 = zext i8 %1170 to i64
  store i64 %1171, i64* %rcx
  store volatile i64 55023, i64* @assembly_address
  %1172 = load i64* %rcx
  %1173 = trunc i64 %1172 to i8
  %1174 = zext i8 %1173 to i64
  store i64 %1174, i64* %rcx
  store volatile i64 55026, i64* @assembly_address
  %1175 = load i64* %rcx
  %1176 = trunc i64 %1175 to i32
  %1177 = load i1* %of
  %1178 = shl i32 %1176, 8
  %1179 = icmp eq i32 %1178, 0
  store i1 %1179, i1* %zf
  %1180 = icmp slt i32 %1178, 0
  store i1 %1180, i1* %sf
  %1181 = trunc i32 %1178 to i8
  %1182 = call i8 @llvm.ctpop.i8(i8 %1181)
  %1183 = and i8 %1182, 1
  %1184 = icmp eq i8 %1183, 0
  store i1 %1184, i1* %pf
  %1185 = zext i32 %1178 to i64
  store i64 %1185, i64* %rcx
  %1186 = shl i32 %1176, 7
  %1187 = lshr i32 %1186, 31
  %1188 = trunc i32 %1187 to i1
  store i1 %1188, i1* %cf
  %1189 = lshr i32 %1178, 31
  %1190 = icmp ne i32 %1189, %1187
  %1191 = select i1 false, i1 %1190, i1 %1177
  store i1 %1191, i1* %of
  store volatile i64 55029, i64* @assembly_address
  %1192 = load i64* %rdx
  %1193 = trunc i64 %1192 to i32
  %1194 = load i64* %rcx
  %1195 = trunc i64 %1194 to i32
  %1196 = or i32 %1193, %1195
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1197 = icmp eq i32 %1196, 0
  store i1 %1197, i1* %zf
  %1198 = icmp slt i32 %1196, 0
  store i1 %1198, i1* %sf
  %1199 = trunc i32 %1196 to i8
  %1200 = call i8 @llvm.ctpop.i8(i8 %1199)
  %1201 = and i8 %1200, 1
  %1202 = icmp eq i8 %1201, 0
  store i1 %1202, i1* %pf
  %1203 = zext i32 %1196 to i64
  store i64 %1203, i64* %rdx
  store volatile i64 55031, i64* @assembly_address
  %1204 = load i64* %rdx
  %1205 = trunc i64 %1204 to i32
  %1206 = sext i32 %1205 to i64
  store i64 %1206, i64* %rdx
  store volatile i64 55034, i64* @assembly_address
  %1207 = load i64* %rdx
  %1208 = load i1* %of
  %1209 = shl i64 %1207, 16
  %1210 = icmp eq i64 %1209, 0
  store i1 %1210, i1* %zf
  %1211 = icmp slt i64 %1209, 0
  store i1 %1211, i1* %sf
  %1212 = trunc i64 %1209 to i8
  %1213 = call i8 @llvm.ctpop.i8(i8 %1212)
  %1214 = and i8 %1213, 1
  %1215 = icmp eq i8 %1214, 0
  store i1 %1215, i1* %pf
  store i64 %1209, i64* %rdx
  %1216 = shl i64 %1207, 15
  %1217 = lshr i64 %1216, 63
  %1218 = trunc i64 %1217 to i1
  store i1 %1218, i1* %cf
  %1219 = lshr i64 %1209, 63
  %1220 = icmp ne i64 %1219, %1217
  %1221 = select i1 false, i1 %1220, i1 %1208
  store i1 %1221, i1* %of
  store volatile i64 55038, i64* @assembly_address
  %1222 = load i64* %rax
  %1223 = load i64* %rdx
  %1224 = or i64 %1222, %1223
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1225 = icmp eq i64 %1224, 0
  store i1 %1225, i1* %zf
  %1226 = icmp slt i64 %1224, 0
  store i1 %1226, i1* %sf
  %1227 = trunc i64 %1224 to i8
  %1228 = call i8 @llvm.ctpop.i8(i8 %1227)
  %1229 = and i8 %1228, 1
  %1230 = icmp eq i8 %1229, 0
  store i1 %1230, i1* %pf
  store i64 %1224, i64* %rax
  store volatile i64 55041, i64* @assembly_address
  %1231 = load i64* %rax
  store i64 %1231, i64* %stack_var_-64
  store volatile i64 55045, i64* @assembly_address
  br label %block_d7d9

block_d70a:                                       ; preds = %block_d636
  store volatile i64 55050, i64* @assembly_address
  %1232 = load i32* bitcast (i64* @global_var_21a404 to i32*)
  %1233 = zext i32 %1232 to i64
  store i64 %1233, i64* %rax
  store volatile i64 55056, i64* @assembly_address
  %1234 = load i64* %rax
  %1235 = trunc i64 %1234 to i32
  %1236 = load i64* %rax
  %1237 = trunc i64 %1236 to i32
  %1238 = and i32 %1235, %1237
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1239 = icmp eq i32 %1238, 0
  store i1 %1239, i1* %zf
  %1240 = icmp slt i32 %1238, 0
  store i1 %1240, i1* %sf
  %1241 = trunc i32 %1238 to i8
  %1242 = call i8 @llvm.ctpop.i8(i8 %1241)
  %1243 = and i8 %1242, 1
  %1244 = icmp eq i8 %1243, 0
  store i1 %1244, i1* %pf
  store volatile i64 55058, i64* @assembly_address
  %1245 = load i1* %zf
  br i1 %1245, label %block_d7d9, label %block_d718

block_d718:                                       ; preds = %block_d70a
  store volatile i64 55064, i64* @assembly_address
  store i32 0, i32* %stack_var_-84
  store volatile i64 55071, i64* @assembly_address
  br label %block_d767

block_d721:                                       ; preds = %block_d767
  store volatile i64 55073, i64* @assembly_address
  %1246 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1247 = zext i32 %1246 to i64
  store i64 %1247, i64* %rdx
  store volatile i64 55079, i64* @assembly_address
  %1248 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1249 = zext i32 %1248 to i64
  store i64 %1249, i64* %rax
  store volatile i64 55085, i64* @assembly_address
  %1250 = load i64* %rdx
  %1251 = trunc i64 %1250 to i32
  %1252 = load i64* %rax
  %1253 = trunc i64 %1252 to i32
  %1254 = sub i32 %1251, %1253
  %1255 = and i32 %1251, 15
  %1256 = and i32 %1253, 15
  %1257 = sub i32 %1255, %1256
  %1258 = icmp ugt i32 %1257, 15
  %1259 = icmp ult i32 %1251, %1253
  %1260 = xor i32 %1251, %1253
  %1261 = xor i32 %1251, %1254
  %1262 = and i32 %1260, %1261
  %1263 = icmp slt i32 %1262, 0
  store i1 %1258, i1* %az
  store i1 %1259, i1* %cf
  store i1 %1263, i1* %of
  %1264 = icmp eq i32 %1254, 0
  store i1 %1264, i1* %zf
  %1265 = icmp slt i32 %1254, 0
  store i1 %1265, i1* %sf
  %1266 = trunc i32 %1254 to i8
  %1267 = call i8 @llvm.ctpop.i8(i8 %1266)
  %1268 = and i8 %1267, 1
  %1269 = icmp eq i8 %1268, 0
  store i1 %1269, i1* %pf
  store volatile i64 55087, i64* @assembly_address
  %1270 = load i1* %cf
  %1271 = icmp eq i1 %1270, false
  br i1 %1271, label %block_d74f, label %block_d731

block_d731:                                       ; preds = %block_d721
  store volatile i64 55089, i64* @assembly_address
  %1272 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1273 = zext i32 %1272 to i64
  store i64 %1273, i64* %rax
  store volatile i64 55095, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rdx
  store volatile i64 55098, i64* @assembly_address
  %1274 = load i64* %rdx
  %1275 = trunc i64 %1274 to i32
  store i32 %1275, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 55104, i64* @assembly_address
  %1276 = load i64* %rax
  %1277 = trunc i64 %1276 to i32
  %1278 = zext i32 %1277 to i64
  store i64 %1278, i64* %rdx
  store volatile i64 55106, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 55113, i64* @assembly_address
  %1279 = load i64* %rdx
  %1280 = load i64* %rax
  %1281 = mul i64 %1280, 1
  %1282 = add i64 %1279, %1281
  %1283 = inttoptr i64 %1282 to i8*
  %1284 = load i8* %1283
  %1285 = zext i8 %1284 to i64
  store i64 %1285, i64* %rax
  store volatile i64 55117, i64* @assembly_address
  br label %block_d759

block_d74f:                                       ; preds = %block_d721
  store volatile i64 55119, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 55124, i64* @assembly_address
  %1286 = load i64* %rdi
  %1287 = trunc i64 %1286 to i32
  %1288 = call i64 @fill_inbuf(i32 %1287)
  store i64 %1288, i64* %rax
  store i64 %1288, i64* %rax
  br label %block_d759

block_d759:                                       ; preds = %block_d74f, %block_d731
  store volatile i64 55129, i64* @assembly_address
  %1289 = load i32* %stack_var_-84
  %1290 = zext i32 %1289 to i64
  store i64 %1290, i64* %rdx
  store volatile i64 55132, i64* @assembly_address
  %1291 = load i64* %rdx
  %1292 = trunc i64 %1291 to i32
  %1293 = sext i32 %1292 to i64
  store i64 %1293, i64* %rdx
  store volatile i64 55135, i64* @assembly_address
  %1294 = load i64* %rax
  %1295 = trunc i64 %1294 to i8
  %1296 = load i64* %rbp
  %1297 = load i64* %rdx
  %1298 = mul i64 %1297, 1
  %1299 = add i64 %1296, -48
  %1300 = add i64 %1299, %1298
  %1301 = inttoptr i64 %1300 to i8*
  store i8 %1295, i8* %1301
  store volatile i64 55139, i64* @assembly_address
  %1302 = load i32* %stack_var_-84
  %1303 = add i32 %1302, 1
  %1304 = and i32 %1302, 15
  %1305 = add i32 %1304, 1
  %1306 = icmp ugt i32 %1305, 15
  %1307 = icmp ult i32 %1303, %1302
  %1308 = xor i32 %1302, %1303
  %1309 = xor i32 1, %1303
  %1310 = and i32 %1308, %1309
  %1311 = icmp slt i32 %1310, 0
  store i1 %1306, i1* %az
  store i1 %1307, i1* %cf
  store i1 %1311, i1* %of
  %1312 = icmp eq i32 %1303, 0
  store i1 %1312, i1* %zf
  %1313 = icmp slt i32 %1303, 0
  store i1 %1313, i1* %sf
  %1314 = trunc i32 %1303 to i8
  %1315 = call i8 @llvm.ctpop.i8(i8 %1314)
  %1316 = and i8 %1315, 1
  %1317 = icmp eq i8 %1316, 0
  store i1 %1317, i1* %pf
  store i32 %1303, i32* %stack_var_-84
  br label %block_d767

block_d767:                                       ; preds = %block_d759, %block_d718
  store volatile i64 55143, i64* @assembly_address
  %1318 = load i32* %stack_var_-84
  store i32 %1318, i32* %14
  store i32 15, i32* %13
  %1319 = sub i32 %1318, 15
  %1320 = and i32 %1318, 15
  %1321 = sub i32 %1320, 15
  %1322 = icmp ugt i32 %1321, 15
  %1323 = icmp ult i32 %1318, 15
  %1324 = xor i32 %1318, 15
  %1325 = xor i32 %1318, %1319
  %1326 = and i32 %1324, %1325
  %1327 = icmp slt i32 %1326, 0
  store i1 %1322, i1* %az
  store i1 %1323, i1* %cf
  store i1 %1327, i1* %of
  %1328 = icmp eq i32 %1319, 0
  store i1 %1328, i1* %zf
  %1329 = icmp slt i32 %1319, 0
  store i1 %1329, i1* %sf
  %1330 = trunc i32 %1319 to i8
  %1331 = call i8 @llvm.ctpop.i8(i8 %1330)
  %1332 = and i8 %1331, 1
  %1333 = icmp eq i8 %1332, 0
  store i1 %1333, i1* %pf
  store volatile i64 55147, i64* @assembly_address
  %1334 = load i32* %14
  %1335 = load i32* %13
  %1336 = icmp sle i32 %1334, %1335
  br i1 %1336, label %block_d721, label %block_d76d

block_d76d:                                       ; preds = %block_d767
  store volatile i64 55149, i64* @assembly_address
  %1337 = load i32* %stack_var_-52
  %1338 = trunc i32 %1337 to i8
  %1339 = zext i8 %1338 to i64
  store i64 %1339, i64* %rax
  store volatile i64 55153, i64* @assembly_address
  %1340 = load i64* %rax
  %1341 = trunc i64 %1340 to i8
  %1342 = zext i8 %1341 to i64
  store i64 %1342, i64* %rax
  store volatile i64 55156, i64* @assembly_address
  %1343 = load i32* %stack_var_-51
  %1344 = trunc i32 %1343 to i8
  %1345 = zext i8 %1344 to i64
  store i64 %1345, i64* %rdx
  store volatile i64 55160, i64* @assembly_address
  %1346 = load i64* %rdx
  %1347 = trunc i64 %1346 to i8
  %1348 = zext i8 %1347 to i64
  store i64 %1348, i64* %rdx
  store volatile i64 55163, i64* @assembly_address
  %1349 = load i64* %rdx
  %1350 = trunc i64 %1349 to i32
  %1351 = load i1* %of
  %1352 = shl i32 %1350, 8
  %1353 = icmp eq i32 %1352, 0
  store i1 %1353, i1* %zf
  %1354 = icmp slt i32 %1352, 0
  store i1 %1354, i1* %sf
  %1355 = trunc i32 %1352 to i8
  %1356 = call i8 @llvm.ctpop.i8(i8 %1355)
  %1357 = and i8 %1356, 1
  %1358 = icmp eq i8 %1357, 0
  store i1 %1358, i1* %pf
  %1359 = zext i32 %1352 to i64
  store i64 %1359, i64* %rdx
  %1360 = shl i32 %1350, 7
  %1361 = lshr i32 %1360, 31
  %1362 = trunc i32 %1361 to i1
  store i1 %1362, i1* %cf
  %1363 = lshr i32 %1352, 31
  %1364 = icmp ne i32 %1363, %1361
  %1365 = select i1 false, i1 %1364, i1 %1351
  store i1 %1365, i1* %of
  store volatile i64 55166, i64* @assembly_address
  %1366 = load i64* %rax
  %1367 = trunc i64 %1366 to i32
  %1368 = load i64* %rdx
  %1369 = trunc i64 %1368 to i32
  %1370 = or i32 %1367, %1369
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1371 = icmp eq i32 %1370, 0
  store i1 %1371, i1* %zf
  %1372 = icmp slt i32 %1370, 0
  store i1 %1372, i1* %sf
  %1373 = trunc i32 %1370 to i8
  %1374 = call i8 @llvm.ctpop.i8(i8 %1373)
  %1375 = and i8 %1374, 1
  %1376 = icmp eq i8 %1375, 0
  store i1 %1376, i1* %pf
  %1377 = zext i32 %1370 to i64
  store i64 %1377, i64* %rax
  store volatile i64 55168, i64* @assembly_address
  %1378 = load i64* %rax
  %1379 = trunc i64 %1378 to i32
  %1380 = sext i32 %1379 to i64
  store i64 %1380, i64* %rax
  store volatile i64 55170, i64* @assembly_address
  %1381 = load i32* %stack_var_-50
  %1382 = trunc i32 %1381 to i8
  %1383 = zext i8 %1382 to i64
  store i64 %1383, i64* %rdx
  store volatile i64 55174, i64* @assembly_address
  %1384 = load i64* %rdx
  %1385 = trunc i64 %1384 to i8
  %1386 = zext i8 %1385 to i64
  store i64 %1386, i64* %rdx
  store volatile i64 55177, i64* @assembly_address
  %1387 = load i32* %stack_var_-49
  %1388 = trunc i32 %1387 to i8
  %1389 = zext i8 %1388 to i64
  store i64 %1389, i64* %rcx
  store volatile i64 55181, i64* @assembly_address
  %1390 = load i64* %rcx
  %1391 = trunc i64 %1390 to i8
  %1392 = zext i8 %1391 to i64
  store i64 %1392, i64* %rcx
  store volatile i64 55184, i64* @assembly_address
  %1393 = load i64* %rcx
  %1394 = trunc i64 %1393 to i32
  %1395 = load i1* %of
  %1396 = shl i32 %1394, 8
  %1397 = icmp eq i32 %1396, 0
  store i1 %1397, i1* %zf
  %1398 = icmp slt i32 %1396, 0
  store i1 %1398, i1* %sf
  %1399 = trunc i32 %1396 to i8
  %1400 = call i8 @llvm.ctpop.i8(i8 %1399)
  %1401 = and i8 %1400, 1
  %1402 = icmp eq i8 %1401, 0
  store i1 %1402, i1* %pf
  %1403 = zext i32 %1396 to i64
  store i64 %1403, i64* %rcx
  %1404 = shl i32 %1394, 7
  %1405 = lshr i32 %1404, 31
  %1406 = trunc i32 %1405 to i1
  store i1 %1406, i1* %cf
  %1407 = lshr i32 %1396, 31
  %1408 = icmp ne i32 %1407, %1405
  %1409 = select i1 false, i1 %1408, i1 %1395
  store i1 %1409, i1* %of
  store volatile i64 55187, i64* @assembly_address
  %1410 = load i64* %rdx
  %1411 = trunc i64 %1410 to i32
  %1412 = load i64* %rcx
  %1413 = trunc i64 %1412 to i32
  %1414 = or i32 %1411, %1413
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1415 = icmp eq i32 %1414, 0
  store i1 %1415, i1* %zf
  %1416 = icmp slt i32 %1414, 0
  store i1 %1416, i1* %sf
  %1417 = trunc i32 %1414 to i8
  %1418 = call i8 @llvm.ctpop.i8(i8 %1417)
  %1419 = and i8 %1418, 1
  %1420 = icmp eq i8 %1419, 0
  store i1 %1420, i1* %pf
  %1421 = zext i32 %1414 to i64
  store i64 %1421, i64* %rdx
  store volatile i64 55189, i64* @assembly_address
  %1422 = load i64* %rdx
  %1423 = trunc i64 %1422 to i32
  %1424 = sext i32 %1423 to i64
  store i64 %1424, i64* %rdx
  store volatile i64 55192, i64* @assembly_address
  %1425 = load i64* %rdx
  %1426 = load i1* %of
  %1427 = shl i64 %1425, 16
  %1428 = icmp eq i64 %1427, 0
  store i1 %1428, i1* %zf
  %1429 = icmp slt i64 %1427, 0
  store i1 %1429, i1* %sf
  %1430 = trunc i64 %1427 to i8
  %1431 = call i8 @llvm.ctpop.i8(i8 %1430)
  %1432 = and i8 %1431, 1
  %1433 = icmp eq i8 %1432, 0
  store i1 %1433, i1* %pf
  store i64 %1427, i64* %rdx
  %1434 = shl i64 %1425, 15
  %1435 = lshr i64 %1434, 63
  %1436 = trunc i64 %1435 to i1
  store i1 %1436, i1* %cf
  %1437 = lshr i64 %1427, 63
  %1438 = icmp ne i64 %1437, %1435
  %1439 = select i1 false, i1 %1438, i1 %1426
  store i1 %1439, i1* %of
  store volatile i64 55196, i64* @assembly_address
  %1440 = load i64* %rax
  %1441 = load i64* %rdx
  %1442 = or i64 %1440, %1441
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1443 = icmp eq i64 %1442, 0
  store i1 %1443, i1* %zf
  %1444 = icmp slt i64 %1442, 0
  store i1 %1444, i1* %sf
  %1445 = trunc i64 %1442 to i8
  %1446 = call i8 @llvm.ctpop.i8(i8 %1445)
  %1447 = and i8 %1446, 1
  %1448 = icmp eq i8 %1447, 0
  store i1 %1448, i1* %pf
  store i64 %1442, i64* %rax
  store volatile i64 55199, i64* @assembly_address
  %1449 = load i64* %rax
  store i64 %1449, i64* %stack_var_-72
  store volatile i64 55203, i64* @assembly_address
  %1450 = load i32* %stack_var_-44
  %1451 = trunc i32 %1450 to i8
  %1452 = zext i8 %1451 to i64
  store i64 %1452, i64* %rax
  store volatile i64 55207, i64* @assembly_address
  %1453 = load i64* %rax
  %1454 = trunc i64 %1453 to i8
  %1455 = zext i8 %1454 to i64
  store i64 %1455, i64* %rax
  store volatile i64 55210, i64* @assembly_address
  %1456 = load i32* %stack_var_-43
  %1457 = trunc i32 %1456 to i8
  %1458 = zext i8 %1457 to i64
  store i64 %1458, i64* %rdx
  store volatile i64 55214, i64* @assembly_address
  %1459 = load i64* %rdx
  %1460 = trunc i64 %1459 to i8
  %1461 = zext i8 %1460 to i64
  store i64 %1461, i64* %rdx
  store volatile i64 55217, i64* @assembly_address
  %1462 = load i64* %rdx
  %1463 = trunc i64 %1462 to i32
  %1464 = load i1* %of
  %1465 = shl i32 %1463, 8
  %1466 = icmp eq i32 %1465, 0
  store i1 %1466, i1* %zf
  %1467 = icmp slt i32 %1465, 0
  store i1 %1467, i1* %sf
  %1468 = trunc i32 %1465 to i8
  %1469 = call i8 @llvm.ctpop.i8(i8 %1468)
  %1470 = and i8 %1469, 1
  %1471 = icmp eq i8 %1470, 0
  store i1 %1471, i1* %pf
  %1472 = zext i32 %1465 to i64
  store i64 %1472, i64* %rdx
  %1473 = shl i32 %1463, 7
  %1474 = lshr i32 %1473, 31
  %1475 = trunc i32 %1474 to i1
  store i1 %1475, i1* %cf
  %1476 = lshr i32 %1465, 31
  %1477 = icmp ne i32 %1476, %1474
  %1478 = select i1 false, i1 %1477, i1 %1464
  store i1 %1478, i1* %of
  store volatile i64 55220, i64* @assembly_address
  %1479 = load i64* %rax
  %1480 = trunc i64 %1479 to i32
  %1481 = load i64* %rdx
  %1482 = trunc i64 %1481 to i32
  %1483 = or i32 %1480, %1482
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1484 = icmp eq i32 %1483, 0
  store i1 %1484, i1* %zf
  %1485 = icmp slt i32 %1483, 0
  store i1 %1485, i1* %sf
  %1486 = trunc i32 %1483 to i8
  %1487 = call i8 @llvm.ctpop.i8(i8 %1486)
  %1488 = and i8 %1487, 1
  %1489 = icmp eq i8 %1488, 0
  store i1 %1489, i1* %pf
  %1490 = zext i32 %1483 to i64
  store i64 %1490, i64* %rax
  store volatile i64 55222, i64* @assembly_address
  %1491 = load i64* %rax
  %1492 = trunc i64 %1491 to i32
  %1493 = sext i32 %1492 to i64
  store i64 %1493, i64* %rax
  store volatile i64 55224, i64* @assembly_address
  %1494 = load i32* %stack_var_-42
  %1495 = trunc i32 %1494 to i8
  %1496 = zext i8 %1495 to i64
  store i64 %1496, i64* %rdx
  store volatile i64 55228, i64* @assembly_address
  %1497 = load i64* %rdx
  %1498 = trunc i64 %1497 to i8
  %1499 = zext i8 %1498 to i64
  store i64 %1499, i64* %rdx
  store volatile i64 55231, i64* @assembly_address
  %1500 = load i32* %stack_var_-41
  %1501 = trunc i32 %1500 to i8
  %1502 = zext i8 %1501 to i64
  store i64 %1502, i64* %rcx
  store volatile i64 55235, i64* @assembly_address
  %1503 = load i64* %rcx
  %1504 = trunc i64 %1503 to i8
  %1505 = zext i8 %1504 to i64
  store i64 %1505, i64* %rcx
  store volatile i64 55238, i64* @assembly_address
  %1506 = load i64* %rcx
  %1507 = trunc i64 %1506 to i32
  %1508 = load i1* %of
  %1509 = shl i32 %1507, 8
  %1510 = icmp eq i32 %1509, 0
  store i1 %1510, i1* %zf
  %1511 = icmp slt i32 %1509, 0
  store i1 %1511, i1* %sf
  %1512 = trunc i32 %1509 to i8
  %1513 = call i8 @llvm.ctpop.i8(i8 %1512)
  %1514 = and i8 %1513, 1
  %1515 = icmp eq i8 %1514, 0
  store i1 %1515, i1* %pf
  %1516 = zext i32 %1509 to i64
  store i64 %1516, i64* %rcx
  %1517 = shl i32 %1507, 7
  %1518 = lshr i32 %1517, 31
  %1519 = trunc i32 %1518 to i1
  store i1 %1519, i1* %cf
  %1520 = lshr i32 %1509, 31
  %1521 = icmp ne i32 %1520, %1518
  %1522 = select i1 false, i1 %1521, i1 %1508
  store i1 %1522, i1* %of
  store volatile i64 55241, i64* @assembly_address
  %1523 = load i64* %rdx
  %1524 = trunc i64 %1523 to i32
  %1525 = load i64* %rcx
  %1526 = trunc i64 %1525 to i32
  %1527 = or i32 %1524, %1526
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1528 = icmp eq i32 %1527, 0
  store i1 %1528, i1* %zf
  %1529 = icmp slt i32 %1527, 0
  store i1 %1529, i1* %sf
  %1530 = trunc i32 %1527 to i8
  %1531 = call i8 @llvm.ctpop.i8(i8 %1530)
  %1532 = and i8 %1531, 1
  %1533 = icmp eq i8 %1532, 0
  store i1 %1533, i1* %pf
  %1534 = zext i32 %1527 to i64
  store i64 %1534, i64* %rdx
  store volatile i64 55243, i64* @assembly_address
  %1535 = load i64* %rdx
  %1536 = trunc i64 %1535 to i32
  %1537 = sext i32 %1536 to i64
  store i64 %1537, i64* %rdx
  store volatile i64 55246, i64* @assembly_address
  %1538 = load i64* %rdx
  %1539 = load i1* %of
  %1540 = shl i64 %1538, 16
  %1541 = icmp eq i64 %1540, 0
  store i1 %1541, i1* %zf
  %1542 = icmp slt i64 %1540, 0
  store i1 %1542, i1* %sf
  %1543 = trunc i64 %1540 to i8
  %1544 = call i8 @llvm.ctpop.i8(i8 %1543)
  %1545 = and i8 %1544, 1
  %1546 = icmp eq i8 %1545, 0
  store i1 %1546, i1* %pf
  store i64 %1540, i64* %rdx
  %1547 = shl i64 %1538, 15
  %1548 = lshr i64 %1547, 63
  %1549 = trunc i64 %1548 to i1
  store i1 %1549, i1* %cf
  %1550 = lshr i64 %1540, 63
  %1551 = icmp ne i64 %1550, %1548
  %1552 = select i1 false, i1 %1551, i1 %1539
  store i1 %1552, i1* %of
  store volatile i64 55250, i64* @assembly_address
  %1553 = load i64* %rax
  %1554 = load i64* %rdx
  %1555 = or i64 %1553, %1554
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1556 = icmp eq i64 %1555, 0
  store i1 %1556, i1* %zf
  %1557 = icmp slt i64 %1555, 0
  store i1 %1557, i1* %sf
  %1558 = trunc i64 %1555 to i8
  %1559 = call i8 @llvm.ctpop.i8(i8 %1558)
  %1560 = and i8 %1559, 1
  %1561 = icmp eq i8 %1560, 0
  store i1 %1561, i1* %pf
  store i64 %1555, i64* %rax
  store volatile i64 55253, i64* @assembly_address
  %1562 = load i64* %rax
  store i64 %1562, i64* %stack_var_-64
  br label %block_d7d9

block_d7d9:                                       ; preds = %block_d76d, %block_d70a, %block_d699
  store volatile i64 55257, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 55262, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rdi
  store volatile i64 55269, i64* @assembly_address
  %1563 = load i64* %rdi
  %1564 = inttoptr i64 %1563 to i8*
  %1565 = load i64* %rsi
  %1566 = trunc i64 %1565 to i32
  %1567 = call i64 @updcrc(i8* %1564, i32 %1566)
  store i64 %1567, i64* %rax
  store i64 %1567, i64* %rax
  store volatile i64 55274, i64* @assembly_address
  %1568 = load i64* %stack_var_-72
  %1569 = load i64* %rax
  %1570 = sub i64 %1568, %1569
  %1571 = and i64 %1568, 15
  %1572 = and i64 %1569, 15
  %1573 = sub i64 %1571, %1572
  %1574 = icmp ugt i64 %1573, 15
  %1575 = icmp ult i64 %1568, %1569
  %1576 = xor i64 %1568, %1569
  %1577 = xor i64 %1568, %1570
  %1578 = and i64 %1576, %1577
  %1579 = icmp slt i64 %1578, 0
  store i1 %1574, i1* %az
  store i1 %1575, i1* %cf
  store i1 %1579, i1* %of
  %1580 = icmp eq i64 %1570, 0
  store i1 %1580, i1* %zf
  %1581 = icmp slt i64 %1570, 0
  store i1 %1581, i1* %sf
  %1582 = trunc i64 %1570 to i8
  %1583 = call i8 @llvm.ctpop.i8(i8 %1582)
  %1584 = and i8 %1583, 1
  %1585 = icmp eq i8 %1584, 0
  store i1 %1585, i1* %pf
  store volatile i64 55278, i64* @assembly_address
  %1586 = load i1* %zf
  br i1 %1586, label %block_d820, label %block_d7f0

block_d7f0:                                       ; preds = %block_d7d9
  store volatile i64 55280, i64* @assembly_address
  %1587 = load i64* @global_var_25f4c8
  store i64 %1587, i64* %rdx
  store volatile i64 55287, i64* @assembly_address
  %1588 = load i64* @global_var_216580
  store i64 %1588, i64* %rax
  store volatile i64 55294, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 55301, i64* @assembly_address
  store i64 ptrtoint ([45 x i8]* @global_var_12368 to i64), i64* %rsi
  store volatile i64 55308, i64* @assembly_address
  %1589 = load i64* %rax
  store i64 %1589, i64* %rdi
  store volatile i64 55311, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 55316, i64* @assembly_address
  %1590 = load i64* %rdi
  %1591 = inttoptr i64 %1590 to %_IO_FILE*
  %1592 = load i64* %rsi
  %1593 = inttoptr i64 %1592 to i8*
  %1594 = load i64* %rdx
  %1595 = inttoptr i64 %1594 to i8*
  %1596 = load i64* %rcx
  %1597 = inttoptr i64 %1596 to i8*
  %1598 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1591, i8* %1593, i8* %1595, i8* %1597)
  %1599 = sext i32 %1598 to i64
  store i64 %1599, i64* %rax
  %1600 = sext i32 %1598 to i64
  store i64 %1600, i64* %rax
  store volatile i64 55321, i64* @assembly_address
  store i32 1, i32* %stack_var_-80
  br label %block_d820

block_d820:                                       ; preds = %block_d7f0, %block_d7d9
  store volatile i64 55328, i64* @assembly_address
  %1601 = load i64* @global_var_25f4c0
  store i64 %1601, i64* %rax
  store volatile i64 55335, i64* @assembly_address
  %1602 = load i64* %rax
  %1603 = trunc i64 %1602 to i32
  %1604 = zext i32 %1603 to i64
  store i64 %1604, i64* %rax
  store volatile i64 55337, i64* @assembly_address
  %1605 = load i64* %stack_var_-64
  %1606 = load i64* %rax
  %1607 = sub i64 %1605, %1606
  %1608 = and i64 %1605, 15
  %1609 = and i64 %1606, 15
  %1610 = sub i64 %1608, %1609
  %1611 = icmp ugt i64 %1610, 15
  %1612 = icmp ult i64 %1605, %1606
  %1613 = xor i64 %1605, %1606
  %1614 = xor i64 %1605, %1607
  %1615 = and i64 %1613, %1614
  %1616 = icmp slt i64 %1615, 0
  store i1 %1611, i1* %az
  store i1 %1612, i1* %cf
  store i1 %1616, i1* %of
  %1617 = icmp eq i64 %1607, 0
  store i1 %1617, i1* %zf
  %1618 = icmp slt i64 %1607, 0
  store i1 %1618, i1* %sf
  %1619 = trunc i64 %1607 to i8
  %1620 = call i8 @llvm.ctpop.i8(i8 %1619)
  %1621 = and i8 %1620, 1
  %1622 = icmp eq i8 %1621, 0
  store i1 %1622, i1* %pf
  store volatile i64 55341, i64* @assembly_address
  %1623 = load i1* %zf
  br i1 %1623, label %block_d85f, label %block_d82f

block_d82f:                                       ; preds = %block_d820
  store volatile i64 55343, i64* @assembly_address
  %1624 = load i64* @global_var_25f4c8
  store i64 %1624, i64* %rdx
  store volatile i64 55350, i64* @assembly_address
  %1625 = load i64* @global_var_216580
  store i64 %1625, i64* %rax
  store volatile i64 55357, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 55364, i64* @assembly_address
  store i64 ptrtoint ([48 x i8]* @global_var_12398 to i64), i64* %rsi
  store volatile i64 55371, i64* @assembly_address
  %1626 = load i64* %rax
  store i64 %1626, i64* %rdi
  store volatile i64 55374, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 55379, i64* @assembly_address
  %1627 = load i64* %rdi
  %1628 = inttoptr i64 %1627 to %_IO_FILE*
  %1629 = load i64* %rsi
  %1630 = inttoptr i64 %1629 to i8*
  %1631 = load i64* %rdx
  %1632 = inttoptr i64 %1631 to i8*
  %1633 = load i64* %rcx
  %1634 = inttoptr i64 %1633 to i8*
  %1635 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1628, i8* %1630, i8* %1632, i8* %1634)
  %1636 = sext i32 %1635 to i64
  store i64 %1636, i64* %rax
  %1637 = sext i32 %1635 to i64
  store i64 %1637, i64* %rax
  store volatile i64 55384, i64* @assembly_address
  store i32 1, i32* %stack_var_-80
  br label %block_d85f

block_d85f:                                       ; preds = %block_d82f, %block_d820
  store volatile i64 55391, i64* @assembly_address
  %1638 = load i32* bitcast (i64* @global_var_21a400 to i32*)
  %1639 = zext i32 %1638 to i64
  store i64 %1639, i64* %rax
  store volatile i64 55397, i64* @assembly_address
  %1640 = load i64* %rax
  %1641 = trunc i64 %1640 to i32
  %1642 = load i64* %rax
  %1643 = trunc i64 %1642 to i32
  %1644 = and i32 %1641, %1643
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1645 = icmp eq i32 %1644, 0
  store i1 %1645, i1* %zf
  %1646 = icmp slt i32 %1644, 0
  store i1 %1646, i1* %sf
  %1647 = trunc i32 %1644 to i8
  %1648 = call i8 @llvm.ctpop.i8(i8 %1647)
  %1649 = and i8 %1648, 1
  %1650 = icmp eq i8 %1649, 0
  store i1 %1650, i1* %pf
  store volatile i64 55399, i64* @assembly_address
  %1651 = load i1* %zf
  br i1 %1651, label %block_d995, label %block_d86d

block_d86d:                                       ; preds = %block_d85f
  store volatile i64 55405, i64* @assembly_address
  %1652 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1653 = zext i32 %1652 to i64
  store i64 %1653, i64* %rax
  store volatile i64 55411, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a888 to i64), i64* %rdx
  store volatile i64 55414, i64* @assembly_address
  %1654 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %1655 = zext i32 %1654 to i64
  store i64 %1655, i64* %rax
  store volatile i64 55420, i64* @assembly_address
  %1656 = load i64* %rdx
  %1657 = trunc i64 %1656 to i32
  %1658 = load i64* %rax
  %1659 = trunc i64 %1658 to i32
  %1660 = sub i32 %1657, %1659
  %1661 = and i32 %1657, 15
  %1662 = and i32 %1659, 15
  %1663 = sub i32 %1661, %1662
  %1664 = icmp ugt i32 %1663, 15
  %1665 = icmp ult i32 %1657, %1659
  %1666 = xor i32 %1657, %1659
  %1667 = xor i32 %1657, %1660
  %1668 = and i32 %1666, %1667
  %1669 = icmp slt i32 %1668, 0
  store i1 %1664, i1* %az
  store i1 %1665, i1* %cf
  store i1 %1669, i1* %of
  %1670 = icmp eq i32 %1660, 0
  store i1 %1670, i1* %zf
  %1671 = icmp slt i32 %1660, 0
  store i1 %1671, i1* %sf
  %1672 = trunc i32 %1660 to i8
  %1673 = call i8 @llvm.ctpop.i8(i8 %1672)
  %1674 = and i8 %1673, 1
  %1675 = icmp eq i8 %1674, 0
  store i1 %1675, i1* %pf
  store volatile i64 55422, i64* @assembly_address
  %1676 = load i1* %cf
  %1677 = icmp eq i1 %1676, false
  br i1 %1677, label %block_d995, label %block_d884

block_d884:                                       ; preds = %block_d86d
  store volatile i64 55428, i64* @assembly_address
  %1678 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1679 = zext i32 %1678 to i64
  store i64 %1679, i64* %rax
  store volatile i64 55434, i64* @assembly_address
  %1680 = load i64* %rax
  %1681 = trunc i64 %1680 to i32
  %1682 = zext i32 %1681 to i64
  store i64 %1682, i64* %rdx
  store volatile i64 55436, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 55443, i64* @assembly_address
  %1683 = load i64* %rax
  %1684 = load i64* %rdx
  %1685 = add i64 %1683, %1684
  %1686 = and i64 %1683, 15
  %1687 = and i64 %1684, 15
  %1688 = add i64 %1686, %1687
  %1689 = icmp ugt i64 %1688, 15
  %1690 = icmp ult i64 %1685, %1683
  %1691 = xor i64 %1683, %1685
  %1692 = xor i64 %1684, %1685
  %1693 = and i64 %1691, %1692
  %1694 = icmp slt i64 %1693, 0
  store i1 %1689, i1* %az
  store i1 %1690, i1* %cf
  store i1 %1694, i1* %of
  %1695 = icmp eq i64 %1685, 0
  store i1 %1695, i1* %zf
  %1696 = icmp slt i64 %1685, 0
  store i1 %1696, i1* %sf
  %1697 = trunc i64 %1685 to i8
  %1698 = call i8 @llvm.ctpop.i8(i8 %1697)
  %1699 = and i8 %1698, 1
  %1700 = icmp eq i8 %1699, 0
  store i1 %1700, i1* %pf
  store i64 %1685, i64* %rax
  store volatile i64 55446, i64* @assembly_address
  %1701 = load i64* %rax
  %1702 = inttoptr i64 %1701 to i8*
  %1703 = load i8* %1702
  %1704 = zext i8 %1703 to i64
  store i64 %1704, i64* %rax
  store volatile i64 55449, i64* @assembly_address
  %1705 = load i64* %rax
  %1706 = trunc i64 %1705 to i8
  %1707 = zext i8 %1706 to i64
  store i64 %1707, i64* %rax
  store volatile i64 55452, i64* @assembly_address
  %1708 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1709 = zext i32 %1708 to i64
  store i64 %1709, i64* %rdx
  store volatile i64 55458, i64* @assembly_address
  %1710 = load i64* %rdx
  %1711 = trunc i64 %1710 to i32
  %1712 = zext i32 %1711 to i64
  store i64 %1712, i64* %rdx
  store volatile i64 55460, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a885 to i64), i64* %rcx
  store volatile i64 55464, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rdx
  store volatile i64 55471, i64* @assembly_address
  %1713 = load i64* %rdx
  %1714 = load i64* %rcx
  %1715 = add i64 %1713, %1714
  %1716 = and i64 %1713, 15
  %1717 = and i64 %1714, 15
  %1718 = add i64 %1716, %1717
  %1719 = icmp ugt i64 %1718, 15
  %1720 = icmp ult i64 %1715, %1713
  %1721 = xor i64 %1713, %1715
  %1722 = xor i64 %1714, %1715
  %1723 = and i64 %1721, %1722
  %1724 = icmp slt i64 %1723, 0
  store i1 %1719, i1* %az
  store i1 %1720, i1* %cf
  store i1 %1724, i1* %of
  %1725 = icmp eq i64 %1715, 0
  store i1 %1725, i1* %zf
  %1726 = icmp slt i64 %1715, 0
  store i1 %1726, i1* %sf
  %1727 = trunc i64 %1715 to i8
  %1728 = call i8 @llvm.ctpop.i8(i8 %1727)
  %1729 = and i8 %1728, 1
  %1730 = icmp eq i8 %1729, 0
  store i1 %1730, i1* %pf
  store i64 %1715, i64* %rdx
  store volatile i64 55474, i64* @assembly_address
  %1731 = load i64* %rdx
  %1732 = inttoptr i64 %1731 to i8*
  %1733 = load i8* %1732
  %1734 = zext i8 %1733 to i64
  store i64 %1734, i64* %rdx
  store volatile i64 55477, i64* @assembly_address
  %1735 = load i64* %rdx
  %1736 = trunc i64 %1735 to i8
  %1737 = zext i8 %1736 to i64
  store i64 %1737, i64* %rdx
  store volatile i64 55480, i64* @assembly_address
  %1738 = load i64* %rdx
  %1739 = trunc i64 %1738 to i32
  %1740 = load i1* %of
  %1741 = shl i32 %1739, 8
  %1742 = icmp eq i32 %1741, 0
  store i1 %1742, i1* %zf
  %1743 = icmp slt i32 %1741, 0
  store i1 %1743, i1* %sf
  %1744 = trunc i32 %1741 to i8
  %1745 = call i8 @llvm.ctpop.i8(i8 %1744)
  %1746 = and i8 %1745, 1
  %1747 = icmp eq i8 %1746, 0
  store i1 %1747, i1* %pf
  %1748 = zext i32 %1741 to i64
  store i64 %1748, i64* %rdx
  %1749 = shl i32 %1739, 7
  %1750 = lshr i32 %1749, 31
  %1751 = trunc i32 %1750 to i1
  store i1 %1751, i1* %cf
  %1752 = lshr i32 %1741, 31
  %1753 = icmp ne i32 %1752, %1750
  %1754 = select i1 false, i1 %1753, i1 %1740
  store i1 %1754, i1* %of
  store volatile i64 55483, i64* @assembly_address
  %1755 = load i64* %rax
  %1756 = trunc i64 %1755 to i32
  %1757 = load i64* %rdx
  %1758 = trunc i64 %1757 to i32
  %1759 = or i32 %1756, %1758
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1760 = icmp eq i32 %1759, 0
  store i1 %1760, i1* %zf
  %1761 = icmp slt i32 %1759, 0
  store i1 %1761, i1* %sf
  %1762 = trunc i32 %1759 to i8
  %1763 = call i8 @llvm.ctpop.i8(i8 %1762)
  %1764 = and i8 %1763, 1
  %1765 = icmp eq i8 %1764, 0
  store i1 %1765, i1* %pf
  %1766 = zext i32 %1759 to i64
  store i64 %1766, i64* %rax
  store volatile i64 55485, i64* @assembly_address
  %1767 = load i64* %rax
  %1768 = trunc i64 %1767 to i32
  %1769 = sext i32 %1768 to i64
  store i64 %1769, i64* %rax
  store volatile i64 55487, i64* @assembly_address
  %1770 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1771 = zext i32 %1770 to i64
  store i64 %1771, i64* %rdx
  store volatile i64 55493, i64* @assembly_address
  %1772 = load i64* %rdx
  %1773 = trunc i64 %1772 to i32
  %1774 = zext i32 %1773 to i64
  store i64 %1774, i64* %rdx
  store volatile i64 55495, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a886 to i64), i64* %rcx
  store volatile i64 55499, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rdx
  store volatile i64 55506, i64* @assembly_address
  %1775 = load i64* %rdx
  %1776 = load i64* %rcx
  %1777 = add i64 %1775, %1776
  %1778 = and i64 %1775, 15
  %1779 = and i64 %1776, 15
  %1780 = add i64 %1778, %1779
  %1781 = icmp ugt i64 %1780, 15
  %1782 = icmp ult i64 %1777, %1775
  %1783 = xor i64 %1775, %1777
  %1784 = xor i64 %1776, %1777
  %1785 = and i64 %1783, %1784
  %1786 = icmp slt i64 %1785, 0
  store i1 %1781, i1* %az
  store i1 %1782, i1* %cf
  store i1 %1786, i1* %of
  %1787 = icmp eq i64 %1777, 0
  store i1 %1787, i1* %zf
  %1788 = icmp slt i64 %1777, 0
  store i1 %1788, i1* %sf
  %1789 = trunc i64 %1777 to i8
  %1790 = call i8 @llvm.ctpop.i8(i8 %1789)
  %1791 = and i8 %1790, 1
  %1792 = icmp eq i8 %1791, 0
  store i1 %1792, i1* %pf
  store i64 %1777, i64* %rdx
  store volatile i64 55509, i64* @assembly_address
  %1793 = load i64* %rdx
  %1794 = inttoptr i64 %1793 to i8*
  %1795 = load i8* %1794
  %1796 = zext i8 %1795 to i64
  store i64 %1796, i64* %rdx
  store volatile i64 55512, i64* @assembly_address
  %1797 = load i64* %rdx
  %1798 = trunc i64 %1797 to i8
  %1799 = zext i8 %1798 to i64
  store i64 %1799, i64* %rdx
  store volatile i64 55515, i64* @assembly_address
  %1800 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %1801 = zext i32 %1800 to i64
  store i64 %1801, i64* %rcx
  store volatile i64 55521, i64* @assembly_address
  %1802 = load i64* %rcx
  %1803 = trunc i64 %1802 to i32
  %1804 = zext i32 %1803 to i64
  store i64 %1804, i64* %rcx
  store volatile i64 55523, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a887 to i64), i64* %rsi
  store volatile i64 55527, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rcx
  store volatile i64 55534, i64* @assembly_address
  %1805 = load i64* %rcx
  %1806 = load i64* %rsi
  %1807 = add i64 %1805, %1806
  %1808 = and i64 %1805, 15
  %1809 = and i64 %1806, 15
  %1810 = add i64 %1808, %1809
  %1811 = icmp ugt i64 %1810, 15
  %1812 = icmp ult i64 %1807, %1805
  %1813 = xor i64 %1805, %1807
  %1814 = xor i64 %1806, %1807
  %1815 = and i64 %1813, %1814
  %1816 = icmp slt i64 %1815, 0
  store i1 %1811, i1* %az
  store i1 %1812, i1* %cf
  store i1 %1816, i1* %of
  %1817 = icmp eq i64 %1807, 0
  store i1 %1817, i1* %zf
  %1818 = icmp slt i64 %1807, 0
  store i1 %1818, i1* %sf
  %1819 = trunc i64 %1807 to i8
  %1820 = call i8 @llvm.ctpop.i8(i8 %1819)
  %1821 = and i8 %1820, 1
  %1822 = icmp eq i8 %1821, 0
  store i1 %1822, i1* %pf
  store i64 %1807, i64* %rcx
  store volatile i64 55537, i64* @assembly_address
  %1823 = load i64* %rcx
  %1824 = inttoptr i64 %1823 to i8*
  %1825 = load i8* %1824
  %1826 = zext i8 %1825 to i64
  store i64 %1826, i64* %rcx
  store volatile i64 55540, i64* @assembly_address
  %1827 = load i64* %rcx
  %1828 = trunc i64 %1827 to i8
  %1829 = zext i8 %1828 to i64
  store i64 %1829, i64* %rcx
  store volatile i64 55543, i64* @assembly_address
  %1830 = load i64* %rcx
  %1831 = trunc i64 %1830 to i32
  %1832 = load i1* %of
  %1833 = shl i32 %1831, 8
  %1834 = icmp eq i32 %1833, 0
  store i1 %1834, i1* %zf
  %1835 = icmp slt i32 %1833, 0
  store i1 %1835, i1* %sf
  %1836 = trunc i32 %1833 to i8
  %1837 = call i8 @llvm.ctpop.i8(i8 %1836)
  %1838 = and i8 %1837, 1
  %1839 = icmp eq i8 %1838, 0
  store i1 %1839, i1* %pf
  %1840 = zext i32 %1833 to i64
  store i64 %1840, i64* %rcx
  %1841 = shl i32 %1831, 7
  %1842 = lshr i32 %1841, 31
  %1843 = trunc i32 %1842 to i1
  store i1 %1843, i1* %cf
  %1844 = lshr i32 %1833, 31
  %1845 = icmp ne i32 %1844, %1842
  %1846 = select i1 false, i1 %1845, i1 %1832
  store i1 %1846, i1* %of
  store volatile i64 55546, i64* @assembly_address
  %1847 = load i64* %rdx
  %1848 = trunc i64 %1847 to i32
  %1849 = load i64* %rcx
  %1850 = trunc i64 %1849 to i32
  %1851 = or i32 %1848, %1850
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1852 = icmp eq i32 %1851, 0
  store i1 %1852, i1* %zf
  %1853 = icmp slt i32 %1851, 0
  store i1 %1853, i1* %sf
  %1854 = trunc i32 %1851 to i8
  %1855 = call i8 @llvm.ctpop.i8(i8 %1854)
  %1856 = and i8 %1855, 1
  %1857 = icmp eq i8 %1856, 0
  store i1 %1857, i1* %pf
  %1858 = zext i32 %1851 to i64
  store i64 %1858, i64* %rdx
  store volatile i64 55548, i64* @assembly_address
  %1859 = load i64* %rdx
  %1860 = trunc i64 %1859 to i32
  %1861 = sext i32 %1860 to i64
  store i64 %1861, i64* %rdx
  store volatile i64 55551, i64* @assembly_address
  %1862 = load i64* %rdx
  %1863 = load i1* %of
  %1864 = shl i64 %1862, 16
  %1865 = icmp eq i64 %1864, 0
  store i1 %1865, i1* %zf
  %1866 = icmp slt i64 %1864, 0
  store i1 %1866, i1* %sf
  %1867 = trunc i64 %1864 to i8
  %1868 = call i8 @llvm.ctpop.i8(i8 %1867)
  %1869 = and i8 %1868, 1
  %1870 = icmp eq i8 %1869, 0
  store i1 %1870, i1* %pf
  store i64 %1864, i64* %rdx
  %1871 = shl i64 %1862, 15
  %1872 = lshr i64 %1871, 63
  %1873 = trunc i64 %1872 to i1
  store i1 %1873, i1* %cf
  %1874 = lshr i64 %1864, 63
  %1875 = icmp ne i64 %1874, %1872
  %1876 = select i1 false, i1 %1875, i1 %1863
  store i1 %1876, i1* %of
  store volatile i64 55555, i64* @assembly_address
  %1877 = load i64* %rax
  %1878 = load i64* %rdx
  %1879 = or i64 %1877, %1878
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1880 = icmp eq i64 %1879, 0
  store i1 %1880, i1* %zf
  %1881 = icmp slt i64 %1879, 0
  store i1 %1881, i1* %sf
  %1882 = trunc i64 %1879 to i8
  %1883 = call i8 @llvm.ctpop.i8(i8 %1882)
  %1884 = and i8 %1883, 1
  %1885 = icmp eq i8 %1884, 0
  store i1 %1885, i1* %pf
  store i64 %1879, i64* %rax
  store volatile i64 55558, i64* @assembly_address
  %1886 = load i64* %rax
  %1887 = sub i64 %1886, 67324752
  %1888 = and i64 %1886, 15
  %1889 = icmp ugt i64 %1888, 15
  %1890 = icmp ult i64 %1886, 67324752
  %1891 = xor i64 %1886, 67324752
  %1892 = xor i64 %1886, %1887
  %1893 = and i64 %1891, %1892
  %1894 = icmp slt i64 %1893, 0
  store i1 %1889, i1* %az
  store i1 %1890, i1* %cf
  store i1 %1894, i1* %of
  %1895 = icmp eq i64 %1887, 0
  store i1 %1895, i1* %zf
  %1896 = icmp slt i64 %1887, 0
  store i1 %1896, i1* %sf
  %1897 = trunc i64 %1887 to i8
  %1898 = call i8 @llvm.ctpop.i8(i8 %1897)
  %1899 = and i8 %1898, 1
  %1900 = icmp eq i8 %1899, 0
  store i1 %1900, i1* %pf
  store volatile i64 55564, i64* @assembly_address
  %1901 = load i1* %zf
  %1902 = icmp eq i1 %1901, false
  br i1 %1902, label %block_d995, label %block_d912

block_d912:                                       ; preds = %block_d884
  store volatile i64 55570, i64* @assembly_address
  %1903 = load i32* bitcast (i64* @global_var_2165e0 to i32*)
  %1904 = zext i32 %1903 to i64
  store i64 %1904, i64* %rax
  store volatile i64 55576, i64* @assembly_address
  %1905 = load i64* %rax
  %1906 = trunc i64 %1905 to i32
  %1907 = load i64* %rax
  %1908 = trunc i64 %1907 to i32
  %1909 = and i32 %1906, %1908
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1910 = icmp eq i32 %1909, 0
  store i1 %1910, i1* %zf
  %1911 = icmp slt i32 %1909, 0
  store i1 %1911, i1* %sf
  %1912 = trunc i32 %1909 to i8
  %1913 = call i8 @llvm.ctpop.i8(i8 %1912)
  %1914 = and i8 %1913, 1
  %1915 = icmp eq i8 %1914, 0
  store i1 %1915, i1* %pf
  store volatile i64 55578, i64* @assembly_address
  %1916 = load i1* %zf
  br i1 %1916, label %block_d965, label %block_d91c

block_d91c:                                       ; preds = %block_d912
  store volatile i64 55580, i64* @assembly_address
  %1917 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %1918 = zext i32 %1917 to i64
  store i64 %1918, i64* %rax
  store volatile i64 55586, i64* @assembly_address
  %1919 = load i64* %rax
  %1920 = trunc i64 %1919 to i32
  %1921 = load i64* %rax
  %1922 = trunc i64 %1921 to i32
  %1923 = and i32 %1920, %1922
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1924 = icmp eq i32 %1923, 0
  store i1 %1924, i1* %zf
  %1925 = icmp slt i32 %1923, 0
  store i1 %1925, i1* %sf
  %1926 = trunc i32 %1923 to i8
  %1927 = call i8 @llvm.ctpop.i8(i8 %1926)
  %1928 = and i8 %1927, 1
  %1929 = icmp eq i8 %1928, 0
  store i1 %1929, i1* %pf
  store volatile i64 55588, i64* @assembly_address
  %1930 = load i1* %zf
  %1931 = icmp eq i1 %1930, false
  br i1 %1931, label %block_d94f, label %block_d926

block_d926:                                       ; preds = %block_d91c
  store volatile i64 55590, i64* @assembly_address
  %1932 = load i64* @global_var_25f4c8
  store i64 %1932, i64* %rdx
  store volatile i64 55597, i64* @assembly_address
  %1933 = load i64* @global_var_216580
  store i64 %1933, i64* %rax
  store volatile i64 55604, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 55611, i64* @assembly_address
  store i64 ptrtoint ([46 x i8]* @global_var_123c8 to i64), i64* %rsi
  store volatile i64 55618, i64* @assembly_address
  %1934 = load i64* %rax
  store i64 %1934, i64* %rdi
  store volatile i64 55621, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 55626, i64* @assembly_address
  %1935 = load i64* %rdi
  %1936 = inttoptr i64 %1935 to %_IO_FILE*
  %1937 = load i64* %rsi
  %1938 = inttoptr i64 %1937 to i8*
  %1939 = load i64* %rdx
  %1940 = inttoptr i64 %1939 to i8*
  %1941 = load i64* %rcx
  %1942 = inttoptr i64 %1941 to i8*
  %1943 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1936, i8* %1938, i8* %1940, i8* %1942)
  %1944 = sext i32 %1943 to i64
  store i64 %1944, i64* %rax
  %1945 = sext i32 %1943 to i64
  store i64 %1945, i64* %rax
  br label %block_d94f

block_d94f:                                       ; preds = %block_d926, %block_d91c
  store volatile i64 55631, i64* @assembly_address
  %1946 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %1947 = zext i32 %1946 to i64
  store i64 %1947, i64* %rax
  store volatile i64 55637, i64* @assembly_address
  %1948 = load i64* %rax
  %1949 = trunc i64 %1948 to i32
  %1950 = load i64* %rax
  %1951 = trunc i64 %1950 to i32
  %1952 = and i32 %1949, %1951
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1953 = icmp eq i32 %1952, 0
  store i1 %1953, i1* %zf
  %1954 = icmp slt i32 %1952, 0
  store i1 %1954, i1* %sf
  %1955 = trunc i32 %1952 to i8
  %1956 = call i8 @llvm.ctpop.i8(i8 %1955)
  %1957 = and i8 %1956, 1
  %1958 = icmp eq i8 %1957, 0
  store i1 %1958, i1* %pf
  store volatile i64 55639, i64* @assembly_address
  %1959 = load i1* %zf
  %1960 = icmp eq i1 %1959, false
  br i1 %1960, label %block_d995, label %block_d959

block_d959:                                       ; preds = %block_d94f
  store volatile i64 55641, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 55651, i64* @assembly_address
  br label %block_d995

block_d965:                                       ; preds = %block_d912
  store volatile i64 55653, i64* @assembly_address
  %1961 = load i64* @global_var_25f4c8
  store i64 %1961, i64* %rdx
  store volatile i64 55660, i64* @assembly_address
  %1962 = load i64* @global_var_216580
  store i64 %1962, i64* %rax
  store volatile i64 55667, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 55674, i64* @assembly_address
  store i64 ptrtoint ([45 x i8]* @global_var_123f8 to i64), i64* %rsi
  store volatile i64 55681, i64* @assembly_address
  %1963 = load i64* %rax
  store i64 %1963, i64* %rdi
  store volatile i64 55684, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 55689, i64* @assembly_address
  %1964 = load i64* %rdi
  %1965 = inttoptr i64 %1964 to %_IO_FILE*
  %1966 = load i64* %rsi
  %1967 = inttoptr i64 %1966 to i8*
  %1968 = load i64* %rdx
  %1969 = inttoptr i64 %1968 to i8*
  %1970 = load i64* %rcx
  %1971 = inttoptr i64 %1970 to i8*
  %1972 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %1965, i8* %1967, i8* %1969, i8* %1971)
  %1973 = sext i32 %1972 to i64
  store i64 %1973, i64* %rax
  %1974 = sext i32 %1972 to i64
  store i64 %1974, i64* %rax
  store volatile i64 55694, i64* @assembly_address
  store i32 1, i32* %stack_var_-80
  br label %block_d995

block_d995:                                       ; preds = %block_d965, %block_d959, %block_d94f, %block_d884, %block_d86d, %block_d85f
  store volatile i64 55701, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_21a400 to i32*)
  store volatile i64 55711, i64* @assembly_address
  %1975 = load i32* bitcast (i64* @global_var_21a400 to i32*)
  %1976 = zext i32 %1975 to i64
  store i64 %1976, i64* %rax
  store volatile i64 55717, i64* @assembly_address
  %1977 = load i64* %rax
  %1978 = trunc i64 %1977 to i32
  store i32 %1978, i32* bitcast (i64* @global_var_21a404 to i32*)
  store volatile i64 55723, i64* @assembly_address
  %1979 = load i32* %stack_var_-80
  %1980 = and i32 %1979, 15
  %1981 = icmp ugt i32 %1980, 15
  %1982 = icmp ult i32 %1979, 0
  %1983 = xor i32 %1979, 0
  %1984 = and i32 %1983, 0
  %1985 = icmp slt i32 %1984, 0
  store i1 %1981, i1* %az
  store i1 %1982, i1* %cf
  store i1 %1985, i1* %of
  %1986 = icmp eq i32 %1979, 0
  store i1 %1986, i1* %zf
  %1987 = icmp slt i32 %1979, 0
  store i1 %1987, i1* %sf
  %1988 = trunc i32 %1979 to i8
  %1989 = call i8 @llvm.ctpop.i8(i8 %1988)
  %1990 = and i8 %1989, 1
  %1991 = icmp eq i8 %1990, 0
  store i1 %1991, i1* %pf
  store volatile i64 55727, i64* @assembly_address
  %1992 = load i1* %zf
  %1993 = icmp eq i1 %1992, false
  br i1 %1993, label %block_d9b8, label %block_d9b1

block_d9b1:                                       ; preds = %block_d995
  store volatile i64 55729, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 55734, i64* @assembly_address
  br label %block_d9d4

block_d9b8:                                       ; preds = %block_d995
  store volatile i64 55736, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_2165f0 to i32*)
  store volatile i64 55746, i64* @assembly_address
  %1994 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %1995 = zext i32 %1994 to i64
  store i64 %1995, i64* %rax
  store volatile i64 55752, i64* @assembly_address
  %1996 = load i64* %rax
  %1997 = trunc i64 %1996 to i32
  %1998 = load i64* %rax
  %1999 = trunc i64 %1998 to i32
  %2000 = and i32 %1997, %1999
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2001 = icmp eq i32 %2000, 0
  store i1 %2001, i1* %zf
  %2002 = icmp slt i32 %2000, 0
  store i1 %2002, i1* %sf
  %2003 = trunc i32 %2000 to i8
  %2004 = call i8 @llvm.ctpop.i8(i8 %2003)
  %2005 = and i8 %2004, 1
  %2006 = icmp eq i8 %2005, 0
  store i1 %2006, i1* %pf
  store volatile i64 55754, i64* @assembly_address
  %2007 = load i1* %zf
  %2008 = icmp eq i1 %2007, false
  br i1 %2008, label %block_d9d1, label %block_d9cc

block_d9cc:                                       ; preds = %block_d9b8
  store volatile i64 55756, i64* @assembly_address
  %2009 = call i64 @abort_gzip()
  store i64 %2009, i64* %rax
  store i64 %2009, i64* %rax
  store i64 %2009, i64* %rax
  unreachable

block_d9d1:                                       ; preds = %block_d9b8
  store volatile i64 55761, i64* @assembly_address
  %2010 = load i32* %stack_var_-80
  %2011 = zext i32 %2010 to i64
  store i64 %2011, i64* %rax
  br label %block_d9d4

block_d9d4:                                       ; preds = %block_d9d1, %block_d9b1
  store volatile i64 55764, i64* @assembly_address
  %2012 = load i64* %stack_var_-32
  store i64 %2012, i64* %rbx
  store volatile i64 55768, i64* @assembly_address
  %2013 = load i64* %rbx
  %2014 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %2015 = xor i64 %2013, %2014
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %2016 = icmp eq i64 %2015, 0
  store i1 %2016, i1* %zf
  %2017 = icmp slt i64 %2015, 0
  store i1 %2017, i1* %sf
  %2018 = trunc i64 %2015 to i8
  %2019 = call i8 @llvm.ctpop.i8(i8 %2018)
  %2020 = and i8 %2019, 1
  %2021 = icmp eq i8 %2020, 0
  store i1 %2021, i1* %pf
  store i64 %2015, i64* %rbx
  store volatile i64 55777, i64* @assembly_address
  %2022 = load i1* %zf
  br i1 %2022, label %block_d9e8, label %block_d9e3

block_d9e3:                                       ; preds = %block_d9d4
  store volatile i64 55779, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_d9e8:                                       ; preds = %block_d9d4
  store volatile i64 55784, i64* @assembly_address
  %2023 = load i64* %rsp
  %2024 = add i64 %2023, 88
  %2025 = and i64 %2023, 15
  %2026 = add i64 %2025, 8
  %2027 = icmp ugt i64 %2026, 15
  %2028 = icmp ult i64 %2024, %2023
  %2029 = xor i64 %2023, %2024
  %2030 = xor i64 88, %2024
  %2031 = and i64 %2029, %2030
  %2032 = icmp slt i64 %2031, 0
  store i1 %2027, i1* %az
  store i1 %2028, i1* %cf
  store i1 %2032, i1* %of
  %2033 = icmp eq i64 %2024, 0
  store i1 %2033, i1* %zf
  %2034 = icmp slt i64 %2024, 0
  store i1 %2034, i1* %sf
  %2035 = trunc i64 %2024 to i8
  %2036 = call i8 @llvm.ctpop.i8(i8 %2035)
  %2037 = and i8 %2036, 1
  %2038 = icmp eq i8 %2037, 0
  store i1 %2038, i1* %pf
  %2039 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %2039, i64* %rsp
  store volatile i64 55788, i64* @assembly_address
  %2040 = load i64* %stack_var_-16
  store i64 %2040, i64* %rbx
  %2041 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2041, i64* %rsp
  store volatile i64 55789, i64* @assembly_address
  %2042 = load i64* %stack_var_-8
  store i64 %2042, i64* %rbp
  %2043 = ptrtoint i64* %stack_var_0 to i64
  store i64 %2043, i64* %rsp
  store volatile i64 55790, i64* @assembly_address
  %2044 = load i64* %rax
  ret i64 %2044
}

declare i64 @238(i64, i32)

declare i64 @239(i64, i64)

define i64 @copy(i32 %arg1, i64 %arg2) {
block_d9ef:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 55791, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 55792, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 55795, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 55799, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-28
  store volatile i64 55802, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-32
  store volatile i64 55805, i64* @assembly_address
  %24 = call i32* @__errno_location()
  %25 = ptrtoint i32* %24 to i64
  store i64 %25, i64* %rax
  %26 = ptrtoint i32* %24 to i64
  store i64 %26, i64* %rax
  %27 = ptrtoint i32* %24 to i64
  store i64 %27, i64* %rax
  store volatile i64 55810, i64* @assembly_address
  %28 = load i64* %rax
  %29 = inttoptr i64 %28 to i32*
  store i32 0, i32* %29
  store volatile i64 55816, i64* @assembly_address
  br label %block_daab

block_da0d:                                       ; preds = %block_daab
  store volatile i64 55821, i64* @assembly_address
  %30 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %31 = zext i32 %30 to i64
  store i64 %31, i64* %rdx
  store volatile i64 55827, i64* @assembly_address
  %32 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %33 = zext i32 %32 to i64
  store i64 %33, i64* %rax
  store volatile i64 55833, i64* @assembly_address
  %34 = load i64* %rdx
  %35 = trunc i64 %34 to i32
  %36 = load i64* %rax
  %37 = trunc i64 %36 to i32
  %38 = sub i32 %35, %37
  %39 = and i32 %35, 15
  %40 = and i32 %37, 15
  %41 = sub i32 %39, %40
  %42 = icmp ugt i32 %41, 15
  %43 = icmp ult i32 %35, %37
  %44 = xor i32 %35, %37
  %45 = xor i32 %35, %38
  %46 = and i32 %44, %45
  %47 = icmp slt i32 %46, 0
  store i1 %42, i1* %az
  store i1 %43, i1* %cf
  store i1 %47, i1* %of
  %48 = icmp eq i32 %38, 0
  store i1 %48, i1* %zf
  %49 = icmp slt i32 %38, 0
  store i1 %49, i1* %sf
  %50 = trunc i32 %38 to i8
  %51 = call i8 @llvm.ctpop.i8(i8 %50)
  %52 = and i8 %51, 1
  %53 = icmp eq i8 %52, 0
  store i1 %53, i1* %pf
  %54 = zext i32 %38 to i64
  store i64 %54, i64* %rdx
  store volatile i64 55835, i64* @assembly_address
  %55 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %56 = zext i32 %55 to i64
  store i64 %56, i64* %rax
  store volatile i64 55841, i64* @assembly_address
  %57 = load i64* %rax
  %58 = trunc i64 %57 to i32
  %59 = zext i32 %58 to i64
  store i64 %59, i64* %rcx
  store volatile i64 55843, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 55850, i64* @assembly_address
  %60 = load i64* %rcx
  %61 = load i64* %rax
  %62 = add i64 %60, %61
  %63 = and i64 %60, 15
  %64 = and i64 %61, 15
  %65 = add i64 %63, %64
  %66 = icmp ugt i64 %65, 15
  %67 = icmp ult i64 %62, %60
  %68 = xor i64 %60, %62
  %69 = xor i64 %61, %62
  %70 = and i64 %68, %69
  %71 = icmp slt i64 %70, 0
  store i1 %66, i1* %az
  store i1 %67, i1* %cf
  store i1 %71, i1* %of
  %72 = icmp eq i64 %62, 0
  store i1 %72, i1* %zf
  %73 = icmp slt i64 %62, 0
  store i1 %73, i1* %sf
  %74 = trunc i64 %62 to i8
  %75 = call i8 @llvm.ctpop.i8(i8 %74)
  %76 = and i8 %75, 1
  %77 = icmp eq i8 %76, 0
  store i1 %77, i1* %pf
  store i64 %62, i64* %rcx
  store volatile i64 55853, i64* @assembly_address
  %78 = load i32* %stack_var_-32
  %79 = zext i32 %78 to i64
  store i64 %79, i64* %rax
  store volatile i64 55856, i64* @assembly_address
  %80 = load i64* %rcx
  store i64 %80, i64* %rsi
  store volatile i64 55859, i64* @assembly_address
  %81 = load i64* %rax
  %82 = trunc i64 %81 to i32
  %83 = zext i32 %82 to i64
  store i64 %83, i64* %rdi
  store volatile i64 55861, i64* @assembly_address
  %84 = load i64* %rdi
  %85 = load i64* %rsi
  %86 = inttoptr i64 %85 to i8*
  %87 = load i64* %rdx
  %88 = trunc i64 %84 to i32
  %89 = call i64 @write_buf(i32 %88, i8* %86, i64 %87)
  store i64 %89, i64* %rax
  store i64 %89, i64* %rax
  store volatile i64 55866, i64* @assembly_address
  %90 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %91 = zext i32 %90 to i64
  store i64 %91, i64* %rdx
  store volatile i64 55872, i64* @assembly_address
  %92 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rax
  store volatile i64 55878, i64* @assembly_address
  %94 = load i64* %rdx
  %95 = trunc i64 %94 to i32
  %96 = load i64* %rax
  %97 = trunc i64 %96 to i32
  %98 = sub i32 %95, %97
  %99 = and i32 %95, 15
  %100 = and i32 %97, 15
  %101 = sub i32 %99, %100
  %102 = icmp ugt i32 %101, 15
  %103 = icmp ult i32 %95, %97
  %104 = xor i32 %95, %97
  %105 = xor i32 %95, %98
  %106 = and i32 %104, %105
  %107 = icmp slt i32 %106, 0
  store i1 %102, i1* %az
  store i1 %103, i1* %cf
  store i1 %107, i1* %of
  %108 = icmp eq i32 %98, 0
  store i1 %108, i1* %zf
  %109 = icmp slt i32 %98, 0
  store i1 %109, i1* %sf
  %110 = trunc i32 %98 to i8
  %111 = call i8 @llvm.ctpop.i8(i8 %110)
  %112 = and i8 %111, 1
  %113 = icmp eq i8 %112, 0
  store i1 %113, i1* %pf
  %114 = zext i32 %98 to i64
  store i64 %114, i64* %rdx
  store volatile i64 55880, i64* @assembly_address
  %115 = load i64* %rdx
  %116 = trunc i64 %115 to i32
  %117 = zext i32 %116 to i64
  store i64 %117, i64* %rax
  store volatile i64 55882, i64* @assembly_address
  %118 = load i64* %rax
  %119 = trunc i64 %118 to i32
  %120 = zext i32 %119 to i64
  store i64 %120, i64* %rdx
  store volatile i64 55884, i64* @assembly_address
  %121 = load i64* @global_var_25f4c0
  store i64 %121, i64* %rax
  store volatile i64 55891, i64* @assembly_address
  %122 = load i64* %rax
  %123 = load i64* %rdx
  %124 = add i64 %122, %123
  %125 = and i64 %122, 15
  %126 = and i64 %123, 15
  %127 = add i64 %125, %126
  %128 = icmp ugt i64 %127, 15
  %129 = icmp ult i64 %124, %122
  %130 = xor i64 %122, %124
  %131 = xor i64 %123, %124
  %132 = and i64 %130, %131
  %133 = icmp slt i64 %132, 0
  store i1 %128, i1* %az
  store i1 %129, i1* %cf
  store i1 %133, i1* %of
  %134 = icmp eq i64 %124, 0
  store i1 %134, i1* %zf
  %135 = icmp slt i64 %124, 0
  store i1 %135, i1* %sf
  %136 = trunc i64 %124 to i8
  %137 = call i8 @llvm.ctpop.i8(i8 %136)
  %138 = and i8 %137, 1
  %139 = icmp eq i8 %138, 0
  store i1 %139, i1* %pf
  store i64 %124, i64* %rax
  store volatile i64 55894, i64* @assembly_address
  %140 = load i64* %rax
  store i64 %140, i64* @global_var_25f4c0
  store volatile i64 55901, i64* @assembly_address
  %141 = load i32* %stack_var_-28
  %142 = zext i32 %141 to i64
  store i64 %142, i64* %rax
  store volatile i64 55904, i64* @assembly_address
  store i64 32768, i64* %rdx
  store volatile i64 55909, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rsi
  store volatile i64 55916, i64* @assembly_address
  %143 = load i64* %rax
  %144 = trunc i64 %143 to i32
  %145 = zext i32 %144 to i64
  store i64 %145, i64* %rdi
  store volatile i64 55918, i64* @assembly_address
  %146 = load i64* %rdi
  %147 = load i64* %rsi
  %148 = load i64* %rdx
  %149 = trunc i64 %146 to i32
  %150 = call i64 @read_buffer(i32 %149, i64 %147, i64 %148)
  store i64 %150, i64* %rax
  store i64 %150, i64* %rax
  store volatile i64 55923, i64* @assembly_address
  %151 = load i64* %rax
  %152 = trunc i64 %151 to i32
  store i32 %152, i32* %stack_var_-12
  store volatile i64 55926, i64* @assembly_address
  %153 = load i32* %stack_var_-12
  %154 = sub i32 %153, -1
  %155 = and i32 %153, 15
  %156 = sub i32 %155, 15
  %157 = icmp ugt i32 %156, 15
  %158 = icmp ult i32 %153, -1
  %159 = xor i32 %153, -1
  %160 = xor i32 %153, %154
  %161 = and i32 %159, %160
  %162 = icmp slt i32 %161, 0
  store i1 %157, i1* %az
  store i1 %158, i1* %cf
  store i1 %162, i1* %of
  %163 = icmp eq i32 %154, 0
  store i1 %163, i1* %zf
  %164 = icmp slt i32 %154, 0
  store i1 %164, i1* %sf
  %165 = trunc i32 %154 to i8
  %166 = call i8 @llvm.ctpop.i8(i8 %165)
  %167 = and i8 %166, 1
  %168 = icmp eq i8 %167, 0
  store i1 %168, i1* %pf
  store volatile i64 55930, i64* @assembly_address
  %169 = load i1* %zf
  %170 = icmp eq i1 %169, false
  br i1 %170, label %block_da81, label %block_da7c

block_da7c:                                       ; preds = %block_da0d
  store volatile i64 55932, i64* @assembly_address
  %171 = call i64 @read_error()
  store i64 %171, i64* %rax
  store i64 %171, i64* %rax
  store i64 %171, i64* %rax
  unreachable

block_da81:                                       ; preds = %block_da0d
  store volatile i64 55937, i64* @assembly_address
  %172 = load i32* %stack_var_-12
  %173 = zext i32 %172 to i64
  store i64 %173, i64* %rax
  store volatile i64 55940, i64* @assembly_address
  %174 = load i64* %rax
  %175 = trunc i64 %174 to i32
  %176 = sext i32 %175 to i64
  store i64 %176, i64* %rdx
  store volatile i64 55943, i64* @assembly_address
  %177 = load i64* @global_var_21a860
  store i64 %177, i64* %rax
  store volatile i64 55950, i64* @assembly_address
  %178 = load i64* %rax
  %179 = load i64* %rdx
  %180 = add i64 %178, %179
  %181 = and i64 %178, 15
  %182 = and i64 %179, 15
  %183 = add i64 %181, %182
  %184 = icmp ugt i64 %183, 15
  %185 = icmp ult i64 %180, %178
  %186 = xor i64 %178, %180
  %187 = xor i64 %179, %180
  %188 = and i64 %186, %187
  %189 = icmp slt i64 %188, 0
  store i1 %184, i1* %az
  store i1 %185, i1* %cf
  store i1 %189, i1* %of
  %190 = icmp eq i64 %180, 0
  store i1 %190, i1* %zf
  %191 = icmp slt i64 %180, 0
  store i1 %191, i1* %sf
  %192 = trunc i64 %180 to i8
  %193 = call i8 @llvm.ctpop.i8(i8 %192)
  %194 = and i8 %193, 1
  %195 = icmp eq i8 %194, 0
  store i1 %195, i1* %pf
  store i64 %180, i64* %rax
  store volatile i64 55953, i64* @assembly_address
  %196 = load i64* %rax
  store i64 %196, i64* @global_var_21a860
  store volatile i64 55960, i64* @assembly_address
  %197 = load i32* %stack_var_-12
  %198 = zext i32 %197 to i64
  store i64 %198, i64* %rax
  store volatile i64 55963, i64* @assembly_address
  %199 = load i64* %rax
  %200 = trunc i64 %199 to i32
  store i32 %200, i32* bitcast (i64* @global_var_25f4e4 to i32*)
  store volatile i64 55969, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_24a884 to i32*)
  br label %block_daab

block_daab:                                       ; preds = %block_da81, %block_d9ef
  store volatile i64 55979, i64* @assembly_address
  %201 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %202 = zext i32 %201 to i64
  store i64 %202, i64* %rdx
  store volatile i64 55985, i64* @assembly_address
  %203 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %204 = zext i32 %203 to i64
  store i64 %204, i64* %rax
  store volatile i64 55991, i64* @assembly_address
  %205 = load i64* %rdx
  %206 = trunc i64 %205 to i32
  %207 = load i64* %rax
  %208 = trunc i64 %207 to i32
  %209 = sub i32 %206, %208
  %210 = and i32 %206, 15
  %211 = and i32 %208, 15
  %212 = sub i32 %210, %211
  %213 = icmp ugt i32 %212, 15
  %214 = icmp ult i32 %206, %208
  %215 = xor i32 %206, %208
  %216 = xor i32 %206, %209
  %217 = and i32 %215, %216
  %218 = icmp slt i32 %217, 0
  store i1 %213, i1* %az
  store i1 %214, i1* %cf
  store i1 %218, i1* %of
  %219 = icmp eq i32 %209, 0
  store i1 %219, i1* %zf
  %220 = icmp slt i32 %209, 0
  store i1 %220, i1* %sf
  %221 = trunc i32 %209 to i8
  %222 = call i8 @llvm.ctpop.i8(i8 %221)
  %223 = and i8 %222, 1
  %224 = icmp eq i8 %223, 0
  store i1 %224, i1* %pf
  store volatile i64 55993, i64* @assembly_address
  %225 = load i1* %cf
  %226 = load i1* %zf
  %227 = or i1 %225, %226
  %228 = icmp ne i1 %227, true
  br i1 %228, label %block_da0d, label %block_dabf

block_dabf:                                       ; preds = %block_daab
  store volatile i64 55999, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 56004, i64* @assembly_address
  %229 = load i64* %stack_var_-8
  store i64 %229, i64* %rbp
  %230 = ptrtoint i64* %stack_var_0 to i64
  store i64 %230, i64* %rsp
  store volatile i64 56005, i64* @assembly_address
  %231 = load i64* %rax
  ret i64 %231
}

declare i64 @240(i64, i32)

declare i64 @241(i64, i64)

define i64 @updcrc(i8* %arg1, i32 %arg2) {
block_dac6:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-24 = alloca i8*
  %2 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56006, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 56007, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 56010, i64* @assembly_address
  %6 = load i64* %rbx
  store i64 %6, i64* %stack_var_-16
  %7 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %7, i64* %rsp
  store volatile i64 56011, i64* @assembly_address
  %8 = load i64* %rdi
  %9 = inttoptr i64 %8 to i8*
  store i8* %9, i8** %stack_var_-24
  store volatile i64 56015, i64* @assembly_address
  %10 = load i64* %rsi
  %11 = trunc i64 %10 to i32
  store i32 %11, i32* %stack_var_-28
  store volatile i64 56018, i64* @assembly_address
  %12 = load i8** %stack_var_-24
  %13 = ptrtoint i8* %12 to i64
  %14 = and i64 %13, 15
  %15 = icmp ugt i64 %14, 15
  %16 = icmp ult i64 %13, 0
  %17 = xor i64 %13, 0
  %18 = and i64 %17, 0
  %19 = icmp slt i64 %18, 0
  store i1 %15, i1* %az
  store i1 %16, i1* %cf
  store i1 %19, i1* %of
  %20 = icmp eq i64 %13, 0
  store i1 %20, i1* %zf
  %21 = icmp slt i64 %13, 0
  store i1 %21, i1* %sf
  %22 = trunc i64 %13 to i8
  %23 = call i8 @llvm.ctpop.i8(i8 %22)
  %24 = and i8 %23, 1
  %25 = icmp eq i8 %24, 0
  store i1 %25, i1* %pf
  store volatile i64 56023, i64* @assembly_address
  %26 = load i1* %zf
  %27 = icmp eq i1 %26, false
  br i1 %27, label %block_dae0, label %block_dad9

block_dad9:                                       ; preds = %block_dac6
  store volatile i64 56025, i64* @assembly_address
  store i64 4294967295, i64* %rbx
  store volatile i64 56030, i64* @assembly_address
  br label %block_db32

block_dae0:                                       ; preds = %block_dac6
  store volatile i64 56032, i64* @assembly_address
  %28 = load i64* @global_var_216550
  store i64 %28, i64* %rbx
  store volatile i64 56039, i64* @assembly_address
  %29 = load i32* %stack_var_-28
  %30 = and i32 %29, 15
  %31 = icmp ugt i32 %30, 15
  %32 = icmp ult i32 %29, 0
  %33 = xor i32 %29, 0
  %34 = and i32 %33, 0
  %35 = icmp slt i32 %34, 0
  store i1 %31, i1* %az
  store i1 %32, i1* %cf
  store i1 %35, i1* %of
  %36 = icmp eq i32 %29, 0
  store i1 %36, i1* %zf
  %37 = icmp slt i32 %29, 0
  store i1 %37, i1* %sf
  %38 = trunc i32 %29 to i8
  %39 = call i8 @llvm.ctpop.i8(i8 %38)
  %40 = and i8 %39, 1
  %41 = icmp eq i8 %40, 0
  store i1 %41, i1* %pf
  store volatile i64 56043, i64* @assembly_address
  %42 = load i1* %zf
  br i1 %42, label %block_db32, label %block_daed

block_daed:                                       ; preds = %block_daed, %block_dae0
  store volatile i64 56045, i64* @assembly_address
  %43 = load i64* %rbx
  %44 = trunc i64 %43 to i32
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rcx
  store volatile i64 56047, i64* @assembly_address
  %46 = load i8** %stack_var_-24
  %47 = ptrtoint i8* %46 to i64
  store i64 %47, i64* %rax
  store volatile i64 56051, i64* @assembly_address
  %48 = load i64* %rax
  %49 = add i64 %48, 1
  store i64 %49, i64* %rdx
  store volatile i64 56055, i64* @assembly_address
  %50 = load i64* %rdx
  %51 = inttoptr i64 %50 to i8*
  store i8* %51, i8** %stack_var_-24
  store volatile i64 56059, i64* @assembly_address
  %52 = load i64* %rax
  %53 = inttoptr i64 %52 to i8*
  %54 = load i8* %53
  %55 = zext i8 %54 to i64
  store i64 %55, i64* %rax
  store volatile i64 56062, i64* @assembly_address
  %56 = load i64* %rax
  %57 = trunc i64 %56 to i8
  %58 = zext i8 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 56065, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  %61 = load i64* %rcx
  %62 = trunc i64 %61 to i32
  %63 = xor i32 %60, %62
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %64 = icmp eq i32 %63, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i32 %63, 0
  store i1 %65, i1* %sf
  %66 = trunc i32 %63 to i8
  %67 = call i8 @llvm.ctpop.i8(i8 %66)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  %70 = zext i32 %63 to i64
  store i64 %70, i64* %rax
  store volatile i64 56067, i64* @assembly_address
  %71 = load i64* %rax
  %72 = trunc i64 %71 to i8
  %73 = zext i8 %72 to i64
  store i64 %73, i64* %rax
  store volatile i64 56070, i64* @assembly_address
  %74 = load i64* %rax
  %75 = trunc i64 %74 to i32
  %76 = sext i32 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 56072, i64* @assembly_address
  %77 = load i64* %rax
  %78 = mul i64 %77, 8
  store i64 %78, i64* %rdx
  store volatile i64 56080, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_12440 to i64), i64* %rax
  store volatile i64 56087, i64* @assembly_address
  %79 = load i64* %rdx
  %80 = load i64* %rax
  %81 = mul i64 %80, 1
  %82 = add i64 %79, %81
  %83 = inttoptr i64 %82 to i64*
  %84 = load i64* %83
  store i64 %84, i64* %rax
  store volatile i64 56091, i64* @assembly_address
  %85 = load i64* %rbx
  store i64 %85, i64* %rdx
  store volatile i64 56094, i64* @assembly_address
  %86 = load i64* %rdx
  %87 = load i1* %of
  %88 = lshr i64 %86, 8
  %89 = icmp eq i64 %88, 0
  store i1 %89, i1* %zf
  %90 = icmp slt i64 %88, 0
  store i1 %90, i1* %sf
  %91 = trunc i64 %88 to i8
  %92 = call i8 @llvm.ctpop.i8(i8 %91)
  %93 = and i8 %92, 1
  %94 = icmp eq i8 %93, 0
  store i1 %94, i1* %pf
  store i64 %88, i64* %rdx
  %95 = and i64 128, %86
  %96 = icmp ne i64 %95, 0
  store i1 %96, i1* %cf
  %97 = icmp slt i64 %86, 0
  %98 = select i1 false, i1 %97, i1 %87
  store i1 %98, i1* %of
  store volatile i64 56098, i64* @assembly_address
  %99 = load i64* %rax
  %100 = load i64* %rdx
  %101 = xor i64 %99, %100
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %102 = icmp eq i64 %101, 0
  store i1 %102, i1* %zf
  %103 = icmp slt i64 %101, 0
  store i1 %103, i1* %sf
  %104 = trunc i64 %101 to i8
  %105 = call i8 @llvm.ctpop.i8(i8 %104)
  %106 = and i8 %105, 1
  %107 = icmp eq i8 %106, 0
  store i1 %107, i1* %pf
  store i64 %101, i64* %rax
  store volatile i64 56101, i64* @assembly_address
  %108 = load i64* %rax
  store i64 %108, i64* %rbx
  store volatile i64 56104, i64* @assembly_address
  %109 = load i32* %stack_var_-28
  %110 = sub i32 %109, 1
  %111 = and i32 %109, 15
  %112 = sub i32 %111, 1
  %113 = icmp ugt i32 %112, 15
  %114 = icmp ult i32 %109, 1
  %115 = xor i32 %109, 1
  %116 = xor i32 %109, %110
  %117 = and i32 %115, %116
  %118 = icmp slt i32 %117, 0
  store i1 %113, i1* %az
  store i1 %114, i1* %cf
  store i1 %118, i1* %of
  %119 = icmp eq i32 %110, 0
  store i1 %119, i1* %zf
  %120 = icmp slt i32 %110, 0
  store i1 %120, i1* %sf
  %121 = trunc i32 %110 to i8
  %122 = call i8 @llvm.ctpop.i8(i8 %121)
  %123 = and i8 %122, 1
  %124 = icmp eq i8 %123, 0
  store i1 %124, i1* %pf
  store i32 %110, i32* %stack_var_-28
  store volatile i64 56108, i64* @assembly_address
  %125 = load i32* %stack_var_-28
  %126 = and i32 %125, 15
  %127 = icmp ugt i32 %126, 15
  %128 = icmp ult i32 %125, 0
  %129 = xor i32 %125, 0
  %130 = and i32 %129, 0
  %131 = icmp slt i32 %130, 0
  store i1 %127, i1* %az
  store i1 %128, i1* %cf
  store i1 %131, i1* %of
  %132 = icmp eq i32 %125, 0
  store i1 %132, i1* %zf
  %133 = icmp slt i32 %125, 0
  store i1 %133, i1* %sf
  %134 = trunc i32 %125 to i8
  %135 = call i8 @llvm.ctpop.i8(i8 %134)
  %136 = and i8 %135, 1
  %137 = icmp eq i8 %136, 0
  store i1 %137, i1* %pf
  store volatile i64 56112, i64* @assembly_address
  %138 = load i1* %zf
  %139 = icmp eq i1 %138, false
  br i1 %139, label %block_daed, label %block_db32

block_db32:                                       ; preds = %block_daed, %block_dae0, %block_dad9
  store volatile i64 56114, i64* @assembly_address
  %140 = load i64* %rbx
  store i64 %140, i64* @global_var_216550
  store volatile i64 56121, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 56126, i64* @assembly_address
  %141 = load i64* %rax
  %142 = load i64* %rbx
  %143 = xor i64 %141, %142
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %144 = icmp eq i64 %143, 0
  store i1 %144, i1* %zf
  %145 = icmp slt i64 %143, 0
  store i1 %145, i1* %sf
  %146 = trunc i64 %143 to i8
  %147 = call i8 @llvm.ctpop.i8(i8 %146)
  %148 = and i8 %147, 1
  %149 = icmp eq i8 %148, 0
  store i1 %149, i1* %pf
  store i64 %143, i64* %rax
  store volatile i64 56129, i64* @assembly_address
  %150 = load i64* %stack_var_-16
  store i64 %150, i64* %rbx
  %151 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %151, i64* %rsp
  store volatile i64 56130, i64* @assembly_address
  %152 = load i64* %stack_var_-8
  store i64 %152, i64* %rbp
  %153 = ptrtoint i64* %stack_var_0 to i64
  store i64 %153, i64* %rsp
  store volatile i64 56131, i64* @assembly_address
  %154 = load i64* %rax
  ret i64 %154
}

declare i64 @242(i8*, i64)

define i64 @clear_bufs() {
block_db44:
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56132, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 56133, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 56136, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 56146, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 56156, i64* @assembly_address
  %3 = load i32* bitcast (i64* @global_var_24a884 to i32*)
  %4 = zext i32 %3 to i64
  store i64 %4, i64* %rax
  store volatile i64 56162, i64* @assembly_address
  %5 = load i64* %rax
  %6 = trunc i64 %5 to i32
  store i32 %6, i32* bitcast (i64* @global_var_25f4e4 to i32*)
  store volatile i64 56168, i64* @assembly_address
  store i64 0, i64* @global_var_25f4c0
  store volatile i64 56179, i64* @assembly_address
  %7 = load i64* @global_var_25f4c0
  store i64 %7, i64* %rax
  store volatile i64 56186, i64* @assembly_address
  %8 = load i64* %rax
  store i64 %8, i64* @global_var_21a860
  store volatile i64 56193, i64* @assembly_address
  store volatile i64 56194, i64* @assembly_address
  %9 = load i64* %stack_var_-8
  store i64 %9, i64* %rbp
  %10 = ptrtoint i64* %stack_var_0 to i64
  store i64 %10, i64* %rsp
  store volatile i64 56195, i64* @assembly_address
  %11 = load i64* %rax
  %12 = load i64* %rax
  ret i64 %12
}

define i64 @fill_inbuf(i32 %arg1) {
block_db84:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56196, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 56197, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 56200, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 56204, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-28
  store volatile i64 56207, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_25f4e4 to i32*)
  br label %block_db99

block_db99:                                       ; preds = %block_dbdc, %block_db84
  store volatile i64 56217, i64* @assembly_address
  %22 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %23 = zext i32 %22 to i64
  store i64 %23, i64* %rax
  store volatile i64 56223, i64* @assembly_address
  store i64 32768, i64* %rdx
  store volatile i64 56228, i64* @assembly_address
  %24 = load i64* %rdx
  %25 = trunc i64 %24 to i32
  %26 = load i64* %rax
  %27 = trunc i64 %26 to i32
  %28 = sub i32 %25, %27
  %29 = and i32 %25, 15
  %30 = and i32 %27, 15
  %31 = sub i32 %29, %30
  %32 = icmp ugt i32 %31, 15
  %33 = icmp ult i32 %25, %27
  %34 = xor i32 %25, %27
  %35 = xor i32 %25, %28
  %36 = and i32 %34, %35
  %37 = icmp slt i32 %36, 0
  store i1 %32, i1* %az
  store i1 %33, i1* %cf
  store i1 %37, i1* %of
  %38 = icmp eq i32 %28, 0
  store i1 %38, i1* %zf
  %39 = icmp slt i32 %28, 0
  store i1 %39, i1* %sf
  %40 = trunc i32 %28 to i8
  %41 = call i8 @llvm.ctpop.i8(i8 %40)
  %42 = and i8 %41, 1
  %43 = icmp eq i8 %42, 0
  store i1 %43, i1* %pf
  %44 = zext i32 %28 to i64
  store i64 %44, i64* %rdx
  store volatile i64 56230, i64* @assembly_address
  %45 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %46 = zext i32 %45 to i64
  store i64 %46, i64* %rax
  store volatile i64 56236, i64* @assembly_address
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i32
  %49 = zext i32 %48 to i64
  store i64 %49, i64* %rcx
  store volatile i64 56238, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f500 to i64), i64* %rax
  store volatile i64 56245, i64* @assembly_address
  %50 = load i64* %rcx
  %51 = load i64* %rax
  %52 = add i64 %50, %51
  %53 = and i64 %50, 15
  %54 = and i64 %51, 15
  %55 = add i64 %53, %54
  %56 = icmp ugt i64 %55, 15
  %57 = icmp ult i64 %52, %50
  %58 = xor i64 %50, %52
  %59 = xor i64 %51, %52
  %60 = and i64 %58, %59
  %61 = icmp slt i64 %60, 0
  store i1 %56, i1* %az
  store i1 %57, i1* %cf
  store i1 %61, i1* %of
  %62 = icmp eq i64 %52, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i64 %52, 0
  store i1 %63, i1* %sf
  %64 = trunc i64 %52 to i8
  %65 = call i8 @llvm.ctpop.i8(i8 %64)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store i64 %52, i64* %rcx
  store volatile i64 56248, i64* @assembly_address
  %68 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %69 = zext i32 %68 to i64
  store i64 %69, i64* %rax
  store volatile i64 56254, i64* @assembly_address
  %70 = load i64* %rcx
  store i64 %70, i64* %rsi
  store volatile i64 56257, i64* @assembly_address
  %71 = load i64* %rax
  %72 = trunc i64 %71 to i32
  %73 = zext i32 %72 to i64
  store i64 %73, i64* %rdi
  store volatile i64 56259, i64* @assembly_address
  %74 = load i64* %rdi
  %75 = load i64* %rsi
  %76 = load i64* %rdx
  %77 = trunc i64 %74 to i32
  %78 = call i64 @read_buffer(i32 %77, i64 %75, i64 %76)
  store i64 %78, i64* %rax
  store i64 %78, i64* %rax
  store volatile i64 56264, i64* @assembly_address
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  store i32 %80, i32* %stack_var_-12
  store volatile i64 56267, i64* @assembly_address
  %81 = load i32* %stack_var_-12
  %82 = and i32 %81, 15
  %83 = icmp ugt i32 %82, 15
  %84 = icmp ult i32 %81, 0
  %85 = xor i32 %81, 0
  %86 = and i32 %85, 0
  %87 = icmp slt i32 %86, 0
  store i1 %83, i1* %az
  store i1 %84, i1* %cf
  store i1 %87, i1* %of
  %88 = icmp eq i32 %81, 0
  store i1 %88, i1* %zf
  %89 = icmp slt i32 %81, 0
  store i1 %89, i1* %sf
  %90 = trunc i32 %81 to i8
  %91 = call i8 @llvm.ctpop.i8(i8 %90)
  %92 = and i8 %91, 1
  %93 = icmp eq i8 %92, 0
  store i1 %93, i1* %pf
  store volatile i64 56271, i64* @assembly_address
  %94 = load i1* %zf
  br i1 %94, label %block_dbfc, label %block_dbd1

block_dbd1:                                       ; preds = %block_db99
  store volatile i64 56273, i64* @assembly_address
  %95 = load i32* %stack_var_-12
  %96 = sub i32 %95, -1
  %97 = and i32 %95, 15
  %98 = sub i32 %97, 15
  %99 = icmp ugt i32 %98, 15
  %100 = icmp ult i32 %95, -1
  %101 = xor i32 %95, -1
  %102 = xor i32 %95, %96
  %103 = and i32 %101, %102
  %104 = icmp slt i32 %103, 0
  store i1 %99, i1* %az
  store i1 %100, i1* %cf
  store i1 %104, i1* %of
  %105 = icmp eq i32 %96, 0
  store i1 %105, i1* %zf
  %106 = icmp slt i32 %96, 0
  store i1 %106, i1* %sf
  %107 = trunc i32 %96 to i8
  %108 = call i8 @llvm.ctpop.i8(i8 %107)
  %109 = and i8 %108, 1
  %110 = icmp eq i8 %109, 0
  store i1 %110, i1* %pf
  store volatile i64 56277, i64* @assembly_address
  %111 = load i1* %zf
  %112 = icmp eq i1 %111, false
  br i1 %112, label %block_dbdc, label %block_dbd7

block_dbd7:                                       ; preds = %block_dbd1
  store volatile i64 56279, i64* @assembly_address
  %113 = call i64 @read_error()
  store i64 %113, i64* %rax
  store i64 %113, i64* %rax
  store i64 %113, i64* %rax
  unreachable

block_dbdc:                                       ; preds = %block_dbd1
  store volatile i64 56284, i64* @assembly_address
  %114 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %115 = zext i32 %114 to i64
  store i64 %115, i64* %rdx
  store volatile i64 56290, i64* @assembly_address
  %116 = load i32* %stack_var_-12
  %117 = zext i32 %116 to i64
  store i64 %117, i64* %rax
  store volatile i64 56293, i64* @assembly_address
  %118 = load i64* %rax
  %119 = trunc i64 %118 to i32
  %120 = load i64* %rdx
  %121 = trunc i64 %120 to i32
  %122 = add i32 %119, %121
  %123 = and i32 %119, 15
  %124 = and i32 %121, 15
  %125 = add i32 %123, %124
  %126 = icmp ugt i32 %125, 15
  %127 = icmp ult i32 %122, %119
  %128 = xor i32 %119, %122
  %129 = xor i32 %121, %122
  %130 = and i32 %128, %129
  %131 = icmp slt i32 %130, 0
  store i1 %126, i1* %az
  store i1 %127, i1* %cf
  store i1 %131, i1* %of
  %132 = icmp eq i32 %122, 0
  store i1 %132, i1* %zf
  %133 = icmp slt i32 %122, 0
  store i1 %133, i1* %sf
  %134 = trunc i32 %122 to i8
  %135 = call i8 @llvm.ctpop.i8(i8 %134)
  %136 = and i8 %135, 1
  %137 = icmp eq i8 %136, 0
  store i1 %137, i1* %pf
  %138 = zext i32 %122 to i64
  store i64 %138, i64* %rax
  store volatile i64 56295, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  store i32 %140, i32* bitcast (i64* @global_var_25f4e4 to i32*)
  store volatile i64 56301, i64* @assembly_address
  %141 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %142 = zext i32 %141 to i64
  store i64 %142, i64* %rax
  store volatile i64 56307, i64* @assembly_address
  %143 = load i64* %rax
  %144 = trunc i64 %143 to i32
  %145 = sub i32 %144, 32767
  %146 = and i32 %144, 15
  %147 = sub i32 %146, 15
  %148 = icmp ugt i32 %147, 15
  %149 = icmp ult i32 %144, 32767
  %150 = xor i32 %144, 32767
  %151 = xor i32 %144, %145
  %152 = and i32 %150, %151
  %153 = icmp slt i32 %152, 0
  store i1 %148, i1* %az
  store i1 %149, i1* %cf
  store i1 %153, i1* %of
  %154 = icmp eq i32 %145, 0
  store i1 %154, i1* %zf
  %155 = icmp slt i32 %145, 0
  store i1 %155, i1* %sf
  %156 = trunc i32 %145 to i8
  %157 = call i8 @llvm.ctpop.i8(i8 %156)
  %158 = and i8 %157, 1
  %159 = icmp eq i8 %158, 0
  store i1 %159, i1* %pf
  store volatile i64 56312, i64* @assembly_address
  %160 = load i1* %cf
  %161 = load i1* %zf
  %162 = or i1 %160, %161
  br i1 %162, label %block_db99, label %block_dbfa

block_dbfa:                                       ; preds = %block_dbdc
  store volatile i64 56314, i64* @assembly_address
  br label %block_dbfd

block_dbfc:                                       ; preds = %block_db99
  store volatile i64 56316, i64* @assembly_address
  br label %block_dbfd

block_dbfd:                                       ; preds = %block_dbfc, %block_dbfa
  store volatile i64 56317, i64* @assembly_address
  %163 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %164 = zext i32 %163 to i64
  store i64 %164, i64* %rax
  store volatile i64 56323, i64* @assembly_address
  %165 = load i64* %rax
  %166 = trunc i64 %165 to i32
  %167 = load i64* %rax
  %168 = trunc i64 %167 to i32
  %169 = and i32 %166, %168
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %170 = icmp eq i32 %169, 0
  store i1 %170, i1* %zf
  %171 = icmp slt i32 %169, 0
  store i1 %171, i1* %sf
  %172 = trunc i32 %169 to i8
  %173 = call i8 @llvm.ctpop.i8(i8 %172)
  %174 = and i8 %173, 1
  %175 = icmp eq i8 %174, 0
  store i1 %175, i1* %pf
  store volatile i64 56325, i64* @assembly_address
  %176 = load i1* %zf
  %177 = icmp eq i1 %176, false
  br i1 %177, label %block_dc29, label %block_dc07

block_dc07:                                       ; preds = %block_dbfd
  store volatile i64 56327, i64* @assembly_address
  %178 = load i32* %stack_var_-28
  %179 = and i32 %178, 15
  %180 = icmp ugt i32 %179, 15
  %181 = icmp ult i32 %178, 0
  %182 = xor i32 %178, 0
  %183 = and i32 %182, 0
  %184 = icmp slt i32 %183, 0
  store i1 %180, i1* %az
  store i1 %181, i1* %cf
  store i1 %184, i1* %of
  %185 = icmp eq i32 %178, 0
  store i1 %185, i1* %zf
  %186 = icmp slt i32 %178, 0
  store i1 %186, i1* %sf
  %187 = trunc i32 %178 to i8
  %188 = call i8 @llvm.ctpop.i8(i8 %187)
  %189 = and i8 %188, 1
  %190 = icmp eq i8 %189, 0
  store i1 %190, i1* %pf
  store volatile i64 56331, i64* @assembly_address
  %191 = load i1* %zf
  br i1 %191, label %block_dc14, label %block_dc0d

block_dc0d:                                       ; preds = %block_dc07
  store volatile i64 56333, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 56338, i64* @assembly_address
  br label %block_dc56

block_dc14:                                       ; preds = %block_dc07
  store volatile i64 56340, i64* @assembly_address
  %192 = call i64 @flush_window()
  store i64 %192, i64* %rax
  store i64 %192, i64* %rax
  store i64 %192, i64* %rax
  store volatile i64 56345, i64* @assembly_address
  %193 = call i32* @__errno_location()
  %194 = ptrtoint i32* %193 to i64
  store i64 %194, i64* %rax
  %195 = ptrtoint i32* %193 to i64
  store i64 %195, i64* %rax
  %196 = ptrtoint i32* %193 to i64
  store i64 %196, i64* %rax
  store volatile i64 56350, i64* @assembly_address
  %197 = load i64* %rax
  %198 = inttoptr i64 %197 to i32*
  store i32 0, i32* %198
  store volatile i64 56356, i64* @assembly_address
  %199 = call i64 @read_error()
  store i64 %199, i64* %rax
  store i64 %199, i64* %rax
  store i64 %199, i64* %rax
  unreachable

block_dc29:                                       ; preds = %block_dbfd
  store volatile i64 56361, i64* @assembly_address
  %200 = load i32* bitcast (i64* @global_var_25f4e4 to i32*)
  %201 = zext i32 %200 to i64
  store i64 %201, i64* %rax
  store volatile i64 56367, i64* @assembly_address
  %202 = load i64* %rax
  %203 = trunc i64 %202 to i32
  %204 = zext i32 %203 to i64
  store i64 %204, i64* %rdx
  store volatile i64 56369, i64* @assembly_address
  %205 = load i64* @global_var_21a860
  store i64 %205, i64* %rax
  store volatile i64 56376, i64* @assembly_address
  %206 = load i64* %rax
  %207 = load i64* %rdx
  %208 = add i64 %206, %207
  %209 = and i64 %206, 15
  %210 = and i64 %207, 15
  %211 = add i64 %209, %210
  %212 = icmp ugt i64 %211, 15
  %213 = icmp ult i64 %208, %206
  %214 = xor i64 %206, %208
  %215 = xor i64 %207, %208
  %216 = and i64 %214, %215
  %217 = icmp slt i64 %216, 0
  store i1 %212, i1* %az
  store i1 %213, i1* %cf
  store i1 %217, i1* %of
  %218 = icmp eq i64 %208, 0
  store i1 %218, i1* %zf
  %219 = icmp slt i64 %208, 0
  store i1 %219, i1* %sf
  %220 = trunc i64 %208 to i8
  %221 = call i8 @llvm.ctpop.i8(i8 %220)
  %222 = and i8 %221, 1
  %223 = icmp eq i8 %222, 0
  store i1 %223, i1* %pf
  store i64 %208, i64* %rax
  store volatile i64 56379, i64* @assembly_address
  %224 = load i64* %rax
  store i64 %224, i64* @global_var_21a860
  store volatile i64 56386, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_24a884 to i32*)
  store volatile i64 56396, i64* @assembly_address
  %225 = load i8* bitcast (i64* @global_var_25f500 to i8*)
  %226 = zext i8 %225 to i64
  store i64 %226, i64* %rax
  store volatile i64 56403, i64* @assembly_address
  %227 = load i64* %rax
  %228 = trunc i64 %227 to i8
  %229 = zext i8 %228 to i64
  store i64 %229, i64* %rax
  br label %block_dc56

block_dc56:                                       ; preds = %block_dc29, %block_dc0d
  store volatile i64 56406, i64* @assembly_address
  %230 = load i64* %stack_var_-8
  store i64 %230, i64* %rbp
  %231 = ptrtoint i64* %stack_var_0 to i64
  store i64 %231, i64* %rsp
  store volatile i64 56407, i64* @assembly_address
  %232 = load i64* %rax
  ret i64 %232
}

declare i64 @243(i64)

define i64 @read_buffer(i32 %arg1, i64 %arg2, i64 %arg3) {
block_dc58:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56408, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 56409, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 56412, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 56416, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-28
  store volatile i64 56419, i64* @assembly_address
  %22 = load i64* %rsi
  store i64 %22, i64* %stack_var_-40
  store volatile i64 56423, i64* @assembly_address
  %23 = load i64* %rdx
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-32
  store volatile i64 56426, i64* @assembly_address
  %25 = load i32* %stack_var_-32
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rax
  store volatile i64 56429, i64* @assembly_address
  %27 = load i64* %rax
  %28 = trunc i64 %27 to i32
  %29 = load i64* %rax
  %30 = trunc i64 %29 to i32
  %31 = and i32 %28, %30
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %32 = icmp eq i32 %31, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i32 %31, 0
  store i1 %33, i1* %sf
  %34 = trunc i32 %31 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  store volatile i64 56431, i64* @assembly_address
  %38 = load i1* %sf
  %39 = icmp eq i1 %38, false
  br i1 %39, label %block_dc78, label %block_dc71

block_dc71:                                       ; preds = %block_dc58
  store volatile i64 56433, i64* @assembly_address
  store i32 2147483647, i32* %stack_var_-32
  br label %block_dc78

block_dc78:                                       ; preds = %block_dc71, %block_dc58
  store volatile i64 56440, i64* @assembly_address
  %40 = load i32* %stack_var_-32
  %41 = zext i32 %40 to i64
  store i64 %41, i64* %rdx
  store volatile i64 56443, i64* @assembly_address
  %42 = load i64* %stack_var_-40
  store i64 %42, i64* %rcx
  store volatile i64 56447, i64* @assembly_address
  %43 = load i32* %stack_var_-28
  %44 = zext i32 %43 to i64
  store i64 %44, i64* %rax
  store volatile i64 56450, i64* @assembly_address
  %45 = load i64* %rcx
  store i64 %45, i64* %rsi
  store volatile i64 56453, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rdi
  store volatile i64 56455, i64* @assembly_address
  %49 = load i64* %rdi
  %50 = trunc i64 %49 to i32
  %51 = load i64* %rsi
  %52 = inttoptr i64 %51 to i64*
  %53 = load i64* %rdx
  %54 = trunc i64 %53 to i32
  %55 = call i32 @read(i32 %50, i64* %52, i32 %54)
  %56 = sext i32 %55 to i64
  store i64 %56, i64* %rax
  %57 = sext i32 %55 to i64
  store i64 %57, i64* %rax
  store volatile i64 56460, i64* @assembly_address
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i32
  store i32 %59, i32* %stack_var_-16
  store volatile i64 56463, i64* @assembly_address
  %60 = load i32* %stack_var_-16
  %61 = and i32 %60, 15
  %62 = icmp ugt i32 %61, 15
  %63 = icmp ult i32 %60, 0
  %64 = xor i32 %60, 0
  %65 = and i32 %64, 0
  %66 = icmp slt i32 %65, 0
  store i1 %62, i1* %az
  store i1 %63, i1* %cf
  store i1 %66, i1* %of
  %67 = icmp eq i32 %60, 0
  store i1 %67, i1* %zf
  %68 = icmp slt i32 %60, 0
  store i1 %68, i1* %sf
  %69 = trunc i32 %60 to i8
  %70 = call i8 @llvm.ctpop.i8(i8 %69)
  %71 = and i8 %70, 1
  %72 = icmp eq i8 %71, 0
  store i1 %72, i1* %pf
  store volatile i64 56467, i64* @assembly_address
  %73 = load i1* %sf
  %74 = icmp eq i1 %73, false
  br i1 %74, label %block_dd0f, label %block_dc95

block_dc95:                                       ; preds = %block_dc78
  store volatile i64 56469, i64* @assembly_address
  %75 = call i32* @__errno_location()
  %76 = ptrtoint i32* %75 to i64
  store i64 %76, i64* %rax
  %77 = ptrtoint i32* %75 to i64
  store i64 %77, i64* %rax
  %78 = ptrtoint i32* %75 to i64
  store i64 %78, i64* %rax
  store volatile i64 56474, i64* @assembly_address
  %79 = load i64* %rax
  %80 = inttoptr i64 %79 to i32*
  %81 = load i32* %80
  %82 = zext i32 %81 to i64
  store i64 %82, i64* %rax
  store volatile i64 56476, i64* @assembly_address
  %83 = load i64* %rax
  %84 = trunc i64 %83 to i32
  %85 = sub i32 %84, 11
  %86 = and i32 %84, 15
  %87 = sub i32 %86, 11
  %88 = icmp ugt i32 %87, 15
  %89 = icmp ult i32 %84, 11
  %90 = xor i32 %84, 11
  %91 = xor i32 %84, %85
  %92 = and i32 %90, %91
  %93 = icmp slt i32 %92, 0
  store i1 %88, i1* %az
  store i1 %89, i1* %cf
  store i1 %93, i1* %of
  %94 = icmp eq i32 %85, 0
  store i1 %94, i1* %zf
  %95 = icmp slt i32 %85, 0
  store i1 %95, i1* %sf
  %96 = trunc i32 %85 to i8
  %97 = call i8 @llvm.ctpop.i8(i8 %96)
  %98 = and i8 %97, 1
  %99 = icmp eq i8 %98, 0
  store i1 %99, i1* %pf
  store volatile i64 56479, i64* @assembly_address
  %100 = load i1* %zf
  %101 = icmp eq i1 %100, false
  br i1 %101, label %block_dd0f, label %block_dca1

block_dca1:                                       ; preds = %block_dc95
  store volatile i64 56481, i64* @assembly_address
  %102 = load i32* %stack_var_-28
  %103 = zext i32 %102 to i64
  store i64 %103, i64* %rax
  store volatile i64 56484, i64* @assembly_address
  store i64 3, i64* %rsi
  store volatile i64 56489, i64* @assembly_address
  %104 = load i64* %rax
  %105 = trunc i64 %104 to i32
  %106 = zext i32 %105 to i64
  store i64 %106, i64* %rdi
  store volatile i64 56491, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 56496, i64* @assembly_address
  %107 = load i64* %rdi
  %108 = load i64* %rsi
  %109 = load i64* %rdx
  %110 = load i64* %rcx
  %111 = load i64* %r8
  %112 = load i64* %r9
  %113 = trunc i64 %107 to i32
  %114 = call i64 @rpl_fcntl(i32 %113, i64 %108, i64 %109, i64 %110, i64 %111, i64 %112)
  store i64 %114, i64* %rax
  store i64 %114, i64* %rax
  store volatile i64 56501, i64* @assembly_address
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i32
  store i32 %116, i32* %stack_var_-12
  store volatile i64 56504, i64* @assembly_address
  %117 = load i32* %stack_var_-12
  %118 = and i32 %117, 15
  %119 = icmp ugt i32 %118, 15
  %120 = icmp ult i32 %117, 0
  %121 = xor i32 %117, 0
  %122 = and i32 %121, 0
  %123 = icmp slt i32 %122, 0
  store i1 %119, i1* %az
  store i1 %120, i1* %cf
  store i1 %123, i1* %of
  %124 = icmp eq i32 %117, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i32 %117, 0
  store i1 %125, i1* %sf
  %126 = trunc i32 %117 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  store volatile i64 56508, i64* @assembly_address
  %130 = load i1* %sf
  br i1 %130, label %block_dd0f, label %block_dcbe

block_dcbe:                                       ; preds = %block_dca1
  store volatile i64 56510, i64* @assembly_address
  %131 = load i32* %stack_var_-12
  %132 = zext i32 %131 to i64
  store i64 %132, i64* %rax
  store volatile i64 56513, i64* @assembly_address
  %133 = load i64* %rax
  %134 = trunc i64 %133 to i32
  %135 = and i32 %134, ptrtoint (i64* @global_var_800 to i32)
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %136 = icmp eq i32 %135, 0
  store i1 %136, i1* %zf
  %137 = icmp slt i32 %135, 0
  store i1 %137, i1* %sf
  %138 = trunc i32 %135 to i8
  %139 = call i8 @llvm.ctpop.i8(i8 %138)
  %140 = and i8 %139, 1
  %141 = icmp eq i8 %140, 0
  store i1 %141, i1* %pf
  %142 = zext i32 %135 to i64
  store i64 %142, i64* %rax
  store volatile i64 56518, i64* @assembly_address
  %143 = load i64* %rax
  %144 = trunc i64 %143 to i32
  %145 = load i64* %rax
  %146 = trunc i64 %145 to i32
  %147 = and i32 %144, %146
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %148 = icmp eq i32 %147, 0
  store i1 %148, i1* %zf
  %149 = icmp slt i32 %147, 0
  store i1 %149, i1* %sf
  %150 = trunc i32 %147 to i8
  %151 = call i8 @llvm.ctpop.i8(i8 %150)
  %152 = and i8 %151, 1
  %153 = icmp eq i8 %152, 0
  store i1 %153, i1* %pf
  store volatile i64 56520, i64* @assembly_address
  %154 = load i1* %zf
  %155 = icmp eq i1 %154, false
  br i1 %155, label %block_dcd7, label %block_dcca

block_dcca:                                       ; preds = %block_dcbe
  store volatile i64 56522, i64* @assembly_address
  %156 = call i32* @__errno_location()
  %157 = ptrtoint i32* %156 to i64
  store i64 %157, i64* %rax
  %158 = ptrtoint i32* %156 to i64
  store i64 %158, i64* %rax
  %159 = ptrtoint i32* %156 to i64
  store i64 %159, i64* %rax
  store volatile i64 56527, i64* @assembly_address
  %160 = load i64* %rax
  %161 = inttoptr i64 %160 to i32*
  store i32 11, i32* %161
  store volatile i64 56533, i64* @assembly_address
  br label %block_dd0f

block_dcd7:                                       ; preds = %block_dcbe
  store volatile i64 56535, i64* @assembly_address
  %162 = load i32* %stack_var_-12
  %163 = zext i32 %162 to i64
  store i64 %163, i64* %rax
  store volatile i64 56538, i64* @assembly_address
  %164 = load i64* %rax
  %165 = lshr i64 %164, 8
  %166 = trunc i64 %165 to i8
  %167 = and i8 %166, -9
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %168 = icmp eq i8 %167, 0
  store i1 %168, i1* %zf
  %169 = icmp slt i8 %167, 0
  store i1 %169, i1* %sf
  %170 = call i8 @llvm.ctpop.i8(i8 %167)
  %171 = and i8 %170, 1
  %172 = icmp eq i8 %171, 0
  store i1 %172, i1* %pf
  %173 = zext i8 %167 to i64
  %174 = load i64* %rax
  %175 = shl i64 %173, 8
  %176 = and i64 %174, -65281
  %177 = or i64 %176, %175
  store i64 %177, i64* %rax
  store volatile i64 56541, i64* @assembly_address
  %178 = load i64* %rax
  %179 = trunc i64 %178 to i32
  %180 = zext i32 %179 to i64
  store i64 %180, i64* %rdx
  store volatile i64 56543, i64* @assembly_address
  %181 = load i32* %stack_var_-28
  %182 = zext i32 %181 to i64
  store i64 %182, i64* %rax
  store volatile i64 56546, i64* @assembly_address
  store i64 4, i64* %rsi
  store volatile i64 56551, i64* @assembly_address
  %183 = load i64* %rax
  %184 = trunc i64 %183 to i32
  %185 = zext i32 %184 to i64
  store i64 %185, i64* %rdi
  store volatile i64 56553, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 56558, i64* @assembly_address
  %186 = load i64* %rdi
  %187 = load i64* %rsi
  %188 = load i64* %rdx
  %189 = load i64* %rcx
  %190 = load i64* %r8
  %191 = load i64* %r9
  %192 = trunc i64 %186 to i32
  %193 = call i64 @rpl_fcntl(i32 %192, i64 %187, i64 %188, i64 %189, i64 %190, i64 %191)
  store i64 %193, i64* %rax
  store i64 %193, i64* %rax
  store volatile i64 56563, i64* @assembly_address
  %194 = load i64* %rax
  %195 = trunc i64 %194 to i32
  %196 = sub i32 %195, -1
  %197 = and i32 %195, 15
  %198 = sub i32 %197, 15
  %199 = icmp ugt i32 %198, 15
  %200 = icmp ult i32 %195, -1
  %201 = xor i32 %195, -1
  %202 = xor i32 %195, %196
  %203 = and i32 %201, %202
  %204 = icmp slt i32 %203, 0
  store i1 %199, i1* %az
  store i1 %200, i1* %cf
  store i1 %204, i1* %of
  %205 = icmp eq i32 %196, 0
  store i1 %205, i1* %zf
  %206 = icmp slt i32 %196, 0
  store i1 %206, i1* %sf
  %207 = trunc i32 %196 to i8
  %208 = call i8 @llvm.ctpop.i8(i8 %207)
  %209 = and i8 %208, 1
  %210 = icmp eq i8 %209, 0
  store i1 %210, i1* %pf
  store volatile i64 56566, i64* @assembly_address
  %211 = load i1* %zf
  br i1 %211, label %block_dd0f, label %block_dcf8

block_dcf8:                                       ; preds = %block_dcd7
  store volatile i64 56568, i64* @assembly_address
  %212 = load i32* %stack_var_-32
  %213 = zext i32 %212 to i64
  store i64 %213, i64* %rdx
  store volatile i64 56571, i64* @assembly_address
  %214 = load i64* %stack_var_-40
  store i64 %214, i64* %rcx
  store volatile i64 56575, i64* @assembly_address
  %215 = load i32* %stack_var_-28
  %216 = zext i32 %215 to i64
  store i64 %216, i64* %rax
  store volatile i64 56578, i64* @assembly_address
  %217 = load i64* %rcx
  store i64 %217, i64* %rsi
  store volatile i64 56581, i64* @assembly_address
  %218 = load i64* %rax
  %219 = trunc i64 %218 to i32
  %220 = zext i32 %219 to i64
  store i64 %220, i64* %rdi
  store volatile i64 56583, i64* @assembly_address
  %221 = load i64* %rdi
  %222 = trunc i64 %221 to i32
  %223 = load i64* %rsi
  %224 = inttoptr i64 %223 to i64*
  %225 = load i64* %rdx
  %226 = trunc i64 %225 to i32
  %227 = call i32 @read(i32 %222, i64* %224, i32 %226)
  %228 = sext i32 %227 to i64
  store i64 %228, i64* %rax
  %229 = sext i32 %227 to i64
  store i64 %229, i64* %rax
  store volatile i64 56588, i64* @assembly_address
  %230 = load i64* %rax
  %231 = trunc i64 %230 to i32
  store i32 %231, i32* %stack_var_-16
  br label %block_dd0f

block_dd0f:                                       ; preds = %block_dcf8, %block_dcd7, %block_dcca, %block_dca1, %block_dc95, %block_dc78
  store volatile i64 56591, i64* @assembly_address
  %232 = load i32* %stack_var_-16
  %233 = zext i32 %232 to i64
  store i64 %233, i64* %rax
  store volatile i64 56594, i64* @assembly_address
  %234 = load i64* %stack_var_-8
  store i64 %234, i64* %rbp
  %235 = ptrtoint i64* %stack_var_0 to i64
  store i64 %235, i64* %rsp
  store volatile i64 56595, i64* @assembly_address
  %236 = load i64* %rax
  ret i64 %236
}

declare i64 @244(i64, i64*, i64)

declare i64 @245(i64, i64, i32)

declare i64 @246(i64, i64, i64)

define i64 @write_buffer(i32 %arg1, i64 %arg2, i32 %arg3) {
block_dd14:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg3 to i64
  store i64 %0, i64* %rdx
  store i64 %arg2, i64* %rsi
  %1 = sext i32 %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i32
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56596, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 56597, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 56600, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 56604, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = trunc i64 %21 to i32
  store i32 %22, i32* %stack_var_-12
  store volatile i64 56607, i64* @assembly_address
  %23 = load i64* %rsi
  store i64 %23, i64* %stack_var_-24
  store volatile i64 56611, i64* @assembly_address
  %24 = load i64* %rdx
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-16
  store volatile i64 56614, i64* @assembly_address
  %26 = load i32* %stack_var_-16
  %27 = zext i32 %26 to i64
  store i64 %27, i64* %rax
  store volatile i64 56617, i64* @assembly_address
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  %30 = load i64* %rax
  %31 = trunc i64 %30 to i32
  %32 = and i32 %29, %31
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %33 = icmp eq i32 %32, 0
  store i1 %33, i1* %zf
  %34 = icmp slt i32 %32, 0
  store i1 %34, i1* %sf
  %35 = trunc i32 %32 to i8
  %36 = call i8 @llvm.ctpop.i8(i8 %35)
  %37 = and i8 %36, 1
  %38 = icmp eq i8 %37, 0
  store i1 %38, i1* %pf
  store volatile i64 56619, i64* @assembly_address
  %39 = load i1* %sf
  %40 = icmp eq i1 %39, false
  br i1 %40, label %block_dd34, label %block_dd2d

block_dd2d:                                       ; preds = %block_dd14
  store volatile i64 56621, i64* @assembly_address
  store i32 2147483647, i32* %stack_var_-16
  br label %block_dd34

block_dd34:                                       ; preds = %block_dd2d, %block_dd14
  store volatile i64 56628, i64* @assembly_address
  %41 = load i32* %stack_var_-16
  %42 = zext i32 %41 to i64
  store i64 %42, i64* %rdx
  store volatile i64 56631, i64* @assembly_address
  %43 = load i64* %stack_var_-24
  store i64 %43, i64* %rcx
  store volatile i64 56635, i64* @assembly_address
  %44 = load i32* %stack_var_-12
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 56638, i64* @assembly_address
  %46 = load i64* %rcx
  store i64 %46, i64* %rsi
  store volatile i64 56641, i64* @assembly_address
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i32
  %49 = zext i32 %48 to i64
  store i64 %49, i64* %rdi
  store volatile i64 56643, i64* @assembly_address
  %50 = load i64* %rdi
  %51 = trunc i64 %50 to i32
  %52 = load i64* %rsi
  %53 = inttoptr i64 %52 to i64*
  %54 = load i64* %rdx
  %55 = trunc i64 %54 to i32
  %56 = call i32 @write(i32 %51, i64* %53, i32 %55)
  %57 = sext i32 %56 to i64
  store i64 %57, i64* %rax
  %58 = sext i32 %56 to i64
  store i64 %58, i64* %rax
  store volatile i64 56648, i64* @assembly_address
  %59 = load i64* %stack_var_-8
  store i64 %59, i64* %rbp
  %60 = ptrtoint i64* %stack_var_0 to i64
  store i64 %60, i64* %rsp
  store volatile i64 56649, i64* @assembly_address
  %61 = load i64* %rax
  ret i64 %61
}

declare i64 @247(i64, i64*, i32)

declare i64 @248(i64, i64, i32)

define i64 @flush_outbuf(i64 %arg1, i64 %arg2, i64 %arg3, i16 %arg4) {
block_dd4a:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i16 %arg4 to i64
  store i64 %0, i64* %rcx
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56650, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 56651, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 56654, i64* @assembly_address
  %4 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %5 = zext i32 %4 to i64
  store i64 %5, i64* %rax
  store volatile i64 56660, i64* @assembly_address
  %6 = load i64* %rax
  %7 = trunc i64 %6 to i32
  %8 = load i64* %rax
  %9 = trunc i64 %8 to i32
  %10 = and i32 %7, %9
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %11 = icmp eq i32 %10, 0
  store i1 %11, i1* %zf
  %12 = icmp slt i32 %10, 0
  store i1 %12, i1* %sf
  %13 = trunc i32 %10 to i8
  %14 = call i8 @llvm.ctpop.i8(i8 %13)
  %15 = and i8 %14, 1
  %16 = icmp eq i8 %15, 0
  store i1 %16, i1* %pf
  store volatile i64 56662, i64* @assembly_address
  %17 = load i1* %zf
  br i1 %17, label %block_dd97, label %block_dd58

block_dd58:                                       ; preds = %block_dd4a
  store volatile i64 56664, i64* @assembly_address
  %18 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %19 = zext i32 %18 to i64
  store i64 %19, i64* %rdx
  store volatile i64 56670, i64* @assembly_address
  %20 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %21 = zext i32 %20 to i64
  store i64 %21, i64* %rax
  store volatile i64 56676, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rsi
  store volatile i64 56683, i64* @assembly_address
  %22 = load i64* %rax
  %23 = trunc i64 %22 to i32
  %24 = zext i32 %23 to i64
  store i64 %24, i64* %rdi
  store volatile i64 56685, i64* @assembly_address
  %25 = load i64* %rdi
  %26 = load i64* %rsi
  %27 = inttoptr i64 %26 to i8*
  %28 = load i64* %rdx
  %29 = trunc i64 %25 to i32
  %30 = call i64 @write_buf(i32 %29, i8* %27, i64 %28)
  store i64 %30, i64* %rax
  store i64 %30, i64* %rax
  store volatile i64 56690, i64* @assembly_address
  %31 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %32 = zext i32 %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 56696, i64* @assembly_address
  %33 = load i64* %rax
  %34 = trunc i64 %33 to i32
  %35 = zext i32 %34 to i64
  store i64 %35, i64* %rdx
  store volatile i64 56698, i64* @assembly_address
  %36 = load i64* @global_var_25f4c0
  store i64 %36, i64* %rax
  store volatile i64 56705, i64* @assembly_address
  %37 = load i64* %rax
  %38 = load i64* %rdx
  %39 = add i64 %37, %38
  %40 = and i64 %37, 15
  %41 = and i64 %38, 15
  %42 = add i64 %40, %41
  %43 = icmp ugt i64 %42, 15
  %44 = icmp ult i64 %39, %37
  %45 = xor i64 %37, %39
  %46 = xor i64 %38, %39
  %47 = and i64 %45, %46
  %48 = icmp slt i64 %47, 0
  store i1 %43, i1* %az
  store i1 %44, i1* %cf
  store i1 %48, i1* %of
  %49 = icmp eq i64 %39, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i64 %39, 0
  store i1 %50, i1* %sf
  %51 = trunc i64 %39 to i8
  %52 = call i8 @llvm.ctpop.i8(i8 %51)
  %53 = and i8 %52, 1
  %54 = icmp eq i8 %53, 0
  store i1 %54, i1* %pf
  store i64 %39, i64* %rax
  store volatile i64 56708, i64* @assembly_address
  %55 = load i64* %rax
  store i64 %55, i64* @global_var_25f4c0
  store volatile i64 56715, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 56725, i64* @assembly_address
  br label %block_dd98

block_dd97:                                       ; preds = %block_dd4a
  store volatile i64 56727, i64* @assembly_address
  br label %block_dd98

block_dd98:                                       ; preds = %block_dd97, %block_dd58
  store volatile i64 56728, i64* @assembly_address
  %56 = load i64* %stack_var_-8
  store i64 %56, i64* %rbp
  %57 = ptrtoint i64* %stack_var_0 to i64
  store i64 %57, i64* %rsp
  store volatile i64 56729, i64* @assembly_address
  %58 = load i64* %rax
  %59 = load i64* %rax
  ret i64 %59
}

define i64 @flush_window() {
block_dd9a:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56730, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 56731, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 56734, i64* @assembly_address
  %3 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %4 = zext i32 %3 to i64
  store i64 %4, i64* %rax
  store volatile i64 56740, i64* @assembly_address
  %5 = load i64* %rax
  %6 = trunc i64 %5 to i32
  %7 = load i64* %rax
  %8 = trunc i64 %7 to i32
  %9 = and i32 %6, %8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %10 = icmp eq i32 %9, 0
  store i1 %10, i1* %zf
  %11 = icmp slt i32 %9, 0
  store i1 %11, i1* %sf
  %12 = trunc i32 %9 to i8
  %13 = call i8 @llvm.ctpop.i8(i8 %12)
  %14 = and i8 %13, 1
  %15 = icmp eq i8 %14, 0
  store i1 %15, i1* %pf
  store volatile i64 56742, i64* @assembly_address
  %16 = load i1* %zf
  br i1 %16, label %block_de05, label %block_dda8

block_dda8:                                       ; preds = %block_dd9a
  store volatile i64 56744, i64* @assembly_address
  %17 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %18 = zext i32 %17 to i64
  store i64 %18, i64* %rax
  store volatile i64 56750, i64* @assembly_address
  %19 = load i64* %rax
  %20 = trunc i64 %19 to i32
  %21 = zext i32 %20 to i64
  store i64 %21, i64* %rsi
  store volatile i64 56752, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rdi
  store volatile i64 56759, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = inttoptr i64 %22 to i8*
  %24 = load i64* %rsi
  %25 = trunc i64 %24 to i32
  %26 = call i64 @updcrc(i8* %23, i32 %25)
  store i64 %26, i64* %rax
  store i64 %26, i64* %rax
  store volatile i64 56764, i64* @assembly_address
  %27 = load i32* bitcast (i64* @global_var_2165ec to i32*)
  %28 = zext i32 %27 to i64
  store i64 %28, i64* %rax
  store volatile i64 56770, i64* @assembly_address
  %29 = load i64* %rax
  %30 = trunc i64 %29 to i32
  %31 = load i64* %rax
  %32 = trunc i64 %31 to i32
  %33 = and i32 %30, %32
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %34 = icmp eq i32 %33, 0
  store i1 %34, i1* %zf
  %35 = icmp slt i32 %33, 0
  store i1 %35, i1* %sf
  %36 = trunc i32 %33 to i8
  %37 = call i8 @llvm.ctpop.i8(i8 %36)
  %38 = and i8 %37, 1
  %39 = icmp eq i8 %38, 0
  store i1 %39, i1* %pf
  store volatile i64 56772, i64* @assembly_address
  %40 = load i1* %zf
  %41 = icmp eq i1 %40, false
  br i1 %41, label %block_dde0, label %block_ddc6

block_ddc6:                                       ; preds = %block_dda8
  store volatile i64 56774, i64* @assembly_address
  %42 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %43 = zext i32 %42 to i64
  store i64 %43, i64* %rdx
  store volatile i64 56780, i64* @assembly_address
  %44 = load i32* bitcast (i64* @global_var_24a880 to i32*)
  %45 = zext i32 %44 to i64
  store i64 %45, i64* %rax
  store volatile i64 56786, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f4c0 to i64), i64* %rsi
  store volatile i64 56793, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rdi
  store volatile i64 56795, i64* @assembly_address
  %49 = load i64* %rdi
  %50 = load i64* %rsi
  %51 = inttoptr i64 %50 to i8*
  %52 = load i64* %rdx
  %53 = trunc i64 %49 to i32
  %54 = call i64 @write_buf(i32 %53, i8* %51, i64 %52)
  store i64 %54, i64* %rax
  store i64 %54, i64* %rax
  br label %block_dde0

block_dde0:                                       ; preds = %block_ddc6, %block_dda8
  store volatile i64 56800, i64* @assembly_address
  %55 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %56 = zext i32 %55 to i64
  store i64 %56, i64* %rax
  store volatile i64 56806, i64* @assembly_address
  %57 = load i64* %rax
  %58 = trunc i64 %57 to i32
  %59 = zext i32 %58 to i64
  store i64 %59, i64* %rdx
  store volatile i64 56808, i64* @assembly_address
  %60 = load i64* @global_var_25f4c0
  store i64 %60, i64* %rax
  store volatile i64 56815, i64* @assembly_address
  %61 = load i64* %rax
  %62 = load i64* %rdx
  %63 = add i64 %61, %62
  %64 = and i64 %61, 15
  %65 = and i64 %62, 15
  %66 = add i64 %64, %65
  %67 = icmp ugt i64 %66, 15
  %68 = icmp ult i64 %63, %61
  %69 = xor i64 %61, %63
  %70 = xor i64 %62, %63
  %71 = and i64 %69, %70
  %72 = icmp slt i64 %71, 0
  store i1 %67, i1* %az
  store i1 %68, i1* %cf
  store i1 %72, i1* %of
  %73 = icmp eq i64 %63, 0
  store i1 %73, i1* %zf
  %74 = icmp slt i64 %63, 0
  store i1 %74, i1* %sf
  %75 = trunc i64 %63 to i8
  %76 = call i8 @llvm.ctpop.i8(i8 %75)
  %77 = and i8 %76, 1
  %78 = icmp eq i8 %77, 0
  store i1 %78, i1* %pf
  store i64 %63, i64* %rax
  store volatile i64 56818, i64* @assembly_address
  %79 = load i64* %rax
  store i64 %79, i64* @global_var_25f4c0
  store volatile i64 56825, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 56835, i64* @assembly_address
  br label %block_de06

block_de05:                                       ; preds = %block_dd9a
  store volatile i64 56837, i64* @assembly_address
  br label %block_de06

block_de06:                                       ; preds = %block_de05, %block_dde0
  store volatile i64 56838, i64* @assembly_address
  %80 = load i64* %stack_var_-8
  store i64 %80, i64* %rbp
  %81 = ptrtoint i64* %stack_var_0 to i64
  store i64 %81, i64* %rsp
  store volatile i64 56839, i64* @assembly_address
  %82 = load i64* %rax
  %83 = load i64* %rax
  ret i64 %83
}

define i64 @write_buf(i32 %arg1, i8* %arg2, i64 %arg3) {
block_de08:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  %0 = ptrtoint i8* %arg2 to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i8*
  %2 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56840, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 56841, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 56844, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i8** %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 56848, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-28
  store volatile i64 56851, i64* @assembly_address
  %24 = load i64* %rsi
  %25 = inttoptr i64 %24 to i8*
  store i8* %25, i8** %stack_var_-40
  store volatile i64 56855, i64* @assembly_address
  %26 = load i64* %rdx
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-32
  store volatile i64 56858, i64* @assembly_address
  br label %block_de34

block_de1c:                                       ; preds = %block_de34
  store volatile i64 56860, i64* @assembly_address
  %28 = load i32* %stack_var_-12
  %29 = sub i32 %28, -1
  %30 = and i32 %28, 15
  %31 = sub i32 %30, 15
  %32 = icmp ugt i32 %31, 15
  %33 = icmp ult i32 %28, -1
  %34 = xor i32 %28, -1
  %35 = xor i32 %28, %29
  %36 = and i32 %34, %35
  %37 = icmp slt i32 %36, 0
  store i1 %32, i1* %az
  store i1 %33, i1* %cf
  store i1 %37, i1* %of
  %38 = icmp eq i32 %29, 0
  store i1 %38, i1* %zf
  %39 = icmp slt i32 %29, 0
  store i1 %39, i1* %sf
  %40 = trunc i32 %29 to i8
  %41 = call i8 @llvm.ctpop.i8(i8 %40)
  %42 = and i8 %41, 1
  %43 = icmp eq i8 %42, 0
  store i1 %43, i1* %pf
  store volatile i64 56864, i64* @assembly_address
  %44 = load i1* %zf
  %45 = icmp eq i1 %44, false
  br i1 %45, label %block_de27, label %block_de22

block_de22:                                       ; preds = %block_de1c
  store volatile i64 56866, i64* @assembly_address
  %46 = call i64 @write_error()
  store i64 %46, i64* %rax
  store i64 %46, i64* %rax
  store i64 %46, i64* %rax
  unreachable

block_de27:                                       ; preds = %block_de1c
  store volatile i64 56871, i64* @assembly_address
  %47 = load i32* %stack_var_-12
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rax
  store volatile i64 56874, i64* @assembly_address
  %49 = load i32* %stack_var_-32
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  %52 = sub i32 %49, %51
  %53 = and i32 %49, 15
  %54 = and i32 %51, 15
  %55 = sub i32 %53, %54
  %56 = icmp ugt i32 %55, 15
  %57 = icmp ult i32 %49, %51
  %58 = xor i32 %49, %51
  %59 = xor i32 %49, %52
  %60 = and i32 %58, %59
  %61 = icmp slt i32 %60, 0
  store i1 %56, i1* %az
  store i1 %57, i1* %cf
  store i1 %61, i1* %of
  %62 = icmp eq i32 %52, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i32 %52, 0
  store i1 %63, i1* %sf
  %64 = trunc i32 %52 to i8
  %65 = call i8 @llvm.ctpop.i8(i8 %64)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store i32 %52, i32* %stack_var_-32
  store volatile i64 56877, i64* @assembly_address
  %68 = load i32* %stack_var_-12
  %69 = zext i32 %68 to i64
  store i64 %69, i64* %rax
  store volatile i64 56880, i64* @assembly_address
  %70 = load i8** %stack_var_-40
  %71 = ptrtoint i8* %70 to i64
  %72 = load i64* %rax
  %73 = add i64 %71, %72
  %74 = and i64 %71, 15
  %75 = and i64 %72, 15
  %76 = add i64 %74, %75
  %77 = icmp ugt i64 %76, 15
  %78 = icmp ult i64 %73, %71
  %79 = xor i64 %71, %73
  %80 = xor i64 %72, %73
  %81 = and i64 %79, %80
  %82 = icmp slt i64 %81, 0
  store i1 %77, i1* %az
  store i1 %78, i1* %cf
  store i1 %82, i1* %of
  %83 = icmp eq i64 %73, 0
  store i1 %83, i1* %zf
  %84 = icmp slt i64 %73, 0
  store i1 %84, i1* %sf
  %85 = trunc i64 %73 to i8
  %86 = call i8 @llvm.ctpop.i8(i8 %85)
  %87 = and i8 %86, 1
  %88 = icmp eq i8 %87, 0
  store i1 %88, i1* %pf
  %89 = inttoptr i64 %73 to i8*
  store i8* %89, i8** %stack_var_-40
  br label %block_de34

block_de34:                                       ; preds = %block_de27, %block_de08
  store volatile i64 56884, i64* @assembly_address
  %90 = load i32* %stack_var_-32
  %91 = zext i32 %90 to i64
  store i64 %91, i64* %rdx
  store volatile i64 56887, i64* @assembly_address
  %92 = load i8** %stack_var_-40
  %93 = ptrtoint i8* %92 to i64
  store i64 %93, i64* %rcx
  store volatile i64 56891, i64* @assembly_address
  %94 = load i32* %stack_var_-28
  %95 = zext i32 %94 to i64
  store i64 %95, i64* %rax
  store volatile i64 56894, i64* @assembly_address
  %96 = load i64* %rcx
  store i64 %96, i64* %rsi
  store volatile i64 56897, i64* @assembly_address
  %97 = load i64* %rax
  %98 = trunc i64 %97 to i32
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rdi
  store volatile i64 56899, i64* @assembly_address
  %100 = load i64* %rdi
  %101 = load i64* %rsi
  %102 = load i64* %rdx
  %103 = trunc i64 %102 to i32
  %104 = trunc i64 %100 to i32
  %105 = call i64 @write_buffer(i32 %104, i64 %101, i32 %103)
  store i64 %105, i64* %rax
  store i64 %105, i64* %rax
  store volatile i64 56904, i64* @assembly_address
  %106 = load i64* %rax
  %107 = trunc i64 %106 to i32
  store i32 %107, i32* %stack_var_-12
  store volatile i64 56907, i64* @assembly_address
  %108 = load i32* %stack_var_-12
  %109 = zext i32 %108 to i64
  store i64 %109, i64* %rax
  store volatile i64 56910, i64* @assembly_address
  %110 = load i64* %rax
  %111 = trunc i64 %110 to i32
  %112 = load i32* %stack_var_-32
  %113 = sub i32 %111, %112
  %114 = and i32 %111, 15
  %115 = and i32 %112, 15
  %116 = sub i32 %114, %115
  %117 = icmp ugt i32 %116, 15
  %118 = icmp ult i32 %111, %112
  %119 = xor i32 %111, %112
  %120 = xor i32 %111, %113
  %121 = and i32 %119, %120
  %122 = icmp slt i32 %121, 0
  store i1 %117, i1* %az
  store i1 %118, i1* %cf
  store i1 %122, i1* %of
  %123 = icmp eq i32 %113, 0
  store i1 %123, i1* %zf
  %124 = icmp slt i32 %113, 0
  store i1 %124, i1* %sf
  %125 = trunc i32 %113 to i8
  %126 = call i8 @llvm.ctpop.i8(i8 %125)
  %127 = and i8 %126, 1
  %128 = icmp eq i8 %127, 0
  store i1 %128, i1* %pf
  store volatile i64 56913, i64* @assembly_address
  %129 = load i1* %zf
  %130 = icmp eq i1 %129, false
  br i1 %130, label %block_de1c, label %block_de53

block_de53:                                       ; preds = %block_de34
  store volatile i64 56915, i64* @assembly_address
  store volatile i64 56916, i64* @assembly_address
  %131 = load i64* %stack_var_-8
  store i64 %131, i64* %rbp
  %132 = ptrtoint i64* %stack_var_0 to i64
  store i64 %132, i64* %rsp
  store volatile i64 56917, i64* @assembly_address
  %133 = load i64* %rax
  ret i64 %133
}

declare i64 @249(i64, i8*, i32)

declare i64 @250(i64, i8*, i64)

define i64 @strlwr(i8* %arg1) {
block_de56:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %1 = alloca i64
  %stack_var_-32 = alloca i8*
  %2 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 56918, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 56919, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 56922, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 56926, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = inttoptr i64 %22 to i8*
  store i8* %23, i8** %stack_var_-32
  store volatile i64 56930, i64* @assembly_address
  %24 = load i8** %stack_var_-32
  %25 = ptrtoint i8* %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 56934, i64* @assembly_address
  %26 = load i64* %rax
  %27 = inttoptr i64 %26 to i8*
  store i8* %27, i8** %stack_var_-16
  store volatile i64 56938, i64* @assembly_address
  br label %block_deb8

block_de6c:                                       ; preds = %block_deb8
  store volatile i64 56940, i64* @assembly_address
  %28 = call i16** @__ctype_b_loc()
  %29 = ptrtoint i16** %28 to i64
  store i64 %29, i64* %rax
  %30 = ptrtoint i16** %28 to i64
  store i64 %30, i64* %rax
  %31 = ptrtoint i16** %28 to i64
  store i64 %31, i64* %rax
  store volatile i64 56945, i64* @assembly_address
  %32 = load i64* %rax
  %33 = inttoptr i64 %32 to i64*
  %34 = load i64* %33
  store i64 %34, i64* %rdx
  store volatile i64 56948, i64* @assembly_address
  %35 = load i8** %stack_var_-16
  %36 = ptrtoint i8* %35 to i64
  store i64 %36, i64* %rax
  store volatile i64 56952, i64* @assembly_address
  %37 = load i64* %rax
  %38 = inttoptr i64 %37 to i8*
  %39 = load i8* %38
  %40 = zext i8 %39 to i64
  store i64 %40, i64* %rax
  store volatile i64 56955, i64* @assembly_address
  %41 = load i64* %rax
  %42 = trunc i64 %41 to i8
  %43 = zext i8 %42 to i64
  store i64 %43, i64* %rax
  store volatile i64 56958, i64* @assembly_address
  %44 = load i64* %rax
  %45 = load i64* %rax
  %46 = add i64 %44, %45
  %47 = and i64 %44, 15
  %48 = and i64 %45, 15
  %49 = add i64 %47, %48
  %50 = icmp ugt i64 %49, 15
  %51 = icmp ult i64 %46, %44
  %52 = xor i64 %44, %46
  %53 = xor i64 %45, %46
  %54 = and i64 %52, %53
  %55 = icmp slt i64 %54, 0
  store i1 %50, i1* %az
  store i1 %51, i1* %cf
  store i1 %55, i1* %of
  %56 = icmp eq i64 %46, 0
  store i1 %56, i1* %zf
  %57 = icmp slt i64 %46, 0
  store i1 %57, i1* %sf
  %58 = trunc i64 %46 to i8
  %59 = call i8 @llvm.ctpop.i8(i8 %58)
  %60 = and i8 %59, 1
  %61 = icmp eq i8 %60, 0
  store i1 %61, i1* %pf
  store i64 %46, i64* %rax
  store volatile i64 56961, i64* @assembly_address
  %62 = load i64* %rax
  %63 = load i64* %rdx
  %64 = add i64 %62, %63
  %65 = and i64 %62, 15
  %66 = and i64 %63, 15
  %67 = add i64 %65, %66
  %68 = icmp ugt i64 %67, 15
  %69 = icmp ult i64 %64, %62
  %70 = xor i64 %62, %64
  %71 = xor i64 %63, %64
  %72 = and i64 %70, %71
  %73 = icmp slt i64 %72, 0
  store i1 %68, i1* %az
  store i1 %69, i1* %cf
  store i1 %73, i1* %of
  %74 = icmp eq i64 %64, 0
  store i1 %74, i1* %zf
  %75 = icmp slt i64 %64, 0
  store i1 %75, i1* %sf
  %76 = trunc i64 %64 to i8
  %77 = call i8 @llvm.ctpop.i8(i8 %76)
  %78 = and i8 %77, 1
  %79 = icmp eq i8 %78, 0
  store i1 %79, i1* %pf
  store i64 %64, i64* %rax
  store volatile i64 56964, i64* @assembly_address
  %80 = load i64* %rax
  %81 = inttoptr i64 %80 to i16*
  %82 = load i16* %81
  %83 = zext i16 %82 to i64
  store i64 %83, i64* %rax
  store volatile i64 56967, i64* @assembly_address
  %84 = load i64* %rax
  %85 = trunc i64 %84 to i16
  %86 = zext i16 %85 to i64
  store i64 %86, i64* %rax
  store volatile i64 56970, i64* @assembly_address
  %87 = load i64* %rax
  %88 = trunc i64 %87 to i32
  %89 = and i32 %88, 256
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %90 = icmp eq i32 %89, 0
  store i1 %90, i1* %zf
  %91 = icmp slt i32 %89, 0
  store i1 %91, i1* %sf
  %92 = trunc i32 %89 to i8
  %93 = call i8 @llvm.ctpop.i8(i8 %92)
  %94 = and i8 %93, 1
  %95 = icmp eq i8 %94, 0
  store i1 %95, i1* %pf
  %96 = zext i32 %89 to i64
  store i64 %96, i64* %rax
  store volatile i64 56975, i64* @assembly_address
  %97 = load i64* %rax
  %98 = trunc i64 %97 to i32
  %99 = load i64* %rax
  %100 = trunc i64 %99 to i32
  %101 = and i32 %98, %100
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %102 = icmp eq i32 %101, 0
  store i1 %102, i1* %zf
  %103 = icmp slt i32 %101, 0
  store i1 %103, i1* %sf
  %104 = trunc i32 %101 to i8
  %105 = call i8 @llvm.ctpop.i8(i8 %104)
  %106 = and i8 %105, 1
  %107 = icmp eq i8 %106, 0
  store i1 %107, i1* %pf
  store volatile i64 56977, i64* @assembly_address
  %108 = load i1* %zf
  br i1 %108, label %block_dea6, label %block_de93

block_de93:                                       ; preds = %block_de6c
  store volatile i64 56979, i64* @assembly_address
  %109 = load i8** %stack_var_-16
  %110 = ptrtoint i8* %109 to i64
  store i64 %110, i64* %rax
  store volatile i64 56983, i64* @assembly_address
  %111 = load i64* %rax
  %112 = inttoptr i64 %111 to i8*
  %113 = load i8* %112
  %114 = zext i8 %113 to i64
  store i64 %114, i64* %rax
  store volatile i64 56986, i64* @assembly_address
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i8
  %117 = zext i8 %116 to i64
  store i64 %117, i64* %rax
  store volatile i64 56989, i64* @assembly_address
  %118 = load i64* %rax
  %119 = trunc i64 %118 to i32
  %120 = zext i32 %119 to i64
  store i64 %120, i64* %rdi
  store volatile i64 56991, i64* @assembly_address
  %121 = load i64* %rdi
  %122 = trunc i64 %121 to i32
  %123 = call i32 @tolower(i32 %122)
  %124 = sext i32 %123 to i64
  store i64 %124, i64* %rax
  %125 = sext i32 %123 to i64
  store i64 %125, i64* %rax
  store volatile i64 56996, i64* @assembly_address
  br label %block_dead

block_dea6:                                       ; preds = %block_de6c
  store volatile i64 56998, i64* @assembly_address
  %126 = load i8** %stack_var_-16
  %127 = ptrtoint i8* %126 to i64
  store i64 %127, i64* %rax
  store volatile i64 57002, i64* @assembly_address
  %128 = load i64* %rax
  %129 = inttoptr i64 %128 to i8*
  %130 = load i8* %129
  %131 = zext i8 %130 to i64
  store i64 %131, i64* %rax
  br label %block_dead

block_dead:                                       ; preds = %block_dea6, %block_de93
  store volatile i64 57005, i64* @assembly_address
  %132 = load i8** %stack_var_-16
  %133 = ptrtoint i8* %132 to i64
  store i64 %133, i64* %rdx
  store volatile i64 57009, i64* @assembly_address
  %134 = load i64* %rax
  %135 = trunc i64 %134 to i8
  %136 = load i64* %rdx
  %137 = inttoptr i64 %136 to i8*
  store i8 %135, i8* %137
  store volatile i64 57011, i64* @assembly_address
  %138 = load i8** %stack_var_-16
  %139 = ptrtoint i8* %138 to i64
  %140 = add i64 %139, 1
  %141 = and i64 %139, 15
  %142 = add i64 %141, 1
  %143 = icmp ugt i64 %142, 15
  %144 = icmp ult i64 %140, %139
  %145 = xor i64 %139, %140
  %146 = xor i64 1, %140
  %147 = and i64 %145, %146
  %148 = icmp slt i64 %147, 0
  store i1 %143, i1* %az
  store i1 %144, i1* %cf
  store i1 %148, i1* %of
  %149 = icmp eq i64 %140, 0
  store i1 %149, i1* %zf
  %150 = icmp slt i64 %140, 0
  store i1 %150, i1* %sf
  %151 = trunc i64 %140 to i8
  %152 = call i8 @llvm.ctpop.i8(i8 %151)
  %153 = and i8 %152, 1
  %154 = icmp eq i8 %153, 0
  store i1 %154, i1* %pf
  %155 = inttoptr i64 %140 to i8*
  store i8* %155, i8** %stack_var_-16
  br label %block_deb8

block_deb8:                                       ; preds = %block_dead, %block_de56
  store volatile i64 57016, i64* @assembly_address
  %156 = load i8** %stack_var_-16
  %157 = ptrtoint i8* %156 to i64
  store i64 %157, i64* %rax
  store volatile i64 57020, i64* @assembly_address
  %158 = load i64* %rax
  %159 = inttoptr i64 %158 to i8*
  %160 = load i8* %159
  %161 = zext i8 %160 to i64
  store i64 %161, i64* %rax
  store volatile i64 57023, i64* @assembly_address
  %162 = load i64* %rax
  %163 = trunc i64 %162 to i8
  %164 = load i64* %rax
  %165 = trunc i64 %164 to i8
  %166 = and i8 %163, %165
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %167 = icmp eq i8 %166, 0
  store i1 %167, i1* %zf
  %168 = icmp slt i8 %166, 0
  store i1 %168, i1* %sf
  %169 = call i8 @llvm.ctpop.i8(i8 %166)
  %170 = and i8 %169, 1
  %171 = icmp eq i8 %170, 0
  store i1 %171, i1* %pf
  store volatile i64 57025, i64* @assembly_address
  %172 = load i1* %zf
  %173 = icmp eq i1 %172, false
  br i1 %173, label %block_de6c, label %block_dec3

block_dec3:                                       ; preds = %block_deb8
  store volatile i64 57027, i64* @assembly_address
  %174 = load i8** %stack_var_-32
  %175 = ptrtoint i8* %174 to i64
  store i64 %175, i64* %rax
  store volatile i64 57031, i64* @assembly_address
  %176 = load i64* %stack_var_-8
  store i64 %176, i64* %rbp
  %177 = ptrtoint i64* %stack_var_0 to i64
  store i64 %177, i64* %rsp
  store volatile i64 57032, i64* @assembly_address
  %178 = load i64* %rax
  ret i64 %178
}

declare i64 @251(i64)

define i64 @gzip_base_name(i64* %arg1) {
block_dec9:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57033, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 57034, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 57037, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 57041, i64* @assembly_address
  %20 = load i64* %rdi
  store i64 %20, i64* %stack_var_-16
  store volatile i64 57045, i64* @assembly_address
  %21 = load i64* %stack_var_-16
  store i64 %21, i64* %rax
  store volatile i64 57049, i64* @assembly_address
  %22 = load i64* %rax
  store i64 %22, i64* %rdi
  store volatile i64 57052, i64* @assembly_address
  %23 = load i64* %rdi
  %24 = inttoptr i64 %23 to i64*
  %25 = bitcast i64* %24 to i8*
  %26 = call i64 @last_component(i8* %25)
  store i64 %26, i64* %rax
  store i64 %26, i64* %rax
  store volatile i64 57057, i64* @assembly_address
  %27 = load i64* %rax
  store i64 %27, i64* %stack_var_-16
  store volatile i64 57061, i64* @assembly_address
  %28 = load i64* %stack_var_-16
  store i64 %28, i64* %rax
  store volatile i64 57065, i64* @assembly_address
  %29 = load i64* %stack_var_-8
  store i64 %29, i64* %rbp
  %30 = ptrtoint i64* %stack_var_0 to i64
  store i64 %30, i64* %rsp
  store volatile i64 57066, i64* @assembly_address
  %31 = load i64* %rax
  ret i64 %31
}

declare i64 @252(i64)

define i64 @xunlink(i8* %arg1) {
block_deeb:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = bitcast i8* %arg1 to i64*
  %1 = ptrtoint i64* %0 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-32 = alloca i8*
  %2 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57067, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 57068, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 57071, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 57075, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = inttoptr i64 %22 to i8*
  store i8* %23, i8** %stack_var_-32
  store volatile i64 57079, i64* @assembly_address
  %24 = load i8** %stack_var_-32
  %25 = ptrtoint i8* %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 57083, i64* @assembly_address
  %26 = load i64* %rax
  store i64 %26, i64* %rdi
  store volatile i64 57086, i64* @assembly_address
  %27 = load i64* %rdi
  %28 = inttoptr i64 %27 to i8*
  %29 = call i32 @unlink(i8* %28)
  %30 = sext i32 %29 to i64
  store i64 %30, i64* %rax
  %31 = sext i32 %29 to i64
  store i64 %31, i64* %rax
  store volatile i64 57091, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i32
  store i32 %33, i32* %stack_var_-12
  store volatile i64 57094, i64* @assembly_address
  %34 = load i32* %stack_var_-12
  %35 = zext i32 %34 to i64
  store i64 %35, i64* %rax
  store volatile i64 57097, i64* @assembly_address
  %36 = load i64* %stack_var_-8
  store i64 %36, i64* %rbp
  %37 = ptrtoint i64* %stack_var_0 to i64
  store i64 %37, i64* %rsp
  store volatile i64 57098, i64* @assembly_address
  %38 = load i64* %rax
  ret i64 %38
}

declare i64 @253(i64*)

define i64 @make_simple_name(i8* %arg1) {
block_df0b:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %1 = alloca i64
  %stack_var_-32 = alloca i8*
  %2 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57099, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 57100, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 57103, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 57107, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = inttoptr i64 %22 to i8*
  store i8* %23, i8** %stack_var_-32
  store volatile i64 57111, i64* @assembly_address
  %24 = load i8** %stack_var_-32
  %25 = ptrtoint i8* %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 57115, i64* @assembly_address
  store i64 46, i64* %rsi
  store volatile i64 57120, i64* @assembly_address
  %26 = load i64* %rax
  store i64 %26, i64* %rdi
  store volatile i64 57123, i64* @assembly_address
  %27 = load i64* %rdi
  %28 = inttoptr i64 %27 to i8*
  %29 = load i64* %rsi
  %30 = trunc i64 %29 to i32
  %31 = call i8* @strrchr(i8* %28, i32 %30)
  %32 = ptrtoint i8* %31 to i64
  store i64 %32, i64* %rax
  %33 = ptrtoint i8* %31 to i64
  store i64 %33, i64* %rax
  store volatile i64 57128, i64* @assembly_address
  %34 = load i64* %rax
  %35 = inttoptr i64 %34 to i8*
  store i8* %35, i8** %stack_var_-16
  store volatile i64 57132, i64* @assembly_address
  %36 = load i8** %stack_var_-16
  %37 = ptrtoint i8* %36 to i64
  %38 = and i64 %37, 15
  %39 = icmp ugt i64 %38, 15
  %40 = icmp ult i64 %37, 0
  %41 = xor i64 %37, 0
  %42 = and i64 %41, 0
  %43 = icmp slt i64 %42, 0
  store i1 %39, i1* %az
  store i1 %40, i1* %cf
  store i1 %43, i1* %of
  %44 = icmp eq i64 %37, 0
  store i1 %44, i1* %zf
  %45 = icmp slt i64 %37, 0
  store i1 %45, i1* %sf
  %46 = trunc i64 %37 to i8
  %47 = call i8 @llvm.ctpop.i8(i8 %46)
  %48 = and i8 %47, 1
  %49 = icmp eq i8 %48, 0
  store i1 %49, i1* %pf
  store volatile i64 57137, i64* @assembly_address
  %50 = load i1* %zf
  br i1 %50, label %block_df65, label %block_df33

block_df33:                                       ; preds = %block_df0b
  store volatile i64 57139, i64* @assembly_address
  %51 = load i8** %stack_var_-16
  %52 = ptrtoint i8* %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 57143, i64* @assembly_address
  %53 = load i64* %rax
  %54 = load i8** %stack_var_-32
  %55 = ptrtoint i8* %54 to i64
  %56 = sub i64 %53, %55
  %57 = and i64 %53, 15
  %58 = and i64 %55, 15
  %59 = sub i64 %57, %58
  %60 = icmp ugt i64 %59, 15
  %61 = icmp ult i64 %53, %55
  %62 = xor i64 %53, %55
  %63 = xor i64 %53, %56
  %64 = and i64 %62, %63
  %65 = icmp slt i64 %64, 0
  store i1 %60, i1* %az
  store i1 %61, i1* %cf
  store i1 %65, i1* %of
  %66 = icmp eq i64 %56, 0
  store i1 %66, i1* %zf
  %67 = icmp slt i64 %56, 0
  store i1 %67, i1* %sf
  %68 = trunc i64 %56 to i8
  %69 = call i8 @llvm.ctpop.i8(i8 %68)
  %70 = and i8 %69, 1
  %71 = icmp eq i8 %70, 0
  store i1 %71, i1* %pf
  store volatile i64 57147, i64* @assembly_address
  %72 = load i1* %zf
  %73 = icmp eq i1 %72, false
  br i1 %73, label %block_df42, label %block_df3d

block_df3d:                                       ; preds = %block_df33
  store volatile i64 57149, i64* @assembly_address
  %74 = load i8** %stack_var_-16
  %75 = ptrtoint i8* %74 to i64
  %76 = add i64 %75, 1
  %77 = and i64 %75, 15
  %78 = add i64 %77, 1
  %79 = icmp ugt i64 %78, 15
  %80 = icmp ult i64 %76, %75
  %81 = xor i64 %75, %76
  %82 = xor i64 1, %76
  %83 = and i64 %81, %82
  %84 = icmp slt i64 %83, 0
  store i1 %79, i1* %az
  store i1 %80, i1* %cf
  store i1 %84, i1* %of
  %85 = icmp eq i64 %76, 0
  store i1 %85, i1* %zf
  %86 = icmp slt i64 %76, 0
  store i1 %86, i1* %sf
  %87 = trunc i64 %76 to i8
  %88 = call i8 @llvm.ctpop.i8(i8 %87)
  %89 = and i8 %88, 1
  %90 = icmp eq i8 %89, 0
  store i1 %90, i1* %pf
  %91 = inttoptr i64 %76 to i8*
  store i8* %91, i8** %stack_var_-16
  br label %block_df42

block_df42:                                       ; preds = %block_df59, %block_df3d, %block_df33
  store volatile i64 57154, i64* @assembly_address
  %92 = load i8** %stack_var_-16
  %93 = ptrtoint i8* %92 to i64
  %94 = sub i64 %93, 1
  %95 = and i64 %93, 15
  %96 = sub i64 %95, 1
  %97 = icmp ugt i64 %96, 15
  %98 = icmp ult i64 %93, 1
  %99 = xor i64 %93, 1
  %100 = xor i64 %93, %94
  %101 = and i64 %99, %100
  %102 = icmp slt i64 %101, 0
  store i1 %97, i1* %az
  store i1 %98, i1* %cf
  store i1 %102, i1* %of
  %103 = icmp eq i64 %94, 0
  store i1 %103, i1* %zf
  %104 = icmp slt i64 %94, 0
  store i1 %104, i1* %sf
  %105 = trunc i64 %94 to i8
  %106 = call i8 @llvm.ctpop.i8(i8 %105)
  %107 = and i8 %106, 1
  %108 = icmp eq i8 %107, 0
  store i1 %108, i1* %pf
  %109 = inttoptr i64 %94 to i8*
  store i8* %109, i8** %stack_var_-16
  store volatile i64 57159, i64* @assembly_address
  %110 = load i8** %stack_var_-16
  %111 = ptrtoint i8* %110 to i64
  store i64 %111, i64* %rax
  store volatile i64 57163, i64* @assembly_address
  %112 = load i64* %rax
  %113 = inttoptr i64 %112 to i8*
  %114 = load i8* %113
  %115 = zext i8 %114 to i64
  store i64 %115, i64* %rax
  store volatile i64 57166, i64* @assembly_address
  %116 = load i64* %rax
  %117 = trunc i64 %116 to i8
  %118 = sub i8 %117, 46
  %119 = and i8 %117, 15
  %120 = sub i8 %119, 14
  %121 = icmp ugt i8 %120, 15
  %122 = icmp ult i8 %117, 46
  %123 = xor i8 %117, 46
  %124 = xor i8 %117, %118
  %125 = and i8 %123, %124
  %126 = icmp slt i8 %125, 0
  store i1 %121, i1* %az
  store i1 %122, i1* %cf
  store i1 %126, i1* %of
  %127 = icmp eq i8 %118, 0
  store i1 %127, i1* %zf
  %128 = icmp slt i8 %118, 0
  store i1 %128, i1* %sf
  %129 = call i8 @llvm.ctpop.i8(i8 %118)
  %130 = and i8 %129, 1
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %pf
  store volatile i64 57168, i64* @assembly_address
  %132 = load i1* %zf
  %133 = icmp eq i1 %132, false
  br i1 %133, label %block_df59, label %block_df52

block_df52:                                       ; preds = %block_df42
  store volatile i64 57170, i64* @assembly_address
  %134 = load i8** %stack_var_-16
  %135 = ptrtoint i8* %134 to i64
  store i64 %135, i64* %rax
  store volatile i64 57174, i64* @assembly_address
  %136 = load i64* %rax
  %137 = inttoptr i64 %136 to i8*
  store i8 95, i8* %137
  br label %block_df59

block_df59:                                       ; preds = %block_df52, %block_df42
  store volatile i64 57177, i64* @assembly_address
  %138 = load i8** %stack_var_-16
  %139 = ptrtoint i8* %138 to i64
  store i64 %139, i64* %rax
  store volatile i64 57181, i64* @assembly_address
  %140 = load i64* %rax
  %141 = load i8** %stack_var_-32
  %142 = ptrtoint i8* %141 to i64
  %143 = sub i64 %140, %142
  %144 = and i64 %140, 15
  %145 = and i64 %142, 15
  %146 = sub i64 %144, %145
  %147 = icmp ugt i64 %146, 15
  %148 = icmp ult i64 %140, %142
  %149 = xor i64 %140, %142
  %150 = xor i64 %140, %143
  %151 = and i64 %149, %150
  %152 = icmp slt i64 %151, 0
  store i1 %147, i1* %az
  store i1 %148, i1* %cf
  store i1 %152, i1* %of
  %153 = icmp eq i64 %143, 0
  store i1 %153, i1* %zf
  %154 = icmp slt i64 %143, 0
  store i1 %154, i1* %sf
  %155 = trunc i64 %143 to i8
  %156 = call i8 @llvm.ctpop.i8(i8 %155)
  %157 = and i8 %156, 1
  %158 = icmp eq i8 %157, 0
  store i1 %158, i1* %pf
  store volatile i64 57185, i64* @assembly_address
  %159 = load i1* %zf
  %160 = icmp eq i1 %159, false
  br i1 %160, label %block_df42, label %block_df63

block_df63:                                       ; preds = %block_df59
  store volatile i64 57187, i64* @assembly_address
  br label %block_df66

block_df65:                                       ; preds = %block_df0b
  store volatile i64 57189, i64* @assembly_address
  br label %block_df66

block_df66:                                       ; preds = %block_df65, %block_df63
  store volatile i64 57190, i64* @assembly_address
  %161 = load i64* %stack_var_-8
  store i64 %161, i64* %rbp
  %162 = ptrtoint i64* %stack_var_0 to i64
  store i64 %162, i64* %rsp
  store volatile i64 57191, i64* @assembly_address
  %163 = load i64* %rax
  ret i64 %163
}

declare i64 @254(i64)

define i64 @add_envopt(i32* %arg1, i64* %arg2, i8* %arg3) {
block_df68:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg3 to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %arg2 to i64
  store i64 %1, i64* %rsi
  %2 = bitcast i32* %arg1 to i64*
  %3 = ptrtoint i64* %2 to i64
  store i64 %3, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-40 = alloca i8*
  %4 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-44 = alloca i32
  %stack_var_-80 = alloca i8*
  %5 = alloca i64
  %stack_var_-72 = alloca i64
  %stack_var_-64 = alloca i32*
  %6 = alloca i64
  %stack_var_-88 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57192, i64* @assembly_address
  %7 = load i64* %rbp
  store i64 %7, i64* %stack_var_-8
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rsp
  store volatile i64 57193, i64* @assembly_address
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rbp
  store volatile i64 57196, i64* @assembly_address
  %10 = load i64* %rsp
  %11 = sub i64 %10, 80
  %12 = and i64 %10, 15
  %13 = icmp ugt i64 %12, 15
  %14 = icmp ult i64 %10, 80
  %15 = xor i64 %10, 80
  %16 = xor i64 %10, %11
  %17 = and i64 %15, %16
  %18 = icmp slt i64 %17, 0
  store i1 %13, i1* %az
  store i1 %14, i1* %cf
  store i1 %18, i1* %of
  %19 = icmp eq i64 %11, 0
  store i1 %19, i1* %zf
  %20 = icmp slt i64 %11, 0
  store i1 %20, i1* %sf
  %21 = trunc i64 %11 to i8
  %22 = call i8 @llvm.ctpop.i8(i8 %21)
  %23 = and i8 %22, 1
  %24 = icmp eq i8 %23, 0
  store i1 %24, i1* %pf
  %25 = ptrtoint i64* %stack_var_-88 to i64
  store i64 %25, i64* %rsp
  store volatile i64 57200, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to i32*
  store i32* %27, i32** %stack_var_-64
  store volatile i64 57204, i64* @assembly_address
  %28 = load i64* %rsi
  store i64 %28, i64* %stack_var_-72
  store volatile i64 57208, i64* @assembly_address
  %29 = load i64* %rdx
  %30 = inttoptr i64 %29 to i8*
  store i8* %30, i8** %stack_var_-80
  store volatile i64 57212, i64* @assembly_address
  store i32 0, i32* %stack_var_-44
  store volatile i64 57219, i64* @assembly_address
  %31 = load i8** %stack_var_-80
  %32 = ptrtoint i8* %31 to i64
  store i64 %32, i64* %rax
  store volatile i64 57223, i64* @assembly_address
  %33 = load i64* %rax
  store i64 %33, i64* %rdi
  store volatile i64 57226, i64* @assembly_address
  %34 = load i64* %rdi
  %35 = inttoptr i64 %34 to i8*
  %36 = call i8* @getenv(i8* %35)
  %37 = ptrtoint i8* %36 to i64
  store i64 %37, i64* %rax
  %38 = ptrtoint i8* %36 to i64
  store i64 %38, i64* %rax
  store volatile i64 57231, i64* @assembly_address
  %39 = load i64* %rax
  store i64 %39, i64* %stack_var_-24
  store volatile i64 57235, i64* @assembly_address
  %40 = load i64* %stack_var_-24
  %41 = and i64 %40, 15
  %42 = icmp ugt i64 %41, 15
  %43 = icmp ult i64 %40, 0
  %44 = xor i64 %40, 0
  %45 = and i64 %44, 0
  %46 = icmp slt i64 %45, 0
  store i1 %42, i1* %az
  store i1 %43, i1* %cf
  store i1 %46, i1* %of
  %47 = icmp eq i64 %40, 0
  store i1 %47, i1* %zf
  %48 = icmp slt i64 %40, 0
  store i1 %48, i1* %sf
  %49 = trunc i64 %40 to i8
  %50 = call i8 @llvm.ctpop.i8(i8 %49)
  %51 = and i8 %50, 1
  %52 = icmp eq i8 %51, 0
  store i1 %52, i1* %pf
  store volatile i64 57240, i64* @assembly_address
  %53 = load i1* %zf
  %54 = icmp eq i1 %53, false
  br i1 %54, label %block_dfa4, label %block_df9a

block_df9a:                                       ; preds = %block_df68
  store volatile i64 57242, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57247, i64* @assembly_address
  br label %block_e0f4

block_dfa4:                                       ; preds = %block_df68
  store volatile i64 57252, i64* @assembly_address
  %55 = load i64* %stack_var_-24
  store i64 %55, i64* %rax
  store volatile i64 57256, i64* @assembly_address
  %56 = load i64* %rax
  store i64 %56, i64* %rdi
  store volatile i64 57259, i64* @assembly_address
  %57 = load i64* %rdi
  %58 = inttoptr i64 %57 to i8*
  %59 = call i64 @xstrdup(i8* %58)
  store i64 %59, i64* %rax
  store i64 %59, i64* %rax
  store volatile i64 57264, i64* @assembly_address
  %60 = load i64* %rax
  store i64 %60, i64* %stack_var_-24
  store volatile i64 57268, i64* @assembly_address
  %61 = load i64* %stack_var_-24
  store i64 %61, i64* %rax
  store volatile i64 57272, i64* @assembly_address
  %62 = load i64* %rax
  %63 = inttoptr i64 %62 to i8*
  store i8* %63, i8** %stack_var_-40
  store volatile i64 57276, i64* @assembly_address
  br label %block_e015

block_dfbe:                                       ; preds = %block_e015
  store volatile i64 57278, i64* @assembly_address
  %64 = load i8** %stack_var_-40
  %65 = ptrtoint i8* %64 to i64
  store i64 %65, i64* %rax
  store volatile i64 57282, i64* @assembly_address
  store i64 ptrtoint ([3 x i8]* @global_var_12c40 to i64), i64* %rsi
  store volatile i64 57289, i64* @assembly_address
  %66 = load i64* %rax
  store i64 %66, i64* %rdi
  store volatile i64 57292, i64* @assembly_address
  %67 = load i64* %rdi
  %68 = inttoptr i64 %67 to i8*
  %69 = load i64* %rsi
  %70 = inttoptr i64 %69 to i8*
  %71 = call i32 @strspn(i8* %68, i8* %70)
  %72 = sext i32 %71 to i64
  store i64 %72, i64* %rax
  %73 = sext i32 %71 to i64
  store i64 %73, i64* %rax
  store volatile i64 57297, i64* @assembly_address
  %74 = load i8** %stack_var_-40
  %75 = ptrtoint i8* %74 to i64
  %76 = load i64* %rax
  %77 = add i64 %75, %76
  %78 = and i64 %75, 15
  %79 = and i64 %76, 15
  %80 = add i64 %78, %79
  %81 = icmp ugt i64 %80, 15
  %82 = icmp ult i64 %77, %75
  %83 = xor i64 %75, %77
  %84 = xor i64 %76, %77
  %85 = and i64 %83, %84
  %86 = icmp slt i64 %85, 0
  store i1 %81, i1* %az
  store i1 %82, i1* %cf
  store i1 %86, i1* %of
  %87 = icmp eq i64 %77, 0
  store i1 %87, i1* %zf
  %88 = icmp slt i64 %77, 0
  store i1 %88, i1* %sf
  %89 = trunc i64 %77 to i8
  %90 = call i8 @llvm.ctpop.i8(i8 %89)
  %91 = and i8 %90, 1
  %92 = icmp eq i8 %91, 0
  store i1 %92, i1* %pf
  %93 = inttoptr i64 %77 to i8*
  store i8* %93, i8** %stack_var_-40
  store volatile i64 57301, i64* @assembly_address
  %94 = load i8** %stack_var_-40
  %95 = ptrtoint i8* %94 to i64
  store i64 %95, i64* %rax
  store volatile i64 57305, i64* @assembly_address
  %96 = load i64* %rax
  %97 = inttoptr i64 %96 to i8*
  %98 = load i8* %97
  %99 = zext i8 %98 to i64
  store i64 %99, i64* %rax
  store volatile i64 57308, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i8
  %102 = load i64* %rax
  %103 = trunc i64 %102 to i8
  %104 = and i8 %101, %103
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %105 = icmp eq i8 %104, 0
  store i1 %105, i1* %zf
  %106 = icmp slt i8 %104, 0
  store i1 %106, i1* %sf
  %107 = call i8 @llvm.ctpop.i8(i8 %104)
  %108 = and i8 %107, 1
  %109 = icmp eq i8 %108, 0
  store i1 %109, i1* %pf
  store volatile i64 57310, i64* @assembly_address
  %110 = load i1* %zf
  br i1 %110, label %block_e022, label %block_dfe0

block_dfe0:                                       ; preds = %block_dfbe
  store volatile i64 57312, i64* @assembly_address
  %111 = load i8** %stack_var_-40
  %112 = ptrtoint i8* %111 to i64
  store i64 %112, i64* %rax
  store volatile i64 57316, i64* @assembly_address
  store i64 ptrtoint ([3 x i8]* @global_var_12c40 to i64), i64* %rsi
  store volatile i64 57323, i64* @assembly_address
  %113 = load i64* %rax
  store i64 %113, i64* %rdi
  store volatile i64 57326, i64* @assembly_address
  %114 = load i64* %rdi
  %115 = inttoptr i64 %114 to i8*
  %116 = load i64* %rsi
  %117 = inttoptr i64 %116 to i8*
  %118 = call i32 @strcspn(i8* %115, i8* %117)
  %119 = sext i32 %118 to i64
  store i64 %119, i64* %rax
  %120 = sext i32 %118 to i64
  store i64 %120, i64* %rax
  store volatile i64 57331, i64* @assembly_address
  %121 = load i8** %stack_var_-40
  %122 = ptrtoint i8* %121 to i64
  %123 = load i64* %rax
  %124 = add i64 %122, %123
  %125 = and i64 %122, 15
  %126 = and i64 %123, 15
  %127 = add i64 %125, %126
  %128 = icmp ugt i64 %127, 15
  %129 = icmp ult i64 %124, %122
  %130 = xor i64 %122, %124
  %131 = xor i64 %123, %124
  %132 = and i64 %130, %131
  %133 = icmp slt i64 %132, 0
  store i1 %128, i1* %az
  store i1 %129, i1* %cf
  store i1 %133, i1* %of
  %134 = icmp eq i64 %124, 0
  store i1 %134, i1* %zf
  %135 = icmp slt i64 %124, 0
  store i1 %135, i1* %sf
  %136 = trunc i64 %124 to i8
  %137 = call i8 @llvm.ctpop.i8(i8 %136)
  %138 = and i8 %137, 1
  %139 = icmp eq i8 %138, 0
  store i1 %139, i1* %pf
  %140 = inttoptr i64 %124 to i8*
  store i8* %140, i8** %stack_var_-40
  store volatile i64 57335, i64* @assembly_address
  %141 = load i8** %stack_var_-40
  %142 = ptrtoint i8* %141 to i64
  store i64 %142, i64* %rax
  store volatile i64 57339, i64* @assembly_address
  %143 = load i64* %rax
  %144 = inttoptr i64 %143 to i8*
  %145 = load i8* %144
  %146 = zext i8 %145 to i64
  store i64 %146, i64* %rax
  store volatile i64 57342, i64* @assembly_address
  %147 = load i64* %rax
  %148 = trunc i64 %147 to i8
  %149 = load i64* %rax
  %150 = trunc i64 %149 to i8
  %151 = and i8 %148, %150
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %152 = icmp eq i8 %151, 0
  store i1 %152, i1* %zf
  %153 = icmp slt i8 %151, 0
  store i1 %153, i1* %sf
  %154 = call i8 @llvm.ctpop.i8(i8 %151)
  %155 = and i8 %154, 1
  %156 = icmp eq i8 %155, 0
  store i1 %156, i1* %pf
  store volatile i64 57344, i64* @assembly_address
  %157 = load i1* %zf
  br i1 %157, label %block_e011, label %block_e002

block_e002:                                       ; preds = %block_dfe0
  store volatile i64 57346, i64* @assembly_address
  %158 = load i8** %stack_var_-40
  %159 = ptrtoint i8* %158 to i64
  store i64 %159, i64* %rax
  store volatile i64 57350, i64* @assembly_address
  %160 = load i64* %rax
  %161 = add i64 %160, 1
  store i64 %161, i64* %rdx
  store volatile i64 57354, i64* @assembly_address
  %162 = load i64* %rdx
  %163 = inttoptr i64 %162 to i8*
  store i8* %163, i8** %stack_var_-40
  store volatile i64 57358, i64* @assembly_address
  %164 = load i64* %rax
  %165 = inttoptr i64 %164 to i8*
  store i8 0, i8* %165
  br label %block_e011

block_e011:                                       ; preds = %block_e002, %block_dfe0
  store volatile i64 57361, i64* @assembly_address
  %166 = load i32* %stack_var_-44
  %167 = add i32 %166, 1
  %168 = and i32 %166, 15
  %169 = add i32 %168, 1
  %170 = icmp ugt i32 %169, 15
  %171 = icmp ult i32 %167, %166
  %172 = xor i32 %166, %167
  %173 = xor i32 1, %167
  %174 = and i32 %172, %173
  %175 = icmp slt i32 %174, 0
  store i1 %170, i1* %az
  store i1 %171, i1* %cf
  store i1 %175, i1* %of
  %176 = icmp eq i32 %167, 0
  store i1 %176, i1* %zf
  %177 = icmp slt i32 %167, 0
  store i1 %177, i1* %sf
  %178 = trunc i32 %167 to i8
  %179 = call i8 @llvm.ctpop.i8(i8 %178)
  %180 = and i8 %179, 1
  %181 = icmp eq i8 %180, 0
  store i1 %181, i1* %pf
  store i32 %167, i32* %stack_var_-44
  br label %block_e015

block_e015:                                       ; preds = %block_e011, %block_dfa4
  store volatile i64 57365, i64* @assembly_address
  %182 = load i8** %stack_var_-40
  %183 = ptrtoint i8* %182 to i64
  store i64 %183, i64* %rax
  store volatile i64 57369, i64* @assembly_address
  %184 = load i64* %rax
  %185 = inttoptr i64 %184 to i8*
  %186 = load i8* %185
  %187 = zext i8 %186 to i64
  store i64 %187, i64* %rax
  store volatile i64 57372, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i8
  %190 = load i64* %rax
  %191 = trunc i64 %190 to i8
  %192 = and i8 %189, %191
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %193 = icmp eq i8 %192, 0
  store i1 %193, i1* %zf
  %194 = icmp slt i8 %192, 0
  store i1 %194, i1* %sf
  %195 = call i8 @llvm.ctpop.i8(i8 %192)
  %196 = and i8 %195, 1
  %197 = icmp eq i8 %196, 0
  store i1 %197, i1* %pf
  store volatile i64 57374, i64* @assembly_address
  %198 = load i1* %zf
  %199 = icmp eq i1 %198, false
  br i1 %199, label %block_dfbe, label %block_e020

block_e020:                                       ; preds = %block_e015
  store volatile i64 57376, i64* @assembly_address
  br label %block_e023

block_e022:                                       ; preds = %block_dfbe
  store volatile i64 57378, i64* @assembly_address
  br label %block_e023

block_e023:                                       ; preds = %block_e022, %block_e020
  store volatile i64 57379, i64* @assembly_address
  %200 = load i32* %stack_var_-44
  %201 = and i32 %200, 15
  %202 = icmp ugt i32 %201, 15
  %203 = icmp ult i32 %200, 0
  %204 = xor i32 %200, 0
  %205 = and i32 %204, 0
  %206 = icmp slt i32 %205, 0
  store i1 %202, i1* %az
  store i1 %203, i1* %cf
  store i1 %206, i1* %of
  %207 = icmp eq i32 %200, 0
  store i1 %207, i1* %zf
  %208 = icmp slt i32 %200, 0
  store i1 %208, i1* %sf
  %209 = trunc i32 %200 to i8
  %210 = call i8 @llvm.ctpop.i8(i8 %209)
  %211 = and i8 %210, 1
  %212 = icmp eq i8 %211, 0
  store i1 %212, i1* %pf
  store volatile i64 57383, i64* @assembly_address
  %213 = load i1* %zf
  %214 = icmp eq i1 %213, false
  br i1 %214, label %block_e03f, label %block_e029

block_e029:                                       ; preds = %block_e023
  store volatile i64 57385, i64* @assembly_address
  %215 = load i64* %stack_var_-24
  store i64 %215, i64* %rax
  store volatile i64 57389, i64* @assembly_address
  %216 = load i64* %rax
  store i64 %216, i64* %rdi
  store volatile i64 57392, i64* @assembly_address
  %217 = load i64* %rdi
  %218 = inttoptr i64 %217 to i64*
  call void @free(i64* %218)
  store volatile i64 57397, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57402, i64* @assembly_address
  br label %block_e0f4

block_e03f:                                       ; preds = %block_e023
  store volatile i64 57407, i64* @assembly_address
  %219 = load i32* %stack_var_-44
  %220 = zext i32 %219 to i64
  store i64 %220, i64* %rax
  store volatile i64 57410, i64* @assembly_address
  %221 = load i64* %rax
  %222 = add i64 %221, 1
  %223 = trunc i64 %222 to i32
  %224 = zext i32 %223 to i64
  store i64 %224, i64* %rdx
  store volatile i64 57413, i64* @assembly_address
  %225 = load i32** %stack_var_-64
  %226 = ptrtoint i32* %225 to i64
  store i64 %226, i64* %rax
  store volatile i64 57417, i64* @assembly_address
  %227 = load i64* %rdx
  %228 = trunc i64 %227 to i32
  %229 = load i64* %rax
  %230 = inttoptr i64 %229 to i32*
  store i32 %228, i32* %230
  store volatile i64 57419, i64* @assembly_address
  %231 = load i32** %stack_var_-64
  %232 = ptrtoint i32* %231 to i64
  store i64 %232, i64* %rax
  store volatile i64 57423, i64* @assembly_address
  %233 = load i64* %rax
  %234 = inttoptr i64 %233 to i32*
  %235 = load i32* %234
  %236 = zext i32 %235 to i64
  store i64 %236, i64* %rax
  store volatile i64 57425, i64* @assembly_address
  %237 = load i64* %rax
  %238 = trunc i64 %237 to i32
  %239 = add i32 %238, 1
  %240 = and i32 %238, 15
  %241 = add i32 %240, 1
  %242 = icmp ugt i32 %241, 15
  %243 = icmp ult i32 %239, %238
  %244 = xor i32 %238, %239
  %245 = xor i32 1, %239
  %246 = and i32 %244, %245
  %247 = icmp slt i32 %246, 0
  store i1 %242, i1* %az
  store i1 %243, i1* %cf
  store i1 %247, i1* %of
  %248 = icmp eq i32 %239, 0
  store i1 %248, i1* %zf
  %249 = icmp slt i32 %239, 0
  store i1 %249, i1* %sf
  %250 = trunc i32 %239 to i8
  %251 = call i8 @llvm.ctpop.i8(i8 %250)
  %252 = and i8 %251, 1
  %253 = icmp eq i8 %252, 0
  store i1 %253, i1* %pf
  %254 = zext i32 %239 to i64
  store i64 %254, i64* %rax
  store volatile i64 57428, i64* @assembly_address
  %255 = load i64* %rax
  %256 = trunc i64 %255 to i32
  %257 = sext i32 %256 to i64
  store i64 %257, i64* %rax
  store volatile i64 57430, i64* @assembly_address
  store i64 8, i64* %rsi
  store volatile i64 57435, i64* @assembly_address
  %258 = load i64* %rax
  store i64 %258, i64* %rdi
  store volatile i64 57438, i64* @assembly_address
  %259 = load i64* %rdi
  %260 = load i64* %rsi
  %261 = trunc i64 %259 to i32
  %262 = call i64 @xcalloc(i32 %261, i64 %260)
  store i64 %262, i64* %rax
  store i64 %262, i64* %rax
  store volatile i64 57443, i64* @assembly_address
  %263 = load i64* %rax
  store i64 %263, i64* %stack_var_-32
  store volatile i64 57447, i64* @assembly_address
  %264 = load i64* %stack_var_-72
  store i64 %264, i64* %rax
  store volatile i64 57451, i64* @assembly_address
  %265 = load i64* %rax
  %266 = inttoptr i64 %265 to i64*
  %267 = load i64* %266
  store i64 %267, i64* %rax
  store volatile i64 57454, i64* @assembly_address
  %268 = load i64* %rax
  store i64 %268, i64* %stack_var_-16
  store volatile i64 57458, i64* @assembly_address
  %269 = load i64* %stack_var_-72
  store i64 %269, i64* %rax
  store volatile i64 57462, i64* @assembly_address
  %270 = load i64* %stack_var_-32
  store i64 %270, i64* %rdx
  store volatile i64 57466, i64* @assembly_address
  %271 = load i64* %rdx
  %272 = load i64* %rax
  %273 = inttoptr i64 %272 to i64*
  store i64 %271, i64* %273
  store volatile i64 57469, i64* @assembly_address
  %274 = load i64* %stack_var_-32
  store i64 %274, i64* %rax
  store volatile i64 57473, i64* @assembly_address
  %275 = load i64* %rax
  %276 = add i64 %275, 8
  store i64 %276, i64* %rdx
  store volatile i64 57477, i64* @assembly_address
  %277 = load i64* %rdx
  store i64 %277, i64* %stack_var_-32
  store volatile i64 57481, i64* @assembly_address
  %278 = load i64* %stack_var_-16
  store i64 %278, i64* %rdx
  store volatile i64 57485, i64* @assembly_address
  %279 = load i64* %rdx
  %280 = inttoptr i64 %279 to i64*
  %281 = load i64* %280
  store i64 %281, i64* %rdx
  store volatile i64 57488, i64* @assembly_address
  %282 = load i64* %rdx
  %283 = load i64* %rax
  %284 = inttoptr i64 %283 to i64*
  store i64 %282, i64* %284
  store volatile i64 57491, i64* @assembly_address
  %285 = load i64* %stack_var_-24
  store i64 %285, i64* %rax
  store volatile i64 57495, i64* @assembly_address
  %286 = load i64* %rax
  %287 = inttoptr i64 %286 to i8*
  store i8* %287, i8** %stack_var_-40
  store volatile i64 57499, i64* @assembly_address
  br label %block_e0df

block_e09d:                                       ; preds = %block_e0df
  store volatile i64 57501, i64* @assembly_address
  %288 = load i8** %stack_var_-40
  %289 = ptrtoint i8* %288 to i64
  store i64 %289, i64* %rax
  store volatile i64 57505, i64* @assembly_address
  store i64 ptrtoint ([3 x i8]* @global_var_12c40 to i64), i64* %rsi
  store volatile i64 57512, i64* @assembly_address
  %290 = load i64* %rax
  store i64 %290, i64* %rdi
  store volatile i64 57515, i64* @assembly_address
  %291 = load i64* %rdi
  %292 = inttoptr i64 %291 to i8*
  %293 = load i64* %rsi
  %294 = inttoptr i64 %293 to i8*
  %295 = call i32 @strspn(i8* %292, i8* %294)
  %296 = sext i32 %295 to i64
  store i64 %296, i64* %rax
  %297 = sext i32 %295 to i64
  store i64 %297, i64* %rax
  store volatile i64 57520, i64* @assembly_address
  %298 = load i8** %stack_var_-40
  %299 = ptrtoint i8* %298 to i64
  %300 = load i64* %rax
  %301 = add i64 %299, %300
  %302 = and i64 %299, 15
  %303 = and i64 %300, 15
  %304 = add i64 %302, %303
  %305 = icmp ugt i64 %304, 15
  %306 = icmp ult i64 %301, %299
  %307 = xor i64 %299, %301
  %308 = xor i64 %300, %301
  %309 = and i64 %307, %308
  %310 = icmp slt i64 %309, 0
  store i1 %305, i1* %az
  store i1 %306, i1* %cf
  store i1 %310, i1* %of
  %311 = icmp eq i64 %301, 0
  store i1 %311, i1* %zf
  %312 = icmp slt i64 %301, 0
  store i1 %312, i1* %sf
  %313 = trunc i64 %301 to i8
  %314 = call i8 @llvm.ctpop.i8(i8 %313)
  %315 = and i8 %314, 1
  %316 = icmp eq i8 %315, 0
  store i1 %316, i1* %pf
  %317 = inttoptr i64 %301 to i8*
  store i8* %317, i8** %stack_var_-40
  store volatile i64 57524, i64* @assembly_address
  %318 = load i64* %stack_var_-32
  store i64 %318, i64* %rax
  store volatile i64 57528, i64* @assembly_address
  %319 = load i64* %rax
  %320 = add i64 %319, 8
  store i64 %320, i64* %rdx
  store volatile i64 57532, i64* @assembly_address
  %321 = load i64* %rdx
  store i64 %321, i64* %stack_var_-32
  store volatile i64 57536, i64* @assembly_address
  %322 = load i8** %stack_var_-40
  %323 = ptrtoint i8* %322 to i64
  store i64 %323, i64* %rdx
  store volatile i64 57540, i64* @assembly_address
  %324 = load i64* %rdx
  %325 = load i64* %rax
  %326 = inttoptr i64 %325 to i64*
  store i64 %324, i64* %326
  store volatile i64 57543, i64* @assembly_address
  br label %block_e0c8

block_e0c8:                                       ; preds = %block_e0c8, %block_e09d
  store volatile i64 57544, i64* @assembly_address
  %327 = load i8** %stack_var_-40
  %328 = ptrtoint i8* %327 to i64
  store i64 %328, i64* %rax
  store volatile i64 57548, i64* @assembly_address
  %329 = load i64* %rax
  %330 = add i64 %329, 1
  store i64 %330, i64* %rdx
  store volatile i64 57552, i64* @assembly_address
  %331 = load i64* %rdx
  %332 = inttoptr i64 %331 to i8*
  store i8* %332, i8** %stack_var_-40
  store volatile i64 57556, i64* @assembly_address
  %333 = load i64* %rax
  %334 = inttoptr i64 %333 to i8*
  %335 = load i8* %334
  %336 = zext i8 %335 to i64
  store i64 %336, i64* %rax
  store volatile i64 57559, i64* @assembly_address
  %337 = load i64* %rax
  %338 = trunc i64 %337 to i8
  %339 = load i64* %rax
  %340 = trunc i64 %339 to i8
  %341 = and i8 %338, %340
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %342 = icmp eq i8 %341, 0
  store i1 %342, i1* %zf
  %343 = icmp slt i8 %341, 0
  store i1 %343, i1* %sf
  %344 = call i8 @llvm.ctpop.i8(i8 %341)
  %345 = and i8 %344, 1
  %346 = icmp eq i8 %345, 0
  store i1 %346, i1* %pf
  store volatile i64 57561, i64* @assembly_address
  %347 = load i1* %zf
  %348 = icmp eq i1 %347, false
  br i1 %348, label %block_e0c8, label %block_e0db

block_e0db:                                       ; preds = %block_e0c8
  store volatile i64 57563, i64* @assembly_address
  %349 = load i32* %stack_var_-44
  %350 = sub i32 %349, 1
  %351 = and i32 %349, 15
  %352 = sub i32 %351, 1
  %353 = icmp ugt i32 %352, 15
  %354 = icmp ult i32 %349, 1
  %355 = xor i32 %349, 1
  %356 = xor i32 %349, %350
  %357 = and i32 %355, %356
  %358 = icmp slt i32 %357, 0
  store i1 %353, i1* %az
  store i1 %354, i1* %cf
  store i1 %358, i1* %of
  %359 = icmp eq i32 %350, 0
  store i1 %359, i1* %zf
  %360 = icmp slt i32 %350, 0
  store i1 %360, i1* %sf
  %361 = trunc i32 %350 to i8
  %362 = call i8 @llvm.ctpop.i8(i8 %361)
  %363 = and i8 %362, 1
  %364 = icmp eq i8 %363, 0
  store i1 %364, i1* %pf
  store i32 %350, i32* %stack_var_-44
  br label %block_e0df

block_e0df:                                       ; preds = %block_e0db, %block_e03f
  store volatile i64 57567, i64* @assembly_address
  %365 = load i32* %stack_var_-44
  %366 = and i32 %365, 15
  %367 = icmp ugt i32 %366, 15
  %368 = icmp ult i32 %365, 0
  %369 = xor i32 %365, 0
  %370 = and i32 %369, 0
  %371 = icmp slt i32 %370, 0
  store i1 %367, i1* %az
  store i1 %368, i1* %cf
  store i1 %371, i1* %of
  %372 = icmp eq i32 %365, 0
  store i1 %372, i1* %zf
  %373 = icmp slt i32 %365, 0
  store i1 %373, i1* %sf
  %374 = trunc i32 %365 to i8
  %375 = call i8 @llvm.ctpop.i8(i8 %374)
  %376 = and i8 %375, 1
  %377 = icmp eq i8 %376, 0
  store i1 %377, i1* %pf
  store volatile i64 57571, i64* @assembly_address
  %378 = load i1* %zf
  %379 = load i1* %sf
  %380 = load i1* %of
  %381 = icmp eq i1 %379, %380
  %382 = icmp eq i1 %378, false
  %383 = icmp eq i1 %381, %382
  br i1 %383, label %block_e09d, label %block_e0e5

block_e0e5:                                       ; preds = %block_e0df
  store volatile i64 57573, i64* @assembly_address
  %384 = load i64* %stack_var_-32
  store i64 %384, i64* %rax
  store volatile i64 57577, i64* @assembly_address
  %385 = load i64* %rax
  %386 = inttoptr i64 %385 to i64*
  store i64 0, i64* %386
  store volatile i64 57584, i64* @assembly_address
  %387 = load i64* %stack_var_-24
  store i64 %387, i64* %rax
  br label %block_e0f4

block_e0f4:                                       ; preds = %block_e0e5, %block_e029, %block_df9a
  store volatile i64 57588, i64* @assembly_address
  %388 = load i64* %stack_var_-8
  store i64 %388, i64* %rbp
  %389 = ptrtoint i64* %stack_var_0 to i64
  store i64 %389, i64* %rsp
  store volatile i64 57589, i64* @assembly_address
  %390 = load i64* %rax
  ret i64 %390
}

declare i64 @255(i64*, i64*, i8*)

define i64 @gzip_error(i8* %arg1) {
block_e0f6:
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_-16 = alloca i8*
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57590, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 57591, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 57594, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 57598, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to i8*
  store i8* %22, i8** %stack_var_-16
  store volatile i64 57602, i64* @assembly_address
  %23 = load i64* @global_var_25f4c8
  store i64 %23, i64* %rdx
  store volatile i64 57609, i64* @assembly_address
  %24 = load i64* @global_var_216580
  store i64 %24, i64* %rax
  store volatile i64 57616, i64* @assembly_address
  %25 = load i8** %stack_var_-16
  %26 = ptrtoint i8* %25 to i64
  store i64 %26, i64* %rcx
  store volatile i64 57620, i64* @assembly_address
  %27 = load i64* %rcx
  store i64 %27, i64* %r8
  store volatile i64 57623, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 57630, i64* @assembly_address
  store i64 ptrtoint ([13 x i8]* @global_var_12c43 to i64), i64* %rsi
  store volatile i64 57637, i64* @assembly_address
  %28 = load i64* %rax
  store i64 %28, i64* %rdi
  store volatile i64 57640, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57645, i64* @assembly_address
  %29 = load i64* %rdi
  %30 = inttoptr i64 %29 to %_IO_FILE*
  %31 = load i64* %rsi
  %32 = inttoptr i64 %31 to i8*
  %33 = load i64* %rdx
  %34 = inttoptr i64 %33 to i8*
  %35 = load i64* %rcx
  %36 = inttoptr i64 %35 to i8*
  %37 = load i64* %r8
  %38 = inttoptr i64 %37 to i8*
  %39 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %30, i8* %32, i8* %34, i8* %36, i8* %38)
  %40 = sext i32 %39 to i64
  store i64 %40, i64* %rax
  %41 = sext i32 %39 to i64
  store i64 %41, i64* %rax
  store volatile i64 57650, i64* @assembly_address
  %42 = call i64 @abort_gzip()
  store i64 %42, i64* %rax
  store i64 %42, i64* %rax
  store i64 %42, i64* %rax
  %43 = load i64* %rax
  ret i64 %43
}

define i64 @xalloc_die() {
block_e137:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57655, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 57656, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 57659, i64* @assembly_address
  %3 = load i64* @global_var_25f4c8
  store i64 %3, i64* %rdx
  store volatile i64 57666, i64* @assembly_address
  %4 = load i64* @global_var_216580
  store i64 %4, i64* %rax
  store volatile i64 57673, i64* @assembly_address
  store i64 ptrtoint ([23 x i8]* @global_var_12c50 to i64), i64* %rsi
  store volatile i64 57680, i64* @assembly_address
  %5 = load i64* %rax
  store i64 %5, i64* %rdi
  store volatile i64 57683, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57688, i64* @assembly_address
  %6 = load i64* %rdi
  %7 = inttoptr i64 %6 to %_IO_FILE*
  %8 = load i64* %rsi
  %9 = inttoptr i64 %8 to i8*
  %10 = load i64* %rdx
  %11 = inttoptr i64 %10 to i8*
  %12 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %7, i8* %9, i8* %11)
  %13 = sext i32 %12 to i64
  store i64 %13, i64* %rax
  %14 = sext i32 %12 to i64
  store i64 %14, i64* %rax
  store volatile i64 57693, i64* @assembly_address
  %15 = call i64 @abort_gzip()
  store i64 %15, i64* %rax
  store i64 %15, i64* %rax
  store i64 %15, i64* %rax
  %16 = load i64* %rax
  %17 = load i64* %rax
  ret i64 %17
}

define i64 @warning(i8* %arg1) {
block_e162:
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57698, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 57699, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 57702, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 57706, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to i8*
  store i8* %22, i8** %stack_var_-16
  store volatile i64 57710, i64* @assembly_address
  %23 = load i32* bitcast (i64* @global_var_2165e8 to i32*)
  %24 = zext i32 %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 57716, i64* @assembly_address
  %25 = load i64* %rax
  %26 = trunc i64 %25 to i32
  %27 = load i64* %rax
  %28 = trunc i64 %27 to i32
  %29 = and i32 %26, %28
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %30 = icmp eq i32 %29, 0
  store i1 %30, i1* %zf
  %31 = icmp slt i32 %29, 0
  store i1 %31, i1* %sf
  %32 = trunc i32 %29 to i8
  %33 = call i8 @llvm.ctpop.i8(i8 %32)
  %34 = and i8 %33, 1
  %35 = icmp eq i8 %34, 0
  store i1 %35, i1* %pf
  store volatile i64 57718, i64* @assembly_address
  %36 = load i1* %zf
  %37 = icmp eq i1 %36, false
  br i1 %37, label %block_e1a8, label %block_e178

block_e178:                                       ; preds = %block_e162
  store volatile i64 57720, i64* @assembly_address
  %38 = load i64* @global_var_25f4c8
  store i64 %38, i64* %rdx
  store volatile i64 57727, i64* @assembly_address
  %39 = load i64* @global_var_216580
  store i64 %39, i64* %rax
  store volatile i64 57734, i64* @assembly_address
  %40 = load i8** %stack_var_-16
  %41 = ptrtoint i8* %40 to i64
  store i64 %41, i64* %rcx
  store volatile i64 57738, i64* @assembly_address
  %42 = load i64* %rcx
  store i64 %42, i64* %r8
  store volatile i64 57741, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 57748, i64* @assembly_address
  store i64 ptrtoint ([21 x i8]* @global_var_12c67 to i64), i64* %rsi
  store volatile i64 57755, i64* @assembly_address
  %43 = load i64* %rax
  store i64 %43, i64* %rdi
  store volatile i64 57758, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57763, i64* @assembly_address
  %44 = load i64* %rdi
  %45 = inttoptr i64 %44 to %_IO_FILE*
  %46 = load i64* %rsi
  %47 = inttoptr i64 %46 to i8*
  %48 = load i64* %rdx
  %49 = inttoptr i64 %48 to i8*
  %50 = load i64* %rcx
  %51 = inttoptr i64 %50 to i8*
  %52 = load i64* %r8
  %53 = inttoptr i64 %52 to i8*
  %54 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %45, i8* %47, i8* %49, i8* %51, i8* %53)
  %55 = sext i32 %54 to i64
  store i64 %55, i64* %rax
  %56 = sext i32 %54 to i64
  store i64 %56, i64* %rax
  br label %block_e1a8

block_e1a8:                                       ; preds = %block_e178, %block_e162
  store volatile i64 57768, i64* @assembly_address
  %57 = load i32* bitcast (i64* @global_var_2165f0 to i32*)
  %58 = zext i32 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 57774, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  %61 = load i64* %rax
  %62 = trunc i64 %61 to i32
  %63 = and i32 %60, %62
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %64 = icmp eq i32 %63, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i32 %63, 0
  store i1 %65, i1* %sf
  %66 = trunc i32 %63 to i8
  %67 = call i8 @llvm.ctpop.i8(i8 %66)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  store volatile i64 57776, i64* @assembly_address
  %70 = load i1* %zf
  %71 = icmp eq i1 %70, false
  br i1 %71, label %block_e1bc, label %block_e1b2

block_e1b2:                                       ; preds = %block_e1a8
  store volatile i64 57778, i64* @assembly_address
  store i32 2, i32* bitcast (i64* @global_var_2165f0 to i32*)
  br label %block_e1bc

block_e1bc:                                       ; preds = %block_e1b2, %block_e1a8
  store volatile i64 57788, i64* @assembly_address
  store volatile i64 57789, i64* @assembly_address
  %72 = load i64* %stack_var_-8
  store i64 %72, i64* %rbp
  %73 = ptrtoint i64* %stack_var_0 to i64
  store i64 %73, i64* %rsp
  store volatile i64 57790, i64* @assembly_address
  %74 = load i64* %rax
  ret i64 %74
}

define i64 @read_error() {
block_e1bf:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57791, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 57792, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 57795, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 16
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 16
  %8 = xor i64 %3, 16
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %18, i64* %rsp
  store volatile i64 57799, i64* @assembly_address
  %19 = call i32* @__errno_location()
  %20 = ptrtoint i32* %19 to i64
  store i64 %20, i64* %rax
  %21 = ptrtoint i32* %19 to i64
  store i64 %21, i64* %rax
  %22 = ptrtoint i32* %19 to i64
  store i64 %22, i64* %rax
  store volatile i64 57804, i64* @assembly_address
  %23 = load i64* %rax
  %24 = inttoptr i64 %23 to i32*
  %25 = load i32* %24
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rax
  store volatile i64 57806, i64* @assembly_address
  %27 = load i64* %rax
  %28 = trunc i64 %27 to i32
  store i32 %28, i32* %stack_var_-12
  store volatile i64 57809, i64* @assembly_address
  %29 = load i64* @global_var_25f4c8
  store i64 %29, i64* %rdx
  store volatile i64 57816, i64* @assembly_address
  %30 = load i64* @global_var_216580
  store i64 %30, i64* %rax
  store volatile i64 57823, i64* @assembly_address
  store i64 ptrtoint ([6 x i8]* @global_var_12c7c to i64), i64* %rsi
  store volatile i64 57830, i64* @assembly_address
  %31 = load i64* %rax
  store i64 %31, i64* %rdi
  store volatile i64 57833, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57838, i64* @assembly_address
  %32 = load i64* %rdi
  %33 = inttoptr i64 %32 to %_IO_FILE*
  %34 = load i64* %rsi
  %35 = inttoptr i64 %34 to i8*
  %36 = load i64* %rdx
  %37 = inttoptr i64 %36 to i8*
  %38 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %33, i8* %35, i8* %37)
  %39 = sext i32 %38 to i64
  store i64 %39, i64* %rax
  %40 = sext i32 %38 to i64
  store i64 %40, i64* %rax
  store volatile i64 57843, i64* @assembly_address
  %41 = load i32* %stack_var_-12
  %42 = and i32 %41, 15
  %43 = icmp ugt i32 %42, 15
  %44 = icmp ult i32 %41, 0
  %45 = xor i32 %41, 0
  %46 = and i32 %45, 0
  %47 = icmp slt i32 %46, 0
  store i1 %43, i1* %az
  store i1 %44, i1* %cf
  store i1 %47, i1* %of
  %48 = icmp eq i32 %41, 0
  store i1 %48, i1* %zf
  %49 = icmp slt i32 %41, 0
  store i1 %49, i1* %sf
  %50 = trunc i32 %41 to i8
  %51 = call i8 @llvm.ctpop.i8(i8 %50)
  %52 = and i8 %51, 1
  %53 = icmp eq i8 %52, 0
  store i1 %53, i1* %pf
  store volatile i64 57847, i64* @assembly_address
  %54 = load i1* %zf
  br i1 %54, label %block_e214, label %block_e1f9

block_e1f9:                                       ; preds = %block_e1bf
  store volatile i64 57849, i64* @assembly_address
  %55 = call i32* @__errno_location()
  %56 = ptrtoint i32* %55 to i64
  store i64 %56, i64* %rax
  %57 = ptrtoint i32* %55 to i64
  store i64 %57, i64* %rax
  %58 = ptrtoint i32* %55 to i64
  store i64 %58, i64* %rax
  store volatile i64 57854, i64* @assembly_address
  %59 = load i64* %rax
  store i64 %59, i64* %rdx
  store volatile i64 57857, i64* @assembly_address
  %60 = load i32* %stack_var_-12
  %61 = zext i32 %60 to i64
  store i64 %61, i64* %rax
  store volatile i64 57860, i64* @assembly_address
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i32
  %64 = load i64* %rdx
  %65 = inttoptr i64 %64 to i32*
  store i32 %63, i32* %65
  store volatile i64 57862, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 57869, i64* @assembly_address
  %66 = load i64* %rdi
  %67 = inttoptr i64 %66 to i8*
  call void @perror(i8* %67)
  store volatile i64 57874, i64* @assembly_address
  br label %block_e236

block_e214:                                       ; preds = %block_e1bf
  store volatile i64 57876, i64* @assembly_address
  %68 = load i64* @global_var_216580
  store i64 %68, i64* %rax
  store volatile i64 57883, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdx
  store volatile i64 57890, i64* @assembly_address
  store i64 ptrtoint ([28 x i8]* @global_var_12c82 to i64), i64* %rsi
  store volatile i64 57897, i64* @assembly_address
  %69 = load i64* %rax
  store i64 %69, i64* %rdi
  store volatile i64 57900, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57905, i64* @assembly_address
  %70 = load i64* %rdi
  %71 = inttoptr i64 %70 to %_IO_FILE*
  %72 = load i64* %rsi
  %73 = inttoptr i64 %72 to i8*
  %74 = load i64* %rdx
  %75 = inttoptr i64 %74 to i8*
  %76 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %71, i8* %73, i8* %75)
  %77 = sext i32 %76 to i64
  store i64 %77, i64* %rax
  %78 = sext i32 %76 to i64
  store i64 %78, i64* %rax
  br label %block_e236

block_e236:                                       ; preds = %block_e214, %block_e1f9
  store volatile i64 57910, i64* @assembly_address
  %79 = call i64 @abort_gzip()
  store i64 %79, i64* %rax
  store i64 %79, i64* %rax
  store i64 %79, i64* %rax
  %80 = load i64* %rax
  %81 = load i64* %rax
  ret i64 %81
}

define i64 @write_error() {
block_e23b:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57915, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 57916, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 57919, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 16
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 16
  %8 = xor i64 %3, 16
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %18, i64* %rsp
  store volatile i64 57923, i64* @assembly_address
  %19 = call i32* @__errno_location()
  %20 = ptrtoint i32* %19 to i64
  store i64 %20, i64* %rax
  %21 = ptrtoint i32* %19 to i64
  store i64 %21, i64* %rax
  %22 = ptrtoint i32* %19 to i64
  store i64 %22, i64* %rax
  store volatile i64 57928, i64* @assembly_address
  %23 = load i64* %rax
  %24 = inttoptr i64 %23 to i32*
  %25 = load i32* %24
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rax
  store volatile i64 57930, i64* @assembly_address
  %27 = load i64* %rax
  %28 = trunc i64 %27 to i32
  store i32 %28, i32* %stack_var_-12
  store volatile i64 57933, i64* @assembly_address
  %29 = load i64* @global_var_25f4c8
  store i64 %29, i64* %rdx
  store volatile i64 57940, i64* @assembly_address
  %30 = load i64* @global_var_216580
  store i64 %30, i64* %rax
  store volatile i64 57947, i64* @assembly_address
  store i64 ptrtoint ([6 x i8]* @global_var_12c7c to i64), i64* %rsi
  store volatile i64 57954, i64* @assembly_address
  %31 = load i64* %rax
  store i64 %31, i64* %rdi
  store volatile i64 57957, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 57962, i64* @assembly_address
  %32 = load i64* %rdi
  %33 = inttoptr i64 %32 to %_IO_FILE*
  %34 = load i64* %rsi
  %35 = inttoptr i64 %34 to i8*
  %36 = load i64* %rdx
  %37 = inttoptr i64 %36 to i8*
  %38 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %33, i8* %35, i8* %37)
  %39 = sext i32 %38 to i64
  store i64 %39, i64* %rax
  %40 = sext i32 %38 to i64
  store i64 %40, i64* %rax
  store volatile i64 57967, i64* @assembly_address
  %41 = call i32* @__errno_location()
  %42 = ptrtoint i32* %41 to i64
  store i64 %42, i64* %rax
  %43 = ptrtoint i32* %41 to i64
  store i64 %43, i64* %rax
  %44 = ptrtoint i32* %41 to i64
  store i64 %44, i64* %rax
  store volatile i64 57972, i64* @assembly_address
  %45 = load i64* %rax
  store i64 %45, i64* %rdx
  store volatile i64 57975, i64* @assembly_address
  %46 = load i32* %stack_var_-12
  %47 = zext i32 %46 to i64
  store i64 %47, i64* %rax
  store volatile i64 57978, i64* @assembly_address
  %48 = load i64* %rax
  %49 = trunc i64 %48 to i32
  %50 = load i64* %rdx
  %51 = inttoptr i64 %50 to i32*
  store i32 %49, i32* %51
  store volatile i64 57980, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24f0c0 to i64), i64* %rdi
  store volatile i64 57987, i64* @assembly_address
  %52 = load i64* %rdi
  %53 = inttoptr i64 %52 to i8*
  call void @perror(i8* %53)
  store volatile i64 57992, i64* @assembly_address
  %54 = call i64 @abort_gzip()
  store i64 %54, i64* %rax
  store i64 %54, i64* %rax
  store i64 %54, i64* %rax
  %55 = load i64* %rax
  %56 = load i64* %rax
  ret i64 %56
}

define i64 @display_ratio(i64 %arg1, i64 %arg2, %_IO_FILE* %arg3) {
block_e28d:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %xmm1 = alloca i128
  %xmm0 = alloca i128
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint %_IO_FILE* %arg3 to i64
  store i64 %0, i64* %rdx
  store i64 %arg2, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca %_IO_FILE*
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 57997, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 57998, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 58001, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 58005, i64* @assembly_address
  %21 = load i64* %rdi
  store i64 %21, i64* %stack_var_-16
  store volatile i64 58009, i64* @assembly_address
  %22 = load i64* %rsi
  store i64 %22, i64* %stack_var_-24
  store volatile i64 58013, i64* @assembly_address
  %23 = load i64* %rdx
  %24 = inttoptr i64 %23 to %_IO_FILE*
  store %_IO_FILE* %24, %_IO_FILE** %stack_var_-32
  store volatile i64 58017, i64* @assembly_address
  %25 = load i64* %stack_var_-24
  %26 = and i64 %25, 15
  %27 = icmp ugt i64 %26, 15
  %28 = icmp ult i64 %25, 0
  %29 = xor i64 %25, 0
  %30 = and i64 %29, 0
  %31 = icmp slt i64 %30, 0
  store i1 %27, i1* %az
  store i1 %28, i1* %cf
  store i1 %31, i1* %of
  %32 = icmp eq i64 %25, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i64 %25, 0
  store i1 %33, i1* %sf
  %34 = trunc i64 %25 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  store volatile i64 58022, i64* @assembly_address
  %38 = load i1* %zf
  br i1 %38, label %block_e2cb, label %block_e2a8

block_e2a8:                                       ; preds = %block_e28d
  store volatile i64 58024, i64* @assembly_address
  %39 = load i64* %stack_var_-16
  %40 = call i128 @__asm_cvtsi2sd(i64 %39)
  store i128 %40, i128* %xmm0
  store volatile i64 58030, i64* @assembly_address
  %41 = load i64* @global_var_12ca8
  %42 = call i128 @__asm_movsd(i64 %41)
  store i128 %42, i128* %xmm1
  store volatile i64 58038, i64* @assembly_address
  %43 = load i128* %xmm0
  %44 = load i128* %xmm1
  %45 = call i128 @__asm_mulsd(i128 %43, i128 %44)
  store i128 %45, i128* %xmm0
  store volatile i64 58042, i64* @assembly_address
  %46 = load i64* %stack_var_-24
  %47 = call i128 @__asm_cvtsi2sd(i64 %46)
  store i128 %47, i128* %xmm1
  store volatile i64 58048, i64* @assembly_address
  %48 = load i128* %xmm0
  %49 = load i128* %xmm1
  %50 = call i128 @__asm_divsd(i128 %48, i128 %49)
  store i128 %50, i128* %xmm0
  store volatile i64 58052, i64* @assembly_address
  %51 = load i128* %xmm0
  %52 = call i64 @__asm_movq(i128 %51)
  store i64 %52, i64* %rax
  store volatile i64 58057, i64* @assembly_address
  br label %block_e2d2

block_e2cb:                                       ; preds = %block_e28d
  store volatile i64 58059, i64* @assembly_address
  %53 = load i64* @global_var_12cb0
  store i64 %53, i64* %rax
  br label %block_e2d2

block_e2d2:                                       ; preds = %block_e2cb, %block_e2a8
  store volatile i64 58066, i64* @assembly_address
  %54 = load %_IO_FILE** %stack_var_-32
  %55 = ptrtoint %_IO_FILE* %54 to i64
  store i64 %55, i64* %rdx
  store volatile i64 58070, i64* @assembly_address
  %56 = load i64* %rax
  store i64 %56, i64* %stack_var_-40
  store volatile i64 58074, i64* @assembly_address
  %57 = load i64* %stack_var_-40
  %58 = call i128 @__asm_movsd(i64 %57)
  store i128 %58, i128* %xmm0
  store volatile i64 58079, i64* @assembly_address
  store i64 ptrtoint ([8 x i8]* @global_var_12c9e to i64), i64* %rsi
  store volatile i64 58086, i64* @assembly_address
  %59 = load i64* %rdx
  store i64 %59, i64* %rdi
  store volatile i64 58089, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 58094, i64* @assembly_address
  %60 = load i64* %rdi
  %61 = inttoptr i64 %60 to %_IO_FILE*
  %62 = load i64* %rsi
  %63 = inttoptr i64 %62 to i8*
  %64 = load i128* %xmm0
  %65 = trunc i128 %64 to i64
  %66 = bitcast i64 %65 to double
  %67 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %61, i8* %63, double %66)
  %68 = sext i32 %67 to i64
  store i64 %68, i64* %rax
  %69 = sext i32 %67 to i64
  store i64 %69, i64* %rax
  store volatile i64 58099, i64* @assembly_address
  store volatile i64 58100, i64* @assembly_address
  %70 = load i64* %stack_var_-8
  store i64 %70, i64* %rbp
  %71 = ptrtoint i64* %stack_var_0 to i64
  store i64 %71, i64* %rsp
  store volatile i64 58101, i64* @assembly_address
  %72 = load i64* %rax
  ret i64 %72
}

declare i64 @256(i64, i64, i64)

define i64 @fprint_off(%_IO_FILE* %arg1, i64 %arg2, i32 %arg3) {
block_e2f6:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg3 to i64
  store i64 %0, i64* %rdx
  store i64 %arg2, i64* %rsi
  %1 = ptrtoint %_IO_FILE* %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-96 = alloca i8*
  %2 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-88 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-124 = alloca i32
  %stack_var_-120 = alloca i128
  %3 = alloca i64
  %stack_var_-112 = alloca %_IO_FILE*
  %4 = alloca i64
  %stack_var_-136 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 58102, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 58103, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 58106, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = add i64 %8, -128
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %9, %8
  %13 = xor i64 %8, %9
  %14 = xor i64 -128, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-136 to i64
  store i64 %23, i64* %rsp
  store volatile i64 58110, i64* @assembly_address
  %24 = load i64* %rdi
  %25 = inttoptr i64 %24 to %_IO_FILE*
  store %_IO_FILE* %25, %_IO_FILE** %stack_var_-112
  store volatile i64 58114, i64* @assembly_address
  %26 = load i64* %rsi
  %27 = sext i64 %26 to i128
  store i128 %27, i128* %stack_var_-120
  store volatile i64 58118, i64* @assembly_address
  %28 = load i64* %rdx
  %29 = trunc i64 %28 to i32
  store i32 %29, i32* %stack_var_-124
  store volatile i64 58121, i64* @assembly_address
  %30 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %30, i64* %rax
  store volatile i64 58130, i64* @assembly_address
  %31 = load i64* %rax
  store i64 %31, i64* %stack_var_-16
  store volatile i64 58134, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %32 = icmp eq i32 0, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i32 0, 0
  store i1 %33, i1* %sf
  %34 = trunc i32 0 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  %38 = zext i32 0 to i64
  store i64 %38, i64* %rax
  store volatile i64 58136, i64* @assembly_address
  %39 = ptrtoint i64* %stack_var_-88 to i64
  store i64 %39, i64* %rax
  store volatile i64 58140, i64* @assembly_address
  %40 = load i64* %rax
  %41 = add i64 %40, 64
  %42 = and i64 %40, 15
  %43 = icmp ugt i64 %42, 15
  %44 = icmp ult i64 %41, %40
  %45 = xor i64 %40, %41
  %46 = xor i64 64, %41
  %47 = and i64 %45, %46
  %48 = icmp slt i64 %47, 0
  store i1 %43, i1* %az
  store i1 %44, i1* %cf
  store i1 %48, i1* %of
  %49 = icmp eq i64 %41, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i64 %41, 0
  store i1 %50, i1* %sf
  %51 = trunc i64 %41 to i8
  %52 = call i8 @llvm.ctpop.i8(i8 %51)
  %53 = and i8 %52, 1
  %54 = icmp eq i8 %53, 0
  store i1 %54, i1* %pf
  %55 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %55, i64* %rax
  store volatile i64 58144, i64* @assembly_address
  %56 = bitcast i64* %stack_var_-24 to i8*
  store i8* %56, i8** %stack_var_-96
  store volatile i64 58148, i64* @assembly_address
  %57 = load i128* %stack_var_-120
  %58 = trunc i128 %57 to i64
  %59 = and i64 %58, 15
  %60 = icmp ugt i64 %59, 15
  %61 = icmp ult i64 %58, 0
  %62 = xor i64 %58, 0
  %63 = and i64 %62, 0
  %64 = icmp slt i64 %63, 0
  store i1 %60, i1* %az
  store i1 %61, i1* %cf
  store i1 %64, i1* %of
  %65 = icmp eq i64 %58, 0
  store i1 %65, i1* %zf
  %66 = icmp slt i64 %58, 0
  store i1 %66, i1* %sf
  %67 = trunc i64 %58 to i8
  %68 = call i8 @llvm.ctpop.i8(i8 %67)
  %69 = and i8 %68, 1
  %70 = icmp eq i8 %69, 0
  store i1 %70, i1* %pf
  store volatile i64 58153, i64* @assembly_address
  %71 = load i1* %sf
  %72 = icmp eq i1 %71, false
  br i1 %72, label %block_e3ba, label %block_e32f

block_e32f:                                       ; preds = %block_e32f, %block_e2f6
  store volatile i64 58159, i64* @assembly_address
  %73 = load i128* %stack_var_-120
  %74 = trunc i128 %73 to i64
  store i64 %74, i64* %rcx
  store volatile i64 58163, i64* @assembly_address
  store i64 7378697629483820647, i64* %rdx
  store volatile i64 58173, i64* @assembly_address
  %75 = load i64* %rcx
  store i64 %75, i64* %rax
  store volatile i64 58176, i64* @assembly_address
  %76 = load i64* %rdx
  %77 = load i64* %rax
  %78 = sext i64 %76 to i128
  %79 = sext i64 %77 to i128
  %80 = mul i128 %78, %79
  %81 = trunc i128 %80 to i64
  %82 = lshr i128 %80, 64
  %83 = trunc i128 %82 to i64
  %84 = icmp ne i64 %83, 0
  store i64 %81, i64* %rax
  store i64 %83, i64* %rdx
  %85 = icmp ne i64 %83, -1
  %86 = icmp eq i1 %84, %85
  store i1 %86, i1* %of
  store i1 %86, i1* %cf
  store volatile i64 58179, i64* @assembly_address
  %87 = load i64* %rdx
  %88 = load i1* %of
  %89 = ashr i64 %87, 2
  %90 = icmp eq i64 %89, 0
  store i1 %90, i1* %zf
  %91 = icmp slt i64 %89, 0
  store i1 %91, i1* %sf
  %92 = trunc i64 %89 to i8
  %93 = call i8 @llvm.ctpop.i8(i8 %92)
  %94 = and i8 %93, 1
  %95 = icmp eq i8 %94, 0
  store i1 %95, i1* %pf
  store i64 %89, i64* %rdx
  %96 = and i64 2, %87
  %97 = icmp ne i64 %96, 0
  store i1 %97, i1* %cf
  %98 = select i1 false, i1 false, i1 %88
  store i1 %98, i1* %of
  store volatile i64 58183, i64* @assembly_address
  %99 = load i64* %rcx
  store i64 %99, i64* %rax
  store volatile i64 58186, i64* @assembly_address
  %100 = load i64* %rax
  %101 = load i1* %of
  %102 = ashr i64 %100, 63
  %103 = icmp eq i64 %102, 0
  store i1 %103, i1* %zf
  %104 = icmp slt i64 %102, 0
  store i1 %104, i1* %sf
  %105 = trunc i64 %102 to i8
  %106 = call i8 @llvm.ctpop.i8(i8 %105)
  %107 = and i8 %106, 1
  %108 = icmp eq i8 %107, 0
  store i1 %108, i1* %pf
  store i64 %102, i64* %rax
  %109 = and i64 4611686018427387904, %100
  %110 = icmp ne i64 %109, 0
  store i1 %110, i1* %cf
  %111 = select i1 false, i1 false, i1 %101
  store i1 %111, i1* %of
  store volatile i64 58190, i64* @assembly_address
  %112 = load i64* %rdx
  %113 = load i64* %rax
  %114 = sub i64 %112, %113
  %115 = and i64 %112, 15
  %116 = and i64 %113, 15
  %117 = sub i64 %115, %116
  %118 = icmp ugt i64 %117, 15
  %119 = icmp ult i64 %112, %113
  %120 = xor i64 %112, %113
  %121 = xor i64 %112, %114
  %122 = and i64 %120, %121
  %123 = icmp slt i64 %122, 0
  store i1 %118, i1* %az
  store i1 %119, i1* %cf
  store i1 %123, i1* %of
  %124 = icmp eq i64 %114, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i64 %114, 0
  store i1 %125, i1* %sf
  %126 = trunc i64 %114 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  store i64 %114, i64* %rdx
  store volatile i64 58193, i64* @assembly_address
  %130 = load i64* %rdx
  store i64 %130, i64* %rax
  store volatile i64 58196, i64* @assembly_address
  %131 = load i64* %rax
  %132 = load i1* %of
  %133 = shl i64 %131, 2
  %134 = icmp eq i64 %133, 0
  store i1 %134, i1* %zf
  %135 = icmp slt i64 %133, 0
  store i1 %135, i1* %sf
  %136 = trunc i64 %133 to i8
  %137 = call i8 @llvm.ctpop.i8(i8 %136)
  %138 = and i8 %137, 1
  %139 = icmp eq i8 %138, 0
  store i1 %139, i1* %pf
  store i64 %133, i64* %rax
  %140 = shl i64 %131, 1
  %141 = lshr i64 %140, 63
  %142 = trunc i64 %141 to i1
  store i1 %142, i1* %cf
  %143 = lshr i64 %133, 63
  %144 = icmp ne i64 %143, %141
  %145 = select i1 false, i1 %144, i1 %132
  store i1 %145, i1* %of
  store volatile i64 58200, i64* @assembly_address
  %146 = load i64* %rax
  %147 = load i64* %rdx
  %148 = add i64 %146, %147
  %149 = and i64 %146, 15
  %150 = and i64 %147, 15
  %151 = add i64 %149, %150
  %152 = icmp ugt i64 %151, 15
  %153 = icmp ult i64 %148, %146
  %154 = xor i64 %146, %148
  %155 = xor i64 %147, %148
  %156 = and i64 %154, %155
  %157 = icmp slt i64 %156, 0
  store i1 %152, i1* %az
  store i1 %153, i1* %cf
  store i1 %157, i1* %of
  %158 = icmp eq i64 %148, 0
  store i1 %158, i1* %zf
  %159 = icmp slt i64 %148, 0
  store i1 %159, i1* %sf
  %160 = trunc i64 %148 to i8
  %161 = call i8 @llvm.ctpop.i8(i8 %160)
  %162 = and i8 %161, 1
  %163 = icmp eq i8 %162, 0
  store i1 %163, i1* %pf
  store i64 %148, i64* %rax
  store volatile i64 58203, i64* @assembly_address
  %164 = load i64* %rax
  %165 = load i64* %rax
  %166 = add i64 %164, %165
  %167 = and i64 %164, 15
  %168 = and i64 %165, 15
  %169 = add i64 %167, %168
  %170 = icmp ugt i64 %169, 15
  %171 = icmp ult i64 %166, %164
  %172 = xor i64 %164, %166
  %173 = xor i64 %165, %166
  %174 = and i64 %172, %173
  %175 = icmp slt i64 %174, 0
  store i1 %170, i1* %az
  store i1 %171, i1* %cf
  store i1 %175, i1* %of
  %176 = icmp eq i64 %166, 0
  store i1 %176, i1* %zf
  %177 = icmp slt i64 %166, 0
  store i1 %177, i1* %sf
  %178 = trunc i64 %166 to i8
  %179 = call i8 @llvm.ctpop.i8(i8 %178)
  %180 = and i8 %179, 1
  %181 = icmp eq i8 %180, 0
  store i1 %181, i1* %pf
  store i64 %166, i64* %rax
  store volatile i64 58206, i64* @assembly_address
  %182 = load i64* %rcx
  %183 = load i64* %rax
  %184 = sub i64 %182, %183
  %185 = and i64 %182, 15
  %186 = and i64 %183, 15
  %187 = sub i64 %185, %186
  %188 = icmp ugt i64 %187, 15
  %189 = icmp ult i64 %182, %183
  %190 = xor i64 %182, %183
  %191 = xor i64 %182, %184
  %192 = and i64 %190, %191
  %193 = icmp slt i64 %192, 0
  store i1 %188, i1* %az
  store i1 %189, i1* %cf
  store i1 %193, i1* %of
  %194 = icmp eq i64 %184, 0
  store i1 %194, i1* %zf
  %195 = icmp slt i64 %184, 0
  store i1 %195, i1* %sf
  %196 = trunc i64 %184 to i8
  %197 = call i8 @llvm.ctpop.i8(i8 %196)
  %198 = and i8 %197, 1
  %199 = icmp eq i8 %198, 0
  store i1 %199, i1* %pf
  store i64 %184, i64* %rcx
  store volatile i64 58209, i64* @assembly_address
  %200 = load i64* %rcx
  store i64 %200, i64* %rdx
  store volatile i64 58212, i64* @assembly_address
  %201 = load i64* %rdx
  %202 = trunc i64 %201 to i32
  %203 = zext i32 %202 to i64
  store i64 %203, i64* %rax
  store volatile i64 58214, i64* @assembly_address
  store i64 48, i64* %rdx
  store volatile i64 58219, i64* @assembly_address
  %204 = load i64* %rdx
  %205 = trunc i64 %204 to i32
  %206 = load i64* %rax
  %207 = trunc i64 %206 to i32
  %208 = sub i32 %205, %207
  %209 = and i32 %205, 15
  %210 = and i32 %207, 15
  %211 = sub i32 %209, %210
  %212 = icmp ugt i32 %211, 15
  %213 = icmp ult i32 %205, %207
  %214 = xor i32 %205, %207
  %215 = xor i32 %205, %208
  %216 = and i32 %214, %215
  %217 = icmp slt i32 %216, 0
  store i1 %212, i1* %az
  store i1 %213, i1* %cf
  store i1 %217, i1* %of
  %218 = icmp eq i32 %208, 0
  store i1 %218, i1* %zf
  %219 = icmp slt i32 %208, 0
  store i1 %219, i1* %sf
  %220 = trunc i32 %208 to i8
  %221 = call i8 @llvm.ctpop.i8(i8 %220)
  %222 = and i8 %221, 1
  %223 = icmp eq i8 %222, 0
  store i1 %223, i1* %pf
  %224 = zext i32 %208 to i64
  store i64 %224, i64* %rdx
  store volatile i64 58221, i64* @assembly_address
  %225 = load i64* %rdx
  %226 = trunc i64 %225 to i32
  %227 = zext i32 %226 to i64
  store i64 %227, i64* %rax
  store volatile i64 58223, i64* @assembly_address
  %228 = load i8** %stack_var_-96
  %229 = ptrtoint i8* %228 to i64
  %230 = sub i64 %229, 1
  %231 = and i64 %229, 15
  %232 = sub i64 %231, 1
  %233 = icmp ugt i64 %232, 15
  %234 = icmp ult i64 %229, 1
  %235 = xor i64 %229, 1
  %236 = xor i64 %229, %230
  %237 = and i64 %235, %236
  %238 = icmp slt i64 %237, 0
  store i1 %233, i1* %az
  store i1 %234, i1* %cf
  store i1 %238, i1* %of
  %239 = icmp eq i64 %230, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i64 %230, 0
  store i1 %240, i1* %sf
  %241 = trunc i64 %230 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  %245 = inttoptr i64 %230 to i8*
  store i8* %245, i8** %stack_var_-96
  store volatile i64 58228, i64* @assembly_address
  %246 = load i64* %rax
  %247 = trunc i64 %246 to i32
  %248 = zext i32 %247 to i64
  store i64 %248, i64* %rdx
  store volatile i64 58230, i64* @assembly_address
  %249 = load i8** %stack_var_-96
  %250 = ptrtoint i8* %249 to i64
  store i64 %250, i64* %rax
  store volatile i64 58234, i64* @assembly_address
  %251 = load i64* %rdx
  %252 = trunc i64 %251 to i8
  %253 = load i64* %rax
  %254 = inttoptr i64 %253 to i8*
  store i8 %252, i8* %254
  store volatile i64 58236, i64* @assembly_address
  %255 = load i128* %stack_var_-120
  %256 = trunc i128 %255 to i64
  store i64 %256, i64* %rcx
  store volatile i64 58240, i64* @assembly_address
  store i64 7378697629483820647, i64* %rdx
  store volatile i64 58250, i64* @assembly_address
  %257 = load i64* %rcx
  store i64 %257, i64* %rax
  store volatile i64 58253, i64* @assembly_address
  %258 = load i64* %rdx
  %259 = load i64* %rax
  %260 = sext i64 %258 to i128
  %261 = sext i64 %259 to i128
  %262 = mul i128 %260, %261
  %263 = trunc i128 %262 to i64
  %264 = lshr i128 %262, 64
  %265 = trunc i128 %264 to i64
  %266 = icmp ne i64 %265, 0
  store i64 %263, i64* %rax
  store i64 %265, i64* %rdx
  %267 = icmp ne i64 %265, -1
  %268 = icmp eq i1 %266, %267
  store i1 %268, i1* %of
  store i1 %268, i1* %cf
  store volatile i64 58256, i64* @assembly_address
  %269 = load i64* %rdx
  %270 = load i1* %of
  %271 = ashr i64 %269, 2
  %272 = icmp eq i64 %271, 0
  store i1 %272, i1* %zf
  %273 = icmp slt i64 %271, 0
  store i1 %273, i1* %sf
  %274 = trunc i64 %271 to i8
  %275 = call i8 @llvm.ctpop.i8(i8 %274)
  %276 = and i8 %275, 1
  %277 = icmp eq i8 %276, 0
  store i1 %277, i1* %pf
  store i64 %271, i64* %rdx
  %278 = and i64 2, %269
  %279 = icmp ne i64 %278, 0
  store i1 %279, i1* %cf
  %280 = select i1 false, i1 false, i1 %270
  store i1 %280, i1* %of
  store volatile i64 58260, i64* @assembly_address
  %281 = load i64* %rcx
  store i64 %281, i64* %rax
  store volatile i64 58263, i64* @assembly_address
  %282 = load i64* %rax
  %283 = load i1* %of
  %284 = ashr i64 %282, 63
  %285 = icmp eq i64 %284, 0
  store i1 %285, i1* %zf
  %286 = icmp slt i64 %284, 0
  store i1 %286, i1* %sf
  %287 = trunc i64 %284 to i8
  %288 = call i8 @llvm.ctpop.i8(i8 %287)
  %289 = and i8 %288, 1
  %290 = icmp eq i8 %289, 0
  store i1 %290, i1* %pf
  store i64 %284, i64* %rax
  %291 = and i64 4611686018427387904, %282
  %292 = icmp ne i64 %291, 0
  store i1 %292, i1* %cf
  %293 = select i1 false, i1 false, i1 %283
  store i1 %293, i1* %of
  store volatile i64 58267, i64* @assembly_address
  %294 = load i64* %rdx
  %295 = load i64* %rax
  %296 = sub i64 %294, %295
  %297 = and i64 %294, 15
  %298 = and i64 %295, 15
  %299 = sub i64 %297, %298
  %300 = icmp ugt i64 %299, 15
  %301 = icmp ult i64 %294, %295
  %302 = xor i64 %294, %295
  %303 = xor i64 %294, %296
  %304 = and i64 %302, %303
  %305 = icmp slt i64 %304, 0
  store i1 %300, i1* %az
  store i1 %301, i1* %cf
  store i1 %305, i1* %of
  %306 = icmp eq i64 %296, 0
  store i1 %306, i1* %zf
  %307 = icmp slt i64 %296, 0
  store i1 %307, i1* %sf
  %308 = trunc i64 %296 to i8
  %309 = call i8 @llvm.ctpop.i8(i8 %308)
  %310 = and i8 %309, 1
  %311 = icmp eq i8 %310, 0
  store i1 %311, i1* %pf
  store i64 %296, i64* %rdx
  store volatile i64 58270, i64* @assembly_address
  %312 = load i64* %rdx
  store i64 %312, i64* %rax
  store volatile i64 58273, i64* @assembly_address
  %313 = load i64* %rax
  %314 = sext i64 %313 to i128
  store i128 %314, i128* %stack_var_-120
  store volatile i64 58277, i64* @assembly_address
  %315 = load i128* %stack_var_-120
  %316 = trunc i128 %315 to i64
  %317 = and i64 %316, 15
  %318 = icmp ugt i64 %317, 15
  %319 = icmp ult i64 %316, 0
  %320 = xor i64 %316, 0
  %321 = and i64 %320, 0
  %322 = icmp slt i64 %321, 0
  store i1 %318, i1* %az
  store i1 %319, i1* %cf
  store i1 %322, i1* %of
  %323 = icmp eq i64 %316, 0
  store i1 %323, i1* %zf
  %324 = icmp slt i64 %316, 0
  store i1 %324, i1* %sf
  %325 = trunc i64 %316 to i8
  %326 = call i8 @llvm.ctpop.i8(i8 %325)
  %327 = and i8 %326, 1
  %328 = icmp eq i8 %327, 0
  store i1 %328, i1* %pf
  store volatile i64 58282, i64* @assembly_address
  %329 = load i1* %zf
  %330 = icmp eq i1 %329, false
  br i1 %330, label %block_e32f, label %block_e3ac

block_e3ac:                                       ; preds = %block_e32f
  store volatile i64 58284, i64* @assembly_address
  %331 = load i8** %stack_var_-96
  %332 = ptrtoint i8* %331 to i64
  %333 = sub i64 %332, 1
  %334 = and i64 %332, 15
  %335 = sub i64 %334, 1
  %336 = icmp ugt i64 %335, 15
  %337 = icmp ult i64 %332, 1
  %338 = xor i64 %332, 1
  %339 = xor i64 %332, %333
  %340 = and i64 %338, %339
  %341 = icmp slt i64 %340, 0
  store i1 %336, i1* %az
  store i1 %337, i1* %cf
  store i1 %341, i1* %of
  %342 = icmp eq i64 %333, 0
  store i1 %342, i1* %zf
  %343 = icmp slt i64 %333, 0
  store i1 %343, i1* %sf
  %344 = trunc i64 %333 to i8
  %345 = call i8 @llvm.ctpop.i8(i8 %344)
  %346 = and i8 %345, 1
  %347 = icmp eq i8 %346, 0
  store i1 %347, i1* %pf
  %348 = inttoptr i64 %333 to i8*
  store i8* %348, i8** %stack_var_-96
  store volatile i64 58289, i64* @assembly_address
  %349 = load i8** %stack_var_-96
  %350 = ptrtoint i8* %349 to i64
  store i64 %350, i64* %rax
  store volatile i64 58293, i64* @assembly_address
  %351 = load i64* %rax
  %352 = inttoptr i64 %351 to i8*
  store i8 45, i8* %352
  store volatile i64 58296, i64* @assembly_address
  br label %block_e431

block_e3ba:                                       ; preds = %block_e3ba, %block_e2f6
  store volatile i64 58298, i64* @assembly_address
  %353 = load i128* %stack_var_-120
  %354 = trunc i128 %353 to i64
  store i64 %354, i64* %rcx
  store volatile i64 58302, i64* @assembly_address
  store i64 7378697629483820647, i64* %rdx
  store volatile i64 58312, i64* @assembly_address
  %355 = load i64* %rcx
  store i64 %355, i64* %rax
  store volatile i64 58315, i64* @assembly_address
  %356 = load i64* %rdx
  %357 = load i64* %rax
  %358 = sext i64 %356 to i128
  %359 = sext i64 %357 to i128
  %360 = mul i128 %358, %359
  %361 = trunc i128 %360 to i64
  %362 = lshr i128 %360, 64
  %363 = trunc i128 %362 to i64
  %364 = icmp ne i64 %363, 0
  store i64 %361, i64* %rax
  store i64 %363, i64* %rdx
  %365 = icmp ne i64 %363, -1
  %366 = icmp eq i1 %364, %365
  store i1 %366, i1* %of
  store i1 %366, i1* %cf
  store volatile i64 58318, i64* @assembly_address
  %367 = load i64* %rdx
  %368 = load i1* %of
  %369 = ashr i64 %367, 2
  %370 = icmp eq i64 %369, 0
  store i1 %370, i1* %zf
  %371 = icmp slt i64 %369, 0
  store i1 %371, i1* %sf
  %372 = trunc i64 %369 to i8
  %373 = call i8 @llvm.ctpop.i8(i8 %372)
  %374 = and i8 %373, 1
  %375 = icmp eq i8 %374, 0
  store i1 %375, i1* %pf
  store i64 %369, i64* %rdx
  %376 = and i64 2, %367
  %377 = icmp ne i64 %376, 0
  store i1 %377, i1* %cf
  %378 = select i1 false, i1 false, i1 %368
  store i1 %378, i1* %of
  store volatile i64 58322, i64* @assembly_address
  %379 = load i64* %rcx
  store i64 %379, i64* %rax
  store volatile i64 58325, i64* @assembly_address
  %380 = load i64* %rax
  %381 = load i1* %of
  %382 = ashr i64 %380, 63
  %383 = icmp eq i64 %382, 0
  store i1 %383, i1* %zf
  %384 = icmp slt i64 %382, 0
  store i1 %384, i1* %sf
  %385 = trunc i64 %382 to i8
  %386 = call i8 @llvm.ctpop.i8(i8 %385)
  %387 = and i8 %386, 1
  %388 = icmp eq i8 %387, 0
  store i1 %388, i1* %pf
  store i64 %382, i64* %rax
  %389 = and i64 4611686018427387904, %380
  %390 = icmp ne i64 %389, 0
  store i1 %390, i1* %cf
  %391 = select i1 false, i1 false, i1 %381
  store i1 %391, i1* %of
  store volatile i64 58329, i64* @assembly_address
  %392 = load i64* %rdx
  %393 = load i64* %rax
  %394 = sub i64 %392, %393
  %395 = and i64 %392, 15
  %396 = and i64 %393, 15
  %397 = sub i64 %395, %396
  %398 = icmp ugt i64 %397, 15
  %399 = icmp ult i64 %392, %393
  %400 = xor i64 %392, %393
  %401 = xor i64 %392, %394
  %402 = and i64 %400, %401
  %403 = icmp slt i64 %402, 0
  store i1 %398, i1* %az
  store i1 %399, i1* %cf
  store i1 %403, i1* %of
  %404 = icmp eq i64 %394, 0
  store i1 %404, i1* %zf
  %405 = icmp slt i64 %394, 0
  store i1 %405, i1* %sf
  %406 = trunc i64 %394 to i8
  %407 = call i8 @llvm.ctpop.i8(i8 %406)
  %408 = and i8 %407, 1
  %409 = icmp eq i8 %408, 0
  store i1 %409, i1* %pf
  store i64 %394, i64* %rdx
  store volatile i64 58332, i64* @assembly_address
  %410 = load i64* %rdx
  store i64 %410, i64* %rax
  store volatile i64 58335, i64* @assembly_address
  %411 = load i64* %rax
  %412 = load i1* %of
  %413 = shl i64 %411, 2
  %414 = icmp eq i64 %413, 0
  store i1 %414, i1* %zf
  %415 = icmp slt i64 %413, 0
  store i1 %415, i1* %sf
  %416 = trunc i64 %413 to i8
  %417 = call i8 @llvm.ctpop.i8(i8 %416)
  %418 = and i8 %417, 1
  %419 = icmp eq i8 %418, 0
  store i1 %419, i1* %pf
  store i64 %413, i64* %rax
  %420 = shl i64 %411, 1
  %421 = lshr i64 %420, 63
  %422 = trunc i64 %421 to i1
  store i1 %422, i1* %cf
  %423 = lshr i64 %413, 63
  %424 = icmp ne i64 %423, %421
  %425 = select i1 false, i1 %424, i1 %412
  store i1 %425, i1* %of
  store volatile i64 58339, i64* @assembly_address
  %426 = load i64* %rax
  %427 = load i64* %rdx
  %428 = add i64 %426, %427
  %429 = and i64 %426, 15
  %430 = and i64 %427, 15
  %431 = add i64 %429, %430
  %432 = icmp ugt i64 %431, 15
  %433 = icmp ult i64 %428, %426
  %434 = xor i64 %426, %428
  %435 = xor i64 %427, %428
  %436 = and i64 %434, %435
  %437 = icmp slt i64 %436, 0
  store i1 %432, i1* %az
  store i1 %433, i1* %cf
  store i1 %437, i1* %of
  %438 = icmp eq i64 %428, 0
  store i1 %438, i1* %zf
  %439 = icmp slt i64 %428, 0
  store i1 %439, i1* %sf
  %440 = trunc i64 %428 to i8
  %441 = call i8 @llvm.ctpop.i8(i8 %440)
  %442 = and i8 %441, 1
  %443 = icmp eq i8 %442, 0
  store i1 %443, i1* %pf
  store i64 %428, i64* %rax
  store volatile i64 58342, i64* @assembly_address
  %444 = load i64* %rax
  %445 = load i64* %rax
  %446 = add i64 %444, %445
  %447 = and i64 %444, 15
  %448 = and i64 %445, 15
  %449 = add i64 %447, %448
  %450 = icmp ugt i64 %449, 15
  %451 = icmp ult i64 %446, %444
  %452 = xor i64 %444, %446
  %453 = xor i64 %445, %446
  %454 = and i64 %452, %453
  %455 = icmp slt i64 %454, 0
  store i1 %450, i1* %az
  store i1 %451, i1* %cf
  store i1 %455, i1* %of
  %456 = icmp eq i64 %446, 0
  store i1 %456, i1* %zf
  %457 = icmp slt i64 %446, 0
  store i1 %457, i1* %sf
  %458 = trunc i64 %446 to i8
  %459 = call i8 @llvm.ctpop.i8(i8 %458)
  %460 = and i8 %459, 1
  %461 = icmp eq i8 %460, 0
  store i1 %461, i1* %pf
  store i64 %446, i64* %rax
  store volatile i64 58345, i64* @assembly_address
  %462 = load i64* %rcx
  %463 = load i64* %rax
  %464 = sub i64 %462, %463
  %465 = and i64 %462, 15
  %466 = and i64 %463, 15
  %467 = sub i64 %465, %466
  %468 = icmp ugt i64 %467, 15
  %469 = icmp ult i64 %462, %463
  %470 = xor i64 %462, %463
  %471 = xor i64 %462, %464
  %472 = and i64 %470, %471
  %473 = icmp slt i64 %472, 0
  store i1 %468, i1* %az
  store i1 %469, i1* %cf
  store i1 %473, i1* %of
  %474 = icmp eq i64 %464, 0
  store i1 %474, i1* %zf
  %475 = icmp slt i64 %464, 0
  store i1 %475, i1* %sf
  %476 = trunc i64 %464 to i8
  %477 = call i8 @llvm.ctpop.i8(i8 %476)
  %478 = and i8 %477, 1
  %479 = icmp eq i8 %478, 0
  store i1 %479, i1* %pf
  store i64 %464, i64* %rcx
  store volatile i64 58348, i64* @assembly_address
  %480 = load i64* %rcx
  store i64 %480, i64* %rdx
  store volatile i64 58351, i64* @assembly_address
  %481 = load i64* %rdx
  %482 = trunc i64 %481 to i32
  %483 = zext i32 %482 to i64
  store i64 %483, i64* %rax
  store volatile i64 58353, i64* @assembly_address
  %484 = load i64* %rax
  %485 = trunc i64 %484 to i32
  %486 = add i32 %485, 48
  %487 = and i32 %485, 15
  %488 = icmp ugt i32 %487, 15
  %489 = icmp ult i32 %486, %485
  %490 = xor i32 %485, %486
  %491 = xor i32 48, %486
  %492 = and i32 %490, %491
  %493 = icmp slt i32 %492, 0
  store i1 %488, i1* %az
  store i1 %489, i1* %cf
  store i1 %493, i1* %of
  %494 = icmp eq i32 %486, 0
  store i1 %494, i1* %zf
  %495 = icmp slt i32 %486, 0
  store i1 %495, i1* %sf
  %496 = trunc i32 %486 to i8
  %497 = call i8 @llvm.ctpop.i8(i8 %496)
  %498 = and i8 %497, 1
  %499 = icmp eq i8 %498, 0
  store i1 %499, i1* %pf
  %500 = zext i32 %486 to i64
  store i64 %500, i64* %rax
  store volatile i64 58356, i64* @assembly_address
  %501 = load i8** %stack_var_-96
  %502 = ptrtoint i8* %501 to i64
  %503 = sub i64 %502, 1
  %504 = and i64 %502, 15
  %505 = sub i64 %504, 1
  %506 = icmp ugt i64 %505, 15
  %507 = icmp ult i64 %502, 1
  %508 = xor i64 %502, 1
  %509 = xor i64 %502, %503
  %510 = and i64 %508, %509
  %511 = icmp slt i64 %510, 0
  store i1 %506, i1* %az
  store i1 %507, i1* %cf
  store i1 %511, i1* %of
  %512 = icmp eq i64 %503, 0
  store i1 %512, i1* %zf
  %513 = icmp slt i64 %503, 0
  store i1 %513, i1* %sf
  %514 = trunc i64 %503 to i8
  %515 = call i8 @llvm.ctpop.i8(i8 %514)
  %516 = and i8 %515, 1
  %517 = icmp eq i8 %516, 0
  store i1 %517, i1* %pf
  %518 = inttoptr i64 %503 to i8*
  store i8* %518, i8** %stack_var_-96
  store volatile i64 58361, i64* @assembly_address
  %519 = load i64* %rax
  %520 = trunc i64 %519 to i32
  %521 = zext i32 %520 to i64
  store i64 %521, i64* %rdx
  store volatile i64 58363, i64* @assembly_address
  %522 = load i8** %stack_var_-96
  %523 = ptrtoint i8* %522 to i64
  store i64 %523, i64* %rax
  store volatile i64 58367, i64* @assembly_address
  %524 = load i64* %rdx
  %525 = trunc i64 %524 to i8
  %526 = load i64* %rax
  %527 = inttoptr i64 %526 to i8*
  store i8 %525, i8* %527
  store volatile i64 58369, i64* @assembly_address
  %528 = load i128* %stack_var_-120
  %529 = trunc i128 %528 to i64
  store i64 %529, i64* %rcx
  store volatile i64 58373, i64* @assembly_address
  store i64 7378697629483820647, i64* %rdx
  store volatile i64 58383, i64* @assembly_address
  %530 = load i64* %rcx
  store i64 %530, i64* %rax
  store volatile i64 58386, i64* @assembly_address
  %531 = load i64* %rdx
  %532 = load i64* %rax
  %533 = sext i64 %531 to i128
  %534 = sext i64 %532 to i128
  %535 = mul i128 %533, %534
  %536 = trunc i128 %535 to i64
  %537 = lshr i128 %535, 64
  %538 = trunc i128 %537 to i64
  %539 = icmp ne i64 %538, 0
  store i64 %536, i64* %rax
  store i64 %538, i64* %rdx
  %540 = icmp ne i64 %538, -1
  %541 = icmp eq i1 %539, %540
  store i1 %541, i1* %of
  store i1 %541, i1* %cf
  store volatile i64 58389, i64* @assembly_address
  %542 = load i64* %rdx
  %543 = load i1* %of
  %544 = ashr i64 %542, 2
  %545 = icmp eq i64 %544, 0
  store i1 %545, i1* %zf
  %546 = icmp slt i64 %544, 0
  store i1 %546, i1* %sf
  %547 = trunc i64 %544 to i8
  %548 = call i8 @llvm.ctpop.i8(i8 %547)
  %549 = and i8 %548, 1
  %550 = icmp eq i8 %549, 0
  store i1 %550, i1* %pf
  store i64 %544, i64* %rdx
  %551 = and i64 2, %542
  %552 = icmp ne i64 %551, 0
  store i1 %552, i1* %cf
  %553 = select i1 false, i1 false, i1 %543
  store i1 %553, i1* %of
  store volatile i64 58393, i64* @assembly_address
  %554 = load i64* %rcx
  store i64 %554, i64* %rax
  store volatile i64 58396, i64* @assembly_address
  %555 = load i64* %rax
  %556 = load i1* %of
  %557 = ashr i64 %555, 63
  %558 = icmp eq i64 %557, 0
  store i1 %558, i1* %zf
  %559 = icmp slt i64 %557, 0
  store i1 %559, i1* %sf
  %560 = trunc i64 %557 to i8
  %561 = call i8 @llvm.ctpop.i8(i8 %560)
  %562 = and i8 %561, 1
  %563 = icmp eq i8 %562, 0
  store i1 %563, i1* %pf
  store i64 %557, i64* %rax
  %564 = and i64 4611686018427387904, %555
  %565 = icmp ne i64 %564, 0
  store i1 %565, i1* %cf
  %566 = select i1 false, i1 false, i1 %556
  store i1 %566, i1* %of
  store volatile i64 58400, i64* @assembly_address
  %567 = load i64* %rdx
  %568 = load i64* %rax
  %569 = sub i64 %567, %568
  %570 = and i64 %567, 15
  %571 = and i64 %568, 15
  %572 = sub i64 %570, %571
  %573 = icmp ugt i64 %572, 15
  %574 = icmp ult i64 %567, %568
  %575 = xor i64 %567, %568
  %576 = xor i64 %567, %569
  %577 = and i64 %575, %576
  %578 = icmp slt i64 %577, 0
  store i1 %573, i1* %az
  store i1 %574, i1* %cf
  store i1 %578, i1* %of
  %579 = icmp eq i64 %569, 0
  store i1 %579, i1* %zf
  %580 = icmp slt i64 %569, 0
  store i1 %580, i1* %sf
  %581 = trunc i64 %569 to i8
  %582 = call i8 @llvm.ctpop.i8(i8 %581)
  %583 = and i8 %582, 1
  %584 = icmp eq i8 %583, 0
  store i1 %584, i1* %pf
  store i64 %569, i64* %rdx
  store volatile i64 58403, i64* @assembly_address
  %585 = load i64* %rdx
  store i64 %585, i64* %rax
  store volatile i64 58406, i64* @assembly_address
  %586 = load i64* %rax
  %587 = sext i64 %586 to i128
  store i128 %587, i128* %stack_var_-120
  store volatile i64 58410, i64* @assembly_address
  %588 = load i128* %stack_var_-120
  %589 = trunc i128 %588 to i64
  %590 = and i64 %589, 15
  %591 = icmp ugt i64 %590, 15
  %592 = icmp ult i64 %589, 0
  %593 = xor i64 %589, 0
  %594 = and i64 %593, 0
  %595 = icmp slt i64 %594, 0
  store i1 %591, i1* %az
  store i1 %592, i1* %cf
  store i1 %595, i1* %of
  %596 = icmp eq i64 %589, 0
  store i1 %596, i1* %zf
  %597 = icmp slt i64 %589, 0
  store i1 %597, i1* %sf
  %598 = trunc i64 %589 to i8
  %599 = call i8 @llvm.ctpop.i8(i8 %598)
  %600 = and i8 %599, 1
  %601 = icmp eq i8 %600, 0
  store i1 %601, i1* %pf
  store volatile i64 58415, i64* @assembly_address
  %602 = load i1* %zf
  %603 = icmp eq i1 %602, false
  br i1 %603, label %block_e3ba, label %block_e431

block_e431:                                       ; preds = %block_e3ba, %block_e3ac
  store volatile i64 58417, i64* @assembly_address
  %604 = load i32* %stack_var_-124
  %605 = zext i32 %604 to i64
  store i64 %605, i64* %rdx
  store volatile i64 58420, i64* @assembly_address
  %606 = ptrtoint i64* %stack_var_-88 to i64
  store i64 %606, i64* %rax
  store volatile i64 58424, i64* @assembly_address
  %607 = load i64* %rax
  %608 = add i64 %607, 64
  %609 = and i64 %607, 15
  %610 = icmp ugt i64 %609, 15
  %611 = icmp ult i64 %608, %607
  %612 = xor i64 %607, %608
  %613 = xor i64 64, %608
  %614 = and i64 %612, %613
  %615 = icmp slt i64 %614, 0
  store i1 %610, i1* %az
  store i1 %611, i1* %cf
  store i1 %615, i1* %of
  %616 = icmp eq i64 %608, 0
  store i1 %616, i1* %zf
  %617 = icmp slt i64 %608, 0
  store i1 %617, i1* %sf
  %618 = trunc i64 %608 to i8
  %619 = call i8 @llvm.ctpop.i8(i8 %618)
  %620 = and i8 %619, 1
  %621 = icmp eq i8 %620, 0
  store i1 %621, i1* %pf
  %622 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %622, i64* %rax
  store volatile i64 58428, i64* @assembly_address
  %623 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %623, i64* %rcx
  store volatile i64 58431, i64* @assembly_address
  %624 = load i8** %stack_var_-96
  %625 = ptrtoint i8* %624 to i64
  store i64 %625, i64* %rax
  store volatile i64 58435, i64* @assembly_address
  %626 = load i64* %rcx
  %627 = load i64* %rax
  %628 = sub i64 %626, %627
  %629 = and i64 %626, 15
  %630 = and i64 %627, 15
  %631 = sub i64 %629, %630
  %632 = icmp ugt i64 %631, 15
  %633 = icmp ult i64 %626, %627
  %634 = xor i64 %626, %627
  %635 = xor i64 %626, %628
  %636 = and i64 %634, %635
  %637 = icmp slt i64 %636, 0
  store i1 %632, i1* %az
  store i1 %633, i1* %cf
  store i1 %637, i1* %of
  %638 = icmp eq i64 %628, 0
  store i1 %638, i1* %zf
  %639 = icmp slt i64 %628, 0
  store i1 %639, i1* %sf
  %640 = trunc i64 %628 to i8
  %641 = call i8 @llvm.ctpop.i8(i8 %640)
  %642 = and i8 %641, 1
  %643 = icmp eq i8 %642, 0
  store i1 %643, i1* %pf
  store i64 %628, i64* %rcx
  store volatile i64 58438, i64* @assembly_address
  %644 = load i64* %rcx
  store i64 %644, i64* %rax
  store volatile i64 58441, i64* @assembly_address
  %645 = load i64* %rdx
  %646 = trunc i64 %645 to i32
  %647 = load i64* %rax
  %648 = trunc i64 %647 to i32
  %649 = sub i32 %646, %648
  %650 = and i32 %646, 15
  %651 = and i32 %648, 15
  %652 = sub i32 %650, %651
  %653 = icmp ugt i32 %652, 15
  %654 = icmp ult i32 %646, %648
  %655 = xor i32 %646, %648
  %656 = xor i32 %646, %649
  %657 = and i32 %655, %656
  %658 = icmp slt i32 %657, 0
  store i1 %653, i1* %az
  store i1 %654, i1* %cf
  store i1 %658, i1* %of
  %659 = icmp eq i32 %649, 0
  store i1 %659, i1* %zf
  %660 = icmp slt i32 %649, 0
  store i1 %660, i1* %sf
  %661 = trunc i32 %649 to i8
  %662 = call i8 @llvm.ctpop.i8(i8 %661)
  %663 = and i8 %662, 1
  %664 = icmp eq i8 %663, 0
  store i1 %664, i1* %pf
  %665 = zext i32 %649 to i64
  store i64 %665, i64* %rdx
  store volatile i64 58443, i64* @assembly_address
  %666 = load i64* %rdx
  %667 = trunc i64 %666 to i32
  %668 = zext i32 %667 to i64
  store i64 %668, i64* %rax
  store volatile i64 58445, i64* @assembly_address
  %669 = load i64* %rax
  %670 = trunc i64 %669 to i32
  store i32 %670, i32* %stack_var_-124
  store volatile i64 58448, i64* @assembly_address
  br label %block_e463

block_e452:                                       ; preds = %block_e463
  store volatile i64 58450, i64* @assembly_address
  %671 = load %_IO_FILE** %stack_var_-112
  %672 = ptrtoint %_IO_FILE* %671 to i64
  store i64 %672, i64* %rax
  store volatile i64 58454, i64* @assembly_address
  %673 = load i64* %rax
  store i64 %673, i64* %rsi
  store volatile i64 58457, i64* @assembly_address
  store i64 32, i64* %rdi
  store volatile i64 58462, i64* @assembly_address
  %674 = load i64* %rdi
  %675 = trunc i64 %674 to i32
  %676 = load i64* %rsi
  %677 = inttoptr i64 %676 to %_IO_FILE*
  %678 = call i32 @_IO_putc(i32 %675, %_IO_FILE* %677)
  %679 = sext i32 %678 to i64
  store i64 %679, i64* %rax
  %680 = sext i32 %678 to i64
  store i64 %680, i64* %rax
  br label %block_e463

block_e463:                                       ; preds = %block_e452, %block_e431
  store volatile i64 58467, i64* @assembly_address
  %681 = load i32* %stack_var_-124
  %682 = zext i32 %681 to i64
  store i64 %682, i64* %rax
  store volatile i64 58470, i64* @assembly_address
  %683 = load i64* %rax
  %684 = add i64 %683, -1
  %685 = trunc i64 %684 to i32
  %686 = zext i32 %685 to i64
  store i64 %686, i64* %rdx
  store volatile i64 58473, i64* @assembly_address
  %687 = load i64* %rdx
  %688 = trunc i64 %687 to i32
  store i32 %688, i32* %stack_var_-124
  store volatile i64 58476, i64* @assembly_address
  %689 = load i64* %rax
  %690 = trunc i64 %689 to i32
  %691 = load i64* %rax
  %692 = trunc i64 %691 to i32
  %693 = and i32 %690, %692
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %694 = icmp eq i32 %693, 0
  store i1 %694, i1* %zf
  %695 = icmp slt i32 %693, 0
  store i1 %695, i1* %sf
  %696 = trunc i32 %693 to i8
  %697 = call i8 @llvm.ctpop.i8(i8 %696)
  %698 = and i8 %697, 1
  %699 = icmp eq i8 %698, 0
  store i1 %699, i1* %pf
  store volatile i64 58478, i64* @assembly_address
  %700 = load i1* %zf
  %701 = load i1* %sf
  %702 = load i1* %of
  %703 = icmp eq i1 %701, %702
  %704 = icmp eq i1 %700, false
  %705 = icmp eq i1 %703, %704
  br i1 %705, label %block_e452, label %block_e470

block_e470:                                       ; preds = %block_e463
  store volatile i64 58480, i64* @assembly_address
  br label %block_e48f

block_e472:                                       ; preds = %block_e48f
  store volatile i64 58482, i64* @assembly_address
  %706 = load i8** %stack_var_-96
  %707 = ptrtoint i8* %706 to i64
  store i64 %707, i64* %rax
  store volatile i64 58486, i64* @assembly_address
  %708 = load i64* %rax
  %709 = inttoptr i64 %708 to i8*
  %710 = load i8* %709
  %711 = zext i8 %710 to i64
  store i64 %711, i64* %rax
  store volatile i64 58489, i64* @assembly_address
  %712 = load i64* %rax
  %713 = trunc i64 %712 to i8
  %714 = sext i8 %713 to i64
  store i64 %714, i64* %rax
  store volatile i64 58492, i64* @assembly_address
  %715 = load %_IO_FILE** %stack_var_-112
  %716 = ptrtoint %_IO_FILE* %715 to i64
  store i64 %716, i64* %rdx
  store volatile i64 58496, i64* @assembly_address
  %717 = load i64* %rdx
  store i64 %717, i64* %rsi
  store volatile i64 58499, i64* @assembly_address
  %718 = load i64* %rax
  %719 = trunc i64 %718 to i32
  %720 = zext i32 %719 to i64
  store i64 %720, i64* %rdi
  store volatile i64 58501, i64* @assembly_address
  %721 = load i64* %rdi
  %722 = trunc i64 %721 to i32
  %723 = load i64* %rsi
  %724 = inttoptr i64 %723 to %_IO_FILE*
  %725 = call i32 @_IO_putc(i32 %722, %_IO_FILE* %724)
  %726 = sext i32 %725 to i64
  store i64 %726, i64* %rax
  %727 = sext i32 %725 to i64
  store i64 %727, i64* %rax
  store volatile i64 58506, i64* @assembly_address
  %728 = load i8** %stack_var_-96
  %729 = ptrtoint i8* %728 to i64
  %730 = add i64 %729, 1
  %731 = and i64 %729, 15
  %732 = add i64 %731, 1
  %733 = icmp ugt i64 %732, 15
  %734 = icmp ult i64 %730, %729
  %735 = xor i64 %729, %730
  %736 = xor i64 1, %730
  %737 = and i64 %735, %736
  %738 = icmp slt i64 %737, 0
  store i1 %733, i1* %az
  store i1 %734, i1* %cf
  store i1 %738, i1* %of
  %739 = icmp eq i64 %730, 0
  store i1 %739, i1* %zf
  %740 = icmp slt i64 %730, 0
  store i1 %740, i1* %sf
  %741 = trunc i64 %730 to i8
  %742 = call i8 @llvm.ctpop.i8(i8 %741)
  %743 = and i8 %742, 1
  %744 = icmp eq i8 %743, 0
  store i1 %744, i1* %pf
  %745 = inttoptr i64 %730 to i8*
  store i8* %745, i8** %stack_var_-96
  br label %block_e48f

block_e48f:                                       ; preds = %block_e472, %block_e470
  store volatile i64 58511, i64* @assembly_address
  %746 = ptrtoint i64* %stack_var_-88 to i64
  store i64 %746, i64* %rax
  store volatile i64 58515, i64* @assembly_address
  %747 = load i64* %rax
  %748 = add i64 %747, 64
  %749 = and i64 %747, 15
  %750 = icmp ugt i64 %749, 15
  %751 = icmp ult i64 %748, %747
  %752 = xor i64 %747, %748
  %753 = xor i64 64, %748
  %754 = and i64 %752, %753
  %755 = icmp slt i64 %754, 0
  store i1 %750, i1* %az
  store i1 %751, i1* %cf
  store i1 %755, i1* %of
  %756 = icmp eq i64 %748, 0
  store i1 %756, i1* %zf
  %757 = icmp slt i64 %748, 0
  store i1 %757, i1* %sf
  %758 = trunc i64 %748 to i8
  %759 = call i8 @llvm.ctpop.i8(i8 %758)
  %760 = and i8 %759, 1
  %761 = icmp eq i8 %760, 0
  store i1 %761, i1* %pf
  %762 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %762, i64* %rax
  store volatile i64 58519, i64* @assembly_address
  %763 = load i8** %stack_var_-96
  %764 = ptrtoint i8* %763 to i64
  %765 = load i64* %rax
  %766 = sub i64 %764, %765
  %767 = and i64 %764, 15
  %768 = and i64 %765, 15
  %769 = sub i64 %767, %768
  %770 = icmp ugt i64 %769, 15
  %771 = icmp ult i64 %764, %765
  %772 = xor i64 %764, %765
  %773 = xor i64 %764, %766
  %774 = and i64 %772, %773
  %775 = icmp slt i64 %774, 0
  store i1 %770, i1* %az
  store i1 %771, i1* %cf
  store i1 %775, i1* %of
  %776 = icmp eq i64 %766, 0
  store i1 %776, i1* %zf
  %777 = icmp slt i64 %766, 0
  store i1 %777, i1* %sf
  %778 = trunc i64 %766 to i8
  %779 = call i8 @llvm.ctpop.i8(i8 %778)
  %780 = and i8 %779, 1
  %781 = icmp eq i8 %780, 0
  store i1 %781, i1* %pf
  store volatile i64 58523, i64* @assembly_address
  %782 = load i1* %cf
  br i1 %782, label %block_e472, label %block_e49d

block_e49d:                                       ; preds = %block_e48f
  store volatile i64 58525, i64* @assembly_address
  store volatile i64 58526, i64* @assembly_address
  %783 = load i64* %stack_var_-16
  store i64 %783, i64* %rax
  store volatile i64 58530, i64* @assembly_address
  %784 = load i64* %rax
  %785 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %786 = xor i64 %784, %785
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %787 = icmp eq i64 %786, 0
  store i1 %787, i1* %zf
  %788 = icmp slt i64 %786, 0
  store i1 %788, i1* %sf
  %789 = trunc i64 %786 to i8
  %790 = call i8 @llvm.ctpop.i8(i8 %789)
  %791 = and i8 %790, 1
  %792 = icmp eq i8 %791, 0
  store i1 %792, i1* %pf
  store i64 %786, i64* %rax
  store volatile i64 58539, i64* @assembly_address
  %793 = load i1* %zf
  br i1 %793, label %block_e4b2, label %block_e4ad

block_e4ad:                                       ; preds = %block_e49d
  store volatile i64 58541, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_e4b2:                                       ; preds = %block_e49d
  store volatile i64 58546, i64* @assembly_address
  %794 = load i64* %stack_var_-8
  store i64 %794, i64* %rbp
  %795 = ptrtoint i64* %stack_var_0 to i64
  store i64 %795, i64* %rsp
  store volatile i64 58547, i64* @assembly_address
  %796 = load i64* %rax
  ret i64 %796
}

declare i64 @257(i64, i128, i32)

declare i64 @258(i64, i64, i32)

define i64 @zip(i32 %arg1, i64 %arg2) {
block_e4b4:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-24 = alloca i8*
  %1 = alloca i64
  %stack_var_-32 = alloca i32*
  %2 = alloca i64
  %stack_var_-34 = alloca i16
  %stack_var_-36 = alloca i16
  %stack_var_-37 = alloca i8
  %stack_var_-16 = alloca i64
  %stack_var_-48 = alloca i32
  %stack_var_-44 = alloca i32
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  %3 = alloca i32*
  %4 = alloca i64
  %5 = alloca i32*
  %6 = alloca i64
  store volatile i64 58548, i64* @assembly_address
  %7 = load i64* %rbp
  store i64 %7, i64* %stack_var_-8
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rsp
  store volatile i64 58549, i64* @assembly_address
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rbp
  store volatile i64 58552, i64* @assembly_address
  %10 = load i64* %rsp
  %11 = sub i64 %10, 48
  %12 = and i64 %10, 15
  %13 = icmp ugt i64 %12, 15
  %14 = icmp ult i64 %10, 48
  %15 = xor i64 %10, 48
  %16 = xor i64 %10, %11
  %17 = and i64 %15, %16
  %18 = icmp slt i64 %17, 0
  store i1 %13, i1* %az
  store i1 %14, i1* %cf
  store i1 %18, i1* %of
  %19 = icmp eq i64 %11, 0
  store i1 %19, i1* %zf
  %20 = icmp slt i64 %11, 0
  store i1 %20, i1* %sf
  %21 = trunc i64 %11 to i8
  %22 = call i8 @llvm.ctpop.i8(i8 %21)
  %23 = and i8 %22, 1
  %24 = icmp eq i8 %23, 0
  store i1 %24, i1* %pf
  %25 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %25, i64* %rsp
  store volatile i64 58556, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-44
  store volatile i64 58559, i64* @assembly_address
  %28 = load i64* %rsi
  %29 = trunc i64 %28 to i32
  store i32 %29, i32* %stack_var_-48
  store volatile i64 58562, i64* @assembly_address
  %30 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %30, i64* %rax
  store volatile i64 58571, i64* @assembly_address
  %31 = load i64* %rax
  store i64 %31, i64* %stack_var_-16
  store volatile i64 58575, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %32 = icmp eq i32 0, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i32 0, 0
  store i1 %33, i1* %sf
  %34 = trunc i32 0 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  %38 = zext i32 0 to i64
  store i64 %38, i64* %rax
  store volatile i64 58577, i64* @assembly_address
  store i8 0, i8* %stack_var_-37
  store volatile i64 58581, i64* @assembly_address
  store i16 0, i16* %stack_var_-36
  store volatile i64 58587, i64* @assembly_address
  store i16 0, i16* %stack_var_-34
  store volatile i64 58593, i64* @assembly_address
  %39 = load i32* %stack_var_-44
  %40 = zext i32 %39 to i64
  store i64 %40, i64* %rax
  store volatile i64 58596, i64* @assembly_address
  %41 = load i64* %rax
  %42 = trunc i64 %41 to i32
  store i32 %42, i32* bitcast (i64* @global_var_24f0a0 to i32*)
  store volatile i64 58602, i64* @assembly_address
  %43 = load i32* %stack_var_-48
  %44 = zext i32 %43 to i64
  store i64 %44, i64* %rax
  store volatile i64 58605, i64* @assembly_address
  %45 = load i64* %rax
  %46 = trunc i64 %45 to i32
  store i32 %46, i32* bitcast (i64* @global_var_24a880 to i32*)
  store volatile i64 58611, i64* @assembly_address
  store i32 0, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 58621, i64* @assembly_address
  store i32 8, i32* bitcast ([2 x i8]* @global_var_21609c to i32*)
  store volatile i64 58631, i64* @assembly_address
  store i64 31, i64* %rcx
  store volatile i64 58636, i64* @assembly_address
  %47 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rax
  store volatile i64 58642, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 58645, i64* @assembly_address
  %49 = load i64* %rdx
  %50 = trunc i64 %49 to i32
  store i32 %50, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 58651, i64* @assembly_address
  %51 = load i64* %rax
  %52 = trunc i64 %51 to i32
  %53 = zext i32 %52 to i64
  store i64 %53, i64* %rdx
  store volatile i64 58653, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 58660, i64* @assembly_address
  %54 = load i64* %rcx
  %55 = trunc i64 %54 to i8
  %56 = load i64* %rdx
  %57 = load i64* %rax
  %58 = mul i64 %57, 1
  %59 = add i64 %56, %58
  %60 = inttoptr i64 %59 to i8*
  store i8 %55, i8* %60
  store volatile i64 58663, i64* @assembly_address
  %61 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %62 = zext i32 %61 to i64
  store i64 %62, i64* %rax
  store volatile i64 58669, i64* @assembly_address
  %63 = load i64* %rax
  %64 = trunc i64 %63 to i32
  %65 = sub i32 %64, 16384
  %66 = and i32 %64, 15
  %67 = icmp ugt i32 %66, 15
  %68 = icmp ult i32 %64, 16384
  %69 = xor i32 %64, 16384
  %70 = xor i32 %64, %65
  %71 = and i32 %69, %70
  %72 = icmp slt i32 %71, 0
  store i1 %67, i1* %az
  store i1 %68, i1* %cf
  store i1 %72, i1* %of
  %73 = icmp eq i32 %65, 0
  store i1 %73, i1* %zf
  %74 = icmp slt i32 %65, 0
  store i1 %74, i1* %sf
  %75 = trunc i32 %65 to i8
  %76 = call i8 @llvm.ctpop.i8(i8 %75)
  %77 = and i8 %76, 1
  %78 = icmp eq i8 %77, 0
  store i1 %78, i1* %pf
  store volatile i64 58674, i64* @assembly_address
  %79 = load i1* %zf
  %80 = icmp eq i1 %79, false
  br i1 %80, label %block_e539, label %block_e534

block_e534:                                       ; preds = %block_e4b4
  store volatile i64 58676, i64* @assembly_address
  %81 = load i64* %rdi
  %82 = load i64* %rsi
  %83 = load i64* %rdx
  %84 = load i64* %rcx
  %85 = trunc i64 %84 to i16
  %86 = call i64 @flush_outbuf(i64 %81, i64 %82, i64 %83, i16 %85)
  store i64 %86, i64* %rax
  store i64 %86, i64* %rax
  store i64 %86, i64* %rax
  br label %block_e539

block_e539:                                       ; preds = %block_e534, %block_e4b4
  store volatile i64 58681, i64* @assembly_address
  store i64 4294967179, i64* %rcx
  store volatile i64 58686, i64* @assembly_address
  %87 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %88 = zext i32 %87 to i64
  store i64 %88, i64* %rax
  store volatile i64 58692, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 58695, i64* @assembly_address
  %89 = load i64* %rdx
  %90 = trunc i64 %89 to i32
  store i32 %90, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 58701, i64* @assembly_address
  %91 = load i64* %rax
  %92 = trunc i64 %91 to i32
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rdx
  store volatile i64 58703, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 58710, i64* @assembly_address
  %94 = load i64* %rcx
  %95 = trunc i64 %94 to i8
  %96 = load i64* %rdx
  %97 = load i64* %rax
  %98 = mul i64 %97, 1
  %99 = add i64 %96, %98
  %100 = inttoptr i64 %99 to i8*
  store i8 %95, i8* %100
  store volatile i64 58713, i64* @assembly_address
  %101 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %102 = zext i32 %101 to i64
  store i64 %102, i64* %rax
  store volatile i64 58719, i64* @assembly_address
  %103 = load i64* %rax
  %104 = trunc i64 %103 to i32
  %105 = sub i32 %104, 16384
  %106 = and i32 %104, 15
  %107 = icmp ugt i32 %106, 15
  %108 = icmp ult i32 %104, 16384
  %109 = xor i32 %104, 16384
  %110 = xor i32 %104, %105
  %111 = and i32 %109, %110
  %112 = icmp slt i32 %111, 0
  store i1 %107, i1* %az
  store i1 %108, i1* %cf
  store i1 %112, i1* %of
  %113 = icmp eq i32 %105, 0
  store i1 %113, i1* %zf
  %114 = icmp slt i32 %105, 0
  store i1 %114, i1* %sf
  %115 = trunc i32 %105 to i8
  %116 = call i8 @llvm.ctpop.i8(i8 %115)
  %117 = and i8 %116, 1
  %118 = icmp eq i8 %117, 0
  store i1 %118, i1* %pf
  store volatile i64 58724, i64* @assembly_address
  %119 = load i1* %zf
  %120 = icmp eq i1 %119, false
  br i1 %120, label %block_e56b, label %block_e566

block_e566:                                       ; preds = %block_e539
  store volatile i64 58726, i64* @assembly_address
  %121 = load i64* %rdi
  %122 = load i64* %rsi
  %123 = load i64* %rdx
  %124 = load i64* %rcx
  %125 = trunc i64 %124 to i16
  %126 = call i64 @flush_outbuf(i64 %121, i64 %122, i64 %123, i16 %125)
  store i64 %126, i64* %rax
  store i64 %126, i64* %rax
  store i64 %126, i64* %rax
  br label %block_e56b

block_e56b:                                       ; preds = %block_e566, %block_e539
  store volatile i64 58731, i64* @assembly_address
  %127 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %128 = zext i32 %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 58737, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 58740, i64* @assembly_address
  %129 = load i64* %rdx
  %130 = trunc i64 %129 to i32
  store i32 %130, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 58746, i64* @assembly_address
  %131 = load i64* %rax
  %132 = trunc i64 %131 to i32
  %133 = zext i32 %132 to i64
  store i64 %133, i64* %rdx
  store volatile i64 58748, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 58755, i64* @assembly_address
  %134 = load i64* %rdx
  %135 = load i64* %rax
  %136 = mul i64 %135, 1
  %137 = add i64 %134, %136
  %138 = inttoptr i64 %137 to i8*
  store i8 8, i8* %138
  store volatile i64 58759, i64* @assembly_address
  %139 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %140 = zext i32 %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 58765, i64* @assembly_address
  %141 = load i64* %rax
  %142 = trunc i64 %141 to i32
  %143 = sub i32 %142, 16384
  %144 = and i32 %142, 15
  %145 = icmp ugt i32 %144, 15
  %146 = icmp ult i32 %142, 16384
  %147 = xor i32 %142, 16384
  %148 = xor i32 %142, %143
  %149 = and i32 %147, %148
  %150 = icmp slt i32 %149, 0
  store i1 %145, i1* %az
  store i1 %146, i1* %cf
  store i1 %150, i1* %of
  %151 = icmp eq i32 %143, 0
  store i1 %151, i1* %zf
  %152 = icmp slt i32 %143, 0
  store i1 %152, i1* %sf
  %153 = trunc i32 %143 to i8
  %154 = call i8 @llvm.ctpop.i8(i8 %153)
  %155 = and i8 %154, 1
  %156 = icmp eq i8 %155, 0
  store i1 %156, i1* %pf
  store volatile i64 58770, i64* @assembly_address
  %157 = load i1* %zf
  %158 = icmp eq i1 %157, false
  br i1 %158, label %block_e599, label %block_e594

block_e594:                                       ; preds = %block_e56b
  store volatile i64 58772, i64* @assembly_address
  %159 = load i64* %rdi
  %160 = load i64* %rsi
  %161 = load i64* %rdx
  %162 = load i64* %rcx
  %163 = trunc i64 %162 to i16
  %164 = call i64 @flush_outbuf(i64 %159, i64 %160, i64 %161, i16 %163)
  store i64 %164, i64* %rax
  store i64 %164, i64* %rax
  store i64 %164, i64* %rax
  br label %block_e599

block_e599:                                       ; preds = %block_e594, %block_e56b
  store volatile i64 58777, i64* @assembly_address
  %165 = load i32* bitcast (i64* @global_var_24a888 to i32*)
  %166 = zext i32 %165 to i64
  store i64 %166, i64* %rax
  store volatile i64 58783, i64* @assembly_address
  %167 = load i64* %rax
  %168 = trunc i64 %167 to i32
  %169 = load i64* %rax
  %170 = trunc i64 %169 to i32
  %171 = and i32 %168, %170
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %172 = icmp eq i32 %171, 0
  store i1 %172, i1* %zf
  %173 = icmp slt i32 %171, 0
  store i1 %173, i1* %sf
  %174 = trunc i32 %171 to i8
  %175 = call i8 @llvm.ctpop.i8(i8 %174)
  %176 = and i8 %175, 1
  %177 = icmp eq i8 %176, 0
  store i1 %177, i1* %pf
  store volatile i64 58785, i64* @assembly_address
  %178 = load i1* %zf
  br i1 %178, label %block_e5a7, label %block_e5a3

block_e5a3:                                       ; preds = %block_e599
  store volatile i64 58787, i64* @assembly_address
  %179 = load i8* %stack_var_-37
  %180 = or i8 %179, 8
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %181 = icmp eq i8 %180, 0
  store i1 %181, i1* %zf
  %182 = icmp slt i8 %180, 0
  store i1 %182, i1* %sf
  %183 = call i8 @llvm.ctpop.i8(i8 %180)
  %184 = and i8 %183, 1
  %185 = icmp eq i8 %184, 0
  store i1 %185, i1* %pf
  store i8 %180, i8* %stack_var_-37
  br label %block_e5a7

block_e5a7:                                       ; preds = %block_e5a3, %block_e599
  store volatile i64 58791, i64* @assembly_address
  %186 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %187 = zext i32 %186 to i64
  store i64 %187, i64* %rax
  store volatile i64 58797, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 58800, i64* @assembly_address
  %188 = load i64* %rdx
  %189 = trunc i64 %188 to i32
  store i32 %189, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 58806, i64* @assembly_address
  %190 = load i64* %rax
  %191 = trunc i64 %190 to i32
  %192 = zext i32 %191 to i64
  store i64 %192, i64* %rcx
  store volatile i64 58808, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 58815, i64* @assembly_address
  %193 = load i8* %stack_var_-37
  %194 = zext i8 %193 to i64
  store i64 %194, i64* %rdx
  store volatile i64 58819, i64* @assembly_address
  %195 = load i64* %rdx
  %196 = trunc i64 %195 to i8
  %197 = load i64* %rcx
  %198 = load i64* %rax
  %199 = mul i64 %198, 1
  %200 = add i64 %197, %199
  %201 = inttoptr i64 %200 to i8*
  store i8 %196, i8* %201
  store volatile i64 58822, i64* @assembly_address
  %202 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %203 = zext i32 %202 to i64
  store i64 %203, i64* %rax
  store volatile i64 58828, i64* @assembly_address
  %204 = load i64* %rax
  %205 = trunc i64 %204 to i32
  %206 = sub i32 %205, 16384
  %207 = and i32 %205, 15
  %208 = icmp ugt i32 %207, 15
  %209 = icmp ult i32 %205, 16384
  %210 = xor i32 %205, 16384
  %211 = xor i32 %205, %206
  %212 = and i32 %210, %211
  %213 = icmp slt i32 %212, 0
  store i1 %208, i1* %az
  store i1 %209, i1* %cf
  store i1 %213, i1* %of
  %214 = icmp eq i32 %206, 0
  store i1 %214, i1* %zf
  %215 = icmp slt i32 %206, 0
  store i1 %215, i1* %sf
  %216 = trunc i32 %206 to i8
  %217 = call i8 @llvm.ctpop.i8(i8 %216)
  %218 = and i8 %217, 1
  %219 = icmp eq i8 %218, 0
  store i1 %219, i1* %pf
  store volatile i64 58833, i64* @assembly_address
  %220 = load i1* %zf
  %221 = icmp eq i1 %220, false
  br i1 %221, label %block_e5d8, label %block_e5d3

block_e5d3:                                       ; preds = %block_e5a7
  store volatile i64 58835, i64* @assembly_address
  %222 = load i64* %rdi
  %223 = load i64* %rsi
  %224 = load i64* %rdx
  %225 = load i64* %rcx
  %226 = trunc i64 %225 to i16
  %227 = call i64 @flush_outbuf(i64 %222, i64 %223, i64 %224, i16 %226)
  store i64 %227, i64* %rax
  store i64 %227, i64* %rax
  store i64 %227, i64* %rax
  br label %block_e5d8

block_e5d8:                                       ; preds = %block_e5d3, %block_e5a7
  store volatile i64 58840, i64* @assembly_address
  %228 = load i64* @global_var_25f4d8
  store i64 %228, i64* %rax
  store volatile i64 58847, i64* @assembly_address
  %229 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %230 = icmp eq i64 %229, 0
  store i1 %230, i1* %zf
  %231 = icmp slt i64 %229, 0
  store i1 %231, i1* %sf
  %232 = trunc i64 %229 to i8
  %233 = call i8 @llvm.ctpop.i8(i8 %232)
  %234 = and i8 %233, 1
  %235 = icmp eq i8 %234, 0
  store i1 %235, i1* %pf
  store volatile i64 58850, i64* @assembly_address
  %236 = load i1* %sf
  %237 = icmp eq i1 %236, false
  br i1 %237, label %block_e5ee, label %block_e5e4

block_e5e4:                                       ; preds = %block_e5d8
  store volatile i64 58852, i64* @assembly_address
  %238 = inttoptr i64 0 to i32*
  store i32* %238, i32** %stack_var_-32
  store volatile i64 58860, i64* @assembly_address
  br label %block_e62c

block_e5ee:                                       ; preds = %block_e5d8
  store volatile i64 58862, i64* @assembly_address
  %239 = load i64* @global_var_25f4d0
  store i64 %239, i64* %rax
  store volatile i64 58869, i64* @assembly_address
  %240 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %241 = icmp eq i64 %240, 0
  store i1 %241, i1* %zf
  %242 = icmp slt i64 %240, 0
  store i1 %242, i1* %sf
  %243 = trunc i64 %240 to i8
  %244 = call i8 @llvm.ctpop.i8(i8 %243)
  %245 = and i8 %244, 1
  %246 = icmp eq i8 %245, 0
  store i1 %246, i1* %pf
  store volatile i64 58872, i64* @assembly_address
  %247 = load i1* %zf
  %248 = load i1* %sf
  %249 = load i1* %of
  %250 = icmp ne i1 %248, %249
  %251 = or i1 %247, %250
  br i1 %251, label %block_e618, label %block_e5fa

block_e5fa:                                       ; preds = %block_e5ee
  store volatile i64 58874, i64* @assembly_address
  %252 = load i64* @global_var_25f4d0
  store i64 %252, i64* %rdx
  store volatile i64 58881, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 58886, i64* @assembly_address
  %253 = load i64* %rdx
  %254 = load i64* %rax
  %255 = inttoptr i64 %253 to i32*
  store i32* %255, i32** %5
  %256 = inttoptr i64 %254 to i32*
  store i32* %256, i32** %3
  %257 = sub i64 %253, %254
  %258 = and i64 %253, 15
  %259 = and i64 %254, 15
  %260 = sub i64 %258, %259
  %261 = icmp ugt i64 %260, 15
  %262 = icmp ult i64 %253, %254
  %263 = xor i64 %253, %254
  %264 = xor i64 %253, %257
  %265 = and i64 %263, %264
  %266 = icmp slt i64 %265, 0
  store i1 %261, i1* %az
  store i1 %262, i1* %cf
  store i1 %266, i1* %of
  %267 = icmp eq i64 %257, 0
  store i1 %267, i1* %zf
  %268 = icmp slt i64 %257, 0
  store i1 %268, i1* %sf
  %269 = trunc i64 %257 to i8
  %270 = call i8 @llvm.ctpop.i8(i8 %269)
  %271 = and i8 %270, 1
  %272 = icmp eq i8 %271, 0
  store i1 %272, i1* %pf
  store volatile i64 58889, i64* @assembly_address
  %273 = load i32** %5
  %274 = ptrtoint i32* %273 to i64
  %275 = load i32** %3
  %276 = ptrtoint i32* %275 to i64
  %277 = icmp sgt i64 %274, %276
  br i1 %277, label %block_e618, label %block_e60b

block_e60b:                                       ; preds = %block_e5fa
  store volatile i64 58891, i64* @assembly_address
  %278 = load i64* @global_var_25f4d0
  store i64 %278, i64* %rax
  store volatile i64 58898, i64* @assembly_address
  %279 = load i64* %rax
  %280 = inttoptr i64 %279 to i32*
  store i32* %280, i32** %stack_var_-32
  store volatile i64 58902, i64* @assembly_address
  br label %block_e62c

block_e618:                                       ; preds = %block_e5fa, %block_e5ee
  store volatile i64 58904, i64* @assembly_address
  store i64 ptrtoint ([44 x i8]* @global_var_12cb8 to i64), i64* %rdi
  store volatile i64 58911, i64* @assembly_address
  %281 = load i64* %rdi
  %282 = inttoptr i64 %281 to i8*
  %283 = call i64 @warning(i8* %282)
  store i64 %283, i64* %rax
  store i64 %283, i64* %rax
  store volatile i64 58916, i64* @assembly_address
  %284 = inttoptr i64 0 to i32*
  store i32* %284, i32** %stack_var_-32
  br label %block_e62c

block_e62c:                                       ; preds = %block_e618, %block_e60b, %block_e5e4
  store volatile i64 58924, i64* @assembly_address
  %285 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %286 = zext i32 %285 to i64
  store i64 %286, i64* %rax
  store volatile i64 58930, i64* @assembly_address
  %287 = load i64* %rax
  %288 = trunc i64 %287 to i32
  %289 = sub i32 %288, 16381
  %290 = and i32 %288, 15
  %291 = sub i32 %290, 13
  %292 = icmp ugt i32 %291, 15
  %293 = icmp ult i32 %288, 16381
  %294 = xor i32 %288, 16381
  %295 = xor i32 %288, %289
  %296 = and i32 %294, %295
  %297 = icmp slt i32 %296, 0
  store i1 %292, i1* %az
  store i1 %293, i1* %cf
  store i1 %297, i1* %of
  %298 = icmp eq i32 %289, 0
  store i1 %298, i1* %zf
  %299 = icmp slt i32 %289, 0
  store i1 %299, i1* %sf
  %300 = trunc i32 %289 to i8
  %301 = call i8 @llvm.ctpop.i8(i8 %300)
  %302 = and i8 %301, 1
  %303 = icmp eq i8 %302, 0
  store i1 %303, i1* %pf
  store volatile i64 58935, i64* @assembly_address
  %304 = load i1* %cf
  %305 = load i1* %zf
  %306 = or i1 %304, %305
  %307 = icmp ne i1 %306, true
  br i1 %307, label %block_e681, label %block_e639

block_e639:                                       ; preds = %block_e62c
  store volatile i64 58937, i64* @assembly_address
  %308 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %309 = zext i32 %308 to i64
  store i64 %309, i64* %rax
  store volatile i64 58943, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 58946, i64* @assembly_address
  %310 = load i64* %rdx
  %311 = trunc i64 %310 to i32
  store i32 %311, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 58952, i64* @assembly_address
  %312 = load i32** %stack_var_-32
  %313 = ptrtoint i32* %312 to i64
  store i64 %313, i64* %rdx
  store volatile i64 58956, i64* @assembly_address
  %314 = load i64* %rdx
  %315 = trunc i64 %314 to i32
  %316 = zext i32 %315 to i64
  store i64 %316, i64* %rcx
  store volatile i64 58958, i64* @assembly_address
  %317 = load i64* %rax
  %318 = trunc i64 %317 to i32
  %319 = zext i32 %318 to i64
  store i64 %319, i64* %rdx
  store volatile i64 58960, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 58967, i64* @assembly_address
  %320 = load i64* %rcx
  %321 = trunc i64 %320 to i8
  %322 = load i64* %rdx
  %323 = load i64* %rax
  %324 = mul i64 %323, 1
  %325 = add i64 %322, %324
  %326 = inttoptr i64 %325 to i8*
  store i8 %321, i8* %326
  store volatile i64 58970, i64* @assembly_address
  %327 = load i32** %stack_var_-32
  %328 = ptrtoint i32* %327 to i64
  store i64 %328, i64* %rax
  store volatile i64 58974, i64* @assembly_address
  %329 = load i64* %rax
  %330 = trunc i64 %329 to i16
  %331 = load i1* %of
  %332 = lshr i16 %330, 8
  %333 = icmp eq i16 %332, 0
  store i1 %333, i1* %zf
  %334 = icmp slt i16 %332, 0
  store i1 %334, i1* %sf
  %335 = trunc i16 %332 to i8
  %336 = call i8 @llvm.ctpop.i8(i8 %335)
  %337 = and i8 %336, 1
  %338 = icmp eq i8 %337, 0
  store i1 %338, i1* %pf
  %339 = zext i16 %332 to i64
  %340 = load i64* %rax
  %341 = and i64 %340, -65536
  %342 = or i64 %341, %339
  store i64 %342, i64* %rax
  %343 = and i16 128, %330
  %344 = icmp ne i16 %343, 0
  store i1 %344, i1* %cf
  %345 = icmp slt i16 %330, 0
  %346 = select i1 false, i1 %345, i1 %331
  store i1 %346, i1* %of
  store volatile i64 58978, i64* @assembly_address
  %347 = load i64* %rax
  %348 = trunc i64 %347 to i32
  %349 = zext i32 %348 to i64
  store i64 %349, i64* %rcx
  store volatile i64 58980, i64* @assembly_address
  %350 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %351 = zext i32 %350 to i64
  store i64 %351, i64* %rax
  store volatile i64 58986, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 58989, i64* @assembly_address
  %352 = load i64* %rdx
  %353 = trunc i64 %352 to i32
  store i32 %353, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 58995, i64* @assembly_address
  %354 = load i64* %rax
  %355 = trunc i64 %354 to i32
  %356 = zext i32 %355 to i64
  store i64 %356, i64* %rdx
  store volatile i64 58997, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59004, i64* @assembly_address
  %357 = load i64* %rcx
  %358 = trunc i64 %357 to i8
  %359 = load i64* %rdx
  %360 = load i64* %rax
  %361 = mul i64 %360, 1
  %362 = add i64 %359, %361
  %363 = inttoptr i64 %362 to i8*
  store i8 %358, i8* %363
  store volatile i64 59007, i64* @assembly_address
  br label %block_e6eb

block_e681:                                       ; preds = %block_e62c
  store volatile i64 59009, i64* @assembly_address
  %364 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %365 = zext i32 %364 to i64
  store i64 %365, i64* %rax
  store volatile i64 59015, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59018, i64* @assembly_address
  %366 = load i64* %rdx
  %367 = trunc i64 %366 to i32
  store i32 %367, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59024, i64* @assembly_address
  %368 = load i32** %stack_var_-32
  %369 = ptrtoint i32* %368 to i64
  store i64 %369, i64* %rdx
  store volatile i64 59028, i64* @assembly_address
  %370 = load i64* %rdx
  %371 = trunc i64 %370 to i32
  %372 = zext i32 %371 to i64
  store i64 %372, i64* %rcx
  store volatile i64 59030, i64* @assembly_address
  %373 = load i64* %rax
  %374 = trunc i64 %373 to i32
  %375 = zext i32 %374 to i64
  store i64 %375, i64* %rdx
  store volatile i64 59032, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59039, i64* @assembly_address
  %376 = load i64* %rcx
  %377 = trunc i64 %376 to i8
  %378 = load i64* %rdx
  %379 = load i64* %rax
  %380 = mul i64 %379, 1
  %381 = add i64 %378, %380
  %382 = inttoptr i64 %381 to i8*
  store i8 %377, i8* %382
  store volatile i64 59042, i64* @assembly_address
  %383 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %384 = zext i32 %383 to i64
  store i64 %384, i64* %rax
  store volatile i64 59048, i64* @assembly_address
  %385 = load i64* %rax
  %386 = trunc i64 %385 to i32
  %387 = sub i32 %386, 16384
  %388 = and i32 %386, 15
  %389 = icmp ugt i32 %388, 15
  %390 = icmp ult i32 %386, 16384
  %391 = xor i32 %386, 16384
  %392 = xor i32 %386, %387
  %393 = and i32 %391, %392
  %394 = icmp slt i32 %393, 0
  store i1 %389, i1* %az
  store i1 %390, i1* %cf
  store i1 %394, i1* %of
  %395 = icmp eq i32 %387, 0
  store i1 %395, i1* %zf
  %396 = icmp slt i32 %387, 0
  store i1 %396, i1* %sf
  %397 = trunc i32 %387 to i8
  %398 = call i8 @llvm.ctpop.i8(i8 %397)
  %399 = and i8 %398, 1
  %400 = icmp eq i8 %399, 0
  store i1 %400, i1* %pf
  store volatile i64 59053, i64* @assembly_address
  %401 = load i1* %zf
  %402 = icmp eq i1 %401, false
  br i1 %402, label %block_e6b4, label %block_e6af

block_e6af:                                       ; preds = %block_e681
  store volatile i64 59055, i64* @assembly_address
  %403 = load i64* %rdi
  %404 = load i64* %rsi
  %405 = load i64* %rdx
  %406 = load i64* %rcx
  %407 = trunc i64 %406 to i16
  %408 = call i64 @flush_outbuf(i64 %403, i64 %404, i64 %405, i16 %407)
  store i64 %408, i64* %rax
  store i64 %408, i64* %rax
  store i64 %408, i64* %rax
  br label %block_e6b4

block_e6b4:                                       ; preds = %block_e6af, %block_e681
  store volatile i64 59060, i64* @assembly_address
  %409 = load i32** %stack_var_-32
  %410 = ptrtoint i32* %409 to i64
  store i64 %410, i64* %rax
  store volatile i64 59064, i64* @assembly_address
  %411 = load i64* %rax
  %412 = trunc i64 %411 to i16
  %413 = load i1* %of
  %414 = lshr i16 %412, 8
  %415 = icmp eq i16 %414, 0
  store i1 %415, i1* %zf
  %416 = icmp slt i16 %414, 0
  store i1 %416, i1* %sf
  %417 = trunc i16 %414 to i8
  %418 = call i8 @llvm.ctpop.i8(i8 %417)
  %419 = and i8 %418, 1
  %420 = icmp eq i8 %419, 0
  store i1 %420, i1* %pf
  %421 = zext i16 %414 to i64
  %422 = load i64* %rax
  %423 = and i64 %422, -65536
  %424 = or i64 %423, %421
  store i64 %424, i64* %rax
  %425 = and i16 128, %412
  %426 = icmp ne i16 %425, 0
  store i1 %426, i1* %cf
  %427 = icmp slt i16 %412, 0
  %428 = select i1 false, i1 %427, i1 %413
  store i1 %428, i1* %of
  store volatile i64 59068, i64* @assembly_address
  %429 = load i64* %rax
  %430 = trunc i64 %429 to i32
  %431 = zext i32 %430 to i64
  store i64 %431, i64* %rcx
  store volatile i64 59070, i64* @assembly_address
  %432 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %433 = zext i32 %432 to i64
  store i64 %433, i64* %rax
  store volatile i64 59076, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59079, i64* @assembly_address
  %434 = load i64* %rdx
  %435 = trunc i64 %434 to i32
  store i32 %435, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59085, i64* @assembly_address
  %436 = load i64* %rax
  %437 = trunc i64 %436 to i32
  %438 = zext i32 %437 to i64
  store i64 %438, i64* %rdx
  store volatile i64 59087, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59094, i64* @assembly_address
  %439 = load i64* %rcx
  %440 = trunc i64 %439 to i8
  %441 = load i64* %rdx
  %442 = load i64* %rax
  %443 = mul i64 %442, 1
  %444 = add i64 %441, %443
  %445 = inttoptr i64 %444 to i8*
  store i8 %440, i8* %445
  store volatile i64 59097, i64* @assembly_address
  %446 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %447 = zext i32 %446 to i64
  store i64 %447, i64* %rax
  store volatile i64 59103, i64* @assembly_address
  %448 = load i64* %rax
  %449 = trunc i64 %448 to i32
  %450 = sub i32 %449, 16384
  %451 = and i32 %449, 15
  %452 = icmp ugt i32 %451, 15
  %453 = icmp ult i32 %449, 16384
  %454 = xor i32 %449, 16384
  %455 = xor i32 %449, %450
  %456 = and i32 %454, %455
  %457 = icmp slt i32 %456, 0
  store i1 %452, i1* %az
  store i1 %453, i1* %cf
  store i1 %457, i1* %of
  %458 = icmp eq i32 %450, 0
  store i1 %458, i1* %zf
  %459 = icmp slt i32 %450, 0
  store i1 %459, i1* %sf
  %460 = trunc i32 %450 to i8
  %461 = call i8 @llvm.ctpop.i8(i8 %460)
  %462 = and i8 %461, 1
  %463 = icmp eq i8 %462, 0
  store i1 %463, i1* %pf
  store volatile i64 59108, i64* @assembly_address
  %464 = load i1* %zf
  %465 = icmp eq i1 %464, false
  br i1 %465, label %block_e6eb, label %block_e6e6

block_e6e6:                                       ; preds = %block_e6b4
  store volatile i64 59110, i64* @assembly_address
  %466 = load i64* %rdi
  %467 = load i64* %rsi
  %468 = load i64* %rdx
  %469 = load i64* %rcx
  %470 = trunc i64 %469 to i16
  %471 = call i64 @flush_outbuf(i64 %466, i64 %467, i64 %468, i16 %470)
  store i64 %471, i64* %rax
  store i64 %471, i64* %rax
  store i64 %471, i64* %rax
  br label %block_e6eb

block_e6eb:                                       ; preds = %block_e6e6, %block_e6b4, %block_e639
  store volatile i64 59115, i64* @assembly_address
  %472 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %473 = zext i32 %472 to i64
  store i64 %473, i64* %rax
  store volatile i64 59121, i64* @assembly_address
  %474 = load i64* %rax
  %475 = trunc i64 %474 to i32
  %476 = sub i32 %475, 16381
  %477 = and i32 %475, 15
  %478 = sub i32 %477, 13
  %479 = icmp ugt i32 %478, 15
  %480 = icmp ult i32 %475, 16381
  %481 = xor i32 %475, 16381
  %482 = xor i32 %475, %476
  %483 = and i32 %481, %482
  %484 = icmp slt i32 %483, 0
  store i1 %479, i1* %az
  store i1 %480, i1* %cf
  store i1 %484, i1* %of
  %485 = icmp eq i32 %476, 0
  store i1 %485, i1* %zf
  %486 = icmp slt i32 %476, 0
  store i1 %486, i1* %sf
  %487 = trunc i32 %476 to i8
  %488 = call i8 @llvm.ctpop.i8(i8 %487)
  %489 = and i8 %488, 1
  %490 = icmp eq i8 %489, 0
  store i1 %490, i1* %pf
  store volatile i64 59126, i64* @assembly_address
  %491 = load i1* %cf
  %492 = load i1* %zf
  %493 = or i1 %491, %492
  %494 = icmp ne i1 %493, true
  br i1 %494, label %block_e749, label %block_e6f8

block_e6f8:                                       ; preds = %block_e6eb
  store volatile i64 59128, i64* @assembly_address
  %495 = load i32** %stack_var_-32
  %496 = ptrtoint i32* %495 to i64
  store i64 %496, i64* %rax
  store volatile i64 59132, i64* @assembly_address
  %497 = load i64* %rax
  %498 = load i1* %of
  %499 = lshr i64 %497, 16
  %500 = icmp eq i64 %499, 0
  store i1 %500, i1* %zf
  %501 = icmp slt i64 %499, 0
  store i1 %501, i1* %sf
  %502 = trunc i64 %499 to i8
  %503 = call i8 @llvm.ctpop.i8(i8 %502)
  %504 = and i8 %503, 1
  %505 = icmp eq i8 %504, 0
  store i1 %505, i1* %pf
  store i64 %499, i64* %rax
  %506 = and i64 32768, %497
  %507 = icmp ne i64 %506, 0
  store i1 %507, i1* %cf
  %508 = icmp slt i64 %497, 0
  %509 = select i1 false, i1 %508, i1 %498
  store i1 %509, i1* %of
  store volatile i64 59136, i64* @assembly_address
  %510 = load i64* %rax
  store i64 %510, i64* %rcx
  store volatile i64 59139, i64* @assembly_address
  %511 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %512 = zext i32 %511 to i64
  store i64 %512, i64* %rax
  store volatile i64 59145, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59148, i64* @assembly_address
  %513 = load i64* %rdx
  %514 = trunc i64 %513 to i32
  store i32 %514, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59154, i64* @assembly_address
  %515 = load i64* %rax
  %516 = trunc i64 %515 to i32
  %517 = zext i32 %516 to i64
  store i64 %517, i64* %rdx
  store volatile i64 59156, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59163, i64* @assembly_address
  %518 = load i64* %rcx
  %519 = trunc i64 %518 to i8
  %520 = load i64* %rdx
  %521 = load i64* %rax
  %522 = mul i64 %521, 1
  %523 = add i64 %520, %522
  %524 = inttoptr i64 %523 to i8*
  store i8 %519, i8* %524
  store volatile i64 59166, i64* @assembly_address
  %525 = load i32** %stack_var_-32
  %526 = ptrtoint i32* %525 to i64
  store i64 %526, i64* %rax
  store volatile i64 59170, i64* @assembly_address
  %527 = load i64* %rax
  %528 = load i1* %of
  %529 = lshr i64 %527, 16
  %530 = icmp eq i64 %529, 0
  store i1 %530, i1* %zf
  %531 = icmp slt i64 %529, 0
  store i1 %531, i1* %sf
  %532 = trunc i64 %529 to i8
  %533 = call i8 @llvm.ctpop.i8(i8 %532)
  %534 = and i8 %533, 1
  %535 = icmp eq i8 %534, 0
  store i1 %535, i1* %pf
  store i64 %529, i64* %rax
  %536 = and i64 32768, %527
  %537 = icmp ne i64 %536, 0
  store i1 %537, i1* %cf
  %538 = icmp slt i64 %527, 0
  %539 = select i1 false, i1 %538, i1 %528
  store i1 %539, i1* %of
  store volatile i64 59174, i64* @assembly_address
  %540 = load i64* %rax
  %541 = trunc i64 %540 to i16
  %542 = load i1* %of
  %543 = lshr i16 %541, 8
  %544 = icmp eq i16 %543, 0
  store i1 %544, i1* %zf
  %545 = icmp slt i16 %543, 0
  store i1 %545, i1* %sf
  %546 = trunc i16 %543 to i8
  %547 = call i8 @llvm.ctpop.i8(i8 %546)
  %548 = and i8 %547, 1
  %549 = icmp eq i8 %548, 0
  store i1 %549, i1* %pf
  %550 = zext i16 %543 to i64
  %551 = load i64* %rax
  %552 = and i64 %551, -65536
  %553 = or i64 %552, %550
  store i64 %553, i64* %rax
  %554 = and i16 128, %541
  %555 = icmp ne i16 %554, 0
  store i1 %555, i1* %cf
  %556 = icmp slt i16 %541, 0
  %557 = select i1 false, i1 %556, i1 %542
  store i1 %557, i1* %of
  store volatile i64 59178, i64* @assembly_address
  %558 = load i64* %rax
  %559 = trunc i64 %558 to i32
  %560 = zext i32 %559 to i64
  store i64 %560, i64* %rcx
  store volatile i64 59180, i64* @assembly_address
  %561 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %562 = zext i32 %561 to i64
  store i64 %562, i64* %rax
  store volatile i64 59186, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59189, i64* @assembly_address
  %563 = load i64* %rdx
  %564 = trunc i64 %563 to i32
  store i32 %564, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59195, i64* @assembly_address
  %565 = load i64* %rax
  %566 = trunc i64 %565 to i32
  %567 = zext i32 %566 to i64
  store i64 %567, i64* %rdx
  store volatile i64 59197, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59204, i64* @assembly_address
  %568 = load i64* %rcx
  %569 = trunc i64 %568 to i8
  %570 = load i64* %rdx
  %571 = load i64* %rax
  %572 = mul i64 %571, 1
  %573 = add i64 %570, %572
  %574 = inttoptr i64 %573 to i8*
  store i8 %569, i8* %574
  store volatile i64 59207, i64* @assembly_address
  br label %block_e7bc

block_e749:                                       ; preds = %block_e6eb
  store volatile i64 59209, i64* @assembly_address
  %575 = load i32** %stack_var_-32
  %576 = ptrtoint i32* %575 to i64
  store i64 %576, i64* %rax
  store volatile i64 59213, i64* @assembly_address
  %577 = load i64* %rax
  %578 = load i1* %of
  %579 = lshr i64 %577, 16
  %580 = icmp eq i64 %579, 0
  store i1 %580, i1* %zf
  %581 = icmp slt i64 %579, 0
  store i1 %581, i1* %sf
  %582 = trunc i64 %579 to i8
  %583 = call i8 @llvm.ctpop.i8(i8 %582)
  %584 = and i8 %583, 1
  %585 = icmp eq i8 %584, 0
  store i1 %585, i1* %pf
  store i64 %579, i64* %rax
  %586 = and i64 32768, %577
  %587 = icmp ne i64 %586, 0
  store i1 %587, i1* %cf
  %588 = icmp slt i64 %577, 0
  %589 = select i1 false, i1 %588, i1 %578
  store i1 %589, i1* %of
  store volatile i64 59217, i64* @assembly_address
  %590 = load i64* %rax
  store i64 %590, i64* %rcx
  store volatile i64 59220, i64* @assembly_address
  %591 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %592 = zext i32 %591 to i64
  store i64 %592, i64* %rax
  store volatile i64 59226, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59229, i64* @assembly_address
  %593 = load i64* %rdx
  %594 = trunc i64 %593 to i32
  store i32 %594, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59235, i64* @assembly_address
  %595 = load i64* %rax
  %596 = trunc i64 %595 to i32
  %597 = zext i32 %596 to i64
  store i64 %597, i64* %rdx
  store volatile i64 59237, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59244, i64* @assembly_address
  %598 = load i64* %rcx
  %599 = trunc i64 %598 to i8
  %600 = load i64* %rdx
  %601 = load i64* %rax
  %602 = mul i64 %601, 1
  %603 = add i64 %600, %602
  %604 = inttoptr i64 %603 to i8*
  store i8 %599, i8* %604
  store volatile i64 59247, i64* @assembly_address
  %605 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %606 = zext i32 %605 to i64
  store i64 %606, i64* %rax
  store volatile i64 59253, i64* @assembly_address
  %607 = load i64* %rax
  %608 = trunc i64 %607 to i32
  %609 = sub i32 %608, 16384
  %610 = and i32 %608, 15
  %611 = icmp ugt i32 %610, 15
  %612 = icmp ult i32 %608, 16384
  %613 = xor i32 %608, 16384
  %614 = xor i32 %608, %609
  %615 = and i32 %613, %614
  %616 = icmp slt i32 %615, 0
  store i1 %611, i1* %az
  store i1 %612, i1* %cf
  store i1 %616, i1* %of
  %617 = icmp eq i32 %609, 0
  store i1 %617, i1* %zf
  %618 = icmp slt i32 %609, 0
  store i1 %618, i1* %sf
  %619 = trunc i32 %609 to i8
  %620 = call i8 @llvm.ctpop.i8(i8 %619)
  %621 = and i8 %620, 1
  %622 = icmp eq i8 %621, 0
  store i1 %622, i1* %pf
  store volatile i64 59258, i64* @assembly_address
  %623 = load i1* %zf
  %624 = icmp eq i1 %623, false
  br i1 %624, label %block_e781, label %block_e77c

block_e77c:                                       ; preds = %block_e749
  store volatile i64 59260, i64* @assembly_address
  %625 = load i64* %rdi
  %626 = load i64* %rsi
  %627 = load i64* %rdx
  %628 = load i64* %rcx
  %629 = trunc i64 %628 to i16
  %630 = call i64 @flush_outbuf(i64 %625, i64 %626, i64 %627, i16 %629)
  store i64 %630, i64* %rax
  store i64 %630, i64* %rax
  store i64 %630, i64* %rax
  br label %block_e781

block_e781:                                       ; preds = %block_e77c, %block_e749
  store volatile i64 59265, i64* @assembly_address
  %631 = load i32** %stack_var_-32
  %632 = ptrtoint i32* %631 to i64
  store i64 %632, i64* %rax
  store volatile i64 59269, i64* @assembly_address
  %633 = load i64* %rax
  %634 = load i1* %of
  %635 = lshr i64 %633, 16
  %636 = icmp eq i64 %635, 0
  store i1 %636, i1* %zf
  %637 = icmp slt i64 %635, 0
  store i1 %637, i1* %sf
  %638 = trunc i64 %635 to i8
  %639 = call i8 @llvm.ctpop.i8(i8 %638)
  %640 = and i8 %639, 1
  %641 = icmp eq i8 %640, 0
  store i1 %641, i1* %pf
  store i64 %635, i64* %rax
  %642 = and i64 32768, %633
  %643 = icmp ne i64 %642, 0
  store i1 %643, i1* %cf
  %644 = icmp slt i64 %633, 0
  %645 = select i1 false, i1 %644, i1 %634
  store i1 %645, i1* %of
  store volatile i64 59273, i64* @assembly_address
  %646 = load i64* %rax
  %647 = trunc i64 %646 to i16
  %648 = load i1* %of
  %649 = lshr i16 %647, 8
  %650 = icmp eq i16 %649, 0
  store i1 %650, i1* %zf
  %651 = icmp slt i16 %649, 0
  store i1 %651, i1* %sf
  %652 = trunc i16 %649 to i8
  %653 = call i8 @llvm.ctpop.i8(i8 %652)
  %654 = and i8 %653, 1
  %655 = icmp eq i8 %654, 0
  store i1 %655, i1* %pf
  %656 = zext i16 %649 to i64
  %657 = load i64* %rax
  %658 = and i64 %657, -65536
  %659 = or i64 %658, %656
  store i64 %659, i64* %rax
  %660 = and i16 128, %647
  %661 = icmp ne i16 %660, 0
  store i1 %661, i1* %cf
  %662 = icmp slt i16 %647, 0
  %663 = select i1 false, i1 %662, i1 %648
  store i1 %663, i1* %of
  store volatile i64 59277, i64* @assembly_address
  %664 = load i64* %rax
  %665 = trunc i64 %664 to i32
  %666 = zext i32 %665 to i64
  store i64 %666, i64* %rcx
  store volatile i64 59279, i64* @assembly_address
  %667 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %668 = zext i32 %667 to i64
  store i64 %668, i64* %rax
  store volatile i64 59285, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59288, i64* @assembly_address
  %669 = load i64* %rdx
  %670 = trunc i64 %669 to i32
  store i32 %670, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59294, i64* @assembly_address
  %671 = load i64* %rax
  %672 = trunc i64 %671 to i32
  %673 = zext i32 %672 to i64
  store i64 %673, i64* %rdx
  store volatile i64 59296, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59303, i64* @assembly_address
  %674 = load i64* %rcx
  %675 = trunc i64 %674 to i8
  %676 = load i64* %rdx
  %677 = load i64* %rax
  %678 = mul i64 %677, 1
  %679 = add i64 %676, %678
  %680 = inttoptr i64 %679 to i8*
  store i8 %675, i8* %680
  store volatile i64 59306, i64* @assembly_address
  %681 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %682 = zext i32 %681 to i64
  store i64 %682, i64* %rax
  store volatile i64 59312, i64* @assembly_address
  %683 = load i64* %rax
  %684 = trunc i64 %683 to i32
  %685 = sub i32 %684, 16384
  %686 = and i32 %684, 15
  %687 = icmp ugt i32 %686, 15
  %688 = icmp ult i32 %684, 16384
  %689 = xor i32 %684, 16384
  %690 = xor i32 %684, %685
  %691 = and i32 %689, %690
  %692 = icmp slt i32 %691, 0
  store i1 %687, i1* %az
  store i1 %688, i1* %cf
  store i1 %692, i1* %of
  %693 = icmp eq i32 %685, 0
  store i1 %693, i1* %zf
  %694 = icmp slt i32 %685, 0
  store i1 %694, i1* %sf
  %695 = trunc i32 %685 to i8
  %696 = call i8 @llvm.ctpop.i8(i8 %695)
  %697 = and i8 %696, 1
  %698 = icmp eq i8 %697, 0
  store i1 %698, i1* %pf
  store volatile i64 59317, i64* @assembly_address
  %699 = load i1* %zf
  %700 = icmp eq i1 %699, false
  br i1 %700, label %block_e7bc, label %block_e7b7

block_e7b7:                                       ; preds = %block_e781
  store volatile i64 59319, i64* @assembly_address
  %701 = load i64* %rdi
  %702 = load i64* %rsi
  %703 = load i64* %rdx
  %704 = load i64* %rcx
  %705 = trunc i64 %704 to i16
  %706 = call i64 @flush_outbuf(i64 %701, i64 %702, i64 %703, i16 %705)
  store i64 %706, i64* %rax
  store i64 %706, i64* %rax
  store i64 %706, i64* %rax
  br label %block_e7bc

block_e7bc:                                       ; preds = %block_e7b7, %block_e781, %block_e6f8
  store volatile i64 59324, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 59329, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 59334, i64* @assembly_address
  %707 = load i64* %rdi
  %708 = inttoptr i64 %707 to i8*
  %709 = load i64* %rsi
  %710 = trunc i64 %709 to i32
  %711 = call i64 @updcrc(i8* %708, i32 %710)
  store i64 %711, i64* %rax
  store i64 %711, i64* %rax
  store volatile i64 59339, i64* @assembly_address
  %712 = load i64* %rax
  store i64 %712, i64* @global_var_21a408
  store volatile i64 59346, i64* @assembly_address
  %713 = load i32* %stack_var_-48
  %714 = zext i32 %713 to i64
  store i64 %714, i64* %rax
  store volatile i64 59349, i64* @assembly_address
  %715 = load i64* %rax
  %716 = trunc i64 %715 to i32
  %717 = zext i32 %716 to i64
  store i64 %717, i64* %rdi
  store volatile i64 59351, i64* @assembly_address
  %718 = load i64* %rdi
  %719 = trunc i64 %718 to i32
  %720 = call i64 @bi_init(i32 %719)
  store i64 %720, i64* %rax
  store i64 %720, i64* %rax
  store volatile i64 59356, i64* @assembly_address
  %721 = ptrtoint i16* %stack_var_-36 to i64
  store i64 %721, i64* %rax
  store volatile i64 59360, i64* @assembly_address
  store i64 ptrtoint ([2 x i8]* @global_var_21609c to i64), i64* %rsi
  store volatile i64 59367, i64* @assembly_address
  %722 = ptrtoint i16* %stack_var_-36 to i64
  store i64 %722, i64* %rdi
  store volatile i64 59370, i64* @assembly_address
  %723 = load i64* %rdi
  %724 = inttoptr i64 %723 to i16*
  %725 = load i64* %rsi
  %726 = inttoptr i64 %725 to i8*
  %727 = call i64 @ct_init(i16* %724, i8* %726)
  store i64 %727, i64* %rax
  store i64 %727, i64* %rax
  store volatile i64 59375, i64* @assembly_address
  %728 = load i32* bitcast (i64* @global_var_2160a0 to i32*)
  %729 = zext i32 %728 to i64
  store i64 %729, i64* %rax
  store volatile i64 59381, i64* @assembly_address
  %730 = ptrtoint i16* %stack_var_-34 to i64
  store i64 %730, i64* %rdx
  store volatile i64 59385, i64* @assembly_address
  %731 = ptrtoint i16* %stack_var_-34 to i64
  store i64 %731, i64* %rsi
  store volatile i64 59388, i64* @assembly_address
  %732 = load i64* %rax
  %733 = trunc i64 %732 to i32
  %734 = zext i32 %733 to i64
  store i64 %734, i64* %rdi
  store volatile i64 59390, i64* @assembly_address
  %735 = load i64* %rdi
  %736 = load i64* %rsi
  %737 = inttoptr i64 %736 to i16*
  %738 = trunc i64 %735 to i32
  %739 = call i64 @lm_init(i32 %738, i16* %737)
  store i64 %739, i64* %rax
  store i64 %739, i64* %rax
  store volatile i64 59395, i64* @assembly_address
  %740 = load i16* %stack_var_-34
  %741 = zext i16 %740 to i64
  store i64 %741, i64* %rcx
  store volatile i64 59399, i64* @assembly_address
  %742 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %743 = zext i32 %742 to i64
  store i64 %743, i64* %rax
  store volatile i64 59405, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59408, i64* @assembly_address
  %744 = load i64* %rdx
  %745 = trunc i64 %744 to i32
  store i32 %745, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59414, i64* @assembly_address
  %746 = load i64* %rax
  %747 = trunc i64 %746 to i32
  %748 = zext i32 %747 to i64
  store i64 %748, i64* %rdx
  store volatile i64 59416, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59423, i64* @assembly_address
  %749 = load i64* %rcx
  %750 = trunc i64 %749 to i8
  %751 = load i64* %rdx
  %752 = load i64* %rax
  %753 = mul i64 %752, 1
  %754 = add i64 %751, %753
  %755 = inttoptr i64 %754 to i8*
  store i8 %750, i8* %755
  store volatile i64 59426, i64* @assembly_address
  %756 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %757 = zext i32 %756 to i64
  store i64 %757, i64* %rax
  store volatile i64 59432, i64* @assembly_address
  %758 = load i64* %rax
  %759 = trunc i64 %758 to i32
  %760 = sub i32 %759, 16384
  %761 = and i32 %759, 15
  %762 = icmp ugt i32 %761, 15
  %763 = icmp ult i32 %759, 16384
  %764 = xor i32 %759, 16384
  %765 = xor i32 %759, %760
  %766 = and i32 %764, %765
  %767 = icmp slt i32 %766, 0
  store i1 %762, i1* %az
  store i1 %763, i1* %cf
  store i1 %767, i1* %of
  %768 = icmp eq i32 %760, 0
  store i1 %768, i1* %zf
  %769 = icmp slt i32 %760, 0
  store i1 %769, i1* %sf
  %770 = trunc i32 %760 to i8
  %771 = call i8 @llvm.ctpop.i8(i8 %770)
  %772 = and i8 %771, 1
  %773 = icmp eq i8 %772, 0
  store i1 %773, i1* %pf
  store volatile i64 59437, i64* @assembly_address
  %774 = load i1* %zf
  %775 = icmp eq i1 %774, false
  br i1 %775, label %block_e834, label %block_e82f

block_e82f:                                       ; preds = %block_e7bc
  store volatile i64 59439, i64* @assembly_address
  %776 = load i64* %rdi
  %777 = load i64* %rsi
  %778 = load i64* %rdx
  %779 = load i64* %rcx
  %780 = trunc i64 %779 to i16
  %781 = call i64 @flush_outbuf(i64 %776, i64 %777, i64 %778, i16 %780)
  store i64 %781, i64* %rax
  store i64 %781, i64* %rax
  store i64 %781, i64* %rax
  br label %block_e834

block_e834:                                       ; preds = %block_e82f, %block_e7bc
  store volatile i64 59444, i64* @assembly_address
  %782 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %783 = zext i32 %782 to i64
  store i64 %783, i64* %rax
  store volatile i64 59450, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59453, i64* @assembly_address
  %784 = load i64* %rdx
  %785 = trunc i64 %784 to i32
  store i32 %785, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59459, i64* @assembly_address
  %786 = load i64* %rax
  %787 = trunc i64 %786 to i32
  %788 = zext i32 %787 to i64
  store i64 %788, i64* %rdx
  store volatile i64 59461, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59468, i64* @assembly_address
  %789 = load i64* %rdx
  %790 = load i64* %rax
  %791 = mul i64 %790, 1
  %792 = add i64 %789, %791
  %793 = inttoptr i64 %792 to i8*
  store i8 3, i8* %793
  store volatile i64 59472, i64* @assembly_address
  %794 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %795 = zext i32 %794 to i64
  store i64 %795, i64* %rax
  store volatile i64 59478, i64* @assembly_address
  %796 = load i64* %rax
  %797 = trunc i64 %796 to i32
  %798 = sub i32 %797, 16384
  %799 = and i32 %797, 15
  %800 = icmp ugt i32 %799, 15
  %801 = icmp ult i32 %797, 16384
  %802 = xor i32 %797, 16384
  %803 = xor i32 %797, %798
  %804 = and i32 %802, %803
  %805 = icmp slt i32 %804, 0
  store i1 %800, i1* %az
  store i1 %801, i1* %cf
  store i1 %805, i1* %of
  %806 = icmp eq i32 %798, 0
  store i1 %806, i1* %zf
  %807 = icmp slt i32 %798, 0
  store i1 %807, i1* %sf
  %808 = trunc i32 %798 to i8
  %809 = call i8 @llvm.ctpop.i8(i8 %808)
  %810 = and i8 %809, 1
  %811 = icmp eq i8 %810, 0
  store i1 %811, i1* %pf
  store volatile i64 59483, i64* @assembly_address
  %812 = load i1* %zf
  %813 = icmp eq i1 %812, false
  br i1 %813, label %block_e862, label %block_e85d

block_e85d:                                       ; preds = %block_e834
  store volatile i64 59485, i64* @assembly_address
  %814 = load i64* %rdi
  %815 = load i64* %rsi
  %816 = load i64* %rdx
  %817 = load i64* %rcx
  %818 = trunc i64 %817 to i16
  %819 = call i64 @flush_outbuf(i64 %814, i64 %815, i64 %816, i16 %818)
  store i64 %819, i64* %rax
  store i64 %819, i64* %rax
  store i64 %819, i64* %rax
  br label %block_e862

block_e862:                                       ; preds = %block_e85d, %block_e834
  store volatile i64 59490, i64* @assembly_address
  %820 = load i32* bitcast (i64* @global_var_24a888 to i32*)
  %821 = zext i32 %820 to i64
  store i64 %821, i64* %rax
  store volatile i64 59496, i64* @assembly_address
  %822 = load i64* %rax
  %823 = trunc i64 %822 to i32
  %824 = load i64* %rax
  %825 = trunc i64 %824 to i32
  %826 = and i32 %823, %825
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %827 = icmp eq i32 %826, 0
  store i1 %827, i1* %zf
  %828 = icmp slt i32 %826, 0
  store i1 %828, i1* %sf
  %829 = trunc i32 %826 to i8
  %830 = call i8 @llvm.ctpop.i8(i8 %829)
  %831 = and i8 %830, 1
  %832 = icmp eq i8 %831, 0
  store i1 %832, i1* %pf
  store volatile i64 59498, i64* @assembly_address
  %833 = load i1* %zf
  br i1 %833, label %block_e8c3, label %block_e86c

block_e86c:                                       ; preds = %block_e862
  store volatile i64 59500, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rdi
  store volatile i64 59507, i64* @assembly_address
  %834 = load i64* %rdi
  %835 = inttoptr i64 %834 to i64*
  %836 = call i64 @gzip_base_name(i64* %835)
  store i64 %836, i64* %rax
  store i64 %836, i64* %rax
  store volatile i64 59512, i64* @assembly_address
  %837 = load i64* %rax
  %838 = inttoptr i64 %837 to i8*
  store i8* %838, i8** %stack_var_-24
  br label %block_e87c

block_e87c:                                       ; preds = %block_e8b0, %block_e86c
  store volatile i64 59516, i64* @assembly_address
  %839 = load i8** %stack_var_-24
  %840 = ptrtoint i8* %839 to i64
  store i64 %840, i64* %rax
  store volatile i64 59520, i64* @assembly_address
  %841 = load i64* %rax
  %842 = inttoptr i64 %841 to i8*
  %843 = load i8* %842
  %844 = zext i8 %843 to i64
  store i64 %844, i64* %rcx
  store volatile i64 59523, i64* @assembly_address
  %845 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %846 = zext i32 %845 to i64
  store i64 %846, i64* %rax
  store volatile i64 59529, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59532, i64* @assembly_address
  %847 = load i64* %rdx
  %848 = trunc i64 %847 to i32
  store i32 %848, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59538, i64* @assembly_address
  %849 = load i64* %rax
  %850 = trunc i64 %849 to i32
  %851 = zext i32 %850 to i64
  store i64 %851, i64* %rdx
  store volatile i64 59540, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59547, i64* @assembly_address
  %852 = load i64* %rcx
  %853 = trunc i64 %852 to i8
  %854 = load i64* %rdx
  %855 = load i64* %rax
  %856 = mul i64 %855, 1
  %857 = add i64 %854, %856
  %858 = inttoptr i64 %857 to i8*
  store i8 %853, i8* %858
  store volatile i64 59550, i64* @assembly_address
  %859 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %860 = zext i32 %859 to i64
  store i64 %860, i64* %rax
  store volatile i64 59556, i64* @assembly_address
  %861 = load i64* %rax
  %862 = trunc i64 %861 to i32
  %863 = sub i32 %862, 16384
  %864 = and i32 %862, 15
  %865 = icmp ugt i32 %864, 15
  %866 = icmp ult i32 %862, 16384
  %867 = xor i32 %862, 16384
  %868 = xor i32 %862, %863
  %869 = and i32 %867, %868
  %870 = icmp slt i32 %869, 0
  store i1 %865, i1* %az
  store i1 %866, i1* %cf
  store i1 %870, i1* %of
  %871 = icmp eq i32 %863, 0
  store i1 %871, i1* %zf
  %872 = icmp slt i32 %863, 0
  store i1 %872, i1* %sf
  %873 = trunc i32 %863 to i8
  %874 = call i8 @llvm.ctpop.i8(i8 %873)
  %875 = and i8 %874, 1
  %876 = icmp eq i8 %875, 0
  store i1 %876, i1* %pf
  store volatile i64 59561, i64* @assembly_address
  %877 = load i1* %zf
  %878 = icmp eq i1 %877, false
  br i1 %878, label %block_e8b0, label %block_e8ab

block_e8ab:                                       ; preds = %block_e87c
  store volatile i64 59563, i64* @assembly_address
  %879 = load i64* %rdi
  %880 = load i64* %rsi
  %881 = load i64* %rdx
  %882 = load i64* %rcx
  %883 = trunc i64 %882 to i16
  %884 = call i64 @flush_outbuf(i64 %879, i64 %880, i64 %881, i16 %883)
  store i64 %884, i64* %rax
  store i64 %884, i64* %rax
  store i64 %884, i64* %rax
  br label %block_e8b0

block_e8b0:                                       ; preds = %block_e8ab, %block_e87c
  store volatile i64 59568, i64* @assembly_address
  %885 = load i8** %stack_var_-24
  %886 = ptrtoint i8* %885 to i64
  store i64 %886, i64* %rax
  store volatile i64 59572, i64* @assembly_address
  %887 = load i64* %rax
  %888 = add i64 %887, 1
  store i64 %888, i64* %rdx
  store volatile i64 59576, i64* @assembly_address
  %889 = load i64* %rdx
  %890 = inttoptr i64 %889 to i8*
  store i8* %890, i8** %stack_var_-24
  store volatile i64 59580, i64* @assembly_address
  %891 = load i64* %rax
  %892 = inttoptr i64 %891 to i8*
  %893 = load i8* %892
  %894 = zext i8 %893 to i64
  store i64 %894, i64* %rax
  store volatile i64 59583, i64* @assembly_address
  %895 = load i64* %rax
  %896 = trunc i64 %895 to i8
  %897 = load i64* %rax
  %898 = trunc i64 %897 to i8
  %899 = and i8 %896, %898
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %900 = icmp eq i8 %899, 0
  store i1 %900, i1* %zf
  %901 = icmp slt i8 %899, 0
  store i1 %901, i1* %sf
  %902 = call i8 @llvm.ctpop.i8(i8 %899)
  %903 = and i8 %902, 1
  %904 = icmp eq i8 %903, 0
  store i1 %904, i1* %pf
  store volatile i64 59585, i64* @assembly_address
  %905 = load i1* %zf
  %906 = icmp eq i1 %905, false
  br i1 %906, label %block_e87c, label %block_e8c3

block_e8c3:                                       ; preds = %block_e8b0, %block_e862
  store volatile i64 59587, i64* @assembly_address
  %907 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %908 = zext i32 %907 to i64
  store i64 %908, i64* %rax
  store volatile i64 59593, i64* @assembly_address
  %909 = load i64* %rax
  %910 = trunc i64 %909 to i32
  %911 = zext i32 %910 to i64
  store i64 %911, i64* %rax
  store volatile i64 59595, i64* @assembly_address
  %912 = load i64* %rax
  store i64 %912, i64* @global_var_267540
  store volatile i64 59602, i64* @assembly_address
  %913 = load i64* %rdi
  %914 = inttoptr i64 %913 to %z_stream_s*
  %915 = load i64* %rsi
  %916 = trunc i64 %915 to i32
  %917 = call i32 @deflate(%z_stream_s* %914, i32 %916)
  %918 = sext i32 %917 to i64
  store i64 %918, i64* %rax
  %919 = sext i32 %917 to i64
  store i64 %919, i64* %rax
  store volatile i64 59607, i64* @assembly_address
  %920 = load i64* @global_var_24a890
  store i64 %920, i64* %rax
  store volatile i64 59614, i64* @assembly_address
  %921 = load i64* %rax
  %922 = sub i64 %921, -1
  %923 = and i64 %921, 15
  %924 = sub i64 %923, 15
  %925 = icmp ugt i64 %924, 15
  %926 = icmp ult i64 %921, -1
  %927 = xor i64 %921, -1
  %928 = xor i64 %921, %922
  %929 = and i64 %927, %928
  %930 = icmp slt i64 %929, 0
  store i1 %925, i1* %az
  store i1 %926, i1* %cf
  store i1 %930, i1* %of
  %931 = icmp eq i64 %922, 0
  store i1 %931, i1* %zf
  %932 = icmp slt i64 %922, 0
  store i1 %932, i1* %sf
  %933 = trunc i64 %922 to i8
  %934 = call i8 @llvm.ctpop.i8(i8 %933)
  %935 = and i8 %934, 1
  %936 = icmp eq i8 %935, 0
  store i1 %936, i1* %pf
  store volatile i64 59618, i64* @assembly_address
  %937 = load i1* %zf
  br i1 %937, label %block_e920, label %block_e8e4

block_e8e4:                                       ; preds = %block_e8c3
  store volatile i64 59620, i64* @assembly_address
  %938 = load i64* @global_var_21a860
  store i64 %938, i64* %rdx
  store volatile i64 59627, i64* @assembly_address
  %939 = load i64* @global_var_24a890
  store i64 %939, i64* %rax
  store volatile i64 59634, i64* @assembly_address
  %940 = load i64* %rdx
  %941 = load i64* %rax
  %942 = sub i64 %940, %941
  %943 = and i64 %940, 15
  %944 = and i64 %941, 15
  %945 = sub i64 %943, %944
  %946 = icmp ugt i64 %945, 15
  %947 = icmp ult i64 %940, %941
  %948 = xor i64 %940, %941
  %949 = xor i64 %940, %942
  %950 = and i64 %948, %949
  %951 = icmp slt i64 %950, 0
  store i1 %946, i1* %az
  store i1 %947, i1* %cf
  store i1 %951, i1* %of
  %952 = icmp eq i64 %942, 0
  store i1 %952, i1* %zf
  %953 = icmp slt i64 %942, 0
  store i1 %953, i1* %sf
  %954 = trunc i64 %942 to i8
  %955 = call i8 @llvm.ctpop.i8(i8 %954)
  %956 = and i8 %955, 1
  %957 = icmp eq i8 %956, 0
  store i1 %957, i1* %pf
  store volatile i64 59637, i64* @assembly_address
  %958 = load i1* %zf
  br i1 %958, label %block_e920, label %block_e8f7

block_e8f7:                                       ; preds = %block_e8e4
  store volatile i64 59639, i64* @assembly_address
  %959 = load i64* @global_var_25f4c8
  store i64 %959, i64* %rdx
  store volatile i64 59646, i64* @assembly_address
  %960 = load i64* @global_var_216580
  store i64 %960, i64* %rax
  store volatile i64 59653, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_21a460 to i64), i64* %rcx
  store volatile i64 59660, i64* @assembly_address
  store i64 ptrtoint ([41 x i8]* @global_var_12ce8 to i64), i64* %rsi
  store volatile i64 59667, i64* @assembly_address
  %961 = load i64* %rax
  store i64 %961, i64* %rdi
  store volatile i64 59670, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 59675, i64* @assembly_address
  %962 = load i64* %rdi
  %963 = inttoptr i64 %962 to %_IO_FILE*
  %964 = load i64* %rsi
  %965 = inttoptr i64 %964 to i8*
  %966 = load i64* %rdx
  %967 = inttoptr i64 %966 to i8*
  %968 = load i64* %rcx
  %969 = inttoptr i64 %968 to i8*
  %970 = call i32 (%_IO_FILE*, i8*, ...)* @fprintf(%_IO_FILE* %963, i8* %965, i8* %967, i8* %969)
  %971 = sext i32 %970 to i64
  store i64 %971, i64* %rax
  %972 = sext i32 %970 to i64
  store i64 %972, i64* %rax
  br label %block_e920

block_e920:                                       ; preds = %block_e8f7, %block_e8e4, %block_e8c3
  store volatile i64 59680, i64* @assembly_address
  %973 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %974 = zext i32 %973 to i64
  store i64 %974, i64* %rax
  store volatile i64 59686, i64* @assembly_address
  %975 = load i64* %rax
  %976 = trunc i64 %975 to i32
  %977 = sub i32 %976, 16381
  %978 = and i32 %976, 15
  %979 = sub i32 %978, 13
  %980 = icmp ugt i32 %979, 15
  %981 = icmp ult i32 %976, 16381
  %982 = xor i32 %976, 16381
  %983 = xor i32 %976, %977
  %984 = and i32 %982, %983
  %985 = icmp slt i32 %984, 0
  store i1 %980, i1* %az
  store i1 %981, i1* %cf
  store i1 %985, i1* %of
  %986 = icmp eq i32 %977, 0
  store i1 %986, i1* %zf
  %987 = icmp slt i32 %977, 0
  store i1 %987, i1* %sf
  %988 = trunc i32 %977 to i8
  %989 = call i8 @llvm.ctpop.i8(i8 %988)
  %990 = and i8 %989, 1
  %991 = icmp eq i8 %990, 0
  store i1 %991, i1* %pf
  store volatile i64 59691, i64* @assembly_address
  %992 = load i1* %cf
  %993 = load i1* %zf
  %994 = or i1 %992, %993
  %995 = icmp ne i1 %994, true
  br i1 %995, label %block_e979, label %block_e92d

block_e92d:                                       ; preds = %block_e920
  store volatile i64 59693, i64* @assembly_address
  %996 = load i64* @global_var_21a408
  store i64 %996, i64* %rcx
  store volatile i64 59700, i64* @assembly_address
  %997 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %998 = zext i32 %997 to i64
  store i64 %998, i64* %rax
  store volatile i64 59706, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59709, i64* @assembly_address
  %999 = load i64* %rdx
  %1000 = trunc i64 %999 to i32
  store i32 %1000, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59715, i64* @assembly_address
  %1001 = load i64* %rax
  %1002 = trunc i64 %1001 to i32
  %1003 = zext i32 %1002 to i64
  store i64 %1003, i64* %rdx
  store volatile i64 59717, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59724, i64* @assembly_address
  %1004 = load i64* %rcx
  %1005 = trunc i64 %1004 to i8
  %1006 = load i64* %rdx
  %1007 = load i64* %rax
  %1008 = mul i64 %1007, 1
  %1009 = add i64 %1006, %1008
  %1010 = inttoptr i64 %1009 to i8*
  store i8 %1005, i8* %1010
  store volatile i64 59727, i64* @assembly_address
  %1011 = load i64* @global_var_21a408
  store i64 %1011, i64* %rax
  store volatile i64 59734, i64* @assembly_address
  %1012 = load i64* %rax
  %1013 = trunc i64 %1012 to i16
  %1014 = load i1* %of
  %1015 = lshr i16 %1013, 8
  %1016 = icmp eq i16 %1015, 0
  store i1 %1016, i1* %zf
  %1017 = icmp slt i16 %1015, 0
  store i1 %1017, i1* %sf
  %1018 = trunc i16 %1015 to i8
  %1019 = call i8 @llvm.ctpop.i8(i8 %1018)
  %1020 = and i8 %1019, 1
  %1021 = icmp eq i8 %1020, 0
  store i1 %1021, i1* %pf
  %1022 = zext i16 %1015 to i64
  %1023 = load i64* %rax
  %1024 = and i64 %1023, -65536
  %1025 = or i64 %1024, %1022
  store i64 %1025, i64* %rax
  %1026 = and i16 128, %1013
  %1027 = icmp ne i16 %1026, 0
  store i1 %1027, i1* %cf
  %1028 = icmp slt i16 %1013, 0
  %1029 = select i1 false, i1 %1028, i1 %1014
  store i1 %1029, i1* %of
  store volatile i64 59738, i64* @assembly_address
  %1030 = load i64* %rax
  %1031 = trunc i64 %1030 to i32
  %1032 = zext i32 %1031 to i64
  store i64 %1032, i64* %rcx
  store volatile i64 59740, i64* @assembly_address
  %1033 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1034 = zext i32 %1033 to i64
  store i64 %1034, i64* %rax
  store volatile i64 59746, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59749, i64* @assembly_address
  %1035 = load i64* %rdx
  %1036 = trunc i64 %1035 to i32
  store i32 %1036, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59755, i64* @assembly_address
  %1037 = load i64* %rax
  %1038 = trunc i64 %1037 to i32
  %1039 = zext i32 %1038 to i64
  store i64 %1039, i64* %rdx
  store volatile i64 59757, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59764, i64* @assembly_address
  %1040 = load i64* %rcx
  %1041 = trunc i64 %1040 to i8
  %1042 = load i64* %rdx
  %1043 = load i64* %rax
  %1044 = mul i64 %1043, 1
  %1045 = add i64 %1042, %1044
  %1046 = inttoptr i64 %1045 to i8*
  store i8 %1041, i8* %1046
  store volatile i64 59767, i64* @assembly_address
  br label %block_e9e7

block_e979:                                       ; preds = %block_e920
  store volatile i64 59769, i64* @assembly_address
  %1047 = load i64* @global_var_21a408
  store i64 %1047, i64* %rcx
  store volatile i64 59776, i64* @assembly_address
  %1048 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1049 = zext i32 %1048 to i64
  store i64 %1049, i64* %rax
  store volatile i64 59782, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59785, i64* @assembly_address
  %1050 = load i64* %rdx
  %1051 = trunc i64 %1050 to i32
  store i32 %1051, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59791, i64* @assembly_address
  %1052 = load i64* %rax
  %1053 = trunc i64 %1052 to i32
  %1054 = zext i32 %1053 to i64
  store i64 %1054, i64* %rdx
  store volatile i64 59793, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59800, i64* @assembly_address
  %1055 = load i64* %rcx
  %1056 = trunc i64 %1055 to i8
  %1057 = load i64* %rdx
  %1058 = load i64* %rax
  %1059 = mul i64 %1058, 1
  %1060 = add i64 %1057, %1059
  %1061 = inttoptr i64 %1060 to i8*
  store i8 %1056, i8* %1061
  store volatile i64 59803, i64* @assembly_address
  %1062 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1063 = zext i32 %1062 to i64
  store i64 %1063, i64* %rax
  store volatile i64 59809, i64* @assembly_address
  %1064 = load i64* %rax
  %1065 = trunc i64 %1064 to i32
  %1066 = sub i32 %1065, 16384
  %1067 = and i32 %1065, 15
  %1068 = icmp ugt i32 %1067, 15
  %1069 = icmp ult i32 %1065, 16384
  %1070 = xor i32 %1065, 16384
  %1071 = xor i32 %1065, %1066
  %1072 = and i32 %1070, %1071
  %1073 = icmp slt i32 %1072, 0
  store i1 %1068, i1* %az
  store i1 %1069, i1* %cf
  store i1 %1073, i1* %of
  %1074 = icmp eq i32 %1066, 0
  store i1 %1074, i1* %zf
  %1075 = icmp slt i32 %1066, 0
  store i1 %1075, i1* %sf
  %1076 = trunc i32 %1066 to i8
  %1077 = call i8 @llvm.ctpop.i8(i8 %1076)
  %1078 = and i8 %1077, 1
  %1079 = icmp eq i8 %1078, 0
  store i1 %1079, i1* %pf
  store volatile i64 59814, i64* @assembly_address
  %1080 = load i1* %zf
  %1081 = icmp eq i1 %1080, false
  br i1 %1081, label %block_e9ad, label %block_e9a8

block_e9a8:                                       ; preds = %block_e979
  store volatile i64 59816, i64* @assembly_address
  %1082 = load i64* %rdi
  %1083 = load i64* %rsi
  %1084 = load i64* %rdx
  %1085 = load i64* %rcx
  %1086 = trunc i64 %1085 to i16
  %1087 = call i64 @flush_outbuf(i64 %1082, i64 %1083, i64 %1084, i16 %1086)
  store i64 %1087, i64* %rax
  store i64 %1087, i64* %rax
  store i64 %1087, i64* %rax
  br label %block_e9ad

block_e9ad:                                       ; preds = %block_e9a8, %block_e979
  store volatile i64 59821, i64* @assembly_address
  %1088 = load i64* @global_var_21a408
  store i64 %1088, i64* %rax
  store volatile i64 59828, i64* @assembly_address
  %1089 = load i64* %rax
  %1090 = trunc i64 %1089 to i16
  %1091 = load i1* %of
  %1092 = lshr i16 %1090, 8
  %1093 = icmp eq i16 %1092, 0
  store i1 %1093, i1* %zf
  %1094 = icmp slt i16 %1092, 0
  store i1 %1094, i1* %sf
  %1095 = trunc i16 %1092 to i8
  %1096 = call i8 @llvm.ctpop.i8(i8 %1095)
  %1097 = and i8 %1096, 1
  %1098 = icmp eq i8 %1097, 0
  store i1 %1098, i1* %pf
  %1099 = zext i16 %1092 to i64
  %1100 = load i64* %rax
  %1101 = and i64 %1100, -65536
  %1102 = or i64 %1101, %1099
  store i64 %1102, i64* %rax
  %1103 = and i16 128, %1090
  %1104 = icmp ne i16 %1103, 0
  store i1 %1104, i1* %cf
  %1105 = icmp slt i16 %1090, 0
  %1106 = select i1 false, i1 %1105, i1 %1091
  store i1 %1106, i1* %of
  store volatile i64 59832, i64* @assembly_address
  %1107 = load i64* %rax
  %1108 = trunc i64 %1107 to i32
  %1109 = zext i32 %1108 to i64
  store i64 %1109, i64* %rcx
  store volatile i64 59834, i64* @assembly_address
  %1110 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1111 = zext i32 %1110 to i64
  store i64 %1111, i64* %rax
  store volatile i64 59840, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59843, i64* @assembly_address
  %1112 = load i64* %rdx
  %1113 = trunc i64 %1112 to i32
  store i32 %1113, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59849, i64* @assembly_address
  %1114 = load i64* %rax
  %1115 = trunc i64 %1114 to i32
  %1116 = zext i32 %1115 to i64
  store i64 %1116, i64* %rdx
  store volatile i64 59851, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59858, i64* @assembly_address
  %1117 = load i64* %rcx
  %1118 = trunc i64 %1117 to i8
  %1119 = load i64* %rdx
  %1120 = load i64* %rax
  %1121 = mul i64 %1120, 1
  %1122 = add i64 %1119, %1121
  %1123 = inttoptr i64 %1122 to i8*
  store i8 %1118, i8* %1123
  store volatile i64 59861, i64* @assembly_address
  %1124 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1125 = zext i32 %1124 to i64
  store i64 %1125, i64* %rax
  store volatile i64 59867, i64* @assembly_address
  %1126 = load i64* %rax
  %1127 = trunc i64 %1126 to i32
  %1128 = sub i32 %1127, 16384
  %1129 = and i32 %1127, 15
  %1130 = icmp ugt i32 %1129, 15
  %1131 = icmp ult i32 %1127, 16384
  %1132 = xor i32 %1127, 16384
  %1133 = xor i32 %1127, %1128
  %1134 = and i32 %1132, %1133
  %1135 = icmp slt i32 %1134, 0
  store i1 %1130, i1* %az
  store i1 %1131, i1* %cf
  store i1 %1135, i1* %of
  %1136 = icmp eq i32 %1128, 0
  store i1 %1136, i1* %zf
  %1137 = icmp slt i32 %1128, 0
  store i1 %1137, i1* %sf
  %1138 = trunc i32 %1128 to i8
  %1139 = call i8 @llvm.ctpop.i8(i8 %1138)
  %1140 = and i8 %1139, 1
  %1141 = icmp eq i8 %1140, 0
  store i1 %1141, i1* %pf
  store volatile i64 59872, i64* @assembly_address
  %1142 = load i1* %zf
  %1143 = icmp eq i1 %1142, false
  br i1 %1143, label %block_e9e7, label %block_e9e2

block_e9e2:                                       ; preds = %block_e9ad
  store volatile i64 59874, i64* @assembly_address
  %1144 = load i64* %rdi
  %1145 = load i64* %rsi
  %1146 = load i64* %rdx
  %1147 = load i64* %rcx
  %1148 = trunc i64 %1147 to i16
  %1149 = call i64 @flush_outbuf(i64 %1144, i64 %1145, i64 %1146, i16 %1148)
  store i64 %1149, i64* %rax
  store i64 %1149, i64* %rax
  store i64 %1149, i64* %rax
  br label %block_e9e7

block_e9e7:                                       ; preds = %block_e9e2, %block_e9ad, %block_e92d
  store volatile i64 59879, i64* @assembly_address
  %1150 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1151 = zext i32 %1150 to i64
  store i64 %1151, i64* %rax
  store volatile i64 59885, i64* @assembly_address
  %1152 = load i64* %rax
  %1153 = trunc i64 %1152 to i32
  %1154 = sub i32 %1153, 16381
  %1155 = and i32 %1153, 15
  %1156 = sub i32 %1155, 13
  %1157 = icmp ugt i32 %1156, 15
  %1158 = icmp ult i32 %1153, 16381
  %1159 = xor i32 %1153, 16381
  %1160 = xor i32 %1153, %1154
  %1161 = and i32 %1159, %1160
  %1162 = icmp slt i32 %1161, 0
  store i1 %1157, i1* %az
  store i1 %1158, i1* %cf
  store i1 %1162, i1* %of
  %1163 = icmp eq i32 %1154, 0
  store i1 %1163, i1* %zf
  %1164 = icmp slt i32 %1154, 0
  store i1 %1164, i1* %sf
  %1165 = trunc i32 %1154 to i8
  %1166 = call i8 @llvm.ctpop.i8(i8 %1165)
  %1167 = and i8 %1166, 1
  %1168 = icmp eq i8 %1167, 0
  store i1 %1168, i1* %pf
  store volatile i64 59890, i64* @assembly_address
  %1169 = load i1* %cf
  %1170 = load i1* %zf
  %1171 = or i1 %1169, %1170
  %1172 = icmp ne i1 %1171, true
  br i1 %1172, label %block_ea4b, label %block_e9f4

block_e9f4:                                       ; preds = %block_e9e7
  store volatile i64 59892, i64* @assembly_address
  %1173 = load i64* @global_var_21a408
  store i64 %1173, i64* %rax
  store volatile i64 59899, i64* @assembly_address
  %1174 = load i64* %rax
  %1175 = load i1* %of
  %1176 = lshr i64 %1174, 16
  %1177 = icmp eq i64 %1176, 0
  store i1 %1177, i1* %zf
  %1178 = icmp slt i64 %1176, 0
  store i1 %1178, i1* %sf
  %1179 = trunc i64 %1176 to i8
  %1180 = call i8 @llvm.ctpop.i8(i8 %1179)
  %1181 = and i8 %1180, 1
  %1182 = icmp eq i8 %1181, 0
  store i1 %1182, i1* %pf
  store i64 %1176, i64* %rax
  %1183 = and i64 32768, %1174
  %1184 = icmp ne i64 %1183, 0
  store i1 %1184, i1* %cf
  %1185 = icmp slt i64 %1174, 0
  %1186 = select i1 false, i1 %1185, i1 %1175
  store i1 %1186, i1* %of
  store volatile i64 59903, i64* @assembly_address
  %1187 = load i64* %rax
  store i64 %1187, i64* %rcx
  store volatile i64 59906, i64* @assembly_address
  %1188 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1189 = zext i32 %1188 to i64
  store i64 %1189, i64* %rax
  store volatile i64 59912, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59915, i64* @assembly_address
  %1190 = load i64* %rdx
  %1191 = trunc i64 %1190 to i32
  store i32 %1191, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59921, i64* @assembly_address
  %1192 = load i64* %rax
  %1193 = trunc i64 %1192 to i32
  %1194 = zext i32 %1193 to i64
  store i64 %1194, i64* %rdx
  store volatile i64 59923, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59930, i64* @assembly_address
  %1195 = load i64* %rcx
  %1196 = trunc i64 %1195 to i8
  %1197 = load i64* %rdx
  %1198 = load i64* %rax
  %1199 = mul i64 %1198, 1
  %1200 = add i64 %1197, %1199
  %1201 = inttoptr i64 %1200 to i8*
  store i8 %1196, i8* %1201
  store volatile i64 59933, i64* @assembly_address
  %1202 = load i64* @global_var_21a408
  store i64 %1202, i64* %rax
  store volatile i64 59940, i64* @assembly_address
  %1203 = load i64* %rax
  %1204 = load i1* %of
  %1205 = lshr i64 %1203, 16
  %1206 = icmp eq i64 %1205, 0
  store i1 %1206, i1* %zf
  %1207 = icmp slt i64 %1205, 0
  store i1 %1207, i1* %sf
  %1208 = trunc i64 %1205 to i8
  %1209 = call i8 @llvm.ctpop.i8(i8 %1208)
  %1210 = and i8 %1209, 1
  %1211 = icmp eq i8 %1210, 0
  store i1 %1211, i1* %pf
  store i64 %1205, i64* %rax
  %1212 = and i64 32768, %1203
  %1213 = icmp ne i64 %1212, 0
  store i1 %1213, i1* %cf
  %1214 = icmp slt i64 %1203, 0
  %1215 = select i1 false, i1 %1214, i1 %1204
  store i1 %1215, i1* %of
  store volatile i64 59944, i64* @assembly_address
  %1216 = load i64* %rax
  %1217 = trunc i64 %1216 to i16
  %1218 = load i1* %of
  %1219 = lshr i16 %1217, 8
  %1220 = icmp eq i16 %1219, 0
  store i1 %1220, i1* %zf
  %1221 = icmp slt i16 %1219, 0
  store i1 %1221, i1* %sf
  %1222 = trunc i16 %1219 to i8
  %1223 = call i8 @llvm.ctpop.i8(i8 %1222)
  %1224 = and i8 %1223, 1
  %1225 = icmp eq i8 %1224, 0
  store i1 %1225, i1* %pf
  %1226 = zext i16 %1219 to i64
  %1227 = load i64* %rax
  %1228 = and i64 %1227, -65536
  %1229 = or i64 %1228, %1226
  store i64 %1229, i64* %rax
  %1230 = and i16 128, %1217
  %1231 = icmp ne i16 %1230, 0
  store i1 %1231, i1* %cf
  %1232 = icmp slt i16 %1217, 0
  %1233 = select i1 false, i1 %1232, i1 %1218
  store i1 %1233, i1* %of
  store volatile i64 59948, i64* @assembly_address
  %1234 = load i64* %rax
  %1235 = trunc i64 %1234 to i32
  %1236 = zext i32 %1235 to i64
  store i64 %1236, i64* %rcx
  store volatile i64 59950, i64* @assembly_address
  %1237 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1238 = zext i32 %1237 to i64
  store i64 %1238, i64* %rax
  store volatile i64 59956, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 59959, i64* @assembly_address
  %1239 = load i64* %rdx
  %1240 = trunc i64 %1239 to i32
  store i32 %1240, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 59965, i64* @assembly_address
  %1241 = load i64* %rax
  %1242 = trunc i64 %1241 to i32
  %1243 = zext i32 %1242 to i64
  store i64 %1243, i64* %rdx
  store volatile i64 59967, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 59974, i64* @assembly_address
  %1244 = load i64* %rcx
  %1245 = trunc i64 %1244 to i8
  %1246 = load i64* %rdx
  %1247 = load i64* %rax
  %1248 = mul i64 %1247, 1
  %1249 = add i64 %1246, %1248
  %1250 = inttoptr i64 %1249 to i8*
  store i8 %1245, i8* %1250
  store volatile i64 59977, i64* @assembly_address
  br label %block_eac4

block_ea4b:                                       ; preds = %block_e9e7
  store volatile i64 59979, i64* @assembly_address
  %1251 = load i64* @global_var_21a408
  store i64 %1251, i64* %rax
  store volatile i64 59986, i64* @assembly_address
  %1252 = load i64* %rax
  %1253 = load i1* %of
  %1254 = lshr i64 %1252, 16
  %1255 = icmp eq i64 %1254, 0
  store i1 %1255, i1* %zf
  %1256 = icmp slt i64 %1254, 0
  store i1 %1256, i1* %sf
  %1257 = trunc i64 %1254 to i8
  %1258 = call i8 @llvm.ctpop.i8(i8 %1257)
  %1259 = and i8 %1258, 1
  %1260 = icmp eq i8 %1259, 0
  store i1 %1260, i1* %pf
  store i64 %1254, i64* %rax
  %1261 = and i64 32768, %1252
  %1262 = icmp ne i64 %1261, 0
  store i1 %1262, i1* %cf
  %1263 = icmp slt i64 %1252, 0
  %1264 = select i1 false, i1 %1263, i1 %1253
  store i1 %1264, i1* %of
  store volatile i64 59990, i64* @assembly_address
  %1265 = load i64* %rax
  store i64 %1265, i64* %rcx
  store volatile i64 59993, i64* @assembly_address
  %1266 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1267 = zext i32 %1266 to i64
  store i64 %1267, i64* %rax
  store volatile i64 59999, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60002, i64* @assembly_address
  %1268 = load i64* %rdx
  %1269 = trunc i64 %1268 to i32
  store i32 %1269, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60008, i64* @assembly_address
  %1270 = load i64* %rax
  %1271 = trunc i64 %1270 to i32
  %1272 = zext i32 %1271 to i64
  store i64 %1272, i64* %rdx
  store volatile i64 60010, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60017, i64* @assembly_address
  %1273 = load i64* %rcx
  %1274 = trunc i64 %1273 to i8
  %1275 = load i64* %rdx
  %1276 = load i64* %rax
  %1277 = mul i64 %1276, 1
  %1278 = add i64 %1275, %1277
  %1279 = inttoptr i64 %1278 to i8*
  store i8 %1274, i8* %1279
  store volatile i64 60020, i64* @assembly_address
  %1280 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1281 = zext i32 %1280 to i64
  store i64 %1281, i64* %rax
  store volatile i64 60026, i64* @assembly_address
  %1282 = load i64* %rax
  %1283 = trunc i64 %1282 to i32
  %1284 = sub i32 %1283, 16384
  %1285 = and i32 %1283, 15
  %1286 = icmp ugt i32 %1285, 15
  %1287 = icmp ult i32 %1283, 16384
  %1288 = xor i32 %1283, 16384
  %1289 = xor i32 %1283, %1284
  %1290 = and i32 %1288, %1289
  %1291 = icmp slt i32 %1290, 0
  store i1 %1286, i1* %az
  store i1 %1287, i1* %cf
  store i1 %1291, i1* %of
  %1292 = icmp eq i32 %1284, 0
  store i1 %1292, i1* %zf
  %1293 = icmp slt i32 %1284, 0
  store i1 %1293, i1* %sf
  %1294 = trunc i32 %1284 to i8
  %1295 = call i8 @llvm.ctpop.i8(i8 %1294)
  %1296 = and i8 %1295, 1
  %1297 = icmp eq i8 %1296, 0
  store i1 %1297, i1* %pf
  store volatile i64 60031, i64* @assembly_address
  %1298 = load i1* %zf
  %1299 = icmp eq i1 %1298, false
  br i1 %1299, label %block_ea86, label %block_ea81

block_ea81:                                       ; preds = %block_ea4b
  store volatile i64 60033, i64* @assembly_address
  %1300 = load i64* %rdi
  %1301 = load i64* %rsi
  %1302 = load i64* %rdx
  %1303 = load i64* %rcx
  %1304 = trunc i64 %1303 to i16
  %1305 = call i64 @flush_outbuf(i64 %1300, i64 %1301, i64 %1302, i16 %1304)
  store i64 %1305, i64* %rax
  store i64 %1305, i64* %rax
  store i64 %1305, i64* %rax
  br label %block_ea86

block_ea86:                                       ; preds = %block_ea81, %block_ea4b
  store volatile i64 60038, i64* @assembly_address
  %1306 = load i64* @global_var_21a408
  store i64 %1306, i64* %rax
  store volatile i64 60045, i64* @assembly_address
  %1307 = load i64* %rax
  %1308 = load i1* %of
  %1309 = lshr i64 %1307, 16
  %1310 = icmp eq i64 %1309, 0
  store i1 %1310, i1* %zf
  %1311 = icmp slt i64 %1309, 0
  store i1 %1311, i1* %sf
  %1312 = trunc i64 %1309 to i8
  %1313 = call i8 @llvm.ctpop.i8(i8 %1312)
  %1314 = and i8 %1313, 1
  %1315 = icmp eq i8 %1314, 0
  store i1 %1315, i1* %pf
  store i64 %1309, i64* %rax
  %1316 = and i64 32768, %1307
  %1317 = icmp ne i64 %1316, 0
  store i1 %1317, i1* %cf
  %1318 = icmp slt i64 %1307, 0
  %1319 = select i1 false, i1 %1318, i1 %1308
  store i1 %1319, i1* %of
  store volatile i64 60049, i64* @assembly_address
  %1320 = load i64* %rax
  %1321 = trunc i64 %1320 to i16
  %1322 = load i1* %of
  %1323 = lshr i16 %1321, 8
  %1324 = icmp eq i16 %1323, 0
  store i1 %1324, i1* %zf
  %1325 = icmp slt i16 %1323, 0
  store i1 %1325, i1* %sf
  %1326 = trunc i16 %1323 to i8
  %1327 = call i8 @llvm.ctpop.i8(i8 %1326)
  %1328 = and i8 %1327, 1
  %1329 = icmp eq i8 %1328, 0
  store i1 %1329, i1* %pf
  %1330 = zext i16 %1323 to i64
  %1331 = load i64* %rax
  %1332 = and i64 %1331, -65536
  %1333 = or i64 %1332, %1330
  store i64 %1333, i64* %rax
  %1334 = and i16 128, %1321
  %1335 = icmp ne i16 %1334, 0
  store i1 %1335, i1* %cf
  %1336 = icmp slt i16 %1321, 0
  %1337 = select i1 false, i1 %1336, i1 %1322
  store i1 %1337, i1* %of
  store volatile i64 60053, i64* @assembly_address
  %1338 = load i64* %rax
  %1339 = trunc i64 %1338 to i32
  %1340 = zext i32 %1339 to i64
  store i64 %1340, i64* %rcx
  store volatile i64 60055, i64* @assembly_address
  %1341 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1342 = zext i32 %1341 to i64
  store i64 %1342, i64* %rax
  store volatile i64 60061, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60064, i64* @assembly_address
  %1343 = load i64* %rdx
  %1344 = trunc i64 %1343 to i32
  store i32 %1344, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60070, i64* @assembly_address
  %1345 = load i64* %rax
  %1346 = trunc i64 %1345 to i32
  %1347 = zext i32 %1346 to i64
  store i64 %1347, i64* %rdx
  store volatile i64 60072, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60079, i64* @assembly_address
  %1348 = load i64* %rcx
  %1349 = trunc i64 %1348 to i8
  %1350 = load i64* %rdx
  %1351 = load i64* %rax
  %1352 = mul i64 %1351, 1
  %1353 = add i64 %1350, %1352
  %1354 = inttoptr i64 %1353 to i8*
  store i8 %1349, i8* %1354
  store volatile i64 60082, i64* @assembly_address
  %1355 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1356 = zext i32 %1355 to i64
  store i64 %1356, i64* %rax
  store volatile i64 60088, i64* @assembly_address
  %1357 = load i64* %rax
  %1358 = trunc i64 %1357 to i32
  %1359 = sub i32 %1358, 16384
  %1360 = and i32 %1358, 15
  %1361 = icmp ugt i32 %1360, 15
  %1362 = icmp ult i32 %1358, 16384
  %1363 = xor i32 %1358, 16384
  %1364 = xor i32 %1358, %1359
  %1365 = and i32 %1363, %1364
  %1366 = icmp slt i32 %1365, 0
  store i1 %1361, i1* %az
  store i1 %1362, i1* %cf
  store i1 %1366, i1* %of
  %1367 = icmp eq i32 %1359, 0
  store i1 %1367, i1* %zf
  %1368 = icmp slt i32 %1359, 0
  store i1 %1368, i1* %sf
  %1369 = trunc i32 %1359 to i8
  %1370 = call i8 @llvm.ctpop.i8(i8 %1369)
  %1371 = and i8 %1370, 1
  %1372 = icmp eq i8 %1371, 0
  store i1 %1372, i1* %pf
  store volatile i64 60093, i64* @assembly_address
  %1373 = load i1* %zf
  %1374 = icmp eq i1 %1373, false
  br i1 %1374, label %block_eac4, label %block_eabf

block_eabf:                                       ; preds = %block_ea86
  store volatile i64 60095, i64* @assembly_address
  %1375 = load i64* %rdi
  %1376 = load i64* %rsi
  %1377 = load i64* %rdx
  %1378 = load i64* %rcx
  %1379 = trunc i64 %1378 to i16
  %1380 = call i64 @flush_outbuf(i64 %1375, i64 %1376, i64 %1377, i16 %1379)
  store i64 %1380, i64* %rax
  store i64 %1380, i64* %rax
  store i64 %1380, i64* %rax
  br label %block_eac4

block_eac4:                                       ; preds = %block_eabf, %block_ea86, %block_e9f4
  store volatile i64 60100, i64* @assembly_address
  %1381 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1382 = zext i32 %1381 to i64
  store i64 %1382, i64* %rax
  store volatile i64 60106, i64* @assembly_address
  %1383 = load i64* %rax
  %1384 = trunc i64 %1383 to i32
  %1385 = sub i32 %1384, 16381
  %1386 = and i32 %1384, 15
  %1387 = sub i32 %1386, 13
  %1388 = icmp ugt i32 %1387, 15
  %1389 = icmp ult i32 %1384, 16381
  %1390 = xor i32 %1384, 16381
  %1391 = xor i32 %1384, %1385
  %1392 = and i32 %1390, %1391
  %1393 = icmp slt i32 %1392, 0
  store i1 %1388, i1* %az
  store i1 %1389, i1* %cf
  store i1 %1393, i1* %of
  %1394 = icmp eq i32 %1385, 0
  store i1 %1394, i1* %zf
  %1395 = icmp slt i32 %1385, 0
  store i1 %1395, i1* %sf
  %1396 = trunc i32 %1385 to i8
  %1397 = call i8 @llvm.ctpop.i8(i8 %1396)
  %1398 = and i8 %1397, 1
  %1399 = icmp eq i8 %1398, 0
  store i1 %1399, i1* %pf
  store volatile i64 60111, i64* @assembly_address
  %1400 = load i1* %cf
  %1401 = load i1* %zf
  %1402 = or i1 %1400, %1401
  %1403 = icmp ne i1 %1402, true
  br i1 %1403, label %block_eb1d, label %block_ead1

block_ead1:                                       ; preds = %block_eac4
  store volatile i64 60113, i64* @assembly_address
  %1404 = load i64* @global_var_21a860
  store i64 %1404, i64* %rcx
  store volatile i64 60120, i64* @assembly_address
  %1405 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1406 = zext i32 %1405 to i64
  store i64 %1406, i64* %rax
  store volatile i64 60126, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60129, i64* @assembly_address
  %1407 = load i64* %rdx
  %1408 = trunc i64 %1407 to i32
  store i32 %1408, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60135, i64* @assembly_address
  %1409 = load i64* %rax
  %1410 = trunc i64 %1409 to i32
  %1411 = zext i32 %1410 to i64
  store i64 %1411, i64* %rdx
  store volatile i64 60137, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60144, i64* @assembly_address
  %1412 = load i64* %rcx
  %1413 = trunc i64 %1412 to i8
  %1414 = load i64* %rdx
  %1415 = load i64* %rax
  %1416 = mul i64 %1415, 1
  %1417 = add i64 %1414, %1416
  %1418 = inttoptr i64 %1417 to i8*
  store i8 %1413, i8* %1418
  store volatile i64 60147, i64* @assembly_address
  %1419 = load i64* @global_var_21a860
  store i64 %1419, i64* %rax
  store volatile i64 60154, i64* @assembly_address
  %1420 = load i64* %rax
  %1421 = trunc i64 %1420 to i16
  %1422 = load i1* %of
  %1423 = lshr i16 %1421, 8
  %1424 = icmp eq i16 %1423, 0
  store i1 %1424, i1* %zf
  %1425 = icmp slt i16 %1423, 0
  store i1 %1425, i1* %sf
  %1426 = trunc i16 %1423 to i8
  %1427 = call i8 @llvm.ctpop.i8(i8 %1426)
  %1428 = and i8 %1427, 1
  %1429 = icmp eq i8 %1428, 0
  store i1 %1429, i1* %pf
  %1430 = zext i16 %1423 to i64
  %1431 = load i64* %rax
  %1432 = and i64 %1431, -65536
  %1433 = or i64 %1432, %1430
  store i64 %1433, i64* %rax
  %1434 = and i16 128, %1421
  %1435 = icmp ne i16 %1434, 0
  store i1 %1435, i1* %cf
  %1436 = icmp slt i16 %1421, 0
  %1437 = select i1 false, i1 %1436, i1 %1422
  store i1 %1437, i1* %of
  store volatile i64 60158, i64* @assembly_address
  %1438 = load i64* %rax
  %1439 = trunc i64 %1438 to i32
  %1440 = zext i32 %1439 to i64
  store i64 %1440, i64* %rcx
  store volatile i64 60160, i64* @assembly_address
  %1441 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1442 = zext i32 %1441 to i64
  store i64 %1442, i64* %rax
  store volatile i64 60166, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60169, i64* @assembly_address
  %1443 = load i64* %rdx
  %1444 = trunc i64 %1443 to i32
  store i32 %1444, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60175, i64* @assembly_address
  %1445 = load i64* %rax
  %1446 = trunc i64 %1445 to i32
  %1447 = zext i32 %1446 to i64
  store i64 %1447, i64* %rdx
  store volatile i64 60177, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60184, i64* @assembly_address
  %1448 = load i64* %rcx
  %1449 = trunc i64 %1448 to i8
  %1450 = load i64* %rdx
  %1451 = load i64* %rax
  %1452 = mul i64 %1451, 1
  %1453 = add i64 %1450, %1452
  %1454 = inttoptr i64 %1453 to i8*
  store i8 %1449, i8* %1454
  store volatile i64 60187, i64* @assembly_address
  br label %block_eb8b

block_eb1d:                                       ; preds = %block_eac4
  store volatile i64 60189, i64* @assembly_address
  %1455 = load i64* @global_var_21a860
  store i64 %1455, i64* %rcx
  store volatile i64 60196, i64* @assembly_address
  %1456 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1457 = zext i32 %1456 to i64
  store i64 %1457, i64* %rax
  store volatile i64 60202, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60205, i64* @assembly_address
  %1458 = load i64* %rdx
  %1459 = trunc i64 %1458 to i32
  store i32 %1459, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60211, i64* @assembly_address
  %1460 = load i64* %rax
  %1461 = trunc i64 %1460 to i32
  %1462 = zext i32 %1461 to i64
  store i64 %1462, i64* %rdx
  store volatile i64 60213, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60220, i64* @assembly_address
  %1463 = load i64* %rcx
  %1464 = trunc i64 %1463 to i8
  %1465 = load i64* %rdx
  %1466 = load i64* %rax
  %1467 = mul i64 %1466, 1
  %1468 = add i64 %1465, %1467
  %1469 = inttoptr i64 %1468 to i8*
  store i8 %1464, i8* %1469
  store volatile i64 60223, i64* @assembly_address
  %1470 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1471 = zext i32 %1470 to i64
  store i64 %1471, i64* %rax
  store volatile i64 60229, i64* @assembly_address
  %1472 = load i64* %rax
  %1473 = trunc i64 %1472 to i32
  %1474 = sub i32 %1473, 16384
  %1475 = and i32 %1473, 15
  %1476 = icmp ugt i32 %1475, 15
  %1477 = icmp ult i32 %1473, 16384
  %1478 = xor i32 %1473, 16384
  %1479 = xor i32 %1473, %1474
  %1480 = and i32 %1478, %1479
  %1481 = icmp slt i32 %1480, 0
  store i1 %1476, i1* %az
  store i1 %1477, i1* %cf
  store i1 %1481, i1* %of
  %1482 = icmp eq i32 %1474, 0
  store i1 %1482, i1* %zf
  %1483 = icmp slt i32 %1474, 0
  store i1 %1483, i1* %sf
  %1484 = trunc i32 %1474 to i8
  %1485 = call i8 @llvm.ctpop.i8(i8 %1484)
  %1486 = and i8 %1485, 1
  %1487 = icmp eq i8 %1486, 0
  store i1 %1487, i1* %pf
  store volatile i64 60234, i64* @assembly_address
  %1488 = load i1* %zf
  %1489 = icmp eq i1 %1488, false
  br i1 %1489, label %block_eb51, label %block_eb4c

block_eb4c:                                       ; preds = %block_eb1d
  store volatile i64 60236, i64* @assembly_address
  %1490 = load i64* %rdi
  %1491 = load i64* %rsi
  %1492 = load i64* %rdx
  %1493 = load i64* %rcx
  %1494 = trunc i64 %1493 to i16
  %1495 = call i64 @flush_outbuf(i64 %1490, i64 %1491, i64 %1492, i16 %1494)
  store i64 %1495, i64* %rax
  store i64 %1495, i64* %rax
  store i64 %1495, i64* %rax
  br label %block_eb51

block_eb51:                                       ; preds = %block_eb4c, %block_eb1d
  store volatile i64 60241, i64* @assembly_address
  %1496 = load i64* @global_var_21a860
  store i64 %1496, i64* %rax
  store volatile i64 60248, i64* @assembly_address
  %1497 = load i64* %rax
  %1498 = trunc i64 %1497 to i16
  %1499 = load i1* %of
  %1500 = lshr i16 %1498, 8
  %1501 = icmp eq i16 %1500, 0
  store i1 %1501, i1* %zf
  %1502 = icmp slt i16 %1500, 0
  store i1 %1502, i1* %sf
  %1503 = trunc i16 %1500 to i8
  %1504 = call i8 @llvm.ctpop.i8(i8 %1503)
  %1505 = and i8 %1504, 1
  %1506 = icmp eq i8 %1505, 0
  store i1 %1506, i1* %pf
  %1507 = zext i16 %1500 to i64
  %1508 = load i64* %rax
  %1509 = and i64 %1508, -65536
  %1510 = or i64 %1509, %1507
  store i64 %1510, i64* %rax
  %1511 = and i16 128, %1498
  %1512 = icmp ne i16 %1511, 0
  store i1 %1512, i1* %cf
  %1513 = icmp slt i16 %1498, 0
  %1514 = select i1 false, i1 %1513, i1 %1499
  store i1 %1514, i1* %of
  store volatile i64 60252, i64* @assembly_address
  %1515 = load i64* %rax
  %1516 = trunc i64 %1515 to i32
  %1517 = zext i32 %1516 to i64
  store i64 %1517, i64* %rcx
  store volatile i64 60254, i64* @assembly_address
  %1518 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1519 = zext i32 %1518 to i64
  store i64 %1519, i64* %rax
  store volatile i64 60260, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60263, i64* @assembly_address
  %1520 = load i64* %rdx
  %1521 = trunc i64 %1520 to i32
  store i32 %1521, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60269, i64* @assembly_address
  %1522 = load i64* %rax
  %1523 = trunc i64 %1522 to i32
  %1524 = zext i32 %1523 to i64
  store i64 %1524, i64* %rdx
  store volatile i64 60271, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60278, i64* @assembly_address
  %1525 = load i64* %rcx
  %1526 = trunc i64 %1525 to i8
  %1527 = load i64* %rdx
  %1528 = load i64* %rax
  %1529 = mul i64 %1528, 1
  %1530 = add i64 %1527, %1529
  %1531 = inttoptr i64 %1530 to i8*
  store i8 %1526, i8* %1531
  store volatile i64 60281, i64* @assembly_address
  %1532 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1533 = zext i32 %1532 to i64
  store i64 %1533, i64* %rax
  store volatile i64 60287, i64* @assembly_address
  %1534 = load i64* %rax
  %1535 = trunc i64 %1534 to i32
  %1536 = sub i32 %1535, 16384
  %1537 = and i32 %1535, 15
  %1538 = icmp ugt i32 %1537, 15
  %1539 = icmp ult i32 %1535, 16384
  %1540 = xor i32 %1535, 16384
  %1541 = xor i32 %1535, %1536
  %1542 = and i32 %1540, %1541
  %1543 = icmp slt i32 %1542, 0
  store i1 %1538, i1* %az
  store i1 %1539, i1* %cf
  store i1 %1543, i1* %of
  %1544 = icmp eq i32 %1536, 0
  store i1 %1544, i1* %zf
  %1545 = icmp slt i32 %1536, 0
  store i1 %1545, i1* %sf
  %1546 = trunc i32 %1536 to i8
  %1547 = call i8 @llvm.ctpop.i8(i8 %1546)
  %1548 = and i8 %1547, 1
  %1549 = icmp eq i8 %1548, 0
  store i1 %1549, i1* %pf
  store volatile i64 60292, i64* @assembly_address
  %1550 = load i1* %zf
  %1551 = icmp eq i1 %1550, false
  br i1 %1551, label %block_eb8b, label %block_eb86

block_eb86:                                       ; preds = %block_eb51
  store volatile i64 60294, i64* @assembly_address
  %1552 = load i64* %rdi
  %1553 = load i64* %rsi
  %1554 = load i64* %rdx
  %1555 = load i64* %rcx
  %1556 = trunc i64 %1555 to i16
  %1557 = call i64 @flush_outbuf(i64 %1552, i64 %1553, i64 %1554, i16 %1556)
  store i64 %1557, i64* %rax
  store i64 %1557, i64* %rax
  store i64 %1557, i64* %rax
  br label %block_eb8b

block_eb8b:                                       ; preds = %block_eb86, %block_eb51, %block_ead1
  store volatile i64 60299, i64* @assembly_address
  %1558 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1559 = zext i32 %1558 to i64
  store i64 %1559, i64* %rax
  store volatile i64 60305, i64* @assembly_address
  %1560 = load i64* %rax
  %1561 = trunc i64 %1560 to i32
  %1562 = sub i32 %1561, 16381
  %1563 = and i32 %1561, 15
  %1564 = sub i32 %1563, 13
  %1565 = icmp ugt i32 %1564, 15
  %1566 = icmp ult i32 %1561, 16381
  %1567 = xor i32 %1561, 16381
  %1568 = xor i32 %1561, %1562
  %1569 = and i32 %1567, %1568
  %1570 = icmp slt i32 %1569, 0
  store i1 %1565, i1* %az
  store i1 %1566, i1* %cf
  store i1 %1570, i1* %of
  %1571 = icmp eq i32 %1562, 0
  store i1 %1571, i1* %zf
  %1572 = icmp slt i32 %1562, 0
  store i1 %1572, i1* %sf
  %1573 = trunc i32 %1562 to i8
  %1574 = call i8 @llvm.ctpop.i8(i8 %1573)
  %1575 = and i8 %1574, 1
  %1576 = icmp eq i8 %1575, 0
  store i1 %1576, i1* %pf
  store volatile i64 60310, i64* @assembly_address
  %1577 = load i1* %cf
  %1578 = load i1* %zf
  %1579 = or i1 %1577, %1578
  %1580 = icmp ne i1 %1579, true
  br i1 %1580, label %block_ebef, label %block_eb98

block_eb98:                                       ; preds = %block_eb8b
  store volatile i64 60312, i64* @assembly_address
  %1581 = load i64* @global_var_21a860
  store i64 %1581, i64* %rax
  store volatile i64 60319, i64* @assembly_address
  %1582 = load i64* %rax
  %1583 = load i1* %of
  %1584 = lshr i64 %1582, 16
  %1585 = icmp eq i64 %1584, 0
  store i1 %1585, i1* %zf
  %1586 = icmp slt i64 %1584, 0
  store i1 %1586, i1* %sf
  %1587 = trunc i64 %1584 to i8
  %1588 = call i8 @llvm.ctpop.i8(i8 %1587)
  %1589 = and i8 %1588, 1
  %1590 = icmp eq i8 %1589, 0
  store i1 %1590, i1* %pf
  store i64 %1584, i64* %rax
  %1591 = and i64 32768, %1582
  %1592 = icmp ne i64 %1591, 0
  store i1 %1592, i1* %cf
  %1593 = icmp slt i64 %1582, 0
  %1594 = select i1 false, i1 %1593, i1 %1583
  store i1 %1594, i1* %of
  store volatile i64 60323, i64* @assembly_address
  %1595 = load i64* %rax
  store i64 %1595, i64* %rcx
  store volatile i64 60326, i64* @assembly_address
  %1596 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1597 = zext i32 %1596 to i64
  store i64 %1597, i64* %rax
  store volatile i64 60332, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60335, i64* @assembly_address
  %1598 = load i64* %rdx
  %1599 = trunc i64 %1598 to i32
  store i32 %1599, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60341, i64* @assembly_address
  %1600 = load i64* %rax
  %1601 = trunc i64 %1600 to i32
  %1602 = zext i32 %1601 to i64
  store i64 %1602, i64* %rdx
  store volatile i64 60343, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60350, i64* @assembly_address
  %1603 = load i64* %rcx
  %1604 = trunc i64 %1603 to i8
  %1605 = load i64* %rdx
  %1606 = load i64* %rax
  %1607 = mul i64 %1606, 1
  %1608 = add i64 %1605, %1607
  %1609 = inttoptr i64 %1608 to i8*
  store i8 %1604, i8* %1609
  store volatile i64 60353, i64* @assembly_address
  %1610 = load i64* @global_var_21a860
  store i64 %1610, i64* %rax
  store volatile i64 60360, i64* @assembly_address
  %1611 = load i64* %rax
  %1612 = load i1* %of
  %1613 = lshr i64 %1611, 16
  %1614 = icmp eq i64 %1613, 0
  store i1 %1614, i1* %zf
  %1615 = icmp slt i64 %1613, 0
  store i1 %1615, i1* %sf
  %1616 = trunc i64 %1613 to i8
  %1617 = call i8 @llvm.ctpop.i8(i8 %1616)
  %1618 = and i8 %1617, 1
  %1619 = icmp eq i8 %1618, 0
  store i1 %1619, i1* %pf
  store i64 %1613, i64* %rax
  %1620 = and i64 32768, %1611
  %1621 = icmp ne i64 %1620, 0
  store i1 %1621, i1* %cf
  %1622 = icmp slt i64 %1611, 0
  %1623 = select i1 false, i1 %1622, i1 %1612
  store i1 %1623, i1* %of
  store volatile i64 60364, i64* @assembly_address
  %1624 = load i64* %rax
  %1625 = trunc i64 %1624 to i16
  %1626 = load i1* %of
  %1627 = lshr i16 %1625, 8
  %1628 = icmp eq i16 %1627, 0
  store i1 %1628, i1* %zf
  %1629 = icmp slt i16 %1627, 0
  store i1 %1629, i1* %sf
  %1630 = trunc i16 %1627 to i8
  %1631 = call i8 @llvm.ctpop.i8(i8 %1630)
  %1632 = and i8 %1631, 1
  %1633 = icmp eq i8 %1632, 0
  store i1 %1633, i1* %pf
  %1634 = zext i16 %1627 to i64
  %1635 = load i64* %rax
  %1636 = and i64 %1635, -65536
  %1637 = or i64 %1636, %1634
  store i64 %1637, i64* %rax
  %1638 = and i16 128, %1625
  %1639 = icmp ne i16 %1638, 0
  store i1 %1639, i1* %cf
  %1640 = icmp slt i16 %1625, 0
  %1641 = select i1 false, i1 %1640, i1 %1626
  store i1 %1641, i1* %of
  store volatile i64 60368, i64* @assembly_address
  %1642 = load i64* %rax
  %1643 = trunc i64 %1642 to i32
  %1644 = zext i32 %1643 to i64
  store i64 %1644, i64* %rcx
  store volatile i64 60370, i64* @assembly_address
  %1645 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1646 = zext i32 %1645 to i64
  store i64 %1646, i64* %rax
  store volatile i64 60376, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60379, i64* @assembly_address
  %1647 = load i64* %rdx
  %1648 = trunc i64 %1647 to i32
  store i32 %1648, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60385, i64* @assembly_address
  %1649 = load i64* %rax
  %1650 = trunc i64 %1649 to i32
  %1651 = zext i32 %1650 to i64
  store i64 %1651, i64* %rdx
  store volatile i64 60387, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60394, i64* @assembly_address
  %1652 = load i64* %rcx
  %1653 = trunc i64 %1652 to i8
  %1654 = load i64* %rdx
  %1655 = load i64* %rax
  %1656 = mul i64 %1655, 1
  %1657 = add i64 %1654, %1656
  %1658 = inttoptr i64 %1657 to i8*
  store i8 %1653, i8* %1658
  store volatile i64 60397, i64* @assembly_address
  br label %block_ec68

block_ebef:                                       ; preds = %block_eb8b
  store volatile i64 60399, i64* @assembly_address
  %1659 = load i64* @global_var_21a860
  store i64 %1659, i64* %rax
  store volatile i64 60406, i64* @assembly_address
  %1660 = load i64* %rax
  %1661 = load i1* %of
  %1662 = lshr i64 %1660, 16
  %1663 = icmp eq i64 %1662, 0
  store i1 %1663, i1* %zf
  %1664 = icmp slt i64 %1662, 0
  store i1 %1664, i1* %sf
  %1665 = trunc i64 %1662 to i8
  %1666 = call i8 @llvm.ctpop.i8(i8 %1665)
  %1667 = and i8 %1666, 1
  %1668 = icmp eq i8 %1667, 0
  store i1 %1668, i1* %pf
  store i64 %1662, i64* %rax
  %1669 = and i64 32768, %1660
  %1670 = icmp ne i64 %1669, 0
  store i1 %1670, i1* %cf
  %1671 = icmp slt i64 %1660, 0
  %1672 = select i1 false, i1 %1671, i1 %1661
  store i1 %1672, i1* %of
  store volatile i64 60410, i64* @assembly_address
  %1673 = load i64* %rax
  store i64 %1673, i64* %rcx
  store volatile i64 60413, i64* @assembly_address
  %1674 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1675 = zext i32 %1674 to i64
  store i64 %1675, i64* %rax
  store volatile i64 60419, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60422, i64* @assembly_address
  %1676 = load i64* %rdx
  %1677 = trunc i64 %1676 to i32
  store i32 %1677, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60428, i64* @assembly_address
  %1678 = load i64* %rax
  %1679 = trunc i64 %1678 to i32
  %1680 = zext i32 %1679 to i64
  store i64 %1680, i64* %rdx
  store volatile i64 60430, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60437, i64* @assembly_address
  %1681 = load i64* %rcx
  %1682 = trunc i64 %1681 to i8
  %1683 = load i64* %rdx
  %1684 = load i64* %rax
  %1685 = mul i64 %1684, 1
  %1686 = add i64 %1683, %1685
  %1687 = inttoptr i64 %1686 to i8*
  store i8 %1682, i8* %1687
  store volatile i64 60440, i64* @assembly_address
  %1688 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1689 = zext i32 %1688 to i64
  store i64 %1689, i64* %rax
  store volatile i64 60446, i64* @assembly_address
  %1690 = load i64* %rax
  %1691 = trunc i64 %1690 to i32
  %1692 = sub i32 %1691, 16384
  %1693 = and i32 %1691, 15
  %1694 = icmp ugt i32 %1693, 15
  %1695 = icmp ult i32 %1691, 16384
  %1696 = xor i32 %1691, 16384
  %1697 = xor i32 %1691, %1692
  %1698 = and i32 %1696, %1697
  %1699 = icmp slt i32 %1698, 0
  store i1 %1694, i1* %az
  store i1 %1695, i1* %cf
  store i1 %1699, i1* %of
  %1700 = icmp eq i32 %1692, 0
  store i1 %1700, i1* %zf
  %1701 = icmp slt i32 %1692, 0
  store i1 %1701, i1* %sf
  %1702 = trunc i32 %1692 to i8
  %1703 = call i8 @llvm.ctpop.i8(i8 %1702)
  %1704 = and i8 %1703, 1
  %1705 = icmp eq i8 %1704, 0
  store i1 %1705, i1* %pf
  store volatile i64 60451, i64* @assembly_address
  %1706 = load i1* %zf
  %1707 = icmp eq i1 %1706, false
  br i1 %1707, label %block_ec2a, label %block_ec25

block_ec25:                                       ; preds = %block_ebef
  store volatile i64 60453, i64* @assembly_address
  %1708 = load i64* %rdi
  %1709 = load i64* %rsi
  %1710 = load i64* %rdx
  %1711 = load i64* %rcx
  %1712 = trunc i64 %1711 to i16
  %1713 = call i64 @flush_outbuf(i64 %1708, i64 %1709, i64 %1710, i16 %1712)
  store i64 %1713, i64* %rax
  store i64 %1713, i64* %rax
  store i64 %1713, i64* %rax
  br label %block_ec2a

block_ec2a:                                       ; preds = %block_ec25, %block_ebef
  store volatile i64 60458, i64* @assembly_address
  %1714 = load i64* @global_var_21a860
  store i64 %1714, i64* %rax
  store volatile i64 60465, i64* @assembly_address
  %1715 = load i64* %rax
  %1716 = load i1* %of
  %1717 = lshr i64 %1715, 16
  %1718 = icmp eq i64 %1717, 0
  store i1 %1718, i1* %zf
  %1719 = icmp slt i64 %1717, 0
  store i1 %1719, i1* %sf
  %1720 = trunc i64 %1717 to i8
  %1721 = call i8 @llvm.ctpop.i8(i8 %1720)
  %1722 = and i8 %1721, 1
  %1723 = icmp eq i8 %1722, 0
  store i1 %1723, i1* %pf
  store i64 %1717, i64* %rax
  %1724 = and i64 32768, %1715
  %1725 = icmp ne i64 %1724, 0
  store i1 %1725, i1* %cf
  %1726 = icmp slt i64 %1715, 0
  %1727 = select i1 false, i1 %1726, i1 %1716
  store i1 %1727, i1* %of
  store volatile i64 60469, i64* @assembly_address
  %1728 = load i64* %rax
  %1729 = trunc i64 %1728 to i16
  %1730 = load i1* %of
  %1731 = lshr i16 %1729, 8
  %1732 = icmp eq i16 %1731, 0
  store i1 %1732, i1* %zf
  %1733 = icmp slt i16 %1731, 0
  store i1 %1733, i1* %sf
  %1734 = trunc i16 %1731 to i8
  %1735 = call i8 @llvm.ctpop.i8(i8 %1734)
  %1736 = and i8 %1735, 1
  %1737 = icmp eq i8 %1736, 0
  store i1 %1737, i1* %pf
  %1738 = zext i16 %1731 to i64
  %1739 = load i64* %rax
  %1740 = and i64 %1739, -65536
  %1741 = or i64 %1740, %1738
  store i64 %1741, i64* %rax
  %1742 = and i16 128, %1729
  %1743 = icmp ne i16 %1742, 0
  store i1 %1743, i1* %cf
  %1744 = icmp slt i16 %1729, 0
  %1745 = select i1 false, i1 %1744, i1 %1730
  store i1 %1745, i1* %of
  store volatile i64 60473, i64* @assembly_address
  %1746 = load i64* %rax
  %1747 = trunc i64 %1746 to i32
  %1748 = zext i32 %1747 to i64
  store i64 %1748, i64* %rcx
  store volatile i64 60475, i64* @assembly_address
  %1749 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1750 = zext i32 %1749 to i64
  store i64 %1750, i64* %rax
  store volatile i64 60481, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_25f4e1 to i64), i64* %rdx
  store volatile i64 60484, i64* @assembly_address
  %1751 = load i64* %rdx
  %1752 = trunc i64 %1751 to i32
  store i32 %1752, i32* bitcast (i64* @global_var_25f4e0 to i32*)
  store volatile i64 60490, i64* @assembly_address
  %1753 = load i64* %rax
  %1754 = trunc i64 %1753 to i32
  %1755 = zext i32 %1754 to i64
  store i64 %1755, i64* %rdx
  store volatile i64 60492, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_24a8a0 to i64), i64* %rax
  store volatile i64 60499, i64* @assembly_address
  %1756 = load i64* %rcx
  %1757 = trunc i64 %1756 to i8
  %1758 = load i64* %rdx
  %1759 = load i64* %rax
  %1760 = mul i64 %1759, 1
  %1761 = add i64 %1758, %1760
  %1762 = inttoptr i64 %1761 to i8*
  store i8 %1757, i8* %1762
  store volatile i64 60502, i64* @assembly_address
  %1763 = load i32* bitcast (i64* @global_var_25f4e0 to i32*)
  %1764 = zext i32 %1763 to i64
  store i64 %1764, i64* %rax
  store volatile i64 60508, i64* @assembly_address
  %1765 = load i64* %rax
  %1766 = trunc i64 %1765 to i32
  %1767 = sub i32 %1766, 16384
  %1768 = and i32 %1766, 15
  %1769 = icmp ugt i32 %1768, 15
  %1770 = icmp ult i32 %1766, 16384
  %1771 = xor i32 %1766, 16384
  %1772 = xor i32 %1766, %1767
  %1773 = and i32 %1771, %1772
  %1774 = icmp slt i32 %1773, 0
  store i1 %1769, i1* %az
  store i1 %1770, i1* %cf
  store i1 %1774, i1* %of
  %1775 = icmp eq i32 %1767, 0
  store i1 %1775, i1* %zf
  %1776 = icmp slt i32 %1767, 0
  store i1 %1776, i1* %sf
  %1777 = trunc i32 %1767 to i8
  %1778 = call i8 @llvm.ctpop.i8(i8 %1777)
  %1779 = and i8 %1778, 1
  %1780 = icmp eq i8 %1779, 0
  store i1 %1780, i1* %pf
  store volatile i64 60513, i64* @assembly_address
  %1781 = load i1* %zf
  %1782 = icmp eq i1 %1781, false
  br i1 %1782, label %block_ec68, label %block_ec63

block_ec63:                                       ; preds = %block_ec2a
  store volatile i64 60515, i64* @assembly_address
  %1783 = load i64* %rdi
  %1784 = load i64* %rsi
  %1785 = load i64* %rdx
  %1786 = load i64* %rcx
  %1787 = trunc i64 %1786 to i16
  %1788 = call i64 @flush_outbuf(i64 %1783, i64 %1784, i64 %1785, i16 %1787)
  store i64 %1788, i64* %rax
  store i64 %1788, i64* %rax
  store i64 %1788, i64* %rax
  br label %block_ec68

block_ec68:                                       ; preds = %block_ec63, %block_ec2a, %block_eb98
  store volatile i64 60520, i64* @assembly_address
  %1789 = load i64* @global_var_267540
  store i64 %1789, i64* %rax
  store volatile i64 60527, i64* @assembly_address
  %1790 = load i64* %rax
  %1791 = add i64 %1790, 8
  %1792 = and i64 %1790, 15
  %1793 = add i64 %1792, 8
  %1794 = icmp ugt i64 %1793, 15
  %1795 = icmp ult i64 %1791, %1790
  %1796 = xor i64 %1790, %1791
  %1797 = xor i64 8, %1791
  %1798 = and i64 %1796, %1797
  %1799 = icmp slt i64 %1798, 0
  store i1 %1794, i1* %az
  store i1 %1795, i1* %cf
  store i1 %1799, i1* %of
  %1800 = icmp eq i64 %1791, 0
  store i1 %1800, i1* %zf
  %1801 = icmp slt i64 %1791, 0
  store i1 %1801, i1* %sf
  %1802 = trunc i64 %1791 to i8
  %1803 = call i8 @llvm.ctpop.i8(i8 %1802)
  %1804 = and i8 %1803, 1
  %1805 = icmp eq i8 %1804, 0
  store i1 %1805, i1* %pf
  store i64 %1791, i64* %rax
  store volatile i64 60531, i64* @assembly_address
  %1806 = load i64* %rax
  store i64 %1806, i64* @global_var_267540
  store volatile i64 60538, i64* @assembly_address
  %1807 = load i64* %rdi
  %1808 = load i64* %rsi
  %1809 = load i64* %rdx
  %1810 = load i64* %rcx
  %1811 = trunc i64 %1810 to i16
  %1812 = call i64 @flush_outbuf(i64 %1807, i64 %1808, i64 %1809, i16 %1811)
  store i64 %1812, i64* %rax
  store i64 %1812, i64* %rax
  store i64 %1812, i64* %rax
  store volatile i64 60543, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 60548, i64* @assembly_address
  %1813 = load i64* %stack_var_-16
  store i64 %1813, i64* %rsi
  store volatile i64 60552, i64* @assembly_address
  %1814 = load i64* %rsi
  %1815 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %1816 = xor i64 %1814, %1815
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1817 = icmp eq i64 %1816, 0
  store i1 %1817, i1* %zf
  %1818 = icmp slt i64 %1816, 0
  store i1 %1818, i1* %sf
  %1819 = trunc i64 %1816 to i8
  %1820 = call i8 @llvm.ctpop.i8(i8 %1819)
  %1821 = and i8 %1820, 1
  %1822 = icmp eq i8 %1821, 0
  store i1 %1822, i1* %pf
  store i64 %1816, i64* %rsi
  store volatile i64 60561, i64* @assembly_address
  %1823 = load i1* %zf
  br i1 %1823, label %block_ec98, label %block_ec93

block_ec93:                                       ; preds = %block_ec68
  store volatile i64 60563, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_ec98:                                       ; preds = %block_ec68
  store volatile i64 60568, i64* @assembly_address
  %1824 = load i64* %stack_var_-8
  store i64 %1824, i64* %rbp
  %1825 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1825, i64* %rsp
  store volatile i64 60569, i64* @assembly_address
  %1826 = load i64* %rax
  ret i64 %1826
}

declare i64 @259(i64, i32)

declare i64 @260(i64, i64)

define i64 @file_read(i8* %arg1, i64 %arg2) {
block_ec9a:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-36 = alloca i32
  %stack_var_-32 = alloca i8*
  %1 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 60570, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 60571, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 60574, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 60578, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to i8*
  store i8* %22, i8** %stack_var_-32
  store volatile i64 60582, i64* @assembly_address
  %23 = load i64* %rsi
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-36
  store volatile i64 60585, i64* @assembly_address
  %25 = load i32* bitcast (i64* @global_var_24f0a0 to i32*)
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rax
  store volatile i64 60591, i64* @assembly_address
  %27 = load i32* %stack_var_-36
  %28 = zext i32 %27 to i64
  store i64 %28, i64* %rdx
  store volatile i64 60594, i64* @assembly_address
  %29 = load i8** %stack_var_-32
  %30 = ptrtoint i8* %29 to i64
  store i64 %30, i64* %rcx
  store volatile i64 60598, i64* @assembly_address
  %31 = load i64* %rcx
  store i64 %31, i64* %rsi
  store volatile i64 60601, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i32
  %34 = zext i32 %33 to i64
  store i64 %34, i64* %rdi
  store volatile i64 60603, i64* @assembly_address
  %35 = load i64* %rdi
  %36 = load i64* %rsi
  %37 = load i64* %rdx
  %38 = trunc i64 %35 to i32
  %39 = call i64 @read_buffer(i32 %38, i64 %36, i64 %37)
  store i64 %39, i64* %rax
  store i64 %39, i64* %rax
  store volatile i64 60608, i64* @assembly_address
  %40 = load i64* %rax
  %41 = trunc i64 %40 to i32
  store i32 %41, i32* %stack_var_-12
  store volatile i64 60611, i64* @assembly_address
  %42 = load i32* %stack_var_-12
  %43 = and i32 %42, 15
  %44 = icmp ugt i32 %43, 15
  %45 = icmp ult i32 %42, 0
  %46 = xor i32 %42, 0
  %47 = and i32 %46, 0
  %48 = icmp slt i32 %47, 0
  store i1 %44, i1* %az
  store i1 %45, i1* %cf
  store i1 %48, i1* %of
  %49 = icmp eq i32 %42, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i32 %42, 0
  store i1 %50, i1* %sf
  %51 = trunc i32 %42 to i8
  %52 = call i8 @llvm.ctpop.i8(i8 %51)
  %53 = and i8 %52, 1
  %54 = icmp eq i8 %53, 0
  store i1 %54, i1* %pf
  store volatile i64 60615, i64* @assembly_address
  %55 = load i1* %zf
  %56 = icmp eq i1 %55, false
  br i1 %56, label %block_ecce, label %block_ecc9

block_ecc9:                                       ; preds = %block_ec9a
  store volatile i64 60617, i64* @assembly_address
  %57 = load i32* %stack_var_-12
  %58 = zext i32 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 60620, i64* @assembly_address
  br label %block_ed08

block_ecce:                                       ; preds = %block_ec9a
  store volatile i64 60622, i64* @assembly_address
  %59 = load i32* %stack_var_-12
  %60 = sub i32 %59, -1
  %61 = and i32 %59, 15
  %62 = sub i32 %61, 15
  %63 = icmp ugt i32 %62, 15
  %64 = icmp ult i32 %59, -1
  %65 = xor i32 %59, -1
  %66 = xor i32 %59, %60
  %67 = and i32 %65, %66
  %68 = icmp slt i32 %67, 0
  store i1 %63, i1* %az
  store i1 %64, i1* %cf
  store i1 %68, i1* %of
  %69 = icmp eq i32 %60, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 %60, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 %60 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  store volatile i64 60626, i64* @assembly_address
  %75 = load i1* %zf
  %76 = icmp eq i1 %75, false
  br i1 %76, label %block_ecd9, label %block_ecd4

block_ecd4:                                       ; preds = %block_ecce
  store volatile i64 60628, i64* @assembly_address
  %77 = call i64 @read_error()
  store i64 %77, i64* %rax
  store i64 %77, i64* %rax
  store i64 %77, i64* %rax
  unreachable

block_ecd9:                                       ; preds = %block_ecce
  store volatile i64 60633, i64* @assembly_address
  %78 = load i32* %stack_var_-12
  %79 = zext i32 %78 to i64
  store i64 %79, i64* %rdx
  store volatile i64 60636, i64* @assembly_address
  %80 = load i8** %stack_var_-32
  %81 = ptrtoint i8* %80 to i64
  store i64 %81, i64* %rax
  store volatile i64 60640, i64* @assembly_address
  %82 = load i64* %rdx
  %83 = trunc i64 %82 to i32
  %84 = zext i32 %83 to i64
  store i64 %84, i64* %rsi
  store volatile i64 60642, i64* @assembly_address
  %85 = load i64* %rax
  store i64 %85, i64* %rdi
  store volatile i64 60645, i64* @assembly_address
  %86 = load i64* %rdi
  %87 = inttoptr i64 %86 to i8*
  %88 = load i64* %rsi
  %89 = trunc i64 %88 to i32
  %90 = call i64 @updcrc(i8* %87, i32 %89)
  store i64 %90, i64* %rax
  store i64 %90, i64* %rax
  store volatile i64 60650, i64* @assembly_address
  %91 = load i64* %rax
  store i64 %91, i64* @global_var_21a408
  store volatile i64 60657, i64* @assembly_address
  %92 = load i32* %stack_var_-12
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rdx
  store volatile i64 60660, i64* @assembly_address
  %94 = load i64* @global_var_21a860
  store i64 %94, i64* %rax
  store volatile i64 60667, i64* @assembly_address
  %95 = load i64* %rax
  %96 = load i64* %rdx
  %97 = add i64 %95, %96
  %98 = and i64 %95, 15
  %99 = and i64 %96, 15
  %100 = add i64 %98, %99
  %101 = icmp ugt i64 %100, 15
  %102 = icmp ult i64 %97, %95
  %103 = xor i64 %95, %97
  %104 = xor i64 %96, %97
  %105 = and i64 %103, %104
  %106 = icmp slt i64 %105, 0
  store i1 %101, i1* %az
  store i1 %102, i1* %cf
  store i1 %106, i1* %of
  %107 = icmp eq i64 %97, 0
  store i1 %107, i1* %zf
  %108 = icmp slt i64 %97, 0
  store i1 %108, i1* %sf
  %109 = trunc i64 %97 to i8
  %110 = call i8 @llvm.ctpop.i8(i8 %109)
  %111 = and i8 %110, 1
  %112 = icmp eq i8 %111, 0
  store i1 %112, i1* %pf
  store i64 %97, i64* %rax
  store volatile i64 60670, i64* @assembly_address
  %113 = load i64* %rax
  store i64 %113, i64* @global_var_21a860
  store volatile i64 60677, i64* @assembly_address
  %114 = load i32* %stack_var_-12
  %115 = zext i32 %114 to i64
  store i64 %115, i64* %rax
  br label %block_ed08

block_ed08:                                       ; preds = %block_ecd9, %block_ecc9
  store volatile i64 60680, i64* @assembly_address
  %116 = load i64* %stack_var_-8
  store i64 %116, i64* %rbp
  %117 = ptrtoint i64* %stack_var_0 to i64
  store i64 %117, i64* %rsp
  store volatile i64 60681, i64* @assembly_address
  %118 = load i64* %rax
  ret i64 %118
}

declare i64 @261(i64, i32)

declare i64 @262(i64, i64)

define i64 @last_component(i8* %arg1) {
block_ed0a:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = bitcast i8* %arg1 to i64*
  %1 = ptrtoint i64* %0 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %2 = alloca i64
  %stack_var_-25 = alloca i8
  %stack_var_-24 = alloca i8*
  %3 = alloca i64
  %stack_var_-48 = alloca i8*
  %4 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 60682, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 60683, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 60686, i64* @assembly_address
  %8 = load i64* %rdi
  %9 = inttoptr i64 %8 to i8*
  store i8* %9, i8** %stack_var_-48
  store volatile i64 60690, i64* @assembly_address
  %10 = load i8** %stack_var_-48
  %11 = ptrtoint i8* %10 to i64
  store i64 %11, i64* %rax
  store volatile i64 60694, i64* @assembly_address
  %12 = load i64* %rax
  %13 = inttoptr i64 %12 to i8*
  store i8* %13, i8** %stack_var_-24
  store volatile i64 60698, i64* @assembly_address
  store i8 0, i8* %stack_var_-25
  store volatile i64 60702, i64* @assembly_address
  br label %block_ed25

block_ed20:                                       ; preds = %block_ed25
  store volatile i64 60704, i64* @assembly_address
  %14 = load i8** %stack_var_-24
  %15 = ptrtoint i8* %14 to i64
  %16 = add i64 %15, 1
  %17 = and i64 %15, 15
  %18 = add i64 %17, 1
  %19 = icmp ugt i64 %18, 15
  %20 = icmp ult i64 %16, %15
  %21 = xor i64 %15, %16
  %22 = xor i64 1, %16
  %23 = and i64 %21, %22
  %24 = icmp slt i64 %23, 0
  store i1 %19, i1* %az
  store i1 %20, i1* %cf
  store i1 %24, i1* %of
  %25 = icmp eq i64 %16, 0
  store i1 %25, i1* %zf
  %26 = icmp slt i64 %16, 0
  store i1 %26, i1* %sf
  %27 = trunc i64 %16 to i8
  %28 = call i8 @llvm.ctpop.i8(i8 %27)
  %29 = and i8 %28, 1
  %30 = icmp eq i8 %29, 0
  store i1 %30, i1* %pf
  %31 = inttoptr i64 %16 to i8*
  store i8* %31, i8** %stack_var_-24
  br label %block_ed25

block_ed25:                                       ; preds = %block_ed20, %block_ed0a
  store volatile i64 60709, i64* @assembly_address
  %32 = load i8** %stack_var_-24
  %33 = ptrtoint i8* %32 to i64
  store i64 %33, i64* %rax
  store volatile i64 60713, i64* @assembly_address
  %34 = load i64* %rax
  %35 = inttoptr i64 %34 to i8*
  %36 = load i8* %35
  %37 = zext i8 %36 to i64
  store i64 %37, i64* %rax
  store volatile i64 60716, i64* @assembly_address
  %38 = load i64* %rax
  %39 = trunc i64 %38 to i8
  %40 = sub i8 %39, 47
  %41 = and i8 %39, 15
  %42 = sub i8 %41, 15
  %43 = icmp ugt i8 %42, 15
  %44 = icmp ult i8 %39, 47
  %45 = xor i8 %39, 47
  %46 = xor i8 %39, %40
  %47 = and i8 %45, %46
  %48 = icmp slt i8 %47, 0
  store i1 %43, i1* %az
  store i1 %44, i1* %cf
  store i1 %48, i1* %of
  %49 = icmp eq i8 %40, 0
  store i1 %49, i1* %zf
  %50 = icmp slt i8 %40, 0
  store i1 %50, i1* %sf
  %51 = call i8 @llvm.ctpop.i8(i8 %40)
  %52 = and i8 %51, 1
  %53 = icmp eq i8 %52, 0
  store i1 %53, i1* %pf
  store volatile i64 60718, i64* @assembly_address
  %54 = load i1* %zf
  br i1 %54, label %block_ed20, label %block_ed30

block_ed30:                                       ; preds = %block_ed25
  store volatile i64 60720, i64* @assembly_address
  %55 = load i8** %stack_var_-24
  %56 = ptrtoint i8* %55 to i64
  store i64 %56, i64* %rax
  store volatile i64 60724, i64* @assembly_address
  %57 = load i64* %rax
  %58 = inttoptr i64 %57 to i8*
  store i8* %58, i8** %stack_var_-16
  store volatile i64 60728, i64* @assembly_address
  br label %block_ed62

block_ed3a:                                       ; preds = %block_ed62
  store volatile i64 60730, i64* @assembly_address
  %59 = load i8** %stack_var_-16
  %60 = ptrtoint i8* %59 to i64
  store i64 %60, i64* %rax
  store volatile i64 60734, i64* @assembly_address
  %61 = load i64* %rax
  %62 = inttoptr i64 %61 to i8*
  %63 = load i8* %62
  %64 = zext i8 %63 to i64
  store i64 %64, i64* %rax
  store volatile i64 60737, i64* @assembly_address
  %65 = load i64* %rax
  %66 = trunc i64 %65 to i8
  %67 = sub i8 %66, 47
  %68 = and i8 %66, 15
  %69 = sub i8 %68, 15
  %70 = icmp ugt i8 %69, 15
  %71 = icmp ult i8 %66, 47
  %72 = xor i8 %66, 47
  %73 = xor i8 %66, %67
  %74 = and i8 %72, %73
  %75 = icmp slt i8 %74, 0
  store i1 %70, i1* %az
  store i1 %71, i1* %cf
  store i1 %75, i1* %of
  %76 = icmp eq i8 %67, 0
  store i1 %76, i1* %zf
  %77 = icmp slt i8 %67, 0
  store i1 %77, i1* %sf
  %78 = call i8 @llvm.ctpop.i8(i8 %67)
  %79 = and i8 %78, 1
  %80 = icmp eq i8 %79, 0
  store i1 %80, i1* %pf
  store volatile i64 60739, i64* @assembly_address
  %81 = load i1* %zf
  %82 = icmp eq i1 %81, false
  br i1 %82, label %block_ed4b, label %block_ed45

block_ed45:                                       ; preds = %block_ed3a
  store volatile i64 60741, i64* @assembly_address
  store i8 1, i8* %stack_var_-25
  store volatile i64 60745, i64* @assembly_address
  br label %block_ed5d

block_ed4b:                                       ; preds = %block_ed3a
  store volatile i64 60747, i64* @assembly_address
  %83 = load i8* %stack_var_-25
  %84 = and i8 %83, 15
  %85 = icmp ugt i8 %84, 15
  %86 = icmp ult i8 %83, 0
  %87 = xor i8 %83, 0
  %88 = and i8 %87, 0
  %89 = icmp slt i8 %88, 0
  store i1 %85, i1* %az
  store i1 %86, i1* %cf
  store i1 %89, i1* %of
  %90 = icmp eq i8 %83, 0
  store i1 %90, i1* %zf
  %91 = icmp slt i8 %83, 0
  store i1 %91, i1* %sf
  %92 = call i8 @llvm.ctpop.i8(i8 %83)
  %93 = and i8 %92, 1
  %94 = icmp eq i8 %93, 0
  store i1 %94, i1* %pf
  store volatile i64 60751, i64* @assembly_address
  %95 = load i1* %zf
  br i1 %95, label %block_ed5d, label %block_ed51

block_ed51:                                       ; preds = %block_ed4b
  store volatile i64 60753, i64* @assembly_address
  %96 = load i8** %stack_var_-16
  %97 = ptrtoint i8* %96 to i64
  store i64 %97, i64* %rax
  store volatile i64 60757, i64* @assembly_address
  %98 = load i64* %rax
  %99 = inttoptr i64 %98 to i8*
  store i8* %99, i8** %stack_var_-24
  store volatile i64 60761, i64* @assembly_address
  store i8 0, i8* %stack_var_-25
  br label %block_ed5d

block_ed5d:                                       ; preds = %block_ed51, %block_ed4b, %block_ed45
  store volatile i64 60765, i64* @assembly_address
  %100 = load i8** %stack_var_-16
  %101 = ptrtoint i8* %100 to i64
  %102 = add i64 %101, 1
  %103 = and i64 %101, 15
  %104 = add i64 %103, 1
  %105 = icmp ugt i64 %104, 15
  %106 = icmp ult i64 %102, %101
  %107 = xor i64 %101, %102
  %108 = xor i64 1, %102
  %109 = and i64 %107, %108
  %110 = icmp slt i64 %109, 0
  store i1 %105, i1* %az
  store i1 %106, i1* %cf
  store i1 %110, i1* %of
  %111 = icmp eq i64 %102, 0
  store i1 %111, i1* %zf
  %112 = icmp slt i64 %102, 0
  store i1 %112, i1* %sf
  %113 = trunc i64 %102 to i8
  %114 = call i8 @llvm.ctpop.i8(i8 %113)
  %115 = and i8 %114, 1
  %116 = icmp eq i8 %115, 0
  store i1 %116, i1* %pf
  %117 = inttoptr i64 %102 to i8*
  store i8* %117, i8** %stack_var_-16
  br label %block_ed62

block_ed62:                                       ; preds = %block_ed5d, %block_ed30
  store volatile i64 60770, i64* @assembly_address
  %118 = load i8** %stack_var_-16
  %119 = ptrtoint i8* %118 to i64
  store i64 %119, i64* %rax
  store volatile i64 60774, i64* @assembly_address
  %120 = load i64* %rax
  %121 = inttoptr i64 %120 to i8*
  %122 = load i8* %121
  %123 = zext i8 %122 to i64
  store i64 %123, i64* %rax
  store volatile i64 60777, i64* @assembly_address
  %124 = load i64* %rax
  %125 = trunc i64 %124 to i8
  %126 = load i64* %rax
  %127 = trunc i64 %126 to i8
  %128 = and i8 %125, %127
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %zf
  %130 = icmp slt i8 %128, 0
  store i1 %130, i1* %sf
  %131 = call i8 @llvm.ctpop.i8(i8 %128)
  %132 = and i8 %131, 1
  %133 = icmp eq i8 %132, 0
  store i1 %133, i1* %pf
  store volatile i64 60779, i64* @assembly_address
  %134 = load i1* %zf
  %135 = icmp eq i1 %134, false
  br i1 %135, label %block_ed3a, label %block_ed6d

block_ed6d:                                       ; preds = %block_ed62
  store volatile i64 60781, i64* @assembly_address
  %136 = load i8** %stack_var_-24
  %137 = ptrtoint i8* %136 to i64
  store i64 %137, i64* %rax
  store volatile i64 60785, i64* @assembly_address
  %138 = load i64* %stack_var_-8
  store i64 %138, i64* %rbp
  %139 = ptrtoint i64* %stack_var_0 to i64
  store i64 %139, i64* %rsp
  store volatile i64 60786, i64* @assembly_address
  %140 = load i64* %rax
  ret i64 %140
}

declare i64 @263(i64*)

define i64 @base_len(i8* %arg1) {
block_ed73:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-32 = alloca i8*
  %1 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 60787, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 60788, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 60791, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 60795, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to i8*
  store i8* %22, i8** %stack_var_-32
  store volatile i64 60799, i64* @assembly_address
  store i64 0, i64* %stack_var_-16
  store volatile i64 60807, i64* @assembly_address
  %23 = load i8** %stack_var_-32
  %24 = ptrtoint i8* %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 60811, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rdi
  store volatile i64 60814, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to i8*
  %28 = call i32 @strlen(i8* %27)
  %29 = sext i32 %28 to i64
  store i64 %29, i64* %rax
  %30 = sext i32 %28 to i64
  store i64 %30, i64* %rax
  store volatile i64 60819, i64* @assembly_address
  %31 = load i64* %rax
  store i64 %31, i64* %stack_var_-24
  store volatile i64 60823, i64* @assembly_address
  br label %block_ed9e

block_ed99:                                       ; preds = %block_eda5
  store volatile i64 60825, i64* @assembly_address
  %32 = load i64* %stack_var_-24
  %33 = sub i64 %32, 1
  %34 = and i64 %32, 15
  %35 = sub i64 %34, 1
  %36 = icmp ugt i64 %35, 15
  %37 = icmp ult i64 %32, 1
  %38 = xor i64 %32, 1
  %39 = xor i64 %32, %33
  %40 = and i64 %38, %39
  %41 = icmp slt i64 %40, 0
  store i1 %36, i1* %az
  store i1 %37, i1* %cf
  store i1 %41, i1* %of
  %42 = icmp eq i64 %33, 0
  store i1 %42, i1* %zf
  %43 = icmp slt i64 %33, 0
  store i1 %43, i1* %sf
  %44 = trunc i64 %33 to i8
  %45 = call i8 @llvm.ctpop.i8(i8 %44)
  %46 = and i8 %45, 1
  %47 = icmp eq i8 %46, 0
  store i1 %47, i1* %pf
  store i64 %33, i64* %stack_var_-24
  br label %block_ed9e

block_ed9e:                                       ; preds = %block_ed99, %block_ed73
  store volatile i64 60830, i64* @assembly_address
  %48 = load i64* %stack_var_-24
  %49 = sub i64 %48, 1
  %50 = and i64 %48, 15
  %51 = sub i64 %50, 1
  %52 = icmp ugt i64 %51, 15
  %53 = icmp ult i64 %48, 1
  %54 = xor i64 %48, 1
  %55 = xor i64 %48, %49
  %56 = and i64 %54, %55
  %57 = icmp slt i64 %56, 0
  store i1 %52, i1* %az
  store i1 %53, i1* %cf
  store i1 %57, i1* %of
  %58 = icmp eq i64 %49, 0
  store i1 %58, i1* %zf
  %59 = icmp slt i64 %49, 0
  store i1 %59, i1* %sf
  %60 = trunc i64 %49 to i8
  %61 = call i8 @llvm.ctpop.i8(i8 %60)
  %62 = and i8 %61, 1
  %63 = icmp eq i8 %62, 0
  store i1 %63, i1* %pf
  store volatile i64 60835, i64* @assembly_address
  %64 = load i1* %cf
  %65 = load i1* %zf
  %66 = or i1 %64, %65
  br i1 %66, label %block_edbb, label %block_eda5

block_eda5:                                       ; preds = %block_ed9e
  store volatile i64 60837, i64* @assembly_address
  %67 = load i64* %stack_var_-24
  store i64 %67, i64* %rax
  store volatile i64 60841, i64* @assembly_address
  %68 = load i64* %rax
  %69 = add i64 %68, -1
  store i64 %69, i64* %rdx
  store volatile i64 60845, i64* @assembly_address
  %70 = load i8** %stack_var_-32
  %71 = ptrtoint i8* %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 60849, i64* @assembly_address
  %72 = load i64* %rax
  %73 = load i64* %rdx
  %74 = add i64 %72, %73
  %75 = and i64 %72, 15
  %76 = and i64 %73, 15
  %77 = add i64 %75, %76
  %78 = icmp ugt i64 %77, 15
  %79 = icmp ult i64 %74, %72
  %80 = xor i64 %72, %74
  %81 = xor i64 %73, %74
  %82 = and i64 %80, %81
  %83 = icmp slt i64 %82, 0
  store i1 %78, i1* %az
  store i1 %79, i1* %cf
  store i1 %83, i1* %of
  %84 = icmp eq i64 %74, 0
  store i1 %84, i1* %zf
  %85 = icmp slt i64 %74, 0
  store i1 %85, i1* %sf
  %86 = trunc i64 %74 to i8
  %87 = call i8 @llvm.ctpop.i8(i8 %86)
  %88 = and i8 %87, 1
  %89 = icmp eq i8 %88, 0
  store i1 %89, i1* %pf
  store i64 %74, i64* %rax
  store volatile i64 60852, i64* @assembly_address
  %90 = load i64* %rax
  %91 = inttoptr i64 %90 to i8*
  %92 = load i8* %91
  %93 = zext i8 %92 to i64
  store i64 %93, i64* %rax
  store volatile i64 60855, i64* @assembly_address
  %94 = load i64* %rax
  %95 = trunc i64 %94 to i8
  %96 = sub i8 %95, 47
  %97 = and i8 %95, 15
  %98 = sub i8 %97, 15
  %99 = icmp ugt i8 %98, 15
  %100 = icmp ult i8 %95, 47
  %101 = xor i8 %95, 47
  %102 = xor i8 %95, %96
  %103 = and i8 %101, %102
  %104 = icmp slt i8 %103, 0
  store i1 %99, i1* %az
  store i1 %100, i1* %cf
  store i1 %104, i1* %of
  %105 = icmp eq i8 %96, 0
  store i1 %105, i1* %zf
  %106 = icmp slt i8 %96, 0
  store i1 %106, i1* %sf
  %107 = call i8 @llvm.ctpop.i8(i8 %96)
  %108 = and i8 %107, 1
  %109 = icmp eq i8 %108, 0
  store i1 %109, i1* %pf
  store volatile i64 60857, i64* @assembly_address
  %110 = load i1* %zf
  br i1 %110, label %block_ed99, label %block_edbb

block_edbb:                                       ; preds = %block_eda5, %block_ed9e
  store volatile i64 60859, i64* @assembly_address
  %111 = load i64* %stack_var_-24
  store i64 %111, i64* %rax
  store volatile i64 60863, i64* @assembly_address
  %112 = load i64* %stack_var_-8
  store i64 %112, i64* %rbp
  %113 = ptrtoint i64* %stack_var_0 to i64
  store i64 %113, i64* %rsp
  store volatile i64 60864, i64* @assembly_address
  %114 = load i64* %rax
  ret i64 %114
}

declare i64 @264(i64)

define i64 @open_safer(i8* %arg1, i64 %arg2, i64* %arg3, i64 %arg4, i64 %arg5, i64 %arg6) {
block_edc1:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %xmm7 = alloca i128
  %xmm6 = alloca i128
  %xmm5 = alloca i128
  %xmm4 = alloca i128
  %xmm3 = alloca i128
  %xmm2 = alloca i128
  %xmm1 = alloca i128
  %xmm0 = alloca i128
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg6, i64* %r9
  store i64 %arg5, i64* %r8
  store i64 %arg4, i64* %rcx
  %0 = ptrtoint i64* %arg3 to i64
  store i64 %0, i64* %rdx
  store i64 %arg2, i64* %rsi
  %1 = bitcast i8* %arg1 to i64*
  %2 = ptrtoint i64* %1 to i64
  store i64 %2, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-200 = alloca i32*
  %3 = alloca i64
  %stack_var_-184 = alloca i64
  %stack_var_-208 = alloca i64
  %stack_var_8 = alloca i64
  %stack_var_-212 = alloca i32
  %stack_var_-216 = alloca i32
  %stack_var_-220 = alloca i32
  %stack_var_-192 = alloca i64
  %stack_var_-24 = alloca i128
  %stack_var_-40 = alloca i128
  %stack_var_-56 = alloca i128
  %stack_var_-72 = alloca i128
  %stack_var_-88 = alloca i128
  %stack_var_-104 = alloca i128
  %stack_var_-120 = alloca i128
  %stack_var_-136 = alloca i128
  %stack_var_-144 = alloca i64
  %stack_var_-152 = alloca i64
  %stack_var_-160 = alloca i64
  %stack_var_-168 = alloca i64
  %stack_var_-244 = alloca i32
  %stack_var_-240 = alloca i8*
  %4 = alloca i64
  %stack_var_-248 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 60865, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 60866, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 60869, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 240
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 240
  %13 = xor i64 %8, 240
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-248 to i64
  store i64 %23, i64* %rsp
  store volatile i64 60876, i64* @assembly_address
  %24 = load i64* %rdi
  %25 = inttoptr i64 %24 to i8*
  store i8* %25, i8** %stack_var_-240
  store volatile i64 60883, i64* @assembly_address
  %26 = load i64* %rsi
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-244
  store volatile i64 60889, i64* @assembly_address
  %28 = load i64* %rdx
  store i64 %28, i64* %stack_var_-168
  store volatile i64 60896, i64* @assembly_address
  %29 = load i64* %rcx
  store i64 %29, i64* %stack_var_-160
  store volatile i64 60903, i64* @assembly_address
  %30 = load i64* %r8
  store i64 %30, i64* %stack_var_-152
  store volatile i64 60910, i64* @assembly_address
  %31 = load i64* %r9
  store i64 %31, i64* %stack_var_-144
  store volatile i64 60917, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i8
  %34 = load i64* %rax
  %35 = trunc i64 %34 to i8
  %36 = and i8 %33, %35
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i8 %36, 0
  store i1 %38, i1* %sf
  %39 = call i8 @llvm.ctpop.i8(i8 %36)
  %40 = and i8 %39, 1
  %41 = icmp eq i8 %40, 0
  store i1 %41, i1* %pf
  store volatile i64 60919, i64* @assembly_address
  %42 = load i1* %zf
  br i1 %42, label %block_ee19, label %block_edf9

block_edf9:                                       ; preds = %block_edc1
  store volatile i64 60921, i64* @assembly_address
  %43 = load i128* %xmm0
  %44 = call i64 @__asm_movaps(i128 %43)
  %45 = sext i64 %44 to i128
  store i128 %45, i128* %stack_var_-136
  store volatile i64 60925, i64* @assembly_address
  %46 = load i128* %xmm1
  %47 = call i64 @__asm_movaps(i128 %46)
  %48 = sext i64 %47 to i128
  store i128 %48, i128* %stack_var_-120
  store volatile i64 60929, i64* @assembly_address
  %49 = load i128* %xmm2
  %50 = call i64 @__asm_movaps(i128 %49)
  %51 = sext i64 %50 to i128
  store i128 %51, i128* %stack_var_-104
  store volatile i64 60933, i64* @assembly_address
  %52 = load i128* %xmm3
  %53 = call i64 @__asm_movaps(i128 %52)
  %54 = sext i64 %53 to i128
  store i128 %54, i128* %stack_var_-88
  store volatile i64 60937, i64* @assembly_address
  %55 = load i128* %xmm4
  %56 = call i64 @__asm_movaps(i128 %55)
  %57 = sext i64 %56 to i128
  store i128 %57, i128* %stack_var_-72
  store volatile i64 60941, i64* @assembly_address
  %58 = load i128* %xmm5
  %59 = call i64 @__asm_movaps(i128 %58)
  %60 = sext i64 %59 to i128
  store i128 %60, i128* %stack_var_-56
  store volatile i64 60945, i64* @assembly_address
  %61 = load i128* %xmm6
  %62 = call i64 @__asm_movaps(i128 %61)
  %63 = sext i64 %62 to i128
  store i128 %63, i128* %stack_var_-40
  store volatile i64 60949, i64* @assembly_address
  %64 = load i128* %xmm7
  %65 = call i64 @__asm_movaps(i128 %64)
  %66 = sext i64 %65 to i128
  store i128 %66, i128* %stack_var_-24
  br label %block_ee19

block_ee19:                                       ; preds = %block_edf9, %block_edc1
  store volatile i64 60953, i64* @assembly_address
  %67 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %67, i64* %rax
  store volatile i64 60962, i64* @assembly_address
  %68 = load i64* %rax
  store i64 %68, i64* %stack_var_-192
  store volatile i64 60969, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %69 = icmp eq i32 0, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 0, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 0 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  %75 = zext i32 0 to i64
  store i64 %75, i64* %rax
  store volatile i64 60971, i64* @assembly_address
  store i32 0, i32* %stack_var_-220
  store volatile i64 60981, i64* @assembly_address
  %76 = load i32* %stack_var_-244
  %77 = zext i32 %76 to i64
  store i64 %77, i64* %rax
  store volatile i64 60987, i64* @assembly_address
  %78 = load i64* %rax
  %79 = trunc i64 %78 to i32
  %80 = and i32 %79, 64
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %81 = icmp eq i32 %80, 0
  store i1 %81, i1* %zf
  %82 = icmp slt i32 %80, 0
  store i1 %82, i1* %sf
  %83 = trunc i32 %80 to i8
  %84 = call i8 @llvm.ctpop.i8(i8 %83)
  %85 = and i8 %84, 1
  %86 = icmp eq i8 %85, 0
  store i1 %86, i1* %pf
  %87 = zext i32 %80 to i64
  store i64 %87, i64* %rax
  store volatile i64 60990, i64* @assembly_address
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  %90 = load i64* %rax
  %91 = trunc i64 %90 to i32
  %92 = and i32 %89, %91
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %93 = icmp eq i32 %92, 0
  store i1 %93, i1* %zf
  %94 = icmp slt i32 %92, 0
  store i1 %94, i1* %sf
  %95 = trunc i32 %92 to i8
  %96 = call i8 @llvm.ctpop.i8(i8 %95)
  %97 = and i8 %96, 1
  %98 = icmp eq i8 %97, 0
  store i1 %98, i1* %pf
  store volatile i64 60992, i64* @assembly_address
  %99 = load i1* %zf
  br i1 %99, label %block_eeb7, label %block_ee42

block_ee42:                                       ; preds = %block_ee19
  store volatile i64 60994, i64* @assembly_address
  store i32 16, i32* %stack_var_-216
  store volatile i64 61004, i64* @assembly_address
  store i32 48, i32* %stack_var_-212
  store volatile i64 61014, i64* @assembly_address
  %100 = ptrtoint i64* %stack_var_8 to i64
  store i64 %100, i64* %rax
  store volatile i64 61018, i64* @assembly_address
  %101 = ptrtoint i64* %stack_var_8 to i64
  store i64 %101, i64* %stack_var_-208
  store volatile i64 61025, i64* @assembly_address
  %102 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %102, i64* %rax
  store volatile i64 61032, i64* @assembly_address
  %103 = bitcast i64* %stack_var_-184 to i32*
  store i32* %103, i32** %stack_var_-200
  store volatile i64 61039, i64* @assembly_address
  %104 = load i32* %stack_var_-216
  %105 = zext i32 %104 to i64
  store i64 %105, i64* %rax
  store volatile i64 61045, i64* @assembly_address
  %106 = load i64* %rax
  %107 = trunc i64 %106 to i32
  %108 = sub i32 %107, 47
  %109 = and i32 %107, 15
  %110 = sub i32 %109, 15
  %111 = icmp ugt i32 %110, 15
  %112 = icmp ult i32 %107, 47
  %113 = xor i32 %107, 47
  %114 = xor i32 %107, %108
  %115 = and i32 %113, %114
  %116 = icmp slt i32 %115, 0
  store i1 %111, i1* %az
  store i1 %112, i1* %cf
  store i1 %116, i1* %of
  %117 = icmp eq i32 %108, 0
  store i1 %117, i1* %zf
  %118 = icmp slt i32 %108, 0
  store i1 %118, i1* %sf
  %119 = trunc i32 %108 to i8
  %120 = call i8 @llvm.ctpop.i8(i8 %119)
  %121 = and i8 %120, 1
  %122 = icmp eq i8 %121, 0
  store i1 %122, i1* %pf
  store volatile i64 61048, i64* @assembly_address
  %123 = load i1* %cf
  %124 = load i1* %zf
  %125 = or i1 %123, %124
  %126 = icmp ne i1 %125, true
  br i1 %126, label %block_ee9d, label %block_ee7a

block_ee7a:                                       ; preds = %block_ee42
  store volatile i64 61050, i64* @assembly_address
  %127 = load i32** %stack_var_-200
  %128 = ptrtoint i32* %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 61057, i64* @assembly_address
  %129 = load i32* %stack_var_-216
  %130 = zext i32 %129 to i64
  store i64 %130, i64* %rdx
  store volatile i64 61063, i64* @assembly_address
  %131 = load i64* %rdx
  %132 = trunc i64 %131 to i32
  %133 = zext i32 %132 to i64
  store i64 %133, i64* %rdx
  store volatile i64 61065, i64* @assembly_address
  %134 = load i64* %rax
  %135 = load i64* %rdx
  %136 = add i64 %134, %135
  %137 = and i64 %134, 15
  %138 = and i64 %135, 15
  %139 = add i64 %137, %138
  %140 = icmp ugt i64 %139, 15
  %141 = icmp ult i64 %136, %134
  %142 = xor i64 %134, %136
  %143 = xor i64 %135, %136
  %144 = and i64 %142, %143
  %145 = icmp slt i64 %144, 0
  store i1 %140, i1* %az
  store i1 %141, i1* %cf
  store i1 %145, i1* %of
  %146 = icmp eq i64 %136, 0
  store i1 %146, i1* %zf
  %147 = icmp slt i64 %136, 0
  store i1 %147, i1* %sf
  %148 = trunc i64 %136 to i8
  %149 = call i8 @llvm.ctpop.i8(i8 %148)
  %150 = and i8 %149, 1
  %151 = icmp eq i8 %150, 0
  store i1 %151, i1* %pf
  store i64 %136, i64* %rax
  store volatile i64 61068, i64* @assembly_address
  %152 = load i32* %stack_var_-216
  %153 = zext i32 %152 to i64
  store i64 %153, i64* %rdx
  store volatile i64 61074, i64* @assembly_address
  %154 = load i64* %rdx
  %155 = trunc i64 %154 to i32
  %156 = add i32 %155, 8
  %157 = and i32 %155, 15
  %158 = add i32 %157, 8
  %159 = icmp ugt i32 %158, 15
  %160 = icmp ult i32 %156, %155
  %161 = xor i32 %155, %156
  %162 = xor i32 8, %156
  %163 = and i32 %161, %162
  %164 = icmp slt i32 %163, 0
  store i1 %159, i1* %az
  store i1 %160, i1* %cf
  store i1 %164, i1* %of
  %165 = icmp eq i32 %156, 0
  store i1 %165, i1* %zf
  %166 = icmp slt i32 %156, 0
  store i1 %166, i1* %sf
  %167 = trunc i32 %156 to i8
  %168 = call i8 @llvm.ctpop.i8(i8 %167)
  %169 = and i8 %168, 1
  %170 = icmp eq i8 %169, 0
  store i1 %170, i1* %pf
  %171 = zext i32 %156 to i64
  store i64 %171, i64* %rdx
  store volatile i64 61077, i64* @assembly_address
  %172 = load i64* %rdx
  %173 = trunc i64 %172 to i32
  store i32 %173, i32* %stack_var_-216
  store volatile i64 61083, i64* @assembly_address
  br label %block_eeaf

block_ee9d:                                       ; preds = %block_ee42
  store volatile i64 61085, i64* @assembly_address
  %174 = load i64* %stack_var_-208
  store i64 %174, i64* %rax
  store volatile i64 61092, i64* @assembly_address
  %175 = load i64* %rax
  %176 = add i64 %175, 8
  store i64 %176, i64* %rdx
  store volatile i64 61096, i64* @assembly_address
  %177 = load i64* %rdx
  store i64 %177, i64* %stack_var_-208
  br label %block_eeaf

block_eeaf:                                       ; preds = %block_ee9d, %block_ee7a
  store volatile i64 61103, i64* @assembly_address
  %178 = load i64* %rax
  %179 = inttoptr i64 %178 to i32*
  %180 = load i32* %179
  %181 = zext i32 %180 to i64
  store i64 %181, i64* %rax
  store volatile i64 61105, i64* @assembly_address
  %182 = load i64* %rax
  %183 = trunc i64 %182 to i32
  store i32 %183, i32* %stack_var_-220
  br label %block_eeb7

block_eeb7:                                       ; preds = %block_eeaf, %block_ee19
  store volatile i64 61111, i64* @assembly_address
  %184 = load i32* %stack_var_-220
  %185 = zext i32 %184 to i64
  store i64 %185, i64* %rdx
  store volatile i64 61117, i64* @assembly_address
  %186 = load i32* %stack_var_-244
  %187 = zext i32 %186 to i64
  store i64 %187, i64* %rcx
  store volatile i64 61123, i64* @assembly_address
  %188 = load i8** %stack_var_-240
  %189 = ptrtoint i8* %188 to i64
  store i64 %189, i64* %rax
  store volatile i64 61130, i64* @assembly_address
  %190 = load i64* %rcx
  %191 = trunc i64 %190 to i32
  %192 = zext i32 %191 to i64
  store i64 %192, i64* %rsi
  store volatile i64 61132, i64* @assembly_address
  %193 = load i64* %rax
  store i64 %193, i64* %rdi
  store volatile i64 61135, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 61140, i64* @assembly_address
  %194 = load i64* %rdi
  %195 = inttoptr i64 %194 to i8*
  %196 = load i64* %rsi
  %197 = trunc i64 %196 to i32
  %198 = call i32 (i8*, i32, ...)* @open(i8* %195, i32 %197)
  %199 = sext i32 %198 to i64
  store i64 %199, i64* %rax
  %200 = sext i32 %198 to i64
  store i64 %200, i64* %rax
  store volatile i64 61145, i64* @assembly_address
  %201 = load i64* %rax
  %202 = trunc i64 %201 to i32
  %203 = zext i32 %202 to i64
  store i64 %203, i64* %rdi
  store volatile i64 61147, i64* @assembly_address
  %204 = load i64* %rdi
  %205 = trunc i64 %204 to i32
  %206 = call i64 @fd_safer(i32 %205)
  store i64 %206, i64* %rax
  store i64 %206, i64* %rax
  store volatile i64 61152, i64* @assembly_address
  %207 = load i64* %stack_var_-192
  store i64 %207, i64* %rsi
  store volatile i64 61159, i64* @assembly_address
  %208 = load i64* %rsi
  %209 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %210 = xor i64 %208, %209
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %211 = icmp eq i64 %210, 0
  store i1 %211, i1* %zf
  %212 = icmp slt i64 %210, 0
  store i1 %212, i1* %sf
  %213 = trunc i64 %210 to i8
  %214 = call i8 @llvm.ctpop.i8(i8 %213)
  %215 = and i8 %214, 1
  %216 = icmp eq i8 %215, 0
  store i1 %216, i1* %pf
  store i64 %210, i64* %rsi
  store volatile i64 61168, i64* @assembly_address
  %217 = load i1* %zf
  br i1 %217, label %block_eef7, label %block_eef2

block_eef2:                                       ; preds = %block_eeb7
  store volatile i64 61170, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_eef7:                                       ; preds = %block_eeb7
  store volatile i64 61175, i64* @assembly_address
  %218 = load i64* %stack_var_-8
  store i64 %218, i64* %rbp
  %219 = ptrtoint i64* %stack_var_0 to i64
  store i64 %219, i64* %rsp
  store volatile i64 61176, i64* @assembly_address
  %220 = load i64* %rax
  ret i64 %220
}

declare i64 @265(i64*, i32, i64*, i64, i64, i64)

declare i64 @266(i64*, i64, i64*, i64, i64, i64)

define i64 @openat_safer(i32 %arg1, i64 %arg2, i32 %arg3, i64 %arg4, i64 %arg5, i64 %arg6) {
block_eef9:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %xmm7 = alloca i128
  %xmm6 = alloca i128
  %xmm5 = alloca i128
  %xmm4 = alloca i128
  %xmm3 = alloca i128
  %xmm2 = alloca i128
  %xmm1 = alloca i128
  %xmm0 = alloca i128
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg6, i64* %r9
  store i64 %arg5, i64* %r8
  store i64 %arg4, i64* %rcx
  %0 = sext i32 %arg3 to i64
  store i64 %0, i64* %rdx
  store i64 %arg2, i64* %rsi
  %1 = sext i32 %arg1 to i64
  store i64 %1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-200 = alloca i32*
  %2 = alloca i64
  %stack_var_-184 = alloca i64
  %stack_var_-208 = alloca i64
  %stack_var_8 = alloca i64
  %stack_var_-212 = alloca i32
  %stack_var_-216 = alloca i32
  %stack_var_-220 = alloca i32
  %stack_var_-192 = alloca i64
  %stack_var_-24 = alloca i128
  %stack_var_-40 = alloca i128
  %stack_var_-56 = alloca i128
  %stack_var_-72 = alloca i128
  %stack_var_-88 = alloca i128
  %stack_var_-104 = alloca i128
  %stack_var_-120 = alloca i128
  %stack_var_-136 = alloca i128
  %stack_var_-144 = alloca i64
  %stack_var_-152 = alloca i64
  %stack_var_-160 = alloca i64
  %stack_var_-240 = alloca i32
  %stack_var_-236 = alloca i32
  %stack_var_-248 = alloca i8*
  %3 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 61177, i64* @assembly_address
  %4 = load i64* %rbp
  store i64 %4, i64* %stack_var_-8
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rsp
  store volatile i64 61178, i64* @assembly_address
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rbp
  store volatile i64 61181, i64* @assembly_address
  %7 = load i64* %rsp
  %8 = sub i64 %7, 240
  %9 = and i64 %7, 15
  %10 = icmp ugt i64 %9, 15
  %11 = icmp ult i64 %7, 240
  %12 = xor i64 %7, 240
  %13 = xor i64 %7, %8
  %14 = and i64 %12, %13
  %15 = icmp slt i64 %14, 0
  store i1 %10, i1* %az
  store i1 %11, i1* %cf
  store i1 %15, i1* %of
  %16 = icmp eq i64 %8, 0
  store i1 %16, i1* %zf
  %17 = icmp slt i64 %8, 0
  store i1 %17, i1* %sf
  %18 = trunc i64 %8 to i8
  %19 = call i8 @llvm.ctpop.i8(i8 %18)
  %20 = and i8 %19, 1
  %21 = icmp eq i8 %20, 0
  store i1 %21, i1* %pf
  %22 = ptrtoint i8** %stack_var_-248 to i64
  store i64 %22, i64* %rsp
  store volatile i64 61188, i64* @assembly_address
  %23 = load i64* %rdi
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-236
  store volatile i64 61194, i64* @assembly_address
  %25 = load i64* %rsi
  %26 = inttoptr i64 %25 to i8*
  store i8* %26, i8** %stack_var_-248
  store volatile i64 61201, i64* @assembly_address
  %27 = load i64* %rdx
  %28 = trunc i64 %27 to i32
  store i32 %28, i32* %stack_var_-240
  store volatile i64 61207, i64* @assembly_address
  %29 = load i64* %rcx
  store i64 %29, i64* %stack_var_-160
  store volatile i64 61214, i64* @assembly_address
  %30 = load i64* %r8
  store i64 %30, i64* %stack_var_-152
  store volatile i64 61221, i64* @assembly_address
  %31 = load i64* %r9
  store i64 %31, i64* %stack_var_-144
  store volatile i64 61228, i64* @assembly_address
  %32 = load i64* %rax
  %33 = trunc i64 %32 to i8
  %34 = load i64* %rax
  %35 = trunc i64 %34 to i8
  %36 = and i8 %33, %35
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i8 %36, 0
  store i1 %38, i1* %sf
  %39 = call i8 @llvm.ctpop.i8(i8 %36)
  %40 = and i8 %39, 1
  %41 = icmp eq i8 %40, 0
  store i1 %41, i1* %pf
  store volatile i64 61230, i64* @assembly_address
  %42 = load i1* %zf
  br i1 %42, label %block_ef50, label %block_ef30

block_ef30:                                       ; preds = %block_eef9
  store volatile i64 61232, i64* @assembly_address
  %43 = load i128* %xmm0
  %44 = call i64 @__asm_movaps(i128 %43)
  %45 = sext i64 %44 to i128
  store i128 %45, i128* %stack_var_-136
  store volatile i64 61236, i64* @assembly_address
  %46 = load i128* %xmm1
  %47 = call i64 @__asm_movaps(i128 %46)
  %48 = sext i64 %47 to i128
  store i128 %48, i128* %stack_var_-120
  store volatile i64 61240, i64* @assembly_address
  %49 = load i128* %xmm2
  %50 = call i64 @__asm_movaps(i128 %49)
  %51 = sext i64 %50 to i128
  store i128 %51, i128* %stack_var_-104
  store volatile i64 61244, i64* @assembly_address
  %52 = load i128* %xmm3
  %53 = call i64 @__asm_movaps(i128 %52)
  %54 = sext i64 %53 to i128
  store i128 %54, i128* %stack_var_-88
  store volatile i64 61248, i64* @assembly_address
  %55 = load i128* %xmm4
  %56 = call i64 @__asm_movaps(i128 %55)
  %57 = sext i64 %56 to i128
  store i128 %57, i128* %stack_var_-72
  store volatile i64 61252, i64* @assembly_address
  %58 = load i128* %xmm5
  %59 = call i64 @__asm_movaps(i128 %58)
  %60 = sext i64 %59 to i128
  store i128 %60, i128* %stack_var_-56
  store volatile i64 61256, i64* @assembly_address
  %61 = load i128* %xmm6
  %62 = call i64 @__asm_movaps(i128 %61)
  %63 = sext i64 %62 to i128
  store i128 %63, i128* %stack_var_-40
  store volatile i64 61260, i64* @assembly_address
  %64 = load i128* %xmm7
  %65 = call i64 @__asm_movaps(i128 %64)
  %66 = sext i64 %65 to i128
  store i128 %66, i128* %stack_var_-24
  br label %block_ef50

block_ef50:                                       ; preds = %block_ef30, %block_eef9
  store volatile i64 61264, i64* @assembly_address
  %67 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %67, i64* %rax
  store volatile i64 61273, i64* @assembly_address
  %68 = load i64* %rax
  store i64 %68, i64* %stack_var_-192
  store volatile i64 61280, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %69 = icmp eq i32 0, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 0, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 0 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  %75 = zext i32 0 to i64
  store i64 %75, i64* %rax
  store volatile i64 61282, i64* @assembly_address
  store i32 0, i32* %stack_var_-220
  store volatile i64 61292, i64* @assembly_address
  %76 = load i32* %stack_var_-240
  %77 = zext i32 %76 to i64
  store i64 %77, i64* %rax
  store volatile i64 61298, i64* @assembly_address
  %78 = load i64* %rax
  %79 = trunc i64 %78 to i32
  %80 = and i32 %79, 64
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %81 = icmp eq i32 %80, 0
  store i1 %81, i1* %zf
  %82 = icmp slt i32 %80, 0
  store i1 %82, i1* %sf
  %83 = trunc i32 %80 to i8
  %84 = call i8 @llvm.ctpop.i8(i8 %83)
  %85 = and i8 %84, 1
  %86 = icmp eq i8 %85, 0
  store i1 %86, i1* %pf
  %87 = zext i32 %80 to i64
  store i64 %87, i64* %rax
  store volatile i64 61301, i64* @assembly_address
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  %90 = load i64* %rax
  %91 = trunc i64 %90 to i32
  %92 = and i32 %89, %91
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %93 = icmp eq i32 %92, 0
  store i1 %93, i1* %zf
  %94 = icmp slt i32 %92, 0
  store i1 %94, i1* %sf
  %95 = trunc i32 %92 to i8
  %96 = call i8 @llvm.ctpop.i8(i8 %95)
  %97 = and i8 %96, 1
  %98 = icmp eq i8 %97, 0
  store i1 %98, i1* %pf
  store volatile i64 61303, i64* @assembly_address
  %99 = load i1* %zf
  br i1 %99, label %block_efee, label %block_ef79

block_ef79:                                       ; preds = %block_ef50
  store volatile i64 61305, i64* @assembly_address
  store i32 24, i32* %stack_var_-216
  store volatile i64 61315, i64* @assembly_address
  store i32 48, i32* %stack_var_-212
  store volatile i64 61325, i64* @assembly_address
  %100 = ptrtoint i64* %stack_var_8 to i64
  store i64 %100, i64* %rax
  store volatile i64 61329, i64* @assembly_address
  %101 = ptrtoint i64* %stack_var_8 to i64
  store i64 %101, i64* %stack_var_-208
  store volatile i64 61336, i64* @assembly_address
  %102 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %102, i64* %rax
  store volatile i64 61343, i64* @assembly_address
  %103 = bitcast i64* %stack_var_-184 to i32*
  store i32* %103, i32** %stack_var_-200
  store volatile i64 61350, i64* @assembly_address
  %104 = load i32* %stack_var_-216
  %105 = zext i32 %104 to i64
  store i64 %105, i64* %rax
  store volatile i64 61356, i64* @assembly_address
  %106 = load i64* %rax
  %107 = trunc i64 %106 to i32
  %108 = sub i32 %107, 47
  %109 = and i32 %107, 15
  %110 = sub i32 %109, 15
  %111 = icmp ugt i32 %110, 15
  %112 = icmp ult i32 %107, 47
  %113 = xor i32 %107, 47
  %114 = xor i32 %107, %108
  %115 = and i32 %113, %114
  %116 = icmp slt i32 %115, 0
  store i1 %111, i1* %az
  store i1 %112, i1* %cf
  store i1 %116, i1* %of
  %117 = icmp eq i32 %108, 0
  store i1 %117, i1* %zf
  %118 = icmp slt i32 %108, 0
  store i1 %118, i1* %sf
  %119 = trunc i32 %108 to i8
  %120 = call i8 @llvm.ctpop.i8(i8 %119)
  %121 = and i8 %120, 1
  %122 = icmp eq i8 %121, 0
  store i1 %122, i1* %pf
  store volatile i64 61359, i64* @assembly_address
  %123 = load i1* %cf
  %124 = load i1* %zf
  %125 = or i1 %123, %124
  %126 = icmp ne i1 %125, true
  br i1 %126, label %block_efd4, label %block_efb1

block_efb1:                                       ; preds = %block_ef79
  store volatile i64 61361, i64* @assembly_address
  %127 = load i32** %stack_var_-200
  %128 = ptrtoint i32* %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 61368, i64* @assembly_address
  %129 = load i32* %stack_var_-216
  %130 = zext i32 %129 to i64
  store i64 %130, i64* %rdx
  store volatile i64 61374, i64* @assembly_address
  %131 = load i64* %rdx
  %132 = trunc i64 %131 to i32
  %133 = zext i32 %132 to i64
  store i64 %133, i64* %rdx
  store volatile i64 61376, i64* @assembly_address
  %134 = load i64* %rax
  %135 = load i64* %rdx
  %136 = add i64 %134, %135
  %137 = and i64 %134, 15
  %138 = and i64 %135, 15
  %139 = add i64 %137, %138
  %140 = icmp ugt i64 %139, 15
  %141 = icmp ult i64 %136, %134
  %142 = xor i64 %134, %136
  %143 = xor i64 %135, %136
  %144 = and i64 %142, %143
  %145 = icmp slt i64 %144, 0
  store i1 %140, i1* %az
  store i1 %141, i1* %cf
  store i1 %145, i1* %of
  %146 = icmp eq i64 %136, 0
  store i1 %146, i1* %zf
  %147 = icmp slt i64 %136, 0
  store i1 %147, i1* %sf
  %148 = trunc i64 %136 to i8
  %149 = call i8 @llvm.ctpop.i8(i8 %148)
  %150 = and i8 %149, 1
  %151 = icmp eq i8 %150, 0
  store i1 %151, i1* %pf
  store i64 %136, i64* %rax
  store volatile i64 61379, i64* @assembly_address
  %152 = load i32* %stack_var_-216
  %153 = zext i32 %152 to i64
  store i64 %153, i64* %rdx
  store volatile i64 61385, i64* @assembly_address
  %154 = load i64* %rdx
  %155 = trunc i64 %154 to i32
  %156 = add i32 %155, 8
  %157 = and i32 %155, 15
  %158 = add i32 %157, 8
  %159 = icmp ugt i32 %158, 15
  %160 = icmp ult i32 %156, %155
  %161 = xor i32 %155, %156
  %162 = xor i32 8, %156
  %163 = and i32 %161, %162
  %164 = icmp slt i32 %163, 0
  store i1 %159, i1* %az
  store i1 %160, i1* %cf
  store i1 %164, i1* %of
  %165 = icmp eq i32 %156, 0
  store i1 %165, i1* %zf
  %166 = icmp slt i32 %156, 0
  store i1 %166, i1* %sf
  %167 = trunc i32 %156 to i8
  %168 = call i8 @llvm.ctpop.i8(i8 %167)
  %169 = and i8 %168, 1
  %170 = icmp eq i8 %169, 0
  store i1 %170, i1* %pf
  %171 = zext i32 %156 to i64
  store i64 %171, i64* %rdx
  store volatile i64 61388, i64* @assembly_address
  %172 = load i64* %rdx
  %173 = trunc i64 %172 to i32
  store i32 %173, i32* %stack_var_-216
  store volatile i64 61394, i64* @assembly_address
  br label %block_efe6

block_efd4:                                       ; preds = %block_ef79
  store volatile i64 61396, i64* @assembly_address
  %174 = load i64* %stack_var_-208
  store i64 %174, i64* %rax
  store volatile i64 61403, i64* @assembly_address
  %175 = load i64* %rax
  %176 = add i64 %175, 8
  store i64 %176, i64* %rdx
  store volatile i64 61407, i64* @assembly_address
  %177 = load i64* %rdx
  store i64 %177, i64* %stack_var_-208
  br label %block_efe6

block_efe6:                                       ; preds = %block_efd4, %block_efb1
  store volatile i64 61414, i64* @assembly_address
  %178 = load i64* %rax
  %179 = inttoptr i64 %178 to i32*
  %180 = load i32* %179
  %181 = zext i32 %180 to i64
  store i64 %181, i64* %rax
  store volatile i64 61416, i64* @assembly_address
  %182 = load i64* %rax
  %183 = trunc i64 %182 to i32
  store i32 %183, i32* %stack_var_-220
  br label %block_efee

block_efee:                                       ; preds = %block_efe6, %block_ef50
  store volatile i64 61422, i64* @assembly_address
  %184 = load i32* %stack_var_-220
  %185 = zext i32 %184 to i64
  store i64 %185, i64* %rcx
  store volatile i64 61428, i64* @assembly_address
  %186 = load i32* %stack_var_-240
  %187 = zext i32 %186 to i64
  store i64 %187, i64* %rdx
  store volatile i64 61434, i64* @assembly_address
  %188 = load i8** %stack_var_-248
  %189 = ptrtoint i8* %188 to i64
  store i64 %189, i64* %rsi
  store volatile i64 61441, i64* @assembly_address
  %190 = load i32* %stack_var_-236
  %191 = zext i32 %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 61447, i64* @assembly_address
  %192 = load i64* %rax
  %193 = trunc i64 %192 to i32
  %194 = zext i32 %193 to i64
  store i64 %194, i64* %rdi
  store volatile i64 61449, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 61454, i64* @assembly_address
  %195 = load i64* %rdi
  %196 = trunc i64 %195 to i32
  %197 = load i64* %rsi
  %198 = inttoptr i64 %197 to i8*
  %199 = load i64* %rdx
  %200 = trunc i64 %199 to i32
  %201 = call i32 (i32, i8*, i32, ...)* @openat(i32 %196, i8* %198, i32 %200)
  %202 = sext i32 %201 to i64
  store i64 %202, i64* %rax
  %203 = sext i32 %201 to i64
  store i64 %203, i64* %rax
  store volatile i64 61459, i64* @assembly_address
  %204 = load i64* %rax
  %205 = trunc i64 %204 to i32
  %206 = zext i32 %205 to i64
  store i64 %206, i64* %rdi
  store volatile i64 61461, i64* @assembly_address
  %207 = load i64* %rdi
  %208 = trunc i64 %207 to i32
  %209 = call i64 @fd_safer(i32 %208)
  store i64 %209, i64* %rax
  store i64 %209, i64* %rax
  store volatile i64 61466, i64* @assembly_address
  %210 = load i64* %stack_var_-192
  store i64 %210, i64* %rdi
  store volatile i64 61473, i64* @assembly_address
  %211 = load i64* %rdi
  %212 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %213 = xor i64 %211, %212
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %214 = icmp eq i64 %213, 0
  store i1 %214, i1* %zf
  %215 = icmp slt i64 %213, 0
  store i1 %215, i1* %sf
  %216 = trunc i64 %213 to i8
  %217 = call i8 @llvm.ctpop.i8(i8 %216)
  %218 = and i8 %217, 1
  %219 = icmp eq i8 %218, 0
  store i1 %219, i1* %pf
  store i64 %213, i64* %rdi
  store volatile i64 61482, i64* @assembly_address
  %220 = load i1* %zf
  br i1 %220, label %block_f031, label %block_f02c

block_f02c:                                       ; preds = %block_efee
  store volatile i64 61484, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_f031:                                       ; preds = %block_efee
  store volatile i64 61489, i64* @assembly_address
  %221 = load i64* %stack_var_-8
  store i64 %221, i64* %rbp
  %222 = ptrtoint i64* %stack_var_0 to i64
  store i64 %222, i64* %rsp
  store volatile i64 61490, i64* @assembly_address
  %223 = load i64* %rax
  ret i64 %223
}

declare i64 @267(i64, i8*, i32, i64, i64, i64)

declare i64 @268(i64, i64, i32, i64, i64, i64)

define i64 @direntry_cmp_name(i64* %arg1, i64 %arg2) {
block_f033:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 61491, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 61492, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 61495, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 61499, i64* @assembly_address
  %20 = load i64* %rdi
  store i64 %20, i64* %stack_var_-32
  store volatile i64 61503, i64* @assembly_address
  %21 = load i64* %rsi
  store i64 %21, i64* %stack_var_-40
  store volatile i64 61507, i64* @assembly_address
  %22 = load i64* %stack_var_-32
  store i64 %22, i64* %rax
  store volatile i64 61511, i64* @assembly_address
  %23 = load i64* %rax
  store i64 %23, i64* %stack_var_-24
  store volatile i64 61515, i64* @assembly_address
  %24 = load i64* %stack_var_-40
  store i64 %24, i64* %rax
  store volatile i64 61519, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %stack_var_-16
  store volatile i64 61523, i64* @assembly_address
  %26 = load i64* %stack_var_-16
  store i64 %26, i64* %rax
  store volatile i64 61527, i64* @assembly_address
  %27 = load i64* %rax
  %28 = inttoptr i64 %27 to i64*
  %29 = load i64* %28
  store i64 %29, i64* %rdx
  store volatile i64 61530, i64* @assembly_address
  %30 = load i64* %stack_var_-24
  store i64 %30, i64* %rax
  store volatile i64 61534, i64* @assembly_address
  %31 = load i64* %rax
  %32 = inttoptr i64 %31 to i64*
  %33 = load i64* %32
  store i64 %33, i64* %rax
  store volatile i64 61537, i64* @assembly_address
  %34 = load i64* %rdx
  store i64 %34, i64* %rsi
  store volatile i64 61540, i64* @assembly_address
  %35 = load i64* %rax
  store i64 %35, i64* %rdi
  store volatile i64 61543, i64* @assembly_address
  %36 = load i64* %rdi
  %37 = inttoptr i64 %36 to i8*
  %38 = load i64* %rsi
  %39 = inttoptr i64 %38 to i8*
  %40 = call i32 @strcmp(i8* %37, i8* %39)
  %41 = sext i32 %40 to i64
  store i64 %41, i64* %rax
  %42 = sext i32 %40 to i64
  store i64 %42, i64* %rax
  store volatile i64 61548, i64* @assembly_address
  %43 = load i64* %stack_var_-8
  store i64 %43, i64* %rbp
  %44 = ptrtoint i64* %stack_var_0 to i64
  store i64 %44, i64* %rsp
  store volatile i64 61549, i64* @assembly_address
  %45 = load i64* %rax
  ret i64 %45
}

declare i64 @269(i64, i64*)

declare i64 @270(i64, i64)

define i64 @streamsavedir(%__dirstream* %arg1, i64 %arg2) {
block_f06e:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint %__dirstream* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-40 = alloca i8*
  %1 = alloca i64
  %stack_var_-80 = alloca i64
  %stack_var_-140 = alloca i32
  %stack_var_-136 = alloca i8*
  %2 = alloca i64
  %stack_var_-48 = alloca i32
  %3 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-64 = alloca i8*
  %4 = alloca i64
  %stack_var_-72 = alloca i32 (i64*, i64*)*
  %5 = alloca i64
  %stack_var_-88 = alloca i8*
  %6 = alloca i64
  %stack_var_-96 = alloca i32
  %7 = alloca i64
  %stack_var_-104 = alloca i8*
  %8 = alloca i64
  %stack_var_-112 = alloca i64
  %stack_var_-120 = alloca i8*
  %9 = alloca i64
  %stack_var_-128 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-164 = alloca i32
  %stack_var_-160 = alloca %__dirstream*
  %10 = alloca i64
  %stack_var_-168 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 61550, i64* @assembly_address
  %11 = load i64* %rbp
  store i64 %11, i64* %stack_var_-8
  %12 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %12, i64* %rsp
  store volatile i64 61551, i64* @assembly_address
  %13 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %13, i64* %rbp
  store volatile i64 61554, i64* @assembly_address
  %14 = load i64* %rbx
  store i64 %14, i64* %stack_var_-16
  %15 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %15, i64* %rsp
  store volatile i64 61555, i64* @assembly_address
  %16 = load i64* %rsp
  %17 = sub i64 %16, 152
  %18 = and i64 %16, 15
  %19 = sub i64 %18, 8
  %20 = icmp ugt i64 %19, 15
  %21 = icmp ult i64 %16, 152
  %22 = xor i64 %16, 152
  %23 = xor i64 %16, %17
  %24 = and i64 %22, %23
  %25 = icmp slt i64 %24, 0
  store i1 %20, i1* %az
  store i1 %21, i1* %cf
  store i1 %25, i1* %of
  %26 = icmp eq i64 %17, 0
  store i1 %26, i1* %zf
  %27 = icmp slt i64 %17, 0
  store i1 %27, i1* %sf
  %28 = trunc i64 %17 to i8
  %29 = call i8 @llvm.ctpop.i8(i8 %28)
  %30 = and i8 %29, 1
  %31 = icmp eq i8 %30, 0
  store i1 %31, i1* %pf
  %32 = ptrtoint i64* %stack_var_-168 to i64
  store i64 %32, i64* %rsp
  store volatile i64 61562, i64* @assembly_address
  %33 = load i64* %rdi
  %34 = inttoptr i64 %33 to %__dirstream*
  store %__dirstream* %34, %__dirstream** %stack_var_-160
  store volatile i64 61569, i64* @assembly_address
  %35 = load i64* %rsi
  %36 = trunc i64 %35 to i32
  store i32 %36, i32* %stack_var_-164
  store volatile i64 61575, i64* @assembly_address
  %37 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %37, i64* %rax
  store volatile i64 61584, i64* @assembly_address
  %38 = load i64* %rax
  store i64 %38, i64* %stack_var_-32
  store volatile i64 61588, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %39 = icmp eq i32 0, 0
  store i1 %39, i1* %zf
  %40 = icmp slt i32 0, 0
  store i1 %40, i1* %sf
  %41 = trunc i32 0 to i8
  %42 = call i8 @llvm.ctpop.i8(i8 %41)
  %43 = and i8 %42, 1
  %44 = icmp eq i8 %43, 0
  store i1 %44, i1* %pf
  %45 = zext i32 0 to i64
  store i64 %45, i64* %rax
  store volatile i64 61590, i64* @assembly_address
  store i64 0, i64* %stack_var_-128
  store volatile i64 61598, i64* @assembly_address
  %46 = inttoptr i64 0 to i8*
  store i8* %46, i8** %stack_var_-120
  store volatile i64 61606, i64* @assembly_address
  store i64 0, i64* %stack_var_-112
  store volatile i64 61614, i64* @assembly_address
  %47 = inttoptr i64 0 to i8*
  store i8* %47, i8** %stack_var_-104
  store volatile i64 61622, i64* @assembly_address
  %48 = trunc i64 0 to i32
  store i32 %48, i32* %stack_var_-96
  store volatile i64 61630, i64* @assembly_address
  %49 = inttoptr i64 0 to i8*
  store i8* %49, i8** %stack_var_-88
  store volatile i64 61638, i64* @assembly_address
  %50 = load i32* %stack_var_-164
  %51 = zext i32 %50 to i64
  store i64 %51, i64* %rax
  store volatile i64 61644, i64* @assembly_address
  %52 = load i64* %rax
  %53 = mul i64 %52, 8
  store i64 %53, i64* %rdx
  store volatile i64 61652, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_215b50 to i64), i64* %rax
  store volatile i64 61659, i64* @assembly_address
  %54 = load i64* %rdx
  %55 = load i64* %rax
  %56 = mul i64 %55, 1
  %57 = add i64 %54, %56
  %58 = inttoptr i64 %57 to i64*
  %59 = load i64* %58
  store i64 %59, i64* %rax
  store volatile i64 61663, i64* @assembly_address
  %60 = load i64* %rax
  %61 = inttoptr i64 %60 to i32 (i64*, i64*)*
  store i32 (i64*, i64*)* %61, i32 (i64*, i64*)** %stack_var_-72
  store volatile i64 61667, i64* @assembly_address
  %62 = load %__dirstream** %stack_var_-160
  %63 = ptrtoint %__dirstream* %62 to i64
  %64 = and i64 %63, 15
  %65 = icmp ugt i64 %64, 15
  %66 = icmp ult i64 %63, 0
  %67 = xor i64 %63, 0
  %68 = and i64 %67, 0
  %69 = icmp slt i64 %68, 0
  store i1 %65, i1* %az
  store i1 %66, i1* %cf
  store i1 %69, i1* %of
  %70 = icmp eq i64 %63, 0
  store i1 %70, i1* %zf
  %71 = icmp slt i64 %63, 0
  store i1 %71, i1* %sf
  %72 = trunc i64 %63 to i8
  %73 = call i8 @llvm.ctpop.i8(i8 %72)
  %74 = and i8 %73, 1
  %75 = icmp eq i8 %74, 0
  store i1 %75, i1* %pf
  store volatile i64 61675, i64* @assembly_address
  %76 = load i1* %zf
  %77 = icmp eq i1 %76, false
  br i1 %77, label %block_f0f7, label %block_f0ed

block_f0ed:                                       ; preds = %block_f06e
  store volatile i64 61677, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 61682, i64* @assembly_address
  br label %block_f3ba

block_f0f7:                                       ; preds = %block_f255, %block_f159, %block_f06e
  store volatile i64 61687, i64* @assembly_address
  %78 = call i32* @__errno_location()
  %79 = ptrtoint i32* %78 to i64
  store i64 %79, i64* %rax
  %80 = ptrtoint i32* %78 to i64
  store i64 %80, i64* %rax
  %81 = ptrtoint i32* %78 to i64
  store i64 %81, i64* %rax
  store volatile i64 61692, i64* @assembly_address
  %82 = load i64* %rax
  %83 = inttoptr i64 %82 to i32*
  store i32 0, i32* %83
  store volatile i64 61698, i64* @assembly_address
  %84 = load %__dirstream** %stack_var_-160
  %85 = ptrtoint %__dirstream* %84 to i64
  store i64 %85, i64* %rax
  store volatile i64 61705, i64* @assembly_address
  %86 = load i64* %rax
  store i64 %86, i64* %rdi
  store volatile i64 61708, i64* @assembly_address
  %87 = load i64* %rdi
  %88 = inttoptr i64 %87 to %__dirstream*
  %89 = call %dirent* @readdir(%__dirstream* %88)
  %90 = ptrtoint %dirent* %89 to i64
  store i64 %90, i64* %rax
  %91 = ptrtoint %dirent* %89 to i64
  store i64 %91, i64* %rax
  store volatile i64 61713, i64* @assembly_address
  %92 = load i64* %rax
  %93 = inttoptr i64 %92 to i8*
  store i8* %93, i8** %stack_var_-64
  store volatile i64 61717, i64* @assembly_address
  %94 = load i8** %stack_var_-64
  %95 = ptrtoint i8* %94 to i64
  %96 = and i64 %95, 15
  %97 = icmp ugt i64 %96, 15
  %98 = icmp ult i64 %95, 0
  %99 = xor i64 %95, 0
  %100 = and i64 %99, 0
  %101 = icmp slt i64 %100, 0
  store i1 %97, i1* %az
  store i1 %98, i1* %cf
  store i1 %101, i1* %of
  %102 = icmp eq i64 %95, 0
  store i1 %102, i1* %zf
  %103 = icmp slt i64 %95, 0
  store i1 %103, i1* %sf
  %104 = trunc i64 %95 to i8
  %105 = call i8 @llvm.ctpop.i8(i8 %104)
  %106 = and i8 %105, 1
  %107 = icmp eq i8 %106, 0
  store i1 %107, i1* %pf
  store volatile i64 61722, i64* @assembly_address
  %108 = load i1* %zf
  br i1 %108, label %block_f262, label %block_f120

block_f120:                                       ; preds = %block_f0f7
  store volatile i64 61728, i64* @assembly_address
  %109 = load i8** %stack_var_-64
  %110 = ptrtoint i8* %109 to i64
  store i64 %110, i64* %rax
  store volatile i64 61732, i64* @assembly_address
  %111 = load i64* %rax
  %112 = add i64 %111, 19
  %113 = and i64 %111, 15
  %114 = add i64 %113, 3
  %115 = icmp ugt i64 %114, 15
  %116 = icmp ult i64 %112, %111
  %117 = xor i64 %111, %112
  %118 = xor i64 19, %112
  %119 = and i64 %117, %118
  %120 = icmp slt i64 %119, 0
  store i1 %115, i1* %az
  store i1 %116, i1* %cf
  store i1 %120, i1* %of
  %121 = icmp eq i64 %112, 0
  store i1 %121, i1* %zf
  %122 = icmp slt i64 %112, 0
  store i1 %122, i1* %sf
  %123 = trunc i64 %112 to i8
  %124 = call i8 @llvm.ctpop.i8(i8 %123)
  %125 = and i8 %124, 1
  %126 = icmp eq i8 %125, 0
  store i1 %126, i1* %pf
  store i64 %112, i64* %rax
  store volatile i64 61736, i64* @assembly_address
  %127 = load i64* %rax
  store i64 %127, i64* %stack_var_-56
  store volatile i64 61740, i64* @assembly_address
  %128 = load i64* %stack_var_-56
  store i64 %128, i64* %rax
  store volatile i64 61744, i64* @assembly_address
  %129 = load i64* %rax
  %130 = inttoptr i64 %129 to i8*
  %131 = load i8* %130
  %132 = zext i8 %131 to i64
  store i64 %132, i64* %rax
  store volatile i64 61747, i64* @assembly_address
  %133 = load i64* %rax
  %134 = trunc i64 %133 to i8
  %135 = sub i8 %134, 46
  %136 = and i8 %134, 15
  %137 = sub i8 %136, 14
  %138 = icmp ugt i8 %137, 15
  %139 = icmp ult i8 %134, 46
  %140 = xor i8 %134, 46
  %141 = xor i8 %134, %135
  %142 = and i8 %140, %141
  %143 = icmp slt i8 %142, 0
  store i1 %138, i1* %az
  store i1 %139, i1* %cf
  store i1 %143, i1* %of
  %144 = icmp eq i8 %135, 0
  store i1 %144, i1* %zf
  %145 = icmp slt i8 %135, 0
  store i1 %145, i1* %sf
  %146 = call i8 @llvm.ctpop.i8(i8 %135)
  %147 = and i8 %146, 1
  %148 = icmp eq i8 %147, 0
  store i1 %148, i1* %pf
  store volatile i64 61749, i64* @assembly_address
  %149 = load i1* %zf
  %150 = icmp eq i1 %149, false
  br i1 %150, label %block_f154, label %block_f137

block_f137:                                       ; preds = %block_f120
  store volatile i64 61751, i64* @assembly_address
  %151 = load i64* %stack_var_-56
  store i64 %151, i64* %rax
  store volatile i64 61755, i64* @assembly_address
  %152 = load i64* %rax
  %153 = add i64 %152, 1
  %154 = and i64 %152, 15
  %155 = add i64 %154, 1
  %156 = icmp ugt i64 %155, 15
  %157 = icmp ult i64 %153, %152
  %158 = xor i64 %152, %153
  %159 = xor i64 1, %153
  %160 = and i64 %158, %159
  %161 = icmp slt i64 %160, 0
  store i1 %156, i1* %az
  store i1 %157, i1* %cf
  store i1 %161, i1* %of
  %162 = icmp eq i64 %153, 0
  store i1 %162, i1* %zf
  %163 = icmp slt i64 %153, 0
  store i1 %163, i1* %sf
  %164 = trunc i64 %153 to i8
  %165 = call i8 @llvm.ctpop.i8(i8 %164)
  %166 = and i8 %165, 1
  %167 = icmp eq i8 %166, 0
  store i1 %167, i1* %pf
  store i64 %153, i64* %rax
  store volatile i64 61759, i64* @assembly_address
  %168 = load i64* %rax
  %169 = inttoptr i64 %168 to i8*
  %170 = load i8* %169
  %171 = zext i8 %170 to i64
  store i64 %171, i64* %rax
  store volatile i64 61762, i64* @assembly_address
  %172 = load i64* %rax
  %173 = trunc i64 %172 to i8
  %174 = sub i8 %173, 46
  %175 = and i8 %173, 15
  %176 = sub i8 %175, 14
  %177 = icmp ugt i8 %176, 15
  %178 = icmp ult i8 %173, 46
  %179 = xor i8 %173, 46
  %180 = xor i8 %173, %174
  %181 = and i8 %179, %180
  %182 = icmp slt i8 %181, 0
  store i1 %177, i1* %az
  store i1 %178, i1* %cf
  store i1 %182, i1* %of
  %183 = icmp eq i8 %174, 0
  store i1 %183, i1* %zf
  %184 = icmp slt i8 %174, 0
  store i1 %184, i1* %sf
  %185 = call i8 @llvm.ctpop.i8(i8 %174)
  %186 = and i8 %185, 1
  %187 = icmp eq i8 %186, 0
  store i1 %187, i1* %pf
  store volatile i64 61764, i64* @assembly_address
  %188 = load i1* %zf
  br i1 %188, label %block_f14d, label %block_f146

block_f146:                                       ; preds = %block_f137
  store volatile i64 61766, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 61771, i64* @assembly_address
  br label %block_f159

block_f14d:                                       ; preds = %block_f137
  store volatile i64 61773, i64* @assembly_address
  store i64 2, i64* %rax
  store volatile i64 61778, i64* @assembly_address
  br label %block_f159

block_f154:                                       ; preds = %block_f120
  store volatile i64 61780, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_f159

block_f159:                                       ; preds = %block_f154, %block_f14d, %block_f146
  store volatile i64 61785, i64* @assembly_address
  %189 = load i64* %stack_var_-56
  store i64 %189, i64* %rdx
  store volatile i64 61789, i64* @assembly_address
  %190 = load i64* %rax
  %191 = load i64* %rdx
  %192 = add i64 %190, %191
  %193 = and i64 %190, 15
  %194 = and i64 %191, 15
  %195 = add i64 %193, %194
  %196 = icmp ugt i64 %195, 15
  %197 = icmp ult i64 %192, %190
  %198 = xor i64 %190, %192
  %199 = xor i64 %191, %192
  %200 = and i64 %198, %199
  %201 = icmp slt i64 %200, 0
  store i1 %196, i1* %az
  store i1 %197, i1* %cf
  store i1 %201, i1* %of
  %202 = icmp eq i64 %192, 0
  store i1 %202, i1* %zf
  %203 = icmp slt i64 %192, 0
  store i1 %203, i1* %sf
  %204 = trunc i64 %192 to i8
  %205 = call i8 @llvm.ctpop.i8(i8 %204)
  %206 = and i8 %205, 1
  %207 = icmp eq i8 %206, 0
  store i1 %207, i1* %pf
  store i64 %192, i64* %rax
  store volatile i64 61792, i64* @assembly_address
  %208 = load i64* %rax
  %209 = inttoptr i64 %208 to i8*
  %210 = load i8* %209
  %211 = zext i8 %210 to i64
  store i64 %211, i64* %rax
  store volatile i64 61795, i64* @assembly_address
  %212 = load i64* %rax
  %213 = trunc i64 %212 to i8
  %214 = load i64* %rax
  %215 = trunc i64 %214 to i8
  %216 = and i8 %213, %215
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %217 = icmp eq i8 %216, 0
  store i1 %217, i1* %zf
  %218 = icmp slt i8 %216, 0
  store i1 %218, i1* %sf
  %219 = call i8 @llvm.ctpop.i8(i8 %216)
  %220 = and i8 %219, 1
  %221 = icmp eq i8 %220, 0
  store i1 %221, i1* %pf
  store volatile i64 61797, i64* @assembly_address
  %222 = load i1* %zf
  br i1 %222, label %block_f0f7, label %block_f167

block_f167:                                       ; preds = %block_f159
  store volatile i64 61799, i64* @assembly_address
  %223 = load i8** %stack_var_-64
  %224 = ptrtoint i8* %223 to i64
  store i64 %224, i64* %rax
  store volatile i64 61803, i64* @assembly_address
  %225 = load i64* %rax
  %226 = add i64 %225, 19
  %227 = and i64 %225, 15
  %228 = add i64 %227, 3
  %229 = icmp ugt i64 %228, 15
  %230 = icmp ult i64 %226, %225
  %231 = xor i64 %225, %226
  %232 = xor i64 19, %226
  %233 = and i64 %231, %232
  %234 = icmp slt i64 %233, 0
  store i1 %229, i1* %az
  store i1 %230, i1* %cf
  store i1 %234, i1* %of
  %235 = icmp eq i64 %226, 0
  store i1 %235, i1* %zf
  %236 = icmp slt i64 %226, 0
  store i1 %236, i1* %sf
  %237 = trunc i64 %226 to i8
  %238 = call i8 @llvm.ctpop.i8(i8 %237)
  %239 = and i8 %238, 1
  %240 = icmp eq i8 %239, 0
  store i1 %240, i1* %pf
  store i64 %226, i64* %rax
  store volatile i64 61807, i64* @assembly_address
  %241 = load i64* %rax
  store i64 %241, i64* %rdi
  store volatile i64 61810, i64* @assembly_address
  %242 = load i64* %rdi
  %243 = inttoptr i64 %242 to i8*
  %244 = call i32 @strlen(i8* %243)
  %245 = sext i32 %244 to i64
  store i64 %245, i64* %rax
  %246 = sext i32 %244 to i64
  store i64 %246, i64* %rax
  store volatile i64 61815, i64* @assembly_address
  %247 = load i64* %rax
  %248 = add i64 %247, 1
  %249 = and i64 %247, 15
  %250 = add i64 %249, 1
  %251 = icmp ugt i64 %250, 15
  %252 = icmp ult i64 %248, %247
  %253 = xor i64 %247, %248
  %254 = xor i64 1, %248
  %255 = and i64 %253, %254
  %256 = icmp slt i64 %255, 0
  store i1 %251, i1* %az
  store i1 %252, i1* %cf
  store i1 %256, i1* %of
  %257 = icmp eq i64 %248, 0
  store i1 %257, i1* %zf
  %258 = icmp slt i64 %248, 0
  store i1 %258, i1* %sf
  %259 = trunc i64 %248 to i8
  %260 = call i8 @llvm.ctpop.i8(i8 %259)
  %261 = and i8 %260, 1
  %262 = icmp eq i8 %261, 0
  store i1 %262, i1* %pf
  store i64 %248, i64* %rax
  store volatile i64 61819, i64* @assembly_address
  %263 = load i64* %rax
  %264 = trunc i64 %263 to i32
  store i32 %264, i32* %stack_var_-48
  store volatile i64 61823, i64* @assembly_address
  %265 = load i32 (i64*, i64*)** %stack_var_-72
  %266 = ptrtoint i32 (i64*, i64*)* %265 to i64
  %267 = and i64 %266, 15
  %268 = icmp ugt i64 %267, 15
  %269 = icmp ult i64 %266, 0
  %270 = xor i64 %266, 0
  %271 = and i64 %270, 0
  %272 = icmp slt i64 %271, 0
  store i1 %268, i1* %az
  store i1 %269, i1* %cf
  store i1 %272, i1* %of
  %273 = icmp eq i64 %266, 0
  store i1 %273, i1* %zf
  %274 = icmp slt i64 %266, 0
  store i1 %274, i1* %sf
  %275 = trunc i64 %266 to i8
  %276 = call i8 @llvm.ctpop.i8(i8 %275)
  %277 = and i8 %276, 1
  %278 = icmp eq i8 %277, 0
  store i1 %278, i1* %pf
  store volatile i64 61828, i64* @assembly_address
  %279 = load i1* %zf
  br i1 %279, label %block_f1e6, label %block_f186

block_f186:                                       ; preds = %block_f167
  store volatile i64 61830, i64* @assembly_address
  %280 = load i8** %stack_var_-104
  %281 = ptrtoint i8* %280 to i64
  store i64 %281, i64* %rax
  store volatile i64 61834, i64* @assembly_address
  %282 = load i64* %rax
  %283 = load i32* %stack_var_-96
  %284 = sext i32 %283 to i64
  %285 = sub i64 %282, %284
  %286 = and i64 %282, 15
  %287 = and i64 %284, 15
  %288 = sub i64 %286, %287
  %289 = icmp ugt i64 %288, 15
  %290 = icmp ult i64 %282, %284
  %291 = xor i64 %282, %284
  %292 = xor i64 %282, %285
  %293 = and i64 %291, %292
  %294 = icmp slt i64 %293, 0
  store i1 %289, i1* %az
  store i1 %290, i1* %cf
  store i1 %294, i1* %of
  %295 = icmp eq i64 %285, 0
  store i1 %295, i1* %zf
  %296 = icmp slt i64 %285, 0
  store i1 %296, i1* %sf
  %297 = trunc i64 %285 to i8
  %298 = call i8 @llvm.ctpop.i8(i8 %297)
  %299 = and i8 %298, 1
  %300 = icmp eq i8 %299, 0
  store i1 %300, i1* %pf
  store volatile i64 61838, i64* @assembly_address
  %301 = load i1* %zf
  %302 = icmp eq i1 %301, false
  br i1 %302, label %block_f1bc, label %block_f190

block_f190:                                       ; preds = %block_f186
  store volatile i64 61840, i64* @assembly_address
  %303 = load i8** %stack_var_-104
  %304 = ptrtoint i8* %303 to i64
  store i64 %304, i64* %rax
  store volatile i64 61844, i64* @assembly_address
  %305 = load i64* %rax
  %306 = inttoptr i64 %305 to i8*
  store i8* %306, i8** %stack_var_-136
  store volatile i64 61848, i64* @assembly_address
  %307 = ptrtoint i8** %stack_var_-136 to i64
  store i64 %307, i64* %rcx
  store volatile i64 61852, i64* @assembly_address
  %308 = load i64* %stack_var_-112
  store i64 %308, i64* %rax
  store volatile i64 61856, i64* @assembly_address
  store i64 8, i64* %rdx
  store volatile i64 61861, i64* @assembly_address
  %309 = ptrtoint i8** %stack_var_-136 to i64
  store i64 %309, i64* %rsi
  store volatile i64 61864, i64* @assembly_address
  %310 = load i64* %rax
  store i64 %310, i64* %rdi
  store volatile i64 61867, i64* @assembly_address
  %311 = load i64* %rdi
  %312 = load i64* %rsi
  %313 = inttoptr i64 %312 to i64*
  %314 = load i64* %rdx
  %315 = sext i64 %314 to i128
  %316 = call i64 @x2nrealloc(i64 %311, i64* %313, i128 %315)
  store i64 %316, i64* %rax
  store i64 %316, i64* %rax
  store volatile i64 61872, i64* @assembly_address
  %317 = load i64* %rax
  store i64 %317, i64* %stack_var_-112
  store volatile i64 61876, i64* @assembly_address
  %318 = load i8** %stack_var_-136
  %319 = ptrtoint i8* %318 to i64
  store i64 %319, i64* %rax
  store volatile i64 61880, i64* @assembly_address
  %320 = load i64* %rax
  %321 = inttoptr i64 %320 to i8*
  store i8* %321, i8** %stack_var_-104
  br label %block_f1bc

block_f1bc:                                       ; preds = %block_f190, %block_f186
  store volatile i64 61884, i64* @assembly_address
  %322 = load i32* %stack_var_-96
  %323 = sext i32 %322 to i64
  store i64 %323, i64* %rax
  store volatile i64 61888, i64* @assembly_address
  %324 = load i64* %rax
  %325 = mul i64 %324, 8
  store i64 %325, i64* %rdx
  store volatile i64 61896, i64* @assembly_address
  %326 = load i64* %stack_var_-112
  store i64 %326, i64* %rax
  store volatile i64 61900, i64* @assembly_address
  %327 = load i64* %rdx
  %328 = load i64* %rax
  %329 = mul i64 %328, 1
  %330 = add i64 %327, %329
  store i64 %330, i64* %rbx
  store volatile i64 61904, i64* @assembly_address
  %331 = load i64* %stack_var_-56
  store i64 %331, i64* %rax
  store volatile i64 61908, i64* @assembly_address
  %332 = load i64* %rax
  store i64 %332, i64* %rdi
  store volatile i64 61911, i64* @assembly_address
  %333 = load i64* %rdi
  %334 = inttoptr i64 %333 to i8*
  %335 = call i64 @xstrdup(i8* %334)
  store i64 %335, i64* %rax
  store i64 %335, i64* %rax
  store volatile i64 61916, i64* @assembly_address
  %336 = load i64* %rax
  %337 = load i64* %rbx
  %338 = inttoptr i64 %337 to i64*
  store i64 %336, i64* %338
  store volatile i64 61919, i64* @assembly_address
  %339 = load i32* %stack_var_-96
  %340 = sext i32 %339 to i64
  %341 = add i64 %340, 1
  %342 = and i64 %340, 15
  %343 = add i64 %342, 1
  %344 = icmp ugt i64 %343, 15
  %345 = icmp ult i64 %341, %340
  %346 = xor i64 %340, %341
  %347 = xor i64 1, %341
  %348 = and i64 %346, %347
  %349 = icmp slt i64 %348, 0
  store i1 %344, i1* %az
  store i1 %345, i1* %cf
  store i1 %349, i1* %of
  %350 = icmp eq i64 %341, 0
  store i1 %350, i1* %zf
  %351 = icmp slt i64 %341, 0
  store i1 %351, i1* %sf
  %352 = trunc i64 %341 to i8
  %353 = call i8 @llvm.ctpop.i8(i8 %352)
  %354 = and i8 %353, 1
  %355 = icmp eq i8 %354, 0
  store i1 %355, i1* %pf
  %356 = trunc i64 %341 to i32
  store i32 %356, i32* %stack_var_-96
  store volatile i64 61924, i64* @assembly_address
  br label %block_f255

block_f1e6:                                       ; preds = %block_f167
  store volatile i64 61926, i64* @assembly_address
  %357 = load i8** %stack_var_-120
  %358 = ptrtoint i8* %357 to i64
  store i64 %358, i64* %rax
  store volatile i64 61930, i64* @assembly_address
  %359 = load i64* %rax
  %360 = load i8** %stack_var_-88
  %361 = ptrtoint i8* %360 to i64
  %362 = sub i64 %359, %361
  %363 = and i64 %359, 15
  %364 = and i64 %361, 15
  %365 = sub i64 %363, %364
  %366 = icmp ugt i64 %365, 15
  %367 = icmp ult i64 %359, %361
  %368 = xor i64 %359, %361
  %369 = xor i64 %359, %362
  %370 = and i64 %368, %369
  %371 = icmp slt i64 %370, 0
  store i1 %366, i1* %az
  store i1 %367, i1* %cf
  store i1 %371, i1* %of
  %372 = icmp eq i64 %362, 0
  store i1 %372, i1* %zf
  %373 = icmp slt i64 %362, 0
  store i1 %373, i1* %sf
  %374 = trunc i64 %362 to i8
  %375 = call i8 @llvm.ctpop.i8(i8 %374)
  %376 = and i8 %375, 1
  %377 = icmp eq i8 %376, 0
  store i1 %377, i1* %pf
  store i64 %362, i64* %rax
  store volatile i64 61934, i64* @assembly_address
  %378 = load i32* %stack_var_-48
  %379 = sext i32 %378 to i64
  %380 = load i64* %rax
  %381 = sub i64 %379, %380
  %382 = and i64 %379, 15
  %383 = and i64 %380, 15
  %384 = sub i64 %382, %383
  %385 = icmp ugt i64 %384, 15
  %386 = icmp ult i64 %379, %380
  %387 = xor i64 %379, %380
  %388 = xor i64 %379, %381
  %389 = and i64 %387, %388
  %390 = icmp slt i64 %389, 0
  store i1 %385, i1* %az
  store i1 %386, i1* %cf
  store i1 %390, i1* %of
  %391 = icmp eq i64 %381, 0
  store i1 %391, i1* %zf
  %392 = icmp slt i64 %381, 0
  store i1 %392, i1* %sf
  %393 = trunc i64 %381 to i8
  %394 = call i8 @llvm.ctpop.i8(i8 %393)
  %395 = and i8 %394, 1
  %396 = icmp eq i8 %395, 0
  store i1 %396, i1* %pf
  store volatile i64 61938, i64* @assembly_address
  %397 = load i1* %cf
  br i1 %397, label %block_f236, label %block_f1f4

block_f1f4:                                       ; preds = %block_f1e6
  store volatile i64 61940, i64* @assembly_address
  %398 = load i8** %stack_var_-88
  %399 = ptrtoint i8* %398 to i64
  store i64 %399, i64* %rdx
  store volatile i64 61944, i64* @assembly_address
  %400 = load i32* %stack_var_-48
  %401 = sext i32 %400 to i64
  store i64 %401, i64* %rax
  store volatile i64 61948, i64* @assembly_address
  %402 = load i64* %rax
  %403 = load i64* %rdx
  %404 = add i64 %402, %403
  %405 = and i64 %402, 15
  %406 = and i64 %403, 15
  %407 = add i64 %405, %406
  %408 = icmp ugt i64 %407, 15
  %409 = icmp ult i64 %404, %402
  %410 = xor i64 %402, %404
  %411 = xor i64 %403, %404
  %412 = and i64 %410, %411
  %413 = icmp slt i64 %412, 0
  store i1 %408, i1* %az
  store i1 %409, i1* %cf
  store i1 %413, i1* %of
  %414 = icmp eq i64 %404, 0
  store i1 %414, i1* %zf
  %415 = icmp slt i64 %404, 0
  store i1 %415, i1* %sf
  %416 = trunc i64 %404 to i8
  %417 = call i8 @llvm.ctpop.i8(i8 %416)
  %418 = and i8 %417, 1
  %419 = icmp eq i8 %418, 0
  store i1 %419, i1* %pf
  store i64 %404, i64* %rax
  store volatile i64 61951, i64* @assembly_address
  %420 = load i64* %rax
  %421 = inttoptr i64 %420 to i8*
  store i8* %421, i8** %stack_var_-136
  store volatile i64 61955, i64* @assembly_address
  %422 = load i8** %stack_var_-136
  %423 = ptrtoint i8* %422 to i64
  store i64 %423, i64* %rax
  store volatile i64 61959, i64* @assembly_address
  %424 = load i8** %stack_var_-88
  %425 = ptrtoint i8* %424 to i64
  %426 = load i64* %rax
  %427 = sub i64 %425, %426
  %428 = and i64 %425, 15
  %429 = and i64 %426, 15
  %430 = sub i64 %428, %429
  %431 = icmp ugt i64 %430, 15
  %432 = icmp ult i64 %425, %426
  %433 = xor i64 %425, %426
  %434 = xor i64 %425, %427
  %435 = and i64 %433, %434
  %436 = icmp slt i64 %435, 0
  store i1 %431, i1* %az
  store i1 %432, i1* %cf
  store i1 %436, i1* %of
  %437 = icmp eq i64 %427, 0
  store i1 %437, i1* %zf
  %438 = icmp slt i64 %427, 0
  store i1 %438, i1* %sf
  %439 = trunc i64 %427 to i8
  %440 = call i8 @llvm.ctpop.i8(i8 %439)
  %441 = and i8 %440, 1
  %442 = icmp eq i8 %441, 0
  store i1 %442, i1* %pf
  store volatile i64 61963, i64* @assembly_address
  %443 = load i1* %cf
  %444 = load i1* %zf
  %445 = or i1 %443, %444
  br i1 %445, label %block_f212, label %block_f20d

block_f20d:                                       ; preds = %block_f1f4
  store volatile i64 61965, i64* @assembly_address
  %446 = call i64 @xalloc_die()
  store i64 %446, i64* %rax
  store i64 %446, i64* %rax
  store i64 %446, i64* %rax
  unreachable

block_f212:                                       ; preds = %block_f1f4
  store volatile i64 61970, i64* @assembly_address
  %447 = ptrtoint i8** %stack_var_-136 to i64
  store i64 %447, i64* %rcx
  store volatile i64 61974, i64* @assembly_address
  %448 = load i64* %stack_var_-128
  store i64 %448, i64* %rax
  store volatile i64 61978, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 61983, i64* @assembly_address
  %449 = ptrtoint i8** %stack_var_-136 to i64
  store i64 %449, i64* %rsi
  store volatile i64 61986, i64* @assembly_address
  %450 = load i64* %rax
  store i64 %450, i64* %rdi
  store volatile i64 61989, i64* @assembly_address
  %451 = load i64* %rdi
  %452 = load i64* %rsi
  %453 = inttoptr i64 %452 to i64*
  %454 = load i64* %rdx
  %455 = sext i64 %454 to i128
  %456 = call i64 @x2nrealloc(i64 %451, i64* %453, i128 %455)
  store i64 %456, i64* %rax
  store i64 %456, i64* %rax
  store volatile i64 61994, i64* @assembly_address
  %457 = load i64* %rax
  store i64 %457, i64* %stack_var_-128
  store volatile i64 61998, i64* @assembly_address
  %458 = load i8** %stack_var_-136
  %459 = ptrtoint i8* %458 to i64
  store i64 %459, i64* %rax
  store volatile i64 62002, i64* @assembly_address
  %460 = load i64* %rax
  %461 = inttoptr i64 %460 to i8*
  store i8* %461, i8** %stack_var_-120
  br label %block_f236

block_f236:                                       ; preds = %block_f212, %block_f1e6
  store volatile i64 62006, i64* @assembly_address
  %462 = load i64* %stack_var_-128
  store i64 %462, i64* %rdx
  store volatile i64 62010, i64* @assembly_address
  %463 = load i8** %stack_var_-88
  %464 = ptrtoint i8* %463 to i64
  store i64 %464, i64* %rax
  store volatile i64 62014, i64* @assembly_address
  %465 = load i64* %rdx
  %466 = load i64* %rax
  %467 = mul i64 %466, 1
  %468 = add i64 %465, %467
  store i64 %468, i64* %rcx
  store volatile i64 62018, i64* @assembly_address
  %469 = load i32* %stack_var_-48
  %470 = sext i32 %469 to i64
  store i64 %470, i64* %rdx
  store volatile i64 62022, i64* @assembly_address
  %471 = load i64* %stack_var_-56
  store i64 %471, i64* %rax
  store volatile i64 62026, i64* @assembly_address
  %472 = load i64* %rax
  store i64 %472, i64* %rsi
  store volatile i64 62029, i64* @assembly_address
  %473 = load i64* %rcx
  store i64 %473, i64* %rdi
  store volatile i64 62032, i64* @assembly_address
  %474 = load i64* %rdi
  %475 = inttoptr i64 %474 to i64*
  %476 = load i64* %rsi
  %477 = inttoptr i64 %476 to i64*
  %478 = load i64* %rdx
  %479 = trunc i64 %478 to i32
  %480 = call i64* @memcpy(i64* %475, i64* %477, i32 %479)
  %481 = ptrtoint i64* %480 to i64
  store i64 %481, i64* %rax
  %482 = ptrtoint i64* %480 to i64
  store i64 %482, i64* %rax
  br label %block_f255

block_f255:                                       ; preds = %block_f236, %block_f1bc
  store volatile i64 62037, i64* @assembly_address
  %483 = load i32* %stack_var_-48
  %484 = sext i32 %483 to i64
  store i64 %484, i64* %rax
  store volatile i64 62041, i64* @assembly_address
  %485 = load i8** %stack_var_-88
  %486 = ptrtoint i8* %485 to i64
  %487 = load i64* %rax
  %488 = add i64 %486, %487
  %489 = and i64 %486, 15
  %490 = and i64 %487, 15
  %491 = add i64 %489, %490
  %492 = icmp ugt i64 %491, 15
  %493 = icmp ult i64 %488, %486
  %494 = xor i64 %486, %488
  %495 = xor i64 %487, %488
  %496 = and i64 %494, %495
  %497 = icmp slt i64 %496, 0
  store i1 %492, i1* %az
  store i1 %493, i1* %cf
  store i1 %497, i1* %of
  %498 = icmp eq i64 %488, 0
  store i1 %498, i1* %zf
  %499 = icmp slt i64 %488, 0
  store i1 %499, i1* %sf
  %500 = trunc i64 %488 to i8
  %501 = call i8 @llvm.ctpop.i8(i8 %500)
  %502 = and i8 %501, 1
  %503 = icmp eq i8 %502, 0
  store i1 %503, i1* %pf
  %504 = inttoptr i64 %488 to i8*
  store i8* %504, i8** %stack_var_-88
  store volatile i64 62045, i64* @assembly_address
  br label %block_f0f7

block_f262:                                       ; preds = %block_f0f7
  store volatile i64 62050, i64* @assembly_address
  store volatile i64 62051, i64* @assembly_address
  %505 = call i32* @__errno_location()
  %506 = ptrtoint i32* %505 to i64
  store i64 %506, i64* %rax
  %507 = ptrtoint i32* %505 to i64
  store i64 %507, i64* %rax
  %508 = ptrtoint i32* %505 to i64
  store i64 %508, i64* %rax
  store volatile i64 62056, i64* @assembly_address
  %509 = load i64* %rax
  %510 = inttoptr i64 %509 to i32*
  %511 = load i32* %510
  %512 = zext i32 %511 to i64
  store i64 %512, i64* %rax
  store volatile i64 62058, i64* @assembly_address
  %513 = load i64* %rax
  %514 = trunc i64 %513 to i32
  store i32 %514, i32* %stack_var_-140
  store volatile i64 62064, i64* @assembly_address
  %515 = load i32* %stack_var_-140
  %516 = and i32 %515, 15
  %517 = icmp ugt i32 %516, 15
  %518 = icmp ult i32 %515, 0
  %519 = xor i32 %515, 0
  %520 = and i32 %519, 0
  %521 = icmp slt i32 %520, 0
  store i1 %517, i1* %az
  store i1 %518, i1* %cf
  store i1 %521, i1* %of
  %522 = icmp eq i32 %515, 0
  store i1 %522, i1* %zf
  %523 = icmp slt i32 %515, 0
  store i1 %523, i1* %sf
  %524 = trunc i32 %515 to i8
  %525 = call i8 @llvm.ctpop.i8(i8 %524)
  %526 = and i8 %525, 1
  %527 = icmp eq i8 %526, 0
  store i1 %527, i1* %pf
  store volatile i64 62071, i64* @assembly_address
  %528 = load i1* %zf
  br i1 %528, label %block_f2ab, label %block_f279

block_f279:                                       ; preds = %block_f262
  store volatile i64 62073, i64* @assembly_address
  %529 = load i64* %stack_var_-112
  store i64 %529, i64* %rax
  store volatile i64 62077, i64* @assembly_address
  %530 = load i64* %rax
  store i64 %530, i64* %rdi
  store volatile i64 62080, i64* @assembly_address
  %531 = load i64* %rdi
  %532 = inttoptr i64 %531 to i64*
  call void @free(i64* %532)
  store volatile i64 62085, i64* @assembly_address
  %533 = load i64* %stack_var_-128
  store i64 %533, i64* %rax
  store volatile i64 62089, i64* @assembly_address
  %534 = load i64* %rax
  store i64 %534, i64* %rdi
  store volatile i64 62092, i64* @assembly_address
  %535 = load i64* %rdi
  %536 = inttoptr i64 %535 to i64*
  call void @free(i64* %536)
  store volatile i64 62097, i64* @assembly_address
  %537 = call i32* @__errno_location()
  %538 = ptrtoint i32* %537 to i64
  store i64 %538, i64* %rax
  %539 = ptrtoint i32* %537 to i64
  store i64 %539, i64* %rax
  %540 = ptrtoint i32* %537 to i64
  store i64 %540, i64* %rax
  store volatile i64 62102, i64* @assembly_address
  %541 = load i64* %rax
  store i64 %541, i64* %rdx
  store volatile i64 62105, i64* @assembly_address
  %542 = load i32* %stack_var_-140
  %543 = zext i32 %542 to i64
  store i64 %543, i64* %rax
  store volatile i64 62111, i64* @assembly_address
  %544 = load i64* %rax
  %545 = trunc i64 %544 to i32
  %546 = load i64* %rdx
  %547 = inttoptr i64 %546 to i32*
  store i32 %545, i32* %547
  store volatile i64 62113, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 62118, i64* @assembly_address
  br label %block_f3ba

block_f2ab:                                       ; preds = %block_f262
  store volatile i64 62123, i64* @assembly_address
  %548 = load i32 (i64*, i64*)** %stack_var_-72
  %549 = ptrtoint i32 (i64*, i64*)* %548 to i64
  %550 = and i64 %549, 15
  %551 = icmp ugt i64 %550, 15
  %552 = icmp ult i64 %549, 0
  %553 = xor i64 %549, 0
  %554 = and i64 %553, 0
  %555 = icmp slt i64 %554, 0
  store i1 %551, i1* %az
  store i1 %552, i1* %cf
  store i1 %555, i1* %of
  %556 = icmp eq i64 %549, 0
  store i1 %556, i1* %zf
  %557 = icmp slt i64 %549, 0
  store i1 %557, i1* %sf
  %558 = trunc i64 %549 to i8
  %559 = call i8 @llvm.ctpop.i8(i8 %558)
  %560 = and i8 %559, 1
  %561 = icmp eq i8 %560, 0
  store i1 %561, i1* %pf
  store volatile i64 62128, i64* @assembly_address
  %562 = load i1* %zf
  br i1 %562, label %block_f383, label %block_f2b6

block_f2b6:                                       ; preds = %block_f2ab
  store volatile i64 62134, i64* @assembly_address
  %563 = load i32* %stack_var_-96
  %564 = sext i32 %563 to i64
  %565 = and i64 %564, 15
  %566 = icmp ugt i64 %565, 15
  %567 = icmp ult i64 %564, 0
  %568 = xor i64 %564, 0
  %569 = and i64 %568, 0
  %570 = icmp slt i64 %569, 0
  store i1 %566, i1* %az
  store i1 %567, i1* %cf
  store i1 %570, i1* %of
  %571 = icmp eq i64 %564, 0
  store i1 %571, i1* %zf
  %572 = icmp slt i64 %564, 0
  store i1 %572, i1* %sf
  %573 = trunc i64 %564 to i8
  %574 = call i8 @llvm.ctpop.i8(i8 %573)
  %575 = and i8 %574, 1
  %576 = icmp eq i8 %575, 0
  store i1 %576, i1* %pf
  store volatile i64 62139, i64* @assembly_address
  %577 = load i1* %zf
  br i1 %577, label %block_f2d9, label %block_f2bd

block_f2bd:                                       ; preds = %block_f2b6
  store volatile i64 62141, i64* @assembly_address
  %578 = load i32 (i64*, i64*)** %stack_var_-72
  %579 = ptrtoint i32 (i64*, i64*)* %578 to i64
  store i64 %579, i64* %rdx
  store volatile i64 62145, i64* @assembly_address
  %580 = load i32* %stack_var_-96
  %581 = sext i32 %580 to i64
  store i64 %581, i64* %rsi
  store volatile i64 62149, i64* @assembly_address
  %582 = load i64* %stack_var_-112
  store i64 %582, i64* %rax
  store volatile i64 62153, i64* @assembly_address
  %583 = load i64* %rdx
  store i64 %583, i64* %rcx
  store volatile i64 62156, i64* @assembly_address
  store i64 8, i64* %rdx
  store volatile i64 62161, i64* @assembly_address
  %584 = load i64* %rax
  store i64 %584, i64* %rdi
  store volatile i64 62164, i64* @assembly_address
  %585 = load i64* %rdi
  %586 = inttoptr i64 %585 to i64*
  %587 = load i64* %rsi
  %588 = trunc i64 %587 to i32
  %589 = load i64* %rdx
  %590 = trunc i64 %589 to i32
  %591 = load i64* %rcx
  %592 = inttoptr i64 %591 to i32 (i64*, i64*)*
  call void @qsort(i64* %586, i32 %588, i32 %590, i32 (i64*, i64*)* %592)
  br label %block_f2d9

block_f2d9:                                       ; preds = %block_f2bd, %block_f2b6
  store volatile i64 62169, i64* @assembly_address
  %593 = load i8** %stack_var_-88
  %594 = ptrtoint i8* %593 to i64
  store i64 %594, i64* %rax
  store volatile i64 62173, i64* @assembly_address
  %595 = load i64* %rax
  %596 = add i64 %595, 1
  %597 = and i64 %595, 15
  %598 = add i64 %597, 1
  %599 = icmp ugt i64 %598, 15
  %600 = icmp ult i64 %596, %595
  %601 = xor i64 %595, %596
  %602 = xor i64 1, %596
  %603 = and i64 %601, %602
  %604 = icmp slt i64 %603, 0
  store i1 %599, i1* %az
  store i1 %600, i1* %cf
  store i1 %604, i1* %of
  %605 = icmp eq i64 %596, 0
  store i1 %605, i1* %zf
  %606 = icmp slt i64 %596, 0
  store i1 %606, i1* %sf
  %607 = trunc i64 %596 to i8
  %608 = call i8 @llvm.ctpop.i8(i8 %607)
  %609 = and i8 %608, 1
  %610 = icmp eq i8 %609, 0
  store i1 %610, i1* %pf
  store i64 %596, i64* %rax
  store volatile i64 62177, i64* @assembly_address
  %611 = load i64* %rax
  store i64 %611, i64* %rdi
  store volatile i64 62180, i64* @assembly_address
  %612 = load i64* %rdi
  %613 = trunc i64 %612 to i32
  %614 = call i64 @xmalloc(i32 %613)
  store i64 %614, i64* %rax
  store i64 %614, i64* %rax
  store volatile i64 62185, i64* @assembly_address
  %615 = load i64* %rax
  store i64 %615, i64* %stack_var_-128
  store volatile i64 62189, i64* @assembly_address
  %616 = inttoptr i64 0 to i8*
  store i8* %616, i8** %stack_var_-88
  store volatile i64 62197, i64* @assembly_address
  store i64 0, i64* %stack_var_-80
  store volatile i64 62205, i64* @assembly_address
  br label %block_f36b

block_f2ff:                                       ; preds = %block_f36b
  store volatile i64 62207, i64* @assembly_address
  %617 = load i64* %stack_var_-128
  store i64 %617, i64* %rdx
  store volatile i64 62211, i64* @assembly_address
  %618 = load i8** %stack_var_-88
  %619 = ptrtoint i8* %618 to i64
  store i64 %619, i64* %rax
  store volatile i64 62215, i64* @assembly_address
  %620 = load i64* %rax
  %621 = load i64* %rdx
  %622 = add i64 %620, %621
  %623 = and i64 %620, 15
  %624 = and i64 %621, 15
  %625 = add i64 %623, %624
  %626 = icmp ugt i64 %625, 15
  %627 = icmp ult i64 %622, %620
  %628 = xor i64 %620, %622
  %629 = xor i64 %621, %622
  %630 = and i64 %628, %629
  %631 = icmp slt i64 %630, 0
  store i1 %626, i1* %az
  store i1 %627, i1* %cf
  store i1 %631, i1* %of
  %632 = icmp eq i64 %622, 0
  store i1 %632, i1* %zf
  %633 = icmp slt i64 %622, 0
  store i1 %633, i1* %sf
  %634 = trunc i64 %622 to i8
  %635 = call i8 @llvm.ctpop.i8(i8 %634)
  %636 = and i8 %635, 1
  %637 = icmp eq i8 %636, 0
  store i1 %637, i1* %pf
  store i64 %622, i64* %rax
  store volatile i64 62218, i64* @assembly_address
  %638 = load i64* %rax
  %639 = inttoptr i64 %638 to i8*
  store i8* %639, i8** %stack_var_-40
  store volatile i64 62222, i64* @assembly_address
  %640 = load i64* %stack_var_-80
  store i64 %640, i64* %rax
  store volatile i64 62226, i64* @assembly_address
  %641 = load i64* %rax
  %642 = mul i64 %641, 8
  store i64 %642, i64* %rdx
  store volatile i64 62234, i64* @assembly_address
  %643 = load i64* %stack_var_-112
  store i64 %643, i64* %rax
  store volatile i64 62238, i64* @assembly_address
  %644 = load i64* %rax
  %645 = load i64* %rdx
  %646 = add i64 %644, %645
  %647 = and i64 %644, 15
  %648 = and i64 %645, 15
  %649 = add i64 %647, %648
  %650 = icmp ugt i64 %649, 15
  %651 = icmp ult i64 %646, %644
  %652 = xor i64 %644, %646
  %653 = xor i64 %645, %646
  %654 = and i64 %652, %653
  %655 = icmp slt i64 %654, 0
  store i1 %650, i1* %az
  store i1 %651, i1* %cf
  store i1 %655, i1* %of
  %656 = icmp eq i64 %646, 0
  store i1 %656, i1* %zf
  %657 = icmp slt i64 %646, 0
  store i1 %657, i1* %sf
  %658 = trunc i64 %646 to i8
  %659 = call i8 @llvm.ctpop.i8(i8 %658)
  %660 = and i8 %659, 1
  %661 = icmp eq i8 %660, 0
  store i1 %661, i1* %pf
  store i64 %646, i64* %rax
  store volatile i64 62241, i64* @assembly_address
  %662 = load i64* %rax
  %663 = inttoptr i64 %662 to i64*
  %664 = load i64* %663
  store i64 %664, i64* %rdx
  store volatile i64 62244, i64* @assembly_address
  %665 = load i8** %stack_var_-40
  %666 = ptrtoint i8* %665 to i64
  store i64 %666, i64* %rax
  store volatile i64 62248, i64* @assembly_address
  %667 = load i64* %rdx
  store i64 %667, i64* %rsi
  store volatile i64 62251, i64* @assembly_address
  %668 = load i64* %rax
  store i64 %668, i64* %rdi
  store volatile i64 62254, i64* @assembly_address
  %669 = load i64* %rdi
  %670 = inttoptr i64 %669 to i8*
  %671 = load i64* %rsi
  %672 = inttoptr i64 %671 to i8*
  %673 = call i8* @stpcpy(i8* %670, i8* %672)
  %674 = ptrtoint i8* %673 to i64
  store i64 %674, i64* %rax
  %675 = ptrtoint i8* %673 to i64
  store i64 %675, i64* %rax
  store volatile i64 62259, i64* @assembly_address
  %676 = load i64* %rax
  store i64 %676, i64* %rdx
  store volatile i64 62262, i64* @assembly_address
  %677 = load i8** %stack_var_-40
  %678 = ptrtoint i8* %677 to i64
  store i64 %678, i64* %rax
  store volatile i64 62266, i64* @assembly_address
  %679 = load i64* %rdx
  %680 = load i64* %rax
  %681 = sub i64 %679, %680
  %682 = and i64 %679, 15
  %683 = and i64 %680, 15
  %684 = sub i64 %682, %683
  %685 = icmp ugt i64 %684, 15
  %686 = icmp ult i64 %679, %680
  %687 = xor i64 %679, %680
  %688 = xor i64 %679, %681
  %689 = and i64 %687, %688
  %690 = icmp slt i64 %689, 0
  store i1 %685, i1* %az
  store i1 %686, i1* %cf
  store i1 %690, i1* %of
  %691 = icmp eq i64 %681, 0
  store i1 %691, i1* %zf
  %692 = icmp slt i64 %681, 0
  store i1 %692, i1* %sf
  %693 = trunc i64 %681 to i8
  %694 = call i8 @llvm.ctpop.i8(i8 %693)
  %695 = and i8 %694, 1
  %696 = icmp eq i8 %695, 0
  store i1 %696, i1* %pf
  store i64 %681, i64* %rdx
  store volatile i64 62269, i64* @assembly_address
  %697 = load i64* %rdx
  store i64 %697, i64* %rax
  store volatile i64 62272, i64* @assembly_address
  %698 = load i64* %rax
  %699 = add i64 %698, 1
  %700 = and i64 %698, 15
  %701 = add i64 %700, 1
  %702 = icmp ugt i64 %701, 15
  %703 = icmp ult i64 %699, %698
  %704 = xor i64 %698, %699
  %705 = xor i64 1, %699
  %706 = and i64 %704, %705
  %707 = icmp slt i64 %706, 0
  store i1 %702, i1* %az
  store i1 %703, i1* %cf
  store i1 %707, i1* %of
  %708 = icmp eq i64 %699, 0
  store i1 %708, i1* %zf
  %709 = icmp slt i64 %699, 0
  store i1 %709, i1* %sf
  %710 = trunc i64 %699 to i8
  %711 = call i8 @llvm.ctpop.i8(i8 %710)
  %712 = and i8 %711, 1
  %713 = icmp eq i8 %712, 0
  store i1 %713, i1* %pf
  store i64 %699, i64* %rax
  store volatile i64 62276, i64* @assembly_address
  %714 = load i8** %stack_var_-88
  %715 = ptrtoint i8* %714 to i64
  %716 = load i64* %rax
  %717 = add i64 %715, %716
  %718 = and i64 %715, 15
  %719 = and i64 %716, 15
  %720 = add i64 %718, %719
  %721 = icmp ugt i64 %720, 15
  %722 = icmp ult i64 %717, %715
  %723 = xor i64 %715, %717
  %724 = xor i64 %716, %717
  %725 = and i64 %723, %724
  %726 = icmp slt i64 %725, 0
  store i1 %721, i1* %az
  store i1 %722, i1* %cf
  store i1 %726, i1* %of
  %727 = icmp eq i64 %717, 0
  store i1 %727, i1* %zf
  %728 = icmp slt i64 %717, 0
  store i1 %728, i1* %sf
  %729 = trunc i64 %717 to i8
  %730 = call i8 @llvm.ctpop.i8(i8 %729)
  %731 = and i8 %730, 1
  %732 = icmp eq i8 %731, 0
  store i1 %732, i1* %pf
  %733 = inttoptr i64 %717 to i8*
  store i8* %733, i8** %stack_var_-88
  store volatile i64 62280, i64* @assembly_address
  %734 = load i64* %stack_var_-80
  store i64 %734, i64* %rax
  store volatile i64 62284, i64* @assembly_address
  %735 = load i64* %rax
  %736 = mul i64 %735, 8
  store i64 %736, i64* %rdx
  store volatile i64 62292, i64* @assembly_address
  %737 = load i64* %stack_var_-112
  store i64 %737, i64* %rax
  store volatile i64 62296, i64* @assembly_address
  %738 = load i64* %rax
  %739 = load i64* %rdx
  %740 = add i64 %738, %739
  %741 = and i64 %738, 15
  %742 = and i64 %739, 15
  %743 = add i64 %741, %742
  %744 = icmp ugt i64 %743, 15
  %745 = icmp ult i64 %740, %738
  %746 = xor i64 %738, %740
  %747 = xor i64 %739, %740
  %748 = and i64 %746, %747
  %749 = icmp slt i64 %748, 0
  store i1 %744, i1* %az
  store i1 %745, i1* %cf
  store i1 %749, i1* %of
  %750 = icmp eq i64 %740, 0
  store i1 %750, i1* %zf
  %751 = icmp slt i64 %740, 0
  store i1 %751, i1* %sf
  %752 = trunc i64 %740 to i8
  %753 = call i8 @llvm.ctpop.i8(i8 %752)
  %754 = and i8 %753, 1
  %755 = icmp eq i8 %754, 0
  store i1 %755, i1* %pf
  store i64 %740, i64* %rax
  store volatile i64 62299, i64* @assembly_address
  %756 = load i64* %rax
  %757 = inttoptr i64 %756 to i64*
  %758 = load i64* %757
  store i64 %758, i64* %rax
  store volatile i64 62302, i64* @assembly_address
  %759 = load i64* %rax
  store i64 %759, i64* %rdi
  store volatile i64 62305, i64* @assembly_address
  %760 = load i64* %rdi
  %761 = inttoptr i64 %760 to i64*
  call void @free(i64* %761)
  store volatile i64 62310, i64* @assembly_address
  %762 = load i64* %stack_var_-80
  %763 = add i64 %762, 1
  %764 = and i64 %762, 15
  %765 = add i64 %764, 1
  %766 = icmp ugt i64 %765, 15
  %767 = icmp ult i64 %763, %762
  %768 = xor i64 %762, %763
  %769 = xor i64 1, %763
  %770 = and i64 %768, %769
  %771 = icmp slt i64 %770, 0
  store i1 %766, i1* %az
  store i1 %767, i1* %cf
  store i1 %771, i1* %of
  %772 = icmp eq i64 %763, 0
  store i1 %772, i1* %zf
  %773 = icmp slt i64 %763, 0
  store i1 %773, i1* %sf
  %774 = trunc i64 %763 to i8
  %775 = call i8 @llvm.ctpop.i8(i8 %774)
  %776 = and i8 %775, 1
  %777 = icmp eq i8 %776, 0
  store i1 %777, i1* %pf
  store i64 %763, i64* %stack_var_-80
  br label %block_f36b

block_f36b:                                       ; preds = %block_f2ff, %block_f2d9
  store volatile i64 62315, i64* @assembly_address
  %778 = load i64* %stack_var_-80
  store i64 %778, i64* %rax
  store volatile i64 62319, i64* @assembly_address
  %779 = load i64* %rax
  %780 = load i32* %stack_var_-96
  %781 = sext i32 %780 to i64
  %782 = sub i64 %779, %781
  %783 = and i64 %779, 15
  %784 = and i64 %781, 15
  %785 = sub i64 %783, %784
  %786 = icmp ugt i64 %785, 15
  %787 = icmp ult i64 %779, %781
  %788 = xor i64 %779, %781
  %789 = xor i64 %779, %782
  %790 = and i64 %788, %789
  %791 = icmp slt i64 %790, 0
  store i1 %786, i1* %az
  store i1 %787, i1* %cf
  store i1 %791, i1* %of
  %792 = icmp eq i64 %782, 0
  store i1 %792, i1* %zf
  %793 = icmp slt i64 %782, 0
  store i1 %793, i1* %sf
  %794 = trunc i64 %782 to i8
  %795 = call i8 @llvm.ctpop.i8(i8 %794)
  %796 = and i8 %795, 1
  %797 = icmp eq i8 %796, 0
  store i1 %797, i1* %pf
  store volatile i64 62323, i64* @assembly_address
  %798 = load i1* %cf
  br i1 %798, label %block_f2ff, label %block_f375

block_f375:                                       ; preds = %block_f36b
  store volatile i64 62325, i64* @assembly_address
  %799 = load i64* %stack_var_-112
  store i64 %799, i64* %rax
  store volatile i64 62329, i64* @assembly_address
  %800 = load i64* %rax
  store i64 %800, i64* %rdi
  store volatile i64 62332, i64* @assembly_address
  %801 = load i64* %rdi
  %802 = inttoptr i64 %801 to i64*
  call void @free(i64* %802)
  store volatile i64 62337, i64* @assembly_address
  br label %block_f3a8

block_f383:                                       ; preds = %block_f2ab
  store volatile i64 62339, i64* @assembly_address
  %803 = load i8** %stack_var_-88
  %804 = ptrtoint i8* %803 to i64
  store i64 %804, i64* %rax
  store volatile i64 62343, i64* @assembly_address
  %805 = load i64* %rax
  %806 = load i8** %stack_var_-120
  %807 = ptrtoint i8* %806 to i64
  %808 = sub i64 %805, %807
  %809 = and i64 %805, 15
  %810 = and i64 %807, 15
  %811 = sub i64 %809, %810
  %812 = icmp ugt i64 %811, 15
  %813 = icmp ult i64 %805, %807
  %814 = xor i64 %805, %807
  %815 = xor i64 %805, %808
  %816 = and i64 %814, %815
  %817 = icmp slt i64 %816, 0
  store i1 %812, i1* %az
  store i1 %813, i1* %cf
  store i1 %817, i1* %of
  %818 = icmp eq i64 %808, 0
  store i1 %818, i1* %zf
  %819 = icmp slt i64 %808, 0
  store i1 %819, i1* %sf
  %820 = trunc i64 %808 to i8
  %821 = call i8 @llvm.ctpop.i8(i8 %820)
  %822 = and i8 %821, 1
  %823 = icmp eq i8 %822, 0
  store i1 %823, i1* %pf
  store volatile i64 62347, i64* @assembly_address
  %824 = load i1* %zf
  %825 = icmp eq i1 %824, false
  br i1 %825, label %block_f3a8, label %block_f38d

block_f38d:                                       ; preds = %block_f383
  store volatile i64 62349, i64* @assembly_address
  %826 = load i8** %stack_var_-88
  %827 = ptrtoint i8* %826 to i64
  store i64 %827, i64* %rax
  store volatile i64 62353, i64* @assembly_address
  %828 = load i64* %rax
  %829 = add i64 %828, 1
  store i64 %829, i64* %rdx
  store volatile i64 62357, i64* @assembly_address
  %830 = load i64* %stack_var_-128
  store i64 %830, i64* %rax
  store volatile i64 62361, i64* @assembly_address
  %831 = load i64* %rdx
  store i64 %831, i64* %rsi
  store volatile i64 62364, i64* @assembly_address
  %832 = load i64* %rax
  store i64 %832, i64* %rdi
  store volatile i64 62367, i64* @assembly_address
  %833 = load i64* %rdi
  %834 = load i64* %rsi
  %835 = inttoptr i64 %833 to i64*
  %836 = call i64 @xrealloc(i64* %835, i64 %834)
  store i64 %836, i64* %rax
  store i64 %836, i64* %rax
  store volatile i64 62372, i64* @assembly_address
  %837 = load i64* %rax
  store i64 %837, i64* %stack_var_-128
  br label %block_f3a8

block_f3a8:                                       ; preds = %block_f38d, %block_f383, %block_f375
  store volatile i64 62376, i64* @assembly_address
  %838 = load i64* %stack_var_-128
  store i64 %838, i64* %rdx
  store volatile i64 62380, i64* @assembly_address
  %839 = load i8** %stack_var_-88
  %840 = ptrtoint i8* %839 to i64
  store i64 %840, i64* %rax
  store volatile i64 62384, i64* @assembly_address
  %841 = load i64* %rax
  %842 = load i64* %rdx
  %843 = add i64 %841, %842
  %844 = and i64 %841, 15
  %845 = and i64 %842, 15
  %846 = add i64 %844, %845
  %847 = icmp ugt i64 %846, 15
  %848 = icmp ult i64 %843, %841
  %849 = xor i64 %841, %843
  %850 = xor i64 %842, %843
  %851 = and i64 %849, %850
  %852 = icmp slt i64 %851, 0
  store i1 %847, i1* %az
  store i1 %848, i1* %cf
  store i1 %852, i1* %of
  %853 = icmp eq i64 %843, 0
  store i1 %853, i1* %zf
  %854 = icmp slt i64 %843, 0
  store i1 %854, i1* %sf
  %855 = trunc i64 %843 to i8
  %856 = call i8 @llvm.ctpop.i8(i8 %855)
  %857 = and i8 %856, 1
  %858 = icmp eq i8 %857, 0
  store i1 %858, i1* %pf
  store i64 %843, i64* %rax
  store volatile i64 62387, i64* @assembly_address
  %859 = load i64* %rax
  %860 = inttoptr i64 %859 to i8*
  store i8 0, i8* %860
  store volatile i64 62390, i64* @assembly_address
  %861 = load i64* %stack_var_-128
  store i64 %861, i64* %rax
  br label %block_f3ba

block_f3ba:                                       ; preds = %block_f3a8, %block_f279, %block_f0ed
  store volatile i64 62394, i64* @assembly_address
  %862 = load i64* %stack_var_-32
  store i64 %862, i64* %rbx
  store volatile i64 62398, i64* @assembly_address
  %863 = load i64* %rbx
  %864 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %865 = xor i64 %863, %864
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %866 = icmp eq i64 %865, 0
  store i1 %866, i1* %zf
  %867 = icmp slt i64 %865, 0
  store i1 %867, i1* %sf
  %868 = trunc i64 %865 to i8
  %869 = call i8 @llvm.ctpop.i8(i8 %868)
  %870 = and i8 %869, 1
  %871 = icmp eq i8 %870, 0
  store i1 %871, i1* %pf
  store i64 %865, i64* %rbx
  store volatile i64 62407, i64* @assembly_address
  %872 = load i1* %zf
  br i1 %872, label %block_f3ce, label %block_f3c9

block_f3c9:                                       ; preds = %block_f3ba
  store volatile i64 62409, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_f3ce:                                       ; preds = %block_f3ba
  store volatile i64 62414, i64* @assembly_address
  %873 = load i64* %rsp
  %874 = add i64 %873, 152
  %875 = and i64 %873, 15
  %876 = add i64 %875, 8
  %877 = icmp ugt i64 %876, 15
  %878 = icmp ult i64 %874, %873
  %879 = xor i64 %873, %874
  %880 = xor i64 152, %874
  %881 = and i64 %879, %880
  %882 = icmp slt i64 %881, 0
  store i1 %877, i1* %az
  store i1 %878, i1* %cf
  store i1 %882, i1* %of
  %883 = icmp eq i64 %874, 0
  store i1 %883, i1* %zf
  %884 = icmp slt i64 %874, 0
  store i1 %884, i1* %sf
  %885 = trunc i64 %874 to i8
  %886 = call i8 @llvm.ctpop.i8(i8 %885)
  %887 = and i8 %886, 1
  %888 = icmp eq i8 %887, 0
  store i1 %888, i1* %pf
  %889 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %889, i64* %rsp
  store volatile i64 62421, i64* @assembly_address
  %890 = load i64* %stack_var_-16
  store i64 %890, i64* %rbx
  %891 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %891, i64* %rsp
  store volatile i64 62422, i64* @assembly_address
  %892 = load i64* %stack_var_-8
  store i64 %892, i64* %rbp
  %893 = ptrtoint i64* %stack_var_0 to i64
  store i64 %893, i64* %rsp
  store volatile i64 62423, i64* @assembly_address
  %894 = load i64* %rax
  ret i64 %894
}

declare i64 @271(i64, i32)

declare i64 @272(i64, i64)

define i64 @savedir(i64 %arg1, i32 %arg2) {
block_f3d8:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg2 to i64
  store i64 %0, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-28 = alloca i32
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca %__dirstream*
  %1 = alloca i64
  %stack_var_-52 = alloca i32
  %stack_var_-48 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62424, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 62425, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 62428, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 48
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 48
  %10 = xor i64 %5, 48
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %20, i64* %rsp
  store volatile i64 62432, i64* @assembly_address
  %21 = load i64* %rdi
  store i64 %21, i64* %stack_var_-48
  store volatile i64 62436, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-52
  store volatile i64 62439, i64* @assembly_address
  %24 = load i64* %stack_var_-48
  store i64 %24, i64* %rax
  store volatile i64 62443, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rdi
  store volatile i64 62446, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to i8*
  %28 = call i64 @opendir_safer(i8* %27)
  store i64 %28, i64* %rax
  store i64 %28, i64* %rax
  store volatile i64 62451, i64* @assembly_address
  %29 = load i64* %rax
  %30 = inttoptr i64 %29 to %__dirstream*
  store %__dirstream* %30, %__dirstream** %stack_var_-24
  store volatile i64 62455, i64* @assembly_address
  %31 = load %__dirstream** %stack_var_-24
  %32 = ptrtoint %__dirstream* %31 to i64
  %33 = and i64 %32, 15
  %34 = icmp ugt i64 %33, 15
  %35 = icmp ult i64 %32, 0
  %36 = xor i64 %32, 0
  %37 = and i64 %36, 0
  %38 = icmp slt i64 %37, 0
  store i1 %34, i1* %az
  store i1 %35, i1* %cf
  store i1 %38, i1* %of
  %39 = icmp eq i64 %32, 0
  store i1 %39, i1* %zf
  %40 = icmp slt i64 %32, 0
  store i1 %40, i1* %sf
  %41 = trunc i64 %32 to i8
  %42 = call i8 @llvm.ctpop.i8(i8 %41)
  %43 = and i8 %42, 1
  %44 = icmp eq i8 %43, 0
  store i1 %44, i1* %pf
  store volatile i64 62460, i64* @assembly_address
  %45 = load i1* %zf
  %46 = icmp eq i1 %45, false
  br i1 %46, label %block_f405, label %block_f3fe

block_f3fe:                                       ; preds = %block_f3d8
  store volatile i64 62462, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 62467, i64* @assembly_address
  br label %block_f458

block_f405:                                       ; preds = %block_f3d8
  store volatile i64 62469, i64* @assembly_address
  %47 = load i32* %stack_var_-52
  %48 = zext i32 %47 to i64
  store i64 %48, i64* %rdx
  store volatile i64 62472, i64* @assembly_address
  %49 = load %__dirstream** %stack_var_-24
  %50 = ptrtoint %__dirstream* %49 to i64
  store i64 %50, i64* %rax
  store volatile i64 62476, i64* @assembly_address
  %51 = load i64* %rdx
  %52 = trunc i64 %51 to i32
  %53 = zext i32 %52 to i64
  store i64 %53, i64* %rsi
  store volatile i64 62478, i64* @assembly_address
  %54 = load i64* %rax
  store i64 %54, i64* %rdi
  store volatile i64 62481, i64* @assembly_address
  %55 = load i64* %rdi
  %56 = load i64* %rsi
  %57 = inttoptr i64 %55 to %__dirstream*
  %58 = call i64 @streamsavedir(%__dirstream* %57, i64 %56)
  store i64 %58, i64* %rax
  store i64 %58, i64* %rax
  store volatile i64 62486, i64* @assembly_address
  %59 = load i64* %rax
  store i64 %59, i64* %stack_var_-16
  store volatile i64 62490, i64* @assembly_address
  %60 = load %__dirstream** %stack_var_-24
  %61 = ptrtoint %__dirstream* %60 to i64
  store i64 %61, i64* %rax
  store volatile i64 62494, i64* @assembly_address
  %62 = load i64* %rax
  store i64 %62, i64* %rdi
  store volatile i64 62497, i64* @assembly_address
  %63 = load i64* %rdi
  %64 = inttoptr i64 %63 to %__dirstream*
  %65 = call i32 @closedir(%__dirstream* %64)
  %66 = sext i32 %65 to i64
  store i64 %66, i64* %rax
  %67 = sext i32 %65 to i64
  store i64 %67, i64* %rax
  store volatile i64 62502, i64* @assembly_address
  %68 = load i64* %rax
  %69 = trunc i64 %68 to i32
  %70 = load i64* %rax
  %71 = trunc i64 %70 to i32
  %72 = and i32 %69, %71
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %73 = icmp eq i32 %72, 0
  store i1 %73, i1* %zf
  %74 = icmp slt i32 %72, 0
  store i1 %74, i1* %sf
  %75 = trunc i32 %72 to i8
  %76 = call i8 @llvm.ctpop.i8(i8 %75)
  %77 = and i8 %76, 1
  %78 = icmp eq i8 %77, 0
  store i1 %78, i1* %pf
  store volatile i64 62504, i64* @assembly_address
  %79 = load i1* %zf
  br i1 %79, label %block_f454, label %block_f42a

block_f42a:                                       ; preds = %block_f405
  store volatile i64 62506, i64* @assembly_address
  %80 = call i32* @__errno_location()
  %81 = ptrtoint i32* %80 to i64
  store i64 %81, i64* %rax
  %82 = ptrtoint i32* %80 to i64
  store i64 %82, i64* %rax
  %83 = ptrtoint i32* %80 to i64
  store i64 %83, i64* %rax
  store volatile i64 62511, i64* @assembly_address
  %84 = load i64* %rax
  %85 = inttoptr i64 %84 to i32*
  %86 = load i32* %85
  %87 = zext i32 %86 to i64
  store i64 %87, i64* %rax
  store volatile i64 62513, i64* @assembly_address
  %88 = load i64* %rax
  %89 = trunc i64 %88 to i32
  store i32 %89, i32* %stack_var_-28
  store volatile i64 62516, i64* @assembly_address
  %90 = load i64* %stack_var_-16
  store i64 %90, i64* %rax
  store volatile i64 62520, i64* @assembly_address
  %91 = load i64* %rax
  store i64 %91, i64* %rdi
  store volatile i64 62523, i64* @assembly_address
  %92 = load i64* %rdi
  %93 = inttoptr i64 %92 to i64*
  call void @free(i64* %93)
  store volatile i64 62528, i64* @assembly_address
  %94 = call i32* @__errno_location()
  %95 = ptrtoint i32* %94 to i64
  store i64 %95, i64* %rax
  %96 = ptrtoint i32* %94 to i64
  store i64 %96, i64* %rax
  %97 = ptrtoint i32* %94 to i64
  store i64 %97, i64* %rax
  store volatile i64 62533, i64* @assembly_address
  %98 = load i64* %rax
  store i64 %98, i64* %rdx
  store volatile i64 62536, i64* @assembly_address
  %99 = load i32* %stack_var_-28
  %100 = zext i32 %99 to i64
  store i64 %100, i64* %rax
  store volatile i64 62539, i64* @assembly_address
  %101 = load i64* %rax
  %102 = trunc i64 %101 to i32
  %103 = load i64* %rdx
  %104 = inttoptr i64 %103 to i32*
  store i32 %102, i32* %104
  store volatile i64 62541, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 62546, i64* @assembly_address
  br label %block_f458

block_f454:                                       ; preds = %block_f405
  store volatile i64 62548, i64* @assembly_address
  %105 = load i64* %stack_var_-16
  store i64 %105, i64* %rax
  br label %block_f458

block_f458:                                       ; preds = %block_f454, %block_f42a, %block_f3fe
  store volatile i64 62552, i64* @assembly_address
  %106 = load i64* %stack_var_-8
  store i64 %106, i64* %rbp
  %107 = ptrtoint i64* %stack_var_0 to i64
  store i64 %107, i64* %rsp
  store volatile i64 62553, i64* @assembly_address
  %108 = load i64* %rax
  ret i64 %108
}

declare i64 @273(i64, i64)

define i64 @get_stat_atime_ns(i64* %arg1) {
block_f45a:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62554, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 62555, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 62558, i64* @assembly_address
  %4 = load i64* %rdi
  store i64 %4, i64* %stack_var_-16
  store volatile i64 62562, i64* @assembly_address
  %5 = load i64* %stack_var_-16
  store i64 %5, i64* %rax
  store volatile i64 62566, i64* @assembly_address
  %6 = load i64* %rax
  %7 = add i64 %6, 80
  %8 = inttoptr i64 %7 to i64*
  %9 = load i64* %8
  store i64 %9, i64* %rax
  store volatile i64 62570, i64* @assembly_address
  %10 = load i64* %stack_var_-8
  store i64 %10, i64* %rbp
  %11 = ptrtoint i64* %stack_var_0 to i64
  store i64 %11, i64* %rsp
  store volatile i64 62571, i64* @assembly_address
  %12 = load i64* %rax
  ret i64 %12
}

define i64 @get_stat_ctime_ns(i64 %arg1) {
block_f46c:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62572, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 62573, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 62576, i64* @assembly_address
  %3 = load i64* %rdi
  store i64 %3, i64* %stack_var_-16
  store volatile i64 62580, i64* @assembly_address
  %4 = load i64* %stack_var_-16
  store i64 %4, i64* %rax
  store volatile i64 62584, i64* @assembly_address
  %5 = load i64* %rax
  %6 = add i64 %5, 112
  %7 = inttoptr i64 %6 to i64*
  %8 = load i64* %7
  store i64 %8, i64* %rax
  store volatile i64 62588, i64* @assembly_address
  %9 = load i64* %stack_var_-8
  store i64 %9, i64* %rbp
  %10 = ptrtoint i64* %stack_var_0 to i64
  store i64 %10, i64* %rsp
  store volatile i64 62589, i64* @assembly_address
  %11 = load i64* %rax
  ret i64 %11
}

define i64 @get_stat_mtime_ns(i64* %arg1) {
block_f47e:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62590, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 62591, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 62594, i64* @assembly_address
  %4 = load i64* %rdi
  store i64 %4, i64* %stack_var_-16
  store volatile i64 62598, i64* @assembly_address
  %5 = load i64* %stack_var_-16
  store i64 %5, i64* %rax
  store volatile i64 62602, i64* @assembly_address
  %6 = load i64* %rax
  %7 = add i64 %6, 96
  %8 = inttoptr i64 %7 to i64*
  %9 = load i64* %8
  store i64 %9, i64* %rax
  store volatile i64 62606, i64* @assembly_address
  %10 = load i64* %stack_var_-8
  store i64 %10, i64* %rbp
  %11 = ptrtoint i64* %stack_var_0 to i64
  store i64 %11, i64* %rsp
  store volatile i64 62607, i64* @assembly_address
  %12 = load i64* %rax
  ret i64 %12
}

define i64 @get_stat_birthtime_ns(i64 %arg1) {
block_f490:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62608, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 62609, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 62612, i64* @assembly_address
  %3 = load i64* %rdi
  store i64 %3, i64* %stack_var_-16
  store volatile i64 62616, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 62621, i64* @assembly_address
  %4 = load i64* %stack_var_-8
  store i64 %4, i64* %rbp
  %5 = ptrtoint i64* %stack_var_0 to i64
  store i64 %5, i64* %rsp
  store volatile i64 62622, i64* @assembly_address
  %6 = load i64* %rax
  ret i64 %6
}

define i64 @get_stat_atime(i64 %arg1) {
block_f49f:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62623, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 62624, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 62627, i64* @assembly_address
  %3 = load i64* %rdi
  store i64 %3, i64* %stack_var_-16
  store volatile i64 62631, i64* @assembly_address
  %4 = load i64* %stack_var_-16
  store i64 %4, i64* %rax
  store volatile i64 62635, i64* @assembly_address
  %5 = load i64* %rax
  %6 = add i64 %5, 80
  %7 = inttoptr i64 %6 to i64*
  %8 = load i64* %7
  store i64 %8, i64* %rdx
  store volatile i64 62639, i64* @assembly_address
  %9 = load i64* %rax
  %10 = add i64 %9, 72
  %11 = inttoptr i64 %10 to i64*
  %12 = load i64* %11
  store i64 %12, i64* %rax
  store volatile i64 62643, i64* @assembly_address
  %13 = load i64* %stack_var_-8
  store i64 %13, i64* %rbp
  %14 = ptrtoint i64* %stack_var_0 to i64
  store i64 %14, i64* %rsp
  store volatile i64 62644, i64* @assembly_address
  %15 = load i64* %rax
  ret i64 %15
}

define i64 @get_stat_ctime(i64 %arg1) {
block_f4b5:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62645, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 62646, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 62649, i64* @assembly_address
  %3 = load i64* %rdi
  store i64 %3, i64* %stack_var_-16
  store volatile i64 62653, i64* @assembly_address
  %4 = load i64* %stack_var_-16
  store i64 %4, i64* %rax
  store volatile i64 62657, i64* @assembly_address
  %5 = load i64* %rax
  %6 = add i64 %5, 112
  %7 = inttoptr i64 %6 to i64*
  %8 = load i64* %7
  store i64 %8, i64* %rdx
  store volatile i64 62661, i64* @assembly_address
  %9 = load i64* %rax
  %10 = add i64 %9, 104
  %11 = inttoptr i64 %10 to i64*
  %12 = load i64* %11
  store i64 %12, i64* %rax
  store volatile i64 62665, i64* @assembly_address
  %13 = load i64* %stack_var_-8
  store i64 %13, i64* %rbp
  %14 = ptrtoint i64* %stack_var_0 to i64
  store i64 %14, i64* %rsp
  store volatile i64 62666, i64* @assembly_address
  %15 = load i64* %rax
  ret i64 %15
}

define i64 @get_stat_mtime(i64* %arg1) {
block_f4cb:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62667, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 62668, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 62671, i64* @assembly_address
  %4 = load i64* %rdi
  store i64 %4, i64* %stack_var_-16
  store volatile i64 62675, i64* @assembly_address
  %5 = load i64* %stack_var_-16
  store i64 %5, i64* %rax
  store volatile i64 62679, i64* @assembly_address
  %6 = load i64* %rax
  %7 = add i64 %6, 96
  %8 = inttoptr i64 %7 to i64*
  %9 = load i64* %8
  store i64 %9, i64* %rdx
  store volatile i64 62683, i64* @assembly_address
  %10 = load i64* %rax
  %11 = add i64 %10, 88
  %12 = inttoptr i64 %11 to i64*
  %13 = load i64* %12
  store i64 %13, i64* %rax
  store volatile i64 62687, i64* @assembly_address
  %14 = load i64* %stack_var_-8
  store i64 %14, i64* %rbp
  %15 = ptrtoint i64* %stack_var_0 to i64
  store i64 %15, i64* %rsp
  store volatile i64 62688, i64* @assembly_address
  %16 = load i64* %rax
  ret i64 %16
}

define i64 @get_stat_birthtime(i64 %arg1) {
block_f4e1:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 62689, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 62690, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 62693, i64* @assembly_address
  %3 = load i64* %rdi
  store i64 %3, i64* %stack_var_-32
  store volatile i64 62697, i64* @assembly_address
  store i64 -1, i64* %stack_var_-24
  store volatile i64 62705, i64* @assembly_address
  store i64 -1, i64* %stack_var_-16
  store volatile i64 62713, i64* @assembly_address
  %4 = load i64* %stack_var_-24
  store i64 %4, i64* %rax
  store volatile i64 62717, i64* @assembly_address
  %5 = load i64* %stack_var_-16
  store i64 %5, i64* %rdx
  store volatile i64 62721, i64* @assembly_address
  %6 = load i64* %stack_var_-8
  store i64 %6, i64* %rbp
  %7 = ptrtoint i64* %stack_var_0 to i64
  store i64 %7, i64* %rsp
  store volatile i64 62722, i64* @assembly_address
  %8 = load i64* %rax
  ret i64 %8
}

define i64 @stat_time_normalize(i32 %arg1, i64 %arg2) {
block_f503:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-8 = alloca i64
  store volatile i64 62723, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 62724, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 62727, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = trunc i64 %4 to i32
  store i32 %5, i32* %stack_var_-12
  store volatile i64 62730, i64* @assembly_address
  %6 = load i64* %rsi
  store i64 %6, i64* %stack_var_-24
  store volatile i64 62734, i64* @assembly_address
  %7 = load i32* %stack_var_-12
  %8 = zext i32 %7 to i64
  store i64 %8, i64* %rax
  store volatile i64 62737, i64* @assembly_address
  %9 = load i64* %stack_var_-8
  store i64 %9, i64* %rbp
  %10 = ptrtoint i64* %stack_var_0 to i64
  store i64 %10, i64* %rsp
  store volatile i64 62738, i64* @assembly_address
  %11 = load i64* %rax
  ret i64 %11
}

declare i64 @274(i64, i64)

define i64 @fd_safer(i32 %arg1) {
block_f513:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i32
  %2 = alloca i32
  store volatile i64 62739, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 62740, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 62743, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 62747, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-28
  store volatile i64 62750, i64* @assembly_address
  %24 = load i32* %stack_var_-28
  %25 = and i32 %24, 15
  %26 = icmp ugt i32 %25, 15
  %27 = icmp ult i32 %24, 0
  %28 = xor i32 %24, 0
  %29 = and i32 %28, 0
  %30 = icmp slt i32 %29, 0
  store i1 %26, i1* %az
  store i1 %27, i1* %cf
  store i1 %30, i1* %of
  %31 = icmp eq i32 %24, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i32 %24, 0
  store i1 %32, i1* %sf
  %33 = trunc i32 %24 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  store volatile i64 62754, i64* @assembly_address
  %37 = load i1* %sf
  br i1 %37, label %block_f55e, label %block_f524

block_f524:                                       ; preds = %block_f513
  store volatile i64 62756, i64* @assembly_address
  %38 = load i32* %stack_var_-28
  store i32 %38, i32* %2
  store i32 2, i32* %1
  %39 = sub i32 %38, 2
  %40 = and i32 %38, 15
  %41 = sub i32 %40, 2
  %42 = icmp ugt i32 %41, 15
  %43 = icmp ult i32 %38, 2
  %44 = xor i32 %38, 2
  %45 = xor i32 %38, %39
  %46 = and i32 %44, %45
  %47 = icmp slt i32 %46, 0
  store i1 %42, i1* %az
  store i1 %43, i1* %cf
  store i1 %47, i1* %of
  %48 = icmp eq i32 %39, 0
  store i1 %48, i1* %zf
  %49 = icmp slt i32 %39, 0
  store i1 %49, i1* %sf
  %50 = trunc i32 %39 to i8
  %51 = call i8 @llvm.ctpop.i8(i8 %50)
  %52 = and i8 %51, 1
  %53 = icmp eq i8 %52, 0
  store i1 %53, i1* %pf
  store volatile i64 62760, i64* @assembly_address
  %54 = load i32* %2
  %55 = load i32* %1
  %56 = icmp sgt i32 %54, %55
  br i1 %56, label %block_f55e, label %block_f52a

block_f52a:                                       ; preds = %block_f524
  store volatile i64 62762, i64* @assembly_address
  %57 = load i32* %stack_var_-28
  %58 = zext i32 %57 to i64
  store i64 %58, i64* %rax
  store volatile i64 62765, i64* @assembly_address
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i32
  %61 = zext i32 %60 to i64
  store i64 %61, i64* %rdi
  store volatile i64 62767, i64* @assembly_address
  %62 = load i64* %rdi
  %63 = trunc i64 %62 to i32
  %64 = call i64 @dup_safer(i32 %63)
  store i64 %64, i64* %rax
  store i64 %64, i64* %rax
  store volatile i64 62772, i64* @assembly_address
  %65 = load i64* %rax
  %66 = trunc i64 %65 to i32
  store i32 %66, i32* %stack_var_-16
  store volatile i64 62775, i64* @assembly_address
  %67 = call i32* @__errno_location()
  %68 = ptrtoint i32* %67 to i64
  store i64 %68, i64* %rax
  %69 = ptrtoint i32* %67 to i64
  store i64 %69, i64* %rax
  %70 = ptrtoint i32* %67 to i64
  store i64 %70, i64* %rax
  store volatile i64 62780, i64* @assembly_address
  %71 = load i64* %rax
  %72 = inttoptr i64 %71 to i32*
  %73 = load i32* %72
  %74 = zext i32 %73 to i64
  store i64 %74, i64* %rax
  store volatile i64 62782, i64* @assembly_address
  %75 = load i64* %rax
  %76 = trunc i64 %75 to i32
  store i32 %76, i32* %stack_var_-12
  store volatile i64 62785, i64* @assembly_address
  %77 = load i32* %stack_var_-28
  %78 = zext i32 %77 to i64
  store i64 %78, i64* %rax
  store volatile i64 62788, i64* @assembly_address
  %79 = load i64* %rax
  %80 = trunc i64 %79 to i32
  %81 = zext i32 %80 to i64
  store i64 %81, i64* %rdi
  store volatile i64 62790, i64* @assembly_address
  %82 = load i64* %rdi
  %83 = trunc i64 %82 to i32
  %84 = call i32 @close(i32 %83)
  %85 = sext i32 %84 to i64
  store i64 %85, i64* %rax
  %86 = sext i32 %84 to i64
  store i64 %86, i64* %rax
  store volatile i64 62795, i64* @assembly_address
  %87 = call i32* @__errno_location()
  %88 = ptrtoint i32* %87 to i64
  store i64 %88, i64* %rax
  %89 = ptrtoint i32* %87 to i64
  store i64 %89, i64* %rax
  %90 = ptrtoint i32* %87 to i64
  store i64 %90, i64* %rax
  store volatile i64 62800, i64* @assembly_address
  %91 = load i64* %rax
  store i64 %91, i64* %rdx
  store volatile i64 62803, i64* @assembly_address
  %92 = load i32* %stack_var_-12
  %93 = zext i32 %92 to i64
  store i64 %93, i64* %rax
  store volatile i64 62806, i64* @assembly_address
  %94 = load i64* %rax
  %95 = trunc i64 %94 to i32
  %96 = load i64* %rdx
  %97 = inttoptr i64 %96 to i32*
  store i32 %95, i32* %97
  store volatile i64 62808, i64* @assembly_address
  %98 = load i32* %stack_var_-16
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rax
  store volatile i64 62811, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i32
  store i32 %101, i32* %stack_var_-28
  br label %block_f55e

block_f55e:                                       ; preds = %block_f52a, %block_f524, %block_f513
  store volatile i64 62814, i64* @assembly_address
  %102 = load i32* %stack_var_-28
  %103 = zext i32 %102 to i64
  store i64 %103, i64* %rax
  store volatile i64 62817, i64* @assembly_address
  %104 = load i64* %stack_var_-8
  store i64 %104, i64* %rbp
  %105 = ptrtoint i64* %stack_var_0 to i64
  store i64 %105, i64* %rsp
  store volatile i64 62818, i64* @assembly_address
  %106 = load i64* %rax
  ret i64 %106
}

declare i64 @275(i64)

define i64 @validate_timespec(i64* %arg1) {
block_f563:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-32 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  %1 = alloca i64
  %2 = alloca i64
  %3 = alloca i64
  %4 = alloca i64
  store volatile i64 62819, i64* @assembly_address
  %5 = load i64* %rbp
  store i64 %5, i64* %stack_var_-8
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rsp
  store volatile i64 62820, i64* @assembly_address
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rbp
  store volatile i64 62823, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 32
  %10 = and i64 %8, 15
  %11 = icmp ugt i64 %10, 15
  %12 = icmp ult i64 %8, 32
  %13 = xor i64 %8, 32
  %14 = xor i64 %8, %9
  %15 = and i64 %13, %14
  %16 = icmp slt i64 %15, 0
  store i1 %11, i1* %az
  store i1 %12, i1* %cf
  store i1 %16, i1* %of
  %17 = icmp eq i64 %9, 0
  store i1 %17, i1* %zf
  %18 = icmp slt i64 %9, 0
  store i1 %18, i1* %sf
  %19 = trunc i64 %9 to i8
  %20 = call i8 @llvm.ctpop.i8(i8 %19)
  %21 = and i8 %20, 1
  %22 = icmp eq i8 %21, 0
  store i1 %22, i1* %pf
  %23 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %23, i64* %rsp
  store volatile i64 62827, i64* @assembly_address
  %24 = load i64* %rdi
  store i64 %24, i64* %stack_var_-32
  store volatile i64 62831, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 62838, i64* @assembly_address
  store i32 0, i32* %stack_var_-12
  store volatile i64 62845, i64* @assembly_address
  %25 = load i64* %stack_var_-32
  store i64 %25, i64* %rax
  store volatile i64 62849, i64* @assembly_address
  %26 = load i64* %rax
  %27 = add i64 %26, 8
  %28 = inttoptr i64 %27 to i64*
  %29 = load i64* %28
  store i64 %29, i64* %rax
  store volatile i64 62853, i64* @assembly_address
  %30 = load i64* %rax
  %31 = sub i64 %30, 1073741823
  %32 = and i64 %30, 15
  %33 = sub i64 %32, 15
  %34 = icmp ugt i64 %33, 15
  %35 = icmp ult i64 %30, 1073741823
  %36 = xor i64 %30, 1073741823
  %37 = xor i64 %30, %31
  %38 = and i64 %36, %37
  %39 = icmp slt i64 %38, 0
  store i1 %34, i1* %az
  store i1 %35, i1* %cf
  store i1 %39, i1* %of
  %40 = icmp eq i64 %31, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i64 %31, 0
  store i1 %41, i1* %sf
  %42 = trunc i64 %31 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  store volatile i64 62859, i64* @assembly_address
  %46 = load i1* %zf
  br i1 %46, label %block_f5ba, label %block_f58d

block_f58d:                                       ; preds = %block_f563
  store volatile i64 62861, i64* @assembly_address
  %47 = load i64* %stack_var_-32
  store i64 %47, i64* %rax
  store volatile i64 62865, i64* @assembly_address
  %48 = load i64* %rax
  %49 = add i64 %48, 8
  %50 = inttoptr i64 %49 to i64*
  %51 = load i64* %50
  store i64 %51, i64* %rax
  store volatile i64 62869, i64* @assembly_address
  %52 = load i64* %rax
  %53 = sub i64 %52, 1073741822
  %54 = and i64 %52, 15
  %55 = sub i64 %54, 14
  %56 = icmp ugt i64 %55, 15
  %57 = icmp ult i64 %52, 1073741822
  %58 = xor i64 %52, 1073741822
  %59 = xor i64 %52, %53
  %60 = and i64 %58, %59
  %61 = icmp slt i64 %60, 0
  store i1 %56, i1* %az
  store i1 %57, i1* %cf
  store i1 %61, i1* %of
  %62 = icmp eq i64 %53, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i64 %53, 0
  store i1 %63, i1* %sf
  %64 = trunc i64 %53 to i8
  %65 = call i8 @llvm.ctpop.i8(i8 %64)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store volatile i64 62875, i64* @assembly_address
  %68 = load i1* %zf
  br i1 %68, label %block_f5ba, label %block_f59d

block_f59d:                                       ; preds = %block_f58d
  store volatile i64 62877, i64* @assembly_address
  %69 = load i64* %stack_var_-32
  store i64 %69, i64* %rax
  store volatile i64 62881, i64* @assembly_address
  %70 = load i64* %rax
  %71 = add i64 %70, 8
  %72 = inttoptr i64 %71 to i64*
  %73 = load i64* %72
  store i64 %73, i64* %rax
  store volatile i64 62885, i64* @assembly_address
  %74 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %75 = icmp eq i64 %74, 0
  store i1 %75, i1* %zf
  %76 = icmp slt i64 %74, 0
  store i1 %76, i1* %sf
  %77 = trunc i64 %74 to i8
  %78 = call i8 @llvm.ctpop.i8(i8 %77)
  %79 = and i8 %78, 1
  %80 = icmp eq i8 %79, 0
  store i1 %80, i1* %pf
  store volatile i64 62888, i64* @assembly_address
  %81 = load i1* %sf
  br i1 %81, label %block_f607, label %block_f5aa

block_f5aa:                                       ; preds = %block_f59d
  store volatile i64 62890, i64* @assembly_address
  %82 = load i64* %stack_var_-32
  store i64 %82, i64* %rax
  store volatile i64 62894, i64* @assembly_address
  %83 = load i64* %rax
  %84 = add i64 %83, 8
  %85 = inttoptr i64 %84 to i64*
  %86 = load i64* %85
  store i64 %86, i64* %rax
  store volatile i64 62898, i64* @assembly_address
  %87 = load i64* %rax
  store i64 %87, i64* %4
  store i64 999999999, i64* %3
  %88 = sub i64 %87, 999999999
  %89 = and i64 %87, 15
  %90 = sub i64 %89, 15
  %91 = icmp ugt i64 %90, 15
  %92 = icmp ult i64 %87, 999999999
  %93 = xor i64 %87, 999999999
  %94 = xor i64 %87, %88
  %95 = and i64 %93, %94
  %96 = icmp slt i64 %95, 0
  store i1 %91, i1* %az
  store i1 %92, i1* %cf
  store i1 %96, i1* %of
  %97 = icmp eq i64 %88, 0
  store i1 %97, i1* %zf
  %98 = icmp slt i64 %88, 0
  store i1 %98, i1* %sf
  %99 = trunc i64 %88 to i8
  %100 = call i8 @llvm.ctpop.i8(i8 %99)
  %101 = and i8 %100, 1
  %102 = icmp eq i8 %101, 0
  store i1 %102, i1* %pf
  store volatile i64 62904, i64* @assembly_address
  %103 = load i64* %4
  %104 = load i64* %3
  %105 = icmp sgt i64 %103, %104
  br i1 %105, label %block_f607, label %block_f5ba

block_f5ba:                                       ; preds = %block_f5aa, %block_f58d, %block_f563
  store volatile i64 62906, i64* @assembly_address
  %106 = load i64* %stack_var_-32
  store i64 %106, i64* %rax
  store volatile i64 62910, i64* @assembly_address
  %107 = load i64* %rax
  %108 = add i64 %107, 16
  %109 = and i64 %107, 15
  %110 = icmp ugt i64 %109, 15
  %111 = icmp ult i64 %108, %107
  %112 = xor i64 %107, %108
  %113 = xor i64 16, %108
  %114 = and i64 %112, %113
  %115 = icmp slt i64 %114, 0
  store i1 %110, i1* %az
  store i1 %111, i1* %cf
  store i1 %115, i1* %of
  %116 = icmp eq i64 %108, 0
  store i1 %116, i1* %zf
  %117 = icmp slt i64 %108, 0
  store i1 %117, i1* %sf
  %118 = trunc i64 %108 to i8
  %119 = call i8 @llvm.ctpop.i8(i8 %118)
  %120 = and i8 %119, 1
  %121 = icmp eq i8 %120, 0
  store i1 %121, i1* %pf
  store i64 %108, i64* %rax
  store volatile i64 62914, i64* @assembly_address
  %122 = load i64* %rax
  %123 = add i64 %122, 8
  %124 = inttoptr i64 %123 to i64*
  %125 = load i64* %124
  store i64 %125, i64* %rax
  store volatile i64 62918, i64* @assembly_address
  %126 = load i64* %rax
  %127 = sub i64 %126, 1073741823
  %128 = and i64 %126, 15
  %129 = sub i64 %128, 15
  %130 = icmp ugt i64 %129, 15
  %131 = icmp ult i64 %126, 1073741823
  %132 = xor i64 %126, 1073741823
  %133 = xor i64 %126, %127
  %134 = and i64 %132, %133
  %135 = icmp slt i64 %134, 0
  store i1 %130, i1* %az
  store i1 %131, i1* %cf
  store i1 %135, i1* %of
  %136 = icmp eq i64 %127, 0
  store i1 %136, i1* %zf
  %137 = icmp slt i64 %127, 0
  store i1 %137, i1* %sf
  %138 = trunc i64 %127 to i8
  %139 = call i8 @llvm.ctpop.i8(i8 %138)
  %140 = and i8 %139, 1
  %141 = icmp eq i8 %140, 0
  store i1 %141, i1* %pf
  store volatile i64 62924, i64* @assembly_address
  %142 = load i1* %zf
  br i1 %142, label %block_f61c, label %block_f5ce

block_f5ce:                                       ; preds = %block_f5ba
  store volatile i64 62926, i64* @assembly_address
  %143 = load i64* %stack_var_-32
  store i64 %143, i64* %rax
  store volatile i64 62930, i64* @assembly_address
  %144 = load i64* %rax
  %145 = add i64 %144, 16
  %146 = and i64 %144, 15
  %147 = icmp ugt i64 %146, 15
  %148 = icmp ult i64 %145, %144
  %149 = xor i64 %144, %145
  %150 = xor i64 16, %145
  %151 = and i64 %149, %150
  %152 = icmp slt i64 %151, 0
  store i1 %147, i1* %az
  store i1 %148, i1* %cf
  store i1 %152, i1* %of
  %153 = icmp eq i64 %145, 0
  store i1 %153, i1* %zf
  %154 = icmp slt i64 %145, 0
  store i1 %154, i1* %sf
  %155 = trunc i64 %145 to i8
  %156 = call i8 @llvm.ctpop.i8(i8 %155)
  %157 = and i8 %156, 1
  %158 = icmp eq i8 %157, 0
  store i1 %158, i1* %pf
  store i64 %145, i64* %rax
  store volatile i64 62934, i64* @assembly_address
  %159 = load i64* %rax
  %160 = add i64 %159, 8
  %161 = inttoptr i64 %160 to i64*
  %162 = load i64* %161
  store i64 %162, i64* %rax
  store volatile i64 62938, i64* @assembly_address
  %163 = load i64* %rax
  %164 = sub i64 %163, 1073741822
  %165 = and i64 %163, 15
  %166 = sub i64 %165, 14
  %167 = icmp ugt i64 %166, 15
  %168 = icmp ult i64 %163, 1073741822
  %169 = xor i64 %163, 1073741822
  %170 = xor i64 %163, %164
  %171 = and i64 %169, %170
  %172 = icmp slt i64 %171, 0
  store i1 %167, i1* %az
  store i1 %168, i1* %cf
  store i1 %172, i1* %of
  %173 = icmp eq i64 %164, 0
  store i1 %173, i1* %zf
  %174 = icmp slt i64 %164, 0
  store i1 %174, i1* %sf
  %175 = trunc i64 %164 to i8
  %176 = call i8 @llvm.ctpop.i8(i8 %175)
  %177 = and i8 %176, 1
  %178 = icmp eq i8 %177, 0
  store i1 %178, i1* %pf
  store volatile i64 62944, i64* @assembly_address
  %179 = load i1* %zf
  br i1 %179, label %block_f61c, label %block_f5e2

block_f5e2:                                       ; preds = %block_f5ce
  store volatile i64 62946, i64* @assembly_address
  %180 = load i64* %stack_var_-32
  store i64 %180, i64* %rax
  store volatile i64 62950, i64* @assembly_address
  %181 = load i64* %rax
  %182 = add i64 %181, 16
  %183 = and i64 %181, 15
  %184 = icmp ugt i64 %183, 15
  %185 = icmp ult i64 %182, %181
  %186 = xor i64 %181, %182
  %187 = xor i64 16, %182
  %188 = and i64 %186, %187
  %189 = icmp slt i64 %188, 0
  store i1 %184, i1* %az
  store i1 %185, i1* %cf
  store i1 %189, i1* %of
  %190 = icmp eq i64 %182, 0
  store i1 %190, i1* %zf
  %191 = icmp slt i64 %182, 0
  store i1 %191, i1* %sf
  %192 = trunc i64 %182 to i8
  %193 = call i8 @llvm.ctpop.i8(i8 %192)
  %194 = and i8 %193, 1
  %195 = icmp eq i8 %194, 0
  store i1 %195, i1* %pf
  store i64 %182, i64* %rax
  store volatile i64 62954, i64* @assembly_address
  %196 = load i64* %rax
  %197 = add i64 %196, 8
  %198 = inttoptr i64 %197 to i64*
  %199 = load i64* %198
  store i64 %199, i64* %rax
  store volatile i64 62958, i64* @assembly_address
  %200 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %201 = icmp eq i64 %200, 0
  store i1 %201, i1* %zf
  %202 = icmp slt i64 %200, 0
  store i1 %202, i1* %sf
  %203 = trunc i64 %200 to i8
  %204 = call i8 @llvm.ctpop.i8(i8 %203)
  %205 = and i8 %204, 1
  %206 = icmp eq i8 %205, 0
  store i1 %206, i1* %pf
  store volatile i64 62961, i64* @assembly_address
  %207 = load i1* %sf
  br i1 %207, label %block_f607, label %block_f5f3

block_f5f3:                                       ; preds = %block_f5e2
  store volatile i64 62963, i64* @assembly_address
  %208 = load i64* %stack_var_-32
  store i64 %208, i64* %rax
  store volatile i64 62967, i64* @assembly_address
  %209 = load i64* %rax
  %210 = add i64 %209, 16
  %211 = and i64 %209, 15
  %212 = icmp ugt i64 %211, 15
  %213 = icmp ult i64 %210, %209
  %214 = xor i64 %209, %210
  %215 = xor i64 16, %210
  %216 = and i64 %214, %215
  %217 = icmp slt i64 %216, 0
  store i1 %212, i1* %az
  store i1 %213, i1* %cf
  store i1 %217, i1* %of
  %218 = icmp eq i64 %210, 0
  store i1 %218, i1* %zf
  %219 = icmp slt i64 %210, 0
  store i1 %219, i1* %sf
  %220 = trunc i64 %210 to i8
  %221 = call i8 @llvm.ctpop.i8(i8 %220)
  %222 = and i8 %221, 1
  %223 = icmp eq i8 %222, 0
  store i1 %223, i1* %pf
  store i64 %210, i64* %rax
  store volatile i64 62971, i64* @assembly_address
  %224 = load i64* %rax
  %225 = add i64 %224, 8
  %226 = inttoptr i64 %225 to i64*
  %227 = load i64* %226
  store i64 %227, i64* %rax
  store volatile i64 62975, i64* @assembly_address
  %228 = load i64* %rax
  store i64 %228, i64* %2
  store i64 999999999, i64* %1
  %229 = sub i64 %228, 999999999
  %230 = and i64 %228, 15
  %231 = sub i64 %230, 15
  %232 = icmp ugt i64 %231, 15
  %233 = icmp ult i64 %228, 999999999
  %234 = xor i64 %228, 999999999
  %235 = xor i64 %228, %229
  %236 = and i64 %234, %235
  %237 = icmp slt i64 %236, 0
  store i1 %232, i1* %az
  store i1 %233, i1* %cf
  store i1 %237, i1* %of
  %238 = icmp eq i64 %229, 0
  store i1 %238, i1* %zf
  %239 = icmp slt i64 %229, 0
  store i1 %239, i1* %sf
  %240 = trunc i64 %229 to i8
  %241 = call i8 @llvm.ctpop.i8(i8 %240)
  %242 = and i8 %241, 1
  %243 = icmp eq i8 %242, 0
  store i1 %243, i1* %pf
  store volatile i64 62981, i64* @assembly_address
  %244 = load i64* %2
  %245 = load i64* %1
  %246 = icmp sle i64 %244, %245
  br i1 %246, label %block_f61c, label %block_f607

block_f607:                                       ; preds = %block_f5f3, %block_f5e2, %block_f5aa, %block_f59d
  store volatile i64 62983, i64* @assembly_address
  %247 = call i32* @__errno_location()
  %248 = ptrtoint i32* %247 to i64
  store i64 %248, i64* %rax
  %249 = ptrtoint i32* %247 to i64
  store i64 %249, i64* %rax
  %250 = ptrtoint i32* %247 to i64
  store i64 %250, i64* %rax
  store volatile i64 62988, i64* @assembly_address
  %251 = load i64* %rax
  %252 = inttoptr i64 %251 to i32*
  store i32 22, i32* %252
  store volatile i64 62994, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 62999, i64* @assembly_address
  br label %block_f6c7

block_f61c:                                       ; preds = %block_f5f3, %block_f5ce, %block_f5ba
  store volatile i64 63004, i64* @assembly_address
  %253 = load i64* %stack_var_-32
  store i64 %253, i64* %rax
  store volatile i64 63008, i64* @assembly_address
  %254 = load i64* %rax
  %255 = add i64 %254, 8
  %256 = inttoptr i64 %255 to i64*
  %257 = load i64* %256
  store i64 %257, i64* %rax
  store volatile i64 63012, i64* @assembly_address
  %258 = load i64* %rax
  %259 = sub i64 %258, 1073741823
  %260 = and i64 %258, 15
  %261 = sub i64 %260, 15
  %262 = icmp ugt i64 %261, 15
  %263 = icmp ult i64 %258, 1073741823
  %264 = xor i64 %258, 1073741823
  %265 = xor i64 %258, %259
  %266 = and i64 %264, %265
  %267 = icmp slt i64 %266, 0
  store i1 %262, i1* %az
  store i1 %263, i1* %cf
  store i1 %267, i1* %of
  %268 = icmp eq i64 %259, 0
  store i1 %268, i1* %zf
  %269 = icmp slt i64 %259, 0
  store i1 %269, i1* %sf
  %270 = trunc i64 %259 to i8
  %271 = call i8 @llvm.ctpop.i8(i8 %270)
  %272 = and i8 %271, 1
  %273 = icmp eq i8 %272, 0
  store i1 %273, i1* %pf
  store volatile i64 63018, i64* @assembly_address
  %274 = load i1* %zf
  br i1 %274, label %block_f63c, label %block_f62c

block_f62c:                                       ; preds = %block_f61c
  store volatile i64 63020, i64* @assembly_address
  %275 = load i64* %stack_var_-32
  store i64 %275, i64* %rax
  store volatile i64 63024, i64* @assembly_address
  %276 = load i64* %rax
  %277 = add i64 %276, 8
  %278 = inttoptr i64 %277 to i64*
  %279 = load i64* %278
  store i64 %279, i64* %rax
  store volatile i64 63028, i64* @assembly_address
  %280 = load i64* %rax
  %281 = sub i64 %280, 1073741822
  %282 = and i64 %280, 15
  %283 = sub i64 %282, 14
  %284 = icmp ugt i64 %283, 15
  %285 = icmp ult i64 %280, 1073741822
  %286 = xor i64 %280, 1073741822
  %287 = xor i64 %280, %281
  %288 = and i64 %286, %287
  %289 = icmp slt i64 %288, 0
  store i1 %284, i1* %az
  store i1 %285, i1* %cf
  store i1 %289, i1* %of
  %290 = icmp eq i64 %281, 0
  store i1 %290, i1* %zf
  %291 = icmp slt i64 %281, 0
  store i1 %291, i1* %sf
  %292 = trunc i64 %281 to i8
  %293 = call i8 @llvm.ctpop.i8(i8 %292)
  %294 = and i8 %293, 1
  %295 = icmp eq i8 %294, 0
  store i1 %295, i1* %pf
  store volatile i64 63034, i64* @assembly_address
  %296 = load i1* %zf
  %297 = icmp eq i1 %296, false
  br i1 %297, label %block_f662, label %block_f63c

block_f63c:                                       ; preds = %block_f62c, %block_f61c
  store volatile i64 63036, i64* @assembly_address
  %298 = load i64* %stack_var_-32
  store i64 %298, i64* %rax
  store volatile i64 63040, i64* @assembly_address
  %299 = load i64* %rax
  %300 = inttoptr i64 %299 to i64*
  store i64 0, i64* %300
  store volatile i64 63047, i64* @assembly_address
  store i32 1, i32* %stack_var_-16
  store volatile i64 63054, i64* @assembly_address
  %301 = load i64* %stack_var_-32
  store i64 %301, i64* %rax
  store volatile i64 63058, i64* @assembly_address
  %302 = load i64* %rax
  %303 = add i64 %302, 8
  %304 = inttoptr i64 %303 to i64*
  %305 = load i64* %304
  store i64 %305, i64* %rax
  store volatile i64 63062, i64* @assembly_address
  %306 = load i64* %rax
  %307 = sub i64 %306, 1073741822
  %308 = and i64 %306, 15
  %309 = sub i64 %308, 14
  %310 = icmp ugt i64 %309, 15
  %311 = icmp ult i64 %306, 1073741822
  %312 = xor i64 %306, 1073741822
  %313 = xor i64 %306, %307
  %314 = and i64 %312, %313
  %315 = icmp slt i64 %314, 0
  store i1 %310, i1* %az
  store i1 %311, i1* %cf
  store i1 %315, i1* %of
  %316 = icmp eq i64 %307, 0
  store i1 %316, i1* %zf
  %317 = icmp slt i64 %307, 0
  store i1 %317, i1* %sf
  %318 = trunc i64 %307 to i8
  %319 = call i8 @llvm.ctpop.i8(i8 %318)
  %320 = and i8 %319, 1
  %321 = icmp eq i8 %320, 0
  store i1 %321, i1* %pf
  store volatile i64 63068, i64* @assembly_address
  %322 = load i1* %zf
  %323 = icmp eq i1 %322, false
  br i1 %323, label %block_f662, label %block_f65e

block_f65e:                                       ; preds = %block_f63c
  store volatile i64 63070, i64* @assembly_address
  %324 = load i32* %stack_var_-12
  %325 = add i32 %324, 1
  %326 = and i32 %324, 15
  %327 = add i32 %326, 1
  %328 = icmp ugt i32 %327, 15
  %329 = icmp ult i32 %325, %324
  %330 = xor i32 %324, %325
  %331 = xor i32 1, %325
  %332 = and i32 %330, %331
  %333 = icmp slt i32 %332, 0
  store i1 %328, i1* %az
  store i1 %329, i1* %cf
  store i1 %333, i1* %of
  %334 = icmp eq i32 %325, 0
  store i1 %334, i1* %zf
  %335 = icmp slt i32 %325, 0
  store i1 %335, i1* %sf
  %336 = trunc i32 %325 to i8
  %337 = call i8 @llvm.ctpop.i8(i8 %336)
  %338 = and i8 %337, 1
  %339 = icmp eq i8 %338, 0
  store i1 %339, i1* %pf
  store i32 %325, i32* %stack_var_-12
  br label %block_f662

block_f662:                                       ; preds = %block_f65e, %block_f63c, %block_f62c
  store volatile i64 63074, i64* @assembly_address
  %340 = load i64* %stack_var_-32
  store i64 %340, i64* %rax
  store volatile i64 63078, i64* @assembly_address
  %341 = load i64* %rax
  %342 = add i64 %341, 16
  %343 = and i64 %341, 15
  %344 = icmp ugt i64 %343, 15
  %345 = icmp ult i64 %342, %341
  %346 = xor i64 %341, %342
  %347 = xor i64 16, %342
  %348 = and i64 %346, %347
  %349 = icmp slt i64 %348, 0
  store i1 %344, i1* %az
  store i1 %345, i1* %cf
  store i1 %349, i1* %of
  %350 = icmp eq i64 %342, 0
  store i1 %350, i1* %zf
  %351 = icmp slt i64 %342, 0
  store i1 %351, i1* %sf
  %352 = trunc i64 %342 to i8
  %353 = call i8 @llvm.ctpop.i8(i8 %352)
  %354 = and i8 %353, 1
  %355 = icmp eq i8 %354, 0
  store i1 %355, i1* %pf
  store i64 %342, i64* %rax
  store volatile i64 63082, i64* @assembly_address
  %356 = load i64* %rax
  %357 = add i64 %356, 8
  %358 = inttoptr i64 %357 to i64*
  %359 = load i64* %358
  store i64 %359, i64* %rax
  store volatile i64 63086, i64* @assembly_address
  %360 = load i64* %rax
  %361 = sub i64 %360, 1073741823
  %362 = and i64 %360, 15
  %363 = sub i64 %362, 15
  %364 = icmp ugt i64 %363, 15
  %365 = icmp ult i64 %360, 1073741823
  %366 = xor i64 %360, 1073741823
  %367 = xor i64 %360, %361
  %368 = and i64 %366, %367
  %369 = icmp slt i64 %368, 0
  store i1 %364, i1* %az
  store i1 %365, i1* %cf
  store i1 %369, i1* %of
  %370 = icmp eq i64 %361, 0
  store i1 %370, i1* %zf
  %371 = icmp slt i64 %361, 0
  store i1 %371, i1* %sf
  %372 = trunc i64 %361 to i8
  %373 = call i8 @llvm.ctpop.i8(i8 %372)
  %374 = and i8 %373, 1
  %375 = icmp eq i8 %374, 0
  store i1 %375, i1* %pf
  store volatile i64 63092, i64* @assembly_address
  %376 = load i1* %zf
  br i1 %376, label %block_f68a, label %block_f676

block_f676:                                       ; preds = %block_f662
  store volatile i64 63094, i64* @assembly_address
  %377 = load i64* %stack_var_-32
  store i64 %377, i64* %rax
  store volatile i64 63098, i64* @assembly_address
  %378 = load i64* %rax
  %379 = add i64 %378, 16
  %380 = and i64 %378, 15
  %381 = icmp ugt i64 %380, 15
  %382 = icmp ult i64 %379, %378
  %383 = xor i64 %378, %379
  %384 = xor i64 16, %379
  %385 = and i64 %383, %384
  %386 = icmp slt i64 %385, 0
  store i1 %381, i1* %az
  store i1 %382, i1* %cf
  store i1 %386, i1* %of
  %387 = icmp eq i64 %379, 0
  store i1 %387, i1* %zf
  %388 = icmp slt i64 %379, 0
  store i1 %388, i1* %sf
  %389 = trunc i64 %379 to i8
  %390 = call i8 @llvm.ctpop.i8(i8 %389)
  %391 = and i8 %390, 1
  %392 = icmp eq i8 %391, 0
  store i1 %392, i1* %pf
  store i64 %379, i64* %rax
  store volatile i64 63102, i64* @assembly_address
  %393 = load i64* %rax
  %394 = add i64 %393, 8
  %395 = inttoptr i64 %394 to i64*
  %396 = load i64* %395
  store i64 %396, i64* %rax
  store volatile i64 63106, i64* @assembly_address
  %397 = load i64* %rax
  %398 = sub i64 %397, 1073741822
  %399 = and i64 %397, 15
  %400 = sub i64 %399, 14
  %401 = icmp ugt i64 %400, 15
  %402 = icmp ult i64 %397, 1073741822
  %403 = xor i64 %397, 1073741822
  %404 = xor i64 %397, %398
  %405 = and i64 %403, %404
  %406 = icmp slt i64 %405, 0
  store i1 %401, i1* %az
  store i1 %402, i1* %cf
  store i1 %406, i1* %of
  %407 = icmp eq i64 %398, 0
  store i1 %407, i1* %zf
  %408 = icmp slt i64 %398, 0
  store i1 %408, i1* %sf
  %409 = trunc i64 %398 to i8
  %410 = call i8 @llvm.ctpop.i8(i8 %409)
  %411 = and i8 %410, 1
  %412 = icmp eq i8 %411, 0
  store i1 %412, i1* %pf
  store volatile i64 63112, i64* @assembly_address
  %413 = load i1* %zf
  %414 = icmp eq i1 %413, false
  br i1 %414, label %block_f6b8, label %block_f68a

block_f68a:                                       ; preds = %block_f676, %block_f662
  store volatile i64 63114, i64* @assembly_address
  %415 = load i64* %stack_var_-32
  store i64 %415, i64* %rax
  store volatile i64 63118, i64* @assembly_address
  %416 = load i64* %rax
  %417 = add i64 %416, 16
  %418 = and i64 %416, 15
  %419 = icmp ugt i64 %418, 15
  %420 = icmp ult i64 %417, %416
  %421 = xor i64 %416, %417
  %422 = xor i64 16, %417
  %423 = and i64 %421, %422
  %424 = icmp slt i64 %423, 0
  store i1 %419, i1* %az
  store i1 %420, i1* %cf
  store i1 %424, i1* %of
  %425 = icmp eq i64 %417, 0
  store i1 %425, i1* %zf
  %426 = icmp slt i64 %417, 0
  store i1 %426, i1* %sf
  %427 = trunc i64 %417 to i8
  %428 = call i8 @llvm.ctpop.i8(i8 %427)
  %429 = and i8 %428, 1
  %430 = icmp eq i8 %429, 0
  store i1 %430, i1* %pf
  store i64 %417, i64* %rax
  store volatile i64 63122, i64* @assembly_address
  %431 = load i64* %rax
  %432 = inttoptr i64 %431 to i64*
  store i64 0, i64* %432
  store volatile i64 63129, i64* @assembly_address
  store i32 1, i32* %stack_var_-16
  store volatile i64 63136, i64* @assembly_address
  %433 = load i64* %stack_var_-32
  store i64 %433, i64* %rax
  store volatile i64 63140, i64* @assembly_address
  %434 = load i64* %rax
  %435 = add i64 %434, 16
  %436 = and i64 %434, 15
  %437 = icmp ugt i64 %436, 15
  %438 = icmp ult i64 %435, %434
  %439 = xor i64 %434, %435
  %440 = xor i64 16, %435
  %441 = and i64 %439, %440
  %442 = icmp slt i64 %441, 0
  store i1 %437, i1* %az
  store i1 %438, i1* %cf
  store i1 %442, i1* %of
  %443 = icmp eq i64 %435, 0
  store i1 %443, i1* %zf
  %444 = icmp slt i64 %435, 0
  store i1 %444, i1* %sf
  %445 = trunc i64 %435 to i8
  %446 = call i8 @llvm.ctpop.i8(i8 %445)
  %447 = and i8 %446, 1
  %448 = icmp eq i8 %447, 0
  store i1 %448, i1* %pf
  store i64 %435, i64* %rax
  store volatile i64 63144, i64* @assembly_address
  %449 = load i64* %rax
  %450 = add i64 %449, 8
  %451 = inttoptr i64 %450 to i64*
  %452 = load i64* %451
  store i64 %452, i64* %rax
  store volatile i64 63148, i64* @assembly_address
  %453 = load i64* %rax
  %454 = sub i64 %453, 1073741822
  %455 = and i64 %453, 15
  %456 = sub i64 %455, 14
  %457 = icmp ugt i64 %456, 15
  %458 = icmp ult i64 %453, 1073741822
  %459 = xor i64 %453, 1073741822
  %460 = xor i64 %453, %454
  %461 = and i64 %459, %460
  %462 = icmp slt i64 %461, 0
  store i1 %457, i1* %az
  store i1 %458, i1* %cf
  store i1 %462, i1* %of
  %463 = icmp eq i64 %454, 0
  store i1 %463, i1* %zf
  %464 = icmp slt i64 %454, 0
  store i1 %464, i1* %sf
  %465 = trunc i64 %454 to i8
  %466 = call i8 @llvm.ctpop.i8(i8 %465)
  %467 = and i8 %466, 1
  %468 = icmp eq i8 %467, 0
  store i1 %468, i1* %pf
  store volatile i64 63154, i64* @assembly_address
  %469 = load i1* %zf
  %470 = icmp eq i1 %469, false
  br i1 %470, label %block_f6b8, label %block_f6b4

block_f6b4:                                       ; preds = %block_f68a
  store volatile i64 63156, i64* @assembly_address
  %471 = load i32* %stack_var_-12
  %472 = add i32 %471, 1
  %473 = and i32 %471, 15
  %474 = add i32 %473, 1
  %475 = icmp ugt i32 %474, 15
  %476 = icmp ult i32 %472, %471
  %477 = xor i32 %471, %472
  %478 = xor i32 1, %472
  %479 = and i32 %477, %478
  %480 = icmp slt i32 %479, 0
  store i1 %475, i1* %az
  store i1 %476, i1* %cf
  store i1 %480, i1* %of
  %481 = icmp eq i32 %472, 0
  store i1 %481, i1* %zf
  %482 = icmp slt i32 %472, 0
  store i1 %482, i1* %sf
  %483 = trunc i32 %472 to i8
  %484 = call i8 @llvm.ctpop.i8(i8 %483)
  %485 = and i8 %484, 1
  %486 = icmp eq i8 %485, 0
  store i1 %486, i1* %pf
  store i32 %472, i32* %stack_var_-12
  br label %block_f6b8

block_f6b8:                                       ; preds = %block_f6b4, %block_f68a, %block_f676
  store volatile i64 63160, i64* @assembly_address
  %487 = load i32* %stack_var_-12
  %488 = sub i32 %487, 1
  %489 = and i32 %487, 15
  %490 = sub i32 %489, 1
  %491 = icmp ugt i32 %490, 15
  %492 = icmp ult i32 %487, 1
  %493 = xor i32 %487, 1
  %494 = xor i32 %487, %488
  %495 = and i32 %493, %494
  %496 = icmp slt i32 %495, 0
  store i1 %491, i1* %az
  store i1 %492, i1* %cf
  store i1 %496, i1* %of
  %497 = icmp eq i32 %488, 0
  store i1 %497, i1* %zf
  %498 = icmp slt i32 %488, 0
  store i1 %498, i1* %sf
  %499 = trunc i32 %488 to i8
  %500 = call i8 @llvm.ctpop.i8(i8 %499)
  %501 = and i8 %500, 1
  %502 = icmp eq i8 %501, 0
  store i1 %502, i1* %pf
  store volatile i64 63164, i64* @assembly_address
  %503 = load i1* %zf
  %504 = zext i1 %503 to i8
  %505 = zext i8 %504 to i64
  %506 = load i64* %rax
  %507 = and i64 %506, -256
  %508 = or i64 %507, %505
  store i64 %508, i64* %rax
  store volatile i64 63167, i64* @assembly_address
  %509 = load i64* %rax
  %510 = trunc i64 %509 to i8
  %511 = zext i8 %510 to i64
  store i64 %511, i64* %rdx
  store volatile i64 63170, i64* @assembly_address
  %512 = load i32* %stack_var_-16
  %513 = zext i32 %512 to i64
  store i64 %513, i64* %rax
  store volatile i64 63173, i64* @assembly_address
  %514 = load i64* %rax
  %515 = trunc i64 %514 to i32
  %516 = load i64* %rdx
  %517 = trunc i64 %516 to i32
  %518 = add i32 %515, %517
  %519 = and i32 %515, 15
  %520 = and i32 %517, 15
  %521 = add i32 %519, %520
  %522 = icmp ugt i32 %521, 15
  %523 = icmp ult i32 %518, %515
  %524 = xor i32 %515, %518
  %525 = xor i32 %517, %518
  %526 = and i32 %524, %525
  %527 = icmp slt i32 %526, 0
  store i1 %522, i1* %az
  store i1 %523, i1* %cf
  store i1 %527, i1* %of
  %528 = icmp eq i32 %518, 0
  store i1 %528, i1* %zf
  %529 = icmp slt i32 %518, 0
  store i1 %529, i1* %sf
  %530 = trunc i32 %518 to i8
  %531 = call i8 @llvm.ctpop.i8(i8 %530)
  %532 = and i8 %531, 1
  %533 = icmp eq i8 %532, 0
  store i1 %533, i1* %pf
  %534 = zext i32 %518 to i64
  store i64 %534, i64* %rax
  br label %block_f6c7

block_f6c7:                                       ; preds = %block_f6b8, %block_f607
  store volatile i64 63175, i64* @assembly_address
  %535 = load i64* %stack_var_-8
  store i64 %535, i64* %rbp
  %536 = ptrtoint i64* %stack_var_0 to i64
  store i64 %536, i64* %rsp
  store volatile i64 63176, i64* @assembly_address
  %537 = load i64* %rax
  ret i64 %537
}

declare i64 @276(i64)

define i64 @update_timespec(i64* %arg1, i64* %arg2, i64* %arg3) {
block_f6c9:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg3 to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %arg2 to i64
  store i64 %1, i64* %rsi
  %2 = ptrtoint i64* %arg1 to i64
  store i64 %2, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-48 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 63177, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 63178, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 63181, i64* @assembly_address
  %6 = load i64* %rbx
  store i64 %6, i64* %stack_var_-16
  %7 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %7, i64* %rsp
  store volatile i64 63182, i64* @assembly_address
  %8 = load i64* %rsp
  %9 = sub i64 %8, 40
  %10 = and i64 %8, 15
  %11 = sub i64 %10, 8
  %12 = icmp ugt i64 %11, 15
  %13 = icmp ult i64 %8, 40
  %14 = xor i64 %8, 40
  %15 = xor i64 %8, %9
  %16 = and i64 %14, %15
  %17 = icmp slt i64 %16, 0
  store i1 %12, i1* %az
  store i1 %13, i1* %cf
  store i1 %17, i1* %of
  %18 = icmp eq i64 %9, 0
  store i1 %18, i1* %zf
  %19 = icmp slt i64 %9, 0
  store i1 %19, i1* %sf
  %20 = trunc i64 %9 to i8
  %21 = call i8 @llvm.ctpop.i8(i8 %20)
  %22 = and i8 %21, 1
  %23 = icmp eq i8 %22, 0
  store i1 %23, i1* %pf
  %24 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %24, i64* %rsp
  store volatile i64 63186, i64* @assembly_address
  %25 = load i64* %rdi
  store i64 %25, i64* %stack_var_-48
  store volatile i64 63190, i64* @assembly_address
  %26 = load i64* %rsi
  store i64 %26, i64* %stack_var_-56
  store volatile i64 63194, i64* @assembly_address
  %27 = load i64* %stack_var_-56
  store i64 %27, i64* %rax
  store volatile i64 63198, i64* @assembly_address
  %28 = load i64* %rax
  %29 = inttoptr i64 %28 to i64*
  %30 = load i64* %29
  store i64 %30, i64* %rax
  store volatile i64 63201, i64* @assembly_address
  %31 = load i64* %rax
  store i64 %31, i64* %stack_var_-32
  store volatile i64 63205, i64* @assembly_address
  %32 = load i64* %stack_var_-32
  store i64 %32, i64* %rax
  store volatile i64 63209, i64* @assembly_address
  %33 = load i64* %rax
  %34 = add i64 %33, 8
  %35 = inttoptr i64 %34 to i64*
  %36 = load i64* %35
  store i64 %36, i64* %rax
  store volatile i64 63213, i64* @assembly_address
  %37 = load i64* %rax
  %38 = sub i64 %37, 1073741822
  %39 = and i64 %37, 15
  %40 = sub i64 %39, 14
  %41 = icmp ugt i64 %40, 15
  %42 = icmp ult i64 %37, 1073741822
  %43 = xor i64 %37, 1073741822
  %44 = xor i64 %37, %38
  %45 = and i64 %43, %44
  %46 = icmp slt i64 %45, 0
  store i1 %41, i1* %az
  store i1 %42, i1* %cf
  store i1 %46, i1* %of
  %47 = icmp eq i64 %38, 0
  store i1 %47, i1* %zf
  %48 = icmp slt i64 %38, 0
  store i1 %48, i1* %sf
  %49 = trunc i64 %38 to i8
  %50 = call i8 @llvm.ctpop.i8(i8 %49)
  %51 = and i8 %50, 1
  %52 = icmp eq i8 %51, 0
  store i1 %52, i1* %pf
  store volatile i64 63219, i64* @assembly_address
  %53 = load i1* %zf
  %54 = icmp eq i1 %53, false
  br i1 %54, label %block_f713, label %block_f6f5

block_f6f5:                                       ; preds = %block_f6c9
  store volatile i64 63221, i64* @assembly_address
  %55 = load i64* %stack_var_-32
  store i64 %55, i64* %rax
  store volatile i64 63225, i64* @assembly_address
  %56 = load i64* %rax
  %57 = add i64 %56, 16
  %58 = and i64 %56, 15
  %59 = icmp ugt i64 %58, 15
  %60 = icmp ult i64 %57, %56
  %61 = xor i64 %56, %57
  %62 = xor i64 16, %57
  %63 = and i64 %61, %62
  %64 = icmp slt i64 %63, 0
  store i1 %59, i1* %az
  store i1 %60, i1* %cf
  store i1 %64, i1* %of
  %65 = icmp eq i64 %57, 0
  store i1 %65, i1* %zf
  %66 = icmp slt i64 %57, 0
  store i1 %66, i1* %sf
  %67 = trunc i64 %57 to i8
  %68 = call i8 @llvm.ctpop.i8(i8 %67)
  %69 = and i8 %68, 1
  %70 = icmp eq i8 %69, 0
  store i1 %70, i1* %pf
  store i64 %57, i64* %rax
  store volatile i64 63229, i64* @assembly_address
  %71 = load i64* %rax
  %72 = add i64 %71, 8
  %73 = inttoptr i64 %72 to i64*
  %74 = load i64* %73
  store i64 %74, i64* %rax
  store volatile i64 63233, i64* @assembly_address
  %75 = load i64* %rax
  %76 = sub i64 %75, 1073741822
  %77 = and i64 %75, 15
  %78 = sub i64 %77, 14
  %79 = icmp ugt i64 %78, 15
  %80 = icmp ult i64 %75, 1073741822
  %81 = xor i64 %75, 1073741822
  %82 = xor i64 %75, %76
  %83 = and i64 %81, %82
  %84 = icmp slt i64 %83, 0
  store i1 %79, i1* %az
  store i1 %80, i1* %cf
  store i1 %84, i1* %of
  %85 = icmp eq i64 %76, 0
  store i1 %85, i1* %zf
  %86 = icmp slt i64 %76, 0
  store i1 %86, i1* %sf
  %87 = trunc i64 %76 to i8
  %88 = call i8 @llvm.ctpop.i8(i8 %87)
  %89 = and i8 %88, 1
  %90 = icmp eq i8 %89, 0
  store i1 %90, i1* %pf
  store volatile i64 63239, i64* @assembly_address
  %91 = load i1* %zf
  %92 = icmp eq i1 %91, false
  br i1 %92, label %block_f713, label %block_f709

block_f709:                                       ; preds = %block_f6f5
  store volatile i64 63241, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 63246, i64* @assembly_address
  br label %block_f7eb

block_f713:                                       ; preds = %block_f6f5, %block_f6c9
  store volatile i64 63251, i64* @assembly_address
  %93 = load i64* %stack_var_-32
  store i64 %93, i64* %rax
  store volatile i64 63255, i64* @assembly_address
  %94 = load i64* %rax
  %95 = add i64 %94, 8
  %96 = inttoptr i64 %95 to i64*
  %97 = load i64* %96
  store i64 %97, i64* %rax
  store volatile i64 63259, i64* @assembly_address
  %98 = load i64* %rax
  %99 = sub i64 %98, 1073741823
  %100 = and i64 %98, 15
  %101 = sub i64 %100, 15
  %102 = icmp ugt i64 %101, 15
  %103 = icmp ult i64 %98, 1073741823
  %104 = xor i64 %98, 1073741823
  %105 = xor i64 %98, %99
  %106 = and i64 %104, %105
  %107 = icmp slt i64 %106, 0
  store i1 %102, i1* %az
  store i1 %103, i1* %cf
  store i1 %107, i1* %of
  %108 = icmp eq i64 %99, 0
  store i1 %108, i1* %zf
  %109 = icmp slt i64 %99, 0
  store i1 %109, i1* %sf
  %110 = trunc i64 %99 to i8
  %111 = call i8 @llvm.ctpop.i8(i8 %110)
  %112 = and i8 %111, 1
  %113 = icmp eq i8 %112, 0
  store i1 %113, i1* %pf
  store volatile i64 63265, i64* @assembly_address
  %114 = load i1* %zf
  %115 = icmp eq i1 %114, false
  br i1 %115, label %block_f74c, label %block_f723

block_f723:                                       ; preds = %block_f713
  store volatile i64 63267, i64* @assembly_address
  %116 = load i64* %stack_var_-32
  store i64 %116, i64* %rax
  store volatile i64 63271, i64* @assembly_address
  %117 = load i64* %rax
  %118 = add i64 %117, 16
  %119 = and i64 %117, 15
  %120 = icmp ugt i64 %119, 15
  %121 = icmp ult i64 %118, %117
  %122 = xor i64 %117, %118
  %123 = xor i64 16, %118
  %124 = and i64 %122, %123
  %125 = icmp slt i64 %124, 0
  store i1 %120, i1* %az
  store i1 %121, i1* %cf
  store i1 %125, i1* %of
  %126 = icmp eq i64 %118, 0
  store i1 %126, i1* %zf
  %127 = icmp slt i64 %118, 0
  store i1 %127, i1* %sf
  %128 = trunc i64 %118 to i8
  %129 = call i8 @llvm.ctpop.i8(i8 %128)
  %130 = and i8 %129, 1
  %131 = icmp eq i8 %130, 0
  store i1 %131, i1* %pf
  store i64 %118, i64* %rax
  store volatile i64 63275, i64* @assembly_address
  %132 = load i64* %rax
  %133 = add i64 %132, 8
  %134 = inttoptr i64 %133 to i64*
  %135 = load i64* %134
  store i64 %135, i64* %rax
  store volatile i64 63279, i64* @assembly_address
  %136 = load i64* %rax
  %137 = sub i64 %136, 1073741823
  %138 = and i64 %136, 15
  %139 = sub i64 %138, 15
  %140 = icmp ugt i64 %139, 15
  %141 = icmp ult i64 %136, 1073741823
  %142 = xor i64 %136, 1073741823
  %143 = xor i64 %136, %137
  %144 = and i64 %142, %143
  %145 = icmp slt i64 %144, 0
  store i1 %140, i1* %az
  store i1 %141, i1* %cf
  store i1 %145, i1* %of
  %146 = icmp eq i64 %137, 0
  store i1 %146, i1* %zf
  %147 = icmp slt i64 %137, 0
  store i1 %147, i1* %sf
  %148 = trunc i64 %137 to i8
  %149 = call i8 @llvm.ctpop.i8(i8 %148)
  %150 = and i8 %149, 1
  %151 = icmp eq i8 %150, 0
  store i1 %151, i1* %pf
  store volatile i64 63285, i64* @assembly_address
  %152 = load i1* %zf
  %153 = icmp eq i1 %152, false
  br i1 %153, label %block_f74c, label %block_f737

block_f737:                                       ; preds = %block_f723
  store volatile i64 63287, i64* @assembly_address
  %154 = load i64* %stack_var_-56
  store i64 %154, i64* %rax
  store volatile i64 63291, i64* @assembly_address
  %155 = load i64* %rax
  %156 = inttoptr i64 %155 to i64*
  store i64 0, i64* %156
  store volatile i64 63298, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 63303, i64* @assembly_address
  br label %block_f7eb

block_f74c:                                       ; preds = %block_f723, %block_f713
  store volatile i64 63308, i64* @assembly_address
  %157 = load i64* %stack_var_-32
  store i64 %157, i64* %rax
  store volatile i64 63312, i64* @assembly_address
  %158 = load i64* %rax
  %159 = add i64 %158, 8
  %160 = inttoptr i64 %159 to i64*
  %161 = load i64* %160
  store i64 %161, i64* %rax
  store volatile i64 63316, i64* @assembly_address
  %162 = load i64* %rax
  %163 = sub i64 %162, 1073741822
  %164 = and i64 %162, 15
  %165 = sub i64 %164, 14
  %166 = icmp ugt i64 %165, 15
  %167 = icmp ult i64 %162, 1073741822
  %168 = xor i64 %162, 1073741822
  %169 = xor i64 %162, %163
  %170 = and i64 %168, %169
  %171 = icmp slt i64 %170, 0
  store i1 %166, i1* %az
  store i1 %167, i1* %cf
  store i1 %171, i1* %of
  %172 = icmp eq i64 %163, 0
  store i1 %172, i1* %zf
  %173 = icmp slt i64 %163, 0
  store i1 %173, i1* %sf
  %174 = trunc i64 %163 to i8
  %175 = call i8 @llvm.ctpop.i8(i8 %174)
  %176 = and i8 %175, 1
  %177 = icmp eq i8 %176, 0
  store i1 %177, i1* %pf
  store volatile i64 63322, i64* @assembly_address
  %178 = load i1* %zf
  %179 = icmp eq i1 %178, false
  br i1 %179, label %block_f775, label %block_f75c

block_f75c:                                       ; preds = %block_f74c
  store volatile i64 63324, i64* @assembly_address
  %180 = load i64* %stack_var_-48
  store i64 %180, i64* %rax
  store volatile i64 63328, i64* @assembly_address
  %181 = load i64* %rax
  store i64 %181, i64* %rdi
  store volatile i64 63331, i64* @assembly_address
  %182 = load i64* %rdi
  %183 = call i64 @get_stat_atime(i64 %182)
  store i64 %183, i64* %rax
  store i64 %183, i64* %rax
  store volatile i64 63336, i64* @assembly_address
  %184 = load i64* %stack_var_-32
  store i64 %184, i64* %rcx
  store volatile i64 63340, i64* @assembly_address
  %185 = load i64* %rax
  %186 = load i64* %rcx
  %187 = inttoptr i64 %186 to i64*
  store i64 %185, i64* %187
  store volatile i64 63343, i64* @assembly_address
  %188 = load i64* %rdx
  %189 = load i64* %rcx
  %190 = add i64 %189, 8
  %191 = inttoptr i64 %190 to i64*
  store i64 %188, i64* %191
  store volatile i64 63347, i64* @assembly_address
  br label %block_f791

block_f775:                                       ; preds = %block_f74c
  store volatile i64 63349, i64* @assembly_address
  %192 = load i64* %stack_var_-32
  store i64 %192, i64* %rax
  store volatile i64 63353, i64* @assembly_address
  %193 = load i64* %rax
  %194 = add i64 %193, 8
  %195 = inttoptr i64 %194 to i64*
  %196 = load i64* %195
  store i64 %196, i64* %rax
  store volatile i64 63357, i64* @assembly_address
  %197 = load i64* %rax
  %198 = sub i64 %197, 1073741823
  %199 = and i64 %197, 15
  %200 = sub i64 %199, 15
  %201 = icmp ugt i64 %200, 15
  %202 = icmp ult i64 %197, 1073741823
  %203 = xor i64 %197, 1073741823
  %204 = xor i64 %197, %198
  %205 = and i64 %203, %204
  %206 = icmp slt i64 %205, 0
  store i1 %201, i1* %az
  store i1 %202, i1* %cf
  store i1 %206, i1* %of
  %207 = icmp eq i64 %198, 0
  store i1 %207, i1* %zf
  %208 = icmp slt i64 %198, 0
  store i1 %208, i1* %sf
  %209 = trunc i64 %198 to i8
  %210 = call i8 @llvm.ctpop.i8(i8 %209)
  %211 = and i8 %210, 1
  %212 = icmp eq i8 %211, 0
  store i1 %212, i1* %pf
  store volatile i64 63363, i64* @assembly_address
  %213 = load i1* %zf
  %214 = icmp eq i1 %213, false
  br i1 %214, label %block_f791, label %block_f785

block_f785:                                       ; preds = %block_f775
  store volatile i64 63365, i64* @assembly_address
  %215 = load i64* %stack_var_-32
  store i64 %215, i64* %rax
  store volatile i64 63369, i64* @assembly_address
  %216 = load i64* %rax
  store i64 %216, i64* %rdi
  store volatile i64 63372, i64* @assembly_address
  %217 = load i64* %rdi
  %218 = inttoptr i64 %217 to %timespec*
  %219 = call i64 @gettime(%timespec* %218)
  store i64 %219, i64* %rax
  store i64 %219, i64* %rax
  br label %block_f791

block_f791:                                       ; preds = %block_f785, %block_f775, %block_f75c
  store volatile i64 63377, i64* @assembly_address
  %220 = load i64* %stack_var_-32
  store i64 %220, i64* %rax
  store volatile i64 63381, i64* @assembly_address
  %221 = load i64* %rax
  %222 = add i64 %221, 16
  %223 = and i64 %221, 15
  %224 = icmp ugt i64 %223, 15
  %225 = icmp ult i64 %222, %221
  %226 = xor i64 %221, %222
  %227 = xor i64 16, %222
  %228 = and i64 %226, %227
  %229 = icmp slt i64 %228, 0
  store i1 %224, i1* %az
  store i1 %225, i1* %cf
  store i1 %229, i1* %of
  %230 = icmp eq i64 %222, 0
  store i1 %230, i1* %zf
  %231 = icmp slt i64 %222, 0
  store i1 %231, i1* %sf
  %232 = trunc i64 %222 to i8
  %233 = call i8 @llvm.ctpop.i8(i8 %232)
  %234 = and i8 %233, 1
  %235 = icmp eq i8 %234, 0
  store i1 %235, i1* %pf
  store i64 %222, i64* %rax
  store volatile i64 63385, i64* @assembly_address
  %236 = load i64* %rax
  %237 = add i64 %236, 8
  %238 = inttoptr i64 %237 to i64*
  %239 = load i64* %238
  store i64 %239, i64* %rax
  store volatile i64 63389, i64* @assembly_address
  %240 = load i64* %rax
  %241 = sub i64 %240, 1073741822
  %242 = and i64 %240, 15
  %243 = sub i64 %242, 14
  %244 = icmp ugt i64 %243, 15
  %245 = icmp ult i64 %240, 1073741822
  %246 = xor i64 %240, 1073741822
  %247 = xor i64 %240, %241
  %248 = and i64 %246, %247
  %249 = icmp slt i64 %248, 0
  store i1 %244, i1* %az
  store i1 %245, i1* %cf
  store i1 %249, i1* %of
  %250 = icmp eq i64 %241, 0
  store i1 %250, i1* %zf
  %251 = icmp slt i64 %241, 0
  store i1 %251, i1* %sf
  %252 = trunc i64 %241 to i8
  %253 = call i8 @llvm.ctpop.i8(i8 %252)
  %254 = and i8 %253, 1
  %255 = icmp eq i8 %254, 0
  store i1 %255, i1* %pf
  store volatile i64 63395, i64* @assembly_address
  %256 = load i1* %zf
  %257 = icmp eq i1 %256, false
  br i1 %257, label %block_f7c2, label %block_f7a5

block_f7a5:                                       ; preds = %block_f791
  store volatile i64 63397, i64* @assembly_address
  %258 = load i64* %stack_var_-32
  store i64 %258, i64* %rax
  store volatile i64 63401, i64* @assembly_address
  %259 = load i64* %rax
  %260 = add i64 %259, 16
  store i64 %260, i64* %rbx
  store volatile i64 63405, i64* @assembly_address
  %261 = load i64* %stack_var_-48
  store i64 %261, i64* %rax
  store volatile i64 63409, i64* @assembly_address
  %262 = load i64* %rax
  store i64 %262, i64* %rdi
  store volatile i64 63412, i64* @assembly_address
  %263 = load i64* %rdi
  %264 = inttoptr i64 %263 to i64*
  %265 = call i64 @get_stat_mtime(i64* %264)
  store i64 %265, i64* %rax
  store i64 %265, i64* %rax
  store volatile i64 63417, i64* @assembly_address
  %266 = load i64* %rax
  %267 = load i64* %rbx
  %268 = inttoptr i64 %267 to i64*
  store i64 %266, i64* %268
  store volatile i64 63420, i64* @assembly_address
  %269 = load i64* %rdx
  %270 = load i64* %rbx
  %271 = add i64 %270, 8
  %272 = inttoptr i64 %271 to i64*
  store i64 %269, i64* %272
  store volatile i64 63424, i64* @assembly_address
  br label %block_f7e6

block_f7c2:                                       ; preds = %block_f791
  store volatile i64 63426, i64* @assembly_address
  %273 = load i64* %stack_var_-32
  store i64 %273, i64* %rax
  store volatile i64 63430, i64* @assembly_address
  %274 = load i64* %rax
  %275 = add i64 %274, 16
  %276 = and i64 %274, 15
  %277 = icmp ugt i64 %276, 15
  %278 = icmp ult i64 %275, %274
  %279 = xor i64 %274, %275
  %280 = xor i64 16, %275
  %281 = and i64 %279, %280
  %282 = icmp slt i64 %281, 0
  store i1 %277, i1* %az
  store i1 %278, i1* %cf
  store i1 %282, i1* %of
  %283 = icmp eq i64 %275, 0
  store i1 %283, i1* %zf
  %284 = icmp slt i64 %275, 0
  store i1 %284, i1* %sf
  %285 = trunc i64 %275 to i8
  %286 = call i8 @llvm.ctpop.i8(i8 %285)
  %287 = and i8 %286, 1
  %288 = icmp eq i8 %287, 0
  store i1 %288, i1* %pf
  store i64 %275, i64* %rax
  store volatile i64 63434, i64* @assembly_address
  %289 = load i64* %rax
  %290 = add i64 %289, 8
  %291 = inttoptr i64 %290 to i64*
  %292 = load i64* %291
  store i64 %292, i64* %rax
  store volatile i64 63438, i64* @assembly_address
  %293 = load i64* %rax
  %294 = sub i64 %293, 1073741823
  %295 = and i64 %293, 15
  %296 = sub i64 %295, 15
  %297 = icmp ugt i64 %296, 15
  %298 = icmp ult i64 %293, 1073741823
  %299 = xor i64 %293, 1073741823
  %300 = xor i64 %293, %294
  %301 = and i64 %299, %300
  %302 = icmp slt i64 %301, 0
  store i1 %297, i1* %az
  store i1 %298, i1* %cf
  store i1 %302, i1* %of
  %303 = icmp eq i64 %294, 0
  store i1 %303, i1* %zf
  %304 = icmp slt i64 %294, 0
  store i1 %304, i1* %sf
  %305 = trunc i64 %294 to i8
  %306 = call i8 @llvm.ctpop.i8(i8 %305)
  %307 = and i8 %306, 1
  %308 = icmp eq i8 %307, 0
  store i1 %308, i1* %pf
  store volatile i64 63444, i64* @assembly_address
  %309 = load i1* %zf
  %310 = icmp eq i1 %309, false
  br i1 %310, label %block_f7e6, label %block_f7d6

block_f7d6:                                       ; preds = %block_f7c2
  store volatile i64 63446, i64* @assembly_address
  %311 = load i64* %stack_var_-32
  store i64 %311, i64* %rax
  store volatile i64 63450, i64* @assembly_address
  %312 = load i64* %rax
  %313 = add i64 %312, 16
  %314 = and i64 %312, 15
  %315 = icmp ugt i64 %314, 15
  %316 = icmp ult i64 %313, %312
  %317 = xor i64 %312, %313
  %318 = xor i64 16, %313
  %319 = and i64 %317, %318
  %320 = icmp slt i64 %319, 0
  store i1 %315, i1* %az
  store i1 %316, i1* %cf
  store i1 %320, i1* %of
  %321 = icmp eq i64 %313, 0
  store i1 %321, i1* %zf
  %322 = icmp slt i64 %313, 0
  store i1 %322, i1* %sf
  %323 = trunc i64 %313 to i8
  %324 = call i8 @llvm.ctpop.i8(i8 %323)
  %325 = and i8 %324, 1
  %326 = icmp eq i8 %325, 0
  store i1 %326, i1* %pf
  store i64 %313, i64* %rax
  store volatile i64 63454, i64* @assembly_address
  %327 = load i64* %rax
  store i64 %327, i64* %rdi
  store volatile i64 63457, i64* @assembly_address
  %328 = load i64* %rdi
  %329 = inttoptr i64 %328 to %timespec*
  %330 = call i64 @gettime(%timespec* %329)
  store i64 %330, i64* %rax
  store i64 %330, i64* %rax
  br label %block_f7e6

block_f7e6:                                       ; preds = %block_f7d6, %block_f7c2, %block_f7a5
  store volatile i64 63462, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_f7eb

block_f7eb:                                       ; preds = %block_f7e6, %block_f737, %block_f709
  store volatile i64 63467, i64* @assembly_address
  %331 = load i64* %rsp
  %332 = add i64 %331, 40
  %333 = and i64 %331, 15
  %334 = add i64 %333, 8
  %335 = icmp ugt i64 %334, 15
  %336 = icmp ult i64 %332, %331
  %337 = xor i64 %331, %332
  %338 = xor i64 40, %332
  %339 = and i64 %337, %338
  %340 = icmp slt i64 %339, 0
  store i1 %335, i1* %az
  store i1 %336, i1* %cf
  store i1 %340, i1* %of
  %341 = icmp eq i64 %332, 0
  store i1 %341, i1* %zf
  %342 = icmp slt i64 %332, 0
  store i1 %342, i1* %sf
  %343 = trunc i64 %332 to i8
  %344 = call i8 @llvm.ctpop.i8(i8 %343)
  %345 = and i8 %344, 1
  %346 = icmp eq i8 %345, 0
  store i1 %346, i1* %pf
  %347 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %347, i64* %rsp
  store volatile i64 63471, i64* @assembly_address
  %348 = load i64* %stack_var_-16
  store i64 %348, i64* %rbx
  %349 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %349, i64* %rsp
  store volatile i64 63472, i64* @assembly_address
  %350 = load i64* %stack_var_-8
  store i64 %350, i64* %rbp
  %351 = ptrtoint i64* %stack_var_0 to i64
  store i64 %351, i64* %rsp
  store volatile i64 63473, i64* @assembly_address
  %352 = load i64* %rax
  ret i64 %352
}

define i64 @fdutimens(i32 %arg1, i64* %arg2, i64* %arg3) {
block_f7f2:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg3 to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %arg2 to i64
  store i64 %1, i64* %rsi
  %2 = sext i32 %arg1 to i64
  store i64 %2, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-192 = alloca i64
  %stack_var_-200 = alloca i64
  %stack_var_-208 = alloca i64
  %stack_var_-216 = alloca i64
  %stack_var_-304 = alloca i64
  %stack_var_-288 = alloca i64
  %stack_var_-96 = alloca i64
  %stack_var_-296 = alloca i64
  %stack_var_-112 = alloca i64
  %stack_var_-329 = alloca i8
  %stack_var_-330 = alloca i8
  %stack_var_-312 = alloca i64
  %stack_var_-224 = alloca i64
  %stack_var_-232 = alloca i64
  %stack_var_-240 = alloca i64
  %stack_var_-248 = alloca i64
  %stack_var_-324 = alloca i32
  %stack_var_-184 = alloca i64
  %stack_var_-256 = alloca i64
  %stack_var_-264 = alloca i64
  %stack_var_-272 = alloca i64
  %stack_var_-328 = alloca i32
  %stack_var_-320 = alloca i64
  %stack_var_-280 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-368 = alloca i64
  %stack_var_-360 = alloca i8*
  %3 = alloca i64
  %stack_var_-348 = alloca i32
  %stack_var_-376 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %4 = alloca i32
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i32
  store volatile i64 63474, i64* @assembly_address
  %8 = load i64* %rbp
  store i64 %8, i64* %stack_var_-8
  %9 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %9, i64* %rsp
  store volatile i64 63475, i64* @assembly_address
  %10 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %10, i64* %rbp
  store volatile i64 63478, i64* @assembly_address
  %11 = load i64* %rbx
  store i64 %11, i64* %stack_var_-16
  %12 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %12, i64* %rsp
  store volatile i64 63479, i64* @assembly_address
  %13 = load i64* %rsp
  %14 = sub i64 %13, 360
  %15 = and i64 %13, 15
  %16 = sub i64 %15, 8
  %17 = icmp ugt i64 %16, 15
  %18 = icmp ult i64 %13, 360
  %19 = xor i64 %13, 360
  %20 = xor i64 %13, %14
  %21 = and i64 %19, %20
  %22 = icmp slt i64 %21, 0
  store i1 %17, i1* %az
  store i1 %18, i1* %cf
  store i1 %22, i1* %of
  %23 = icmp eq i64 %14, 0
  store i1 %23, i1* %zf
  %24 = icmp slt i64 %14, 0
  store i1 %24, i1* %sf
  %25 = trunc i64 %14 to i8
  %26 = call i8 @llvm.ctpop.i8(i8 %25)
  %27 = and i8 %26, 1
  %28 = icmp eq i8 %27, 0
  store i1 %28, i1* %pf
  %29 = ptrtoint i64* %stack_var_-376 to i64
  store i64 %29, i64* %rsp
  store volatile i64 63486, i64* @assembly_address
  %30 = load i64* %rdi
  %31 = trunc i64 %30 to i32
  store i32 %31, i32* %stack_var_-348
  store volatile i64 63492, i64* @assembly_address
  %32 = load i64* %rsi
  %33 = inttoptr i64 %32 to i8*
  store i8* %33, i8** %stack_var_-360
  store volatile i64 63499, i64* @assembly_address
  %34 = load i64* %rdx
  store i64 %34, i64* %stack_var_-368
  store volatile i64 63506, i64* @assembly_address
  %35 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %35, i64* %rax
  store volatile i64 63515, i64* @assembly_address
  %36 = load i64* %rax
  store i64 %36, i64* %stack_var_-32
  store volatile i64 63519, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %37 = icmp eq i32 0, 0
  store i1 %37, i1* %zf
  %38 = icmp slt i32 0, 0
  store i1 %38, i1* %sf
  %39 = trunc i32 0 to i8
  %40 = call i8 @llvm.ctpop.i8(i8 %39)
  %41 = and i8 %40, 1
  %42 = icmp eq i8 %41, 0
  store i1 %42, i1* %pf
  %43 = zext i32 0 to i64
  store i64 %43, i64* %rax
  store volatile i64 63521, i64* @assembly_address
  %44 = load i64* %stack_var_-368
  %45 = and i64 %44, 15
  %46 = icmp ugt i64 %45, 15
  %47 = icmp ult i64 %44, 0
  %48 = xor i64 %44, 0
  %49 = and i64 %48, 0
  %50 = icmp slt i64 %49, 0
  store i1 %46, i1* %az
  store i1 %47, i1* %cf
  store i1 %50, i1* %of
  %51 = icmp eq i64 %44, 0
  store i1 %51, i1* %zf
  %52 = icmp slt i64 %44, 0
  store i1 %52, i1* %sf
  %53 = trunc i64 %44 to i8
  %54 = call i8 @llvm.ctpop.i8(i8 %53)
  %55 = and i8 %54, 1
  %56 = icmp eq i8 %55, 0
  store i1 %56, i1* %pf
  store volatile i64 63529, i64* @assembly_address
  %57 = load i1* %zf
  br i1 %57, label %block_f834, label %block_f82b

block_f82b:                                       ; preds = %block_f7f2
  store volatile i64 63531, i64* @assembly_address
  %58 = ptrtoint i64* %stack_var_-280 to i64
  store i64 %58, i64* %rax
  store volatile i64 63538, i64* @assembly_address
  br label %block_f839

block_f834:                                       ; preds = %block_f7f2
  store volatile i64 63540, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_f839

block_f839:                                       ; preds = %block_f834, %block_f82b
  store volatile i64 63545, i64* @assembly_address
  %59 = load i64* %rax
  store i64 %59, i64* %stack_var_-320
  store volatile i64 63552, i64* @assembly_address
  store i32 0, i32* %stack_var_-328
  store volatile i64 63562, i64* @assembly_address
  %60 = load i64* %stack_var_-320
  store i64 %60, i64* %rax
  store volatile i64 63569, i64* @assembly_address
  %61 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %62 = icmp eq i64 %61, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i64 %61, 0
  store i1 %63, i1* %sf
  %64 = trunc i64 %61 to i8
  %65 = call i8 @llvm.ctpop.i8(i8 %64)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store volatile i64 63572, i64* @assembly_address
  %68 = load i1* %zf
  br i1 %68, label %block_f8a4, label %block_f856

block_f856:                                       ; preds = %block_f839
  store volatile i64 63574, i64* @assembly_address
  %69 = load i64* %stack_var_-368
  store i64 %69, i64* %rax
  store volatile i64 63581, i64* @assembly_address
  %70 = load i64* %rax
  %71 = add i64 %70, 8
  %72 = inttoptr i64 %71 to i64*
  %73 = load i64* %72
  store i64 %73, i64* %rdx
  store volatile i64 63585, i64* @assembly_address
  %74 = load i64* %rax
  %75 = inttoptr i64 %74 to i64*
  %76 = load i64* %75
  store i64 %76, i64* %rax
  store volatile i64 63588, i64* @assembly_address
  %77 = load i64* %rax
  store i64 %77, i64* %stack_var_-280
  store volatile i64 63595, i64* @assembly_address
  %78 = load i64* %rdx
  store i64 %78, i64* %stack_var_-272
  store volatile i64 63602, i64* @assembly_address
  %79 = load i64* %stack_var_-368
  store i64 %79, i64* %rax
  store volatile i64 63609, i64* @assembly_address
  %80 = load i64* %rax
  %81 = add i64 %80, 24
  %82 = inttoptr i64 %81 to i64*
  %83 = load i64* %82
  store i64 %83, i64* %rdx
  store volatile i64 63613, i64* @assembly_address
  %84 = load i64* %rax
  %85 = add i64 %84, 16
  %86 = inttoptr i64 %85 to i64*
  %87 = load i64* %86
  store i64 %87, i64* %rax
  store volatile i64 63617, i64* @assembly_address
  %88 = load i64* %rax
  store i64 %88, i64* %stack_var_-264
  store volatile i64 63624, i64* @assembly_address
  %89 = load i64* %rdx
  store i64 %89, i64* %stack_var_-256
  store volatile i64 63631, i64* @assembly_address
  %90 = load i64* %stack_var_-320
  store i64 %90, i64* %rax
  store volatile i64 63638, i64* @assembly_address
  %91 = load i64* %rax
  store i64 %91, i64* %rdi
  store volatile i64 63641, i64* @assembly_address
  %92 = load i64* %rdi
  %93 = inttoptr i64 %92 to i64*
  %94 = call i64 @validate_timespec(i64* %93)
  store i64 %94, i64* %rax
  store i64 %94, i64* %rax
  store volatile i64 63646, i64* @assembly_address
  %95 = load i64* %rax
  %96 = trunc i64 %95 to i32
  store i32 %96, i32* %stack_var_-328
  br label %block_f8a4

block_f8a4:                                       ; preds = %block_f856, %block_f839
  store volatile i64 63652, i64* @assembly_address
  %97 = load i32* %stack_var_-328
  %98 = and i32 %97, 15
  %99 = icmp ugt i32 %98, 15
  %100 = icmp ult i32 %97, 0
  %101 = xor i32 %97, 0
  %102 = and i32 %101, 0
  %103 = icmp slt i32 %102, 0
  store i1 %99, i1* %az
  store i1 %100, i1* %cf
  store i1 %103, i1* %of
  %104 = icmp eq i32 %97, 0
  store i1 %104, i1* %zf
  %105 = icmp slt i32 %97, 0
  store i1 %105, i1* %sf
  %106 = trunc i32 %97 to i8
  %107 = call i8 @llvm.ctpop.i8(i8 %106)
  %108 = and i8 %107, 1
  %109 = icmp eq i8 %108, 0
  store i1 %109, i1* %pf
  store volatile i64 63659, i64* @assembly_address
  %110 = load i1* %sf
  %111 = icmp eq i1 %110, false
  br i1 %111, label %block_f8b7, label %block_f8ad

block_f8ad:                                       ; preds = %block_f8a4
  store volatile i64 63661, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 63666, i64* @assembly_address
  br label %block_fe1c

block_f8b7:                                       ; preds = %block_f8a4
  store volatile i64 63671, i64* @assembly_address
  %112 = load i32* %stack_var_-348
  %113 = and i32 %112, 15
  %114 = icmp ugt i32 %113, 15
  %115 = icmp ult i32 %112, 0
  %116 = xor i32 %112, 0
  %117 = and i32 %116, 0
  %118 = icmp slt i32 %117, 0
  store i1 %114, i1* %az
  store i1 %115, i1* %cf
  store i1 %118, i1* %of
  %119 = icmp eq i32 %112, 0
  store i1 %119, i1* %zf
  %120 = icmp slt i32 %112, 0
  store i1 %120, i1* %sf
  %121 = trunc i32 %112 to i8
  %122 = call i8 @llvm.ctpop.i8(i8 %121)
  %123 = and i8 %122, 1
  %124 = icmp eq i8 %123, 0
  store i1 %124, i1* %pf
  store volatile i64 63678, i64* @assembly_address
  %125 = load i1* %sf
  %126 = icmp eq i1 %125, false
  br i1 %126, label %block_f8df, label %block_f8c0

block_f8c0:                                       ; preds = %block_f8b7
  store volatile i64 63680, i64* @assembly_address
  %127 = load i8** %stack_var_-360
  %128 = ptrtoint i8* %127 to i64
  %129 = and i64 %128, 15
  %130 = icmp ugt i64 %129, 15
  %131 = icmp ult i64 %128, 0
  %132 = xor i64 %128, 0
  %133 = and i64 %132, 0
  %134 = icmp slt i64 %133, 0
  store i1 %130, i1* %az
  store i1 %131, i1* %cf
  store i1 %134, i1* %of
  %135 = icmp eq i64 %128, 0
  store i1 %135, i1* %zf
  %136 = icmp slt i64 %128, 0
  store i1 %136, i1* %sf
  %137 = trunc i64 %128 to i8
  %138 = call i8 @llvm.ctpop.i8(i8 %137)
  %139 = and i8 %138, 1
  %140 = icmp eq i8 %139, 0
  store i1 %140, i1* %pf
  store volatile i64 63688, i64* @assembly_address
  %141 = load i1* %zf
  %142 = icmp eq i1 %141, false
  br i1 %142, label %block_f8df, label %block_f8ca

block_f8ca:                                       ; preds = %block_f8c0
  store volatile i64 63690, i64* @assembly_address
  %143 = call i32* @__errno_location()
  %144 = ptrtoint i32* %143 to i64
  store i64 %144, i64* %rax
  %145 = ptrtoint i32* %143 to i64
  store i64 %145, i64* %rax
  %146 = ptrtoint i32* %143 to i64
  store i64 %146, i64* %rax
  store volatile i64 63695, i64* @assembly_address
  %147 = load i64* %rax
  %148 = inttoptr i64 %147 to i32*
  store i32 9, i32* %148
  store volatile i64 63701, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 63706, i64* @assembly_address
  br label %block_fe1c

block_f8df:                                       ; preds = %block_f8c0, %block_f8b7
  store volatile i64 63711, i64* @assembly_address
  %149 = load i32* bitcast (i64* @global_var_21a410 to i32*)
  %150 = zext i32 %149 to i64
  store i64 %150, i64* %rax
  store volatile i64 63717, i64* @assembly_address
  %151 = load i64* %rax
  %152 = trunc i64 %151 to i32
  %153 = load i64* %rax
  %154 = trunc i64 %153 to i32
  %155 = and i32 %152, %154
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %156 = icmp eq i32 %155, 0
  store i1 %156, i1* %zf
  %157 = icmp slt i32 %155, 0
  store i1 %157, i1* %sf
  %158 = trunc i32 %155 to i8
  %159 = call i8 @llvm.ctpop.i8(i8 %158)
  %160 = and i8 %159, 1
  %161 = icmp eq i8 %160, 0
  store i1 %161, i1* %pf
  store volatile i64 63719, i64* @assembly_address
  %162 = load i1* %sf
  br i1 %162, label %block_fa8f, label %block_f8ed

block_f8ed:                                       ; preds = %block_f8df
  store volatile i64 63725, i64* @assembly_address
  %163 = load i32* %stack_var_-328
  %164 = sub i32 %163, 2
  %165 = and i32 %163, 15
  %166 = sub i32 %165, 2
  %167 = icmp ugt i32 %166, 15
  %168 = icmp ult i32 %163, 2
  %169 = xor i32 %163, 2
  %170 = xor i32 %163, %164
  %171 = and i32 %169, %170
  %172 = icmp slt i32 %171, 0
  store i1 %167, i1* %az
  store i1 %168, i1* %cf
  store i1 %172, i1* %of
  %173 = icmp eq i32 %164, 0
  store i1 %173, i1* %zf
  %174 = icmp slt i32 %164, 0
  store i1 %174, i1* %sf
  %175 = trunc i32 %164 to i8
  %176 = call i8 @llvm.ctpop.i8(i8 %175)
  %177 = and i8 %176, 1
  %178 = icmp eq i8 %177, 0
  store i1 %178, i1* %pf
  store volatile i64 63732, i64* @assembly_address
  %179 = load i1* %zf
  %180 = icmp eq i1 %179, false
  br i1 %180, label %block_f9be, label %block_f8fa

block_f8fa:                                       ; preds = %block_f8ed
  store volatile i64 63738, i64* @assembly_address
  %181 = load i32* %stack_var_-348
  %182 = and i32 %181, 15
  %183 = icmp ugt i32 %182, 15
  %184 = icmp ult i32 %181, 0
  %185 = xor i32 %181, 0
  %186 = and i32 %185, 0
  %187 = icmp slt i32 %186, 0
  store i1 %183, i1* %az
  store i1 %184, i1* %cf
  store i1 %187, i1* %of
  %188 = icmp eq i32 %181, 0
  store i1 %188, i1* %zf
  %189 = icmp slt i32 %181, 0
  store i1 %189, i1* %sf
  %190 = trunc i32 %181 to i8
  %191 = call i8 @llvm.ctpop.i8(i8 %190)
  %192 = and i8 %191, 1
  %193 = icmp eq i8 %192, 0
  store i1 %193, i1* %pf
  store volatile i64 63745, i64* @assembly_address
  %194 = load i1* %sf
  %195 = icmp eq i1 %194, false
  br i1 %195, label %block_f923, label %block_f903

block_f903:                                       ; preds = %block_f8fa
  store volatile i64 63747, i64* @assembly_address
  %196 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %196, i64* %rdx
  store volatile i64 63754, i64* @assembly_address
  %197 = load i8** %stack_var_-360
  %198 = ptrtoint i8* %197 to i64
  store i64 %198, i64* %rax
  store volatile i64 63761, i64* @assembly_address
  %199 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %199, i64* %rsi
  store volatile i64 63764, i64* @assembly_address
  %200 = load i64* %rax
  store i64 %200, i64* %rdi
  store volatile i64 63767, i64* @assembly_address
  %201 = load i64* %rdi
  %202 = inttoptr i64 %201 to i8*
  %203 = load i64* %rsi
  %204 = inttoptr i64 %203 to %stat*
  %205 = call i32 @stat(i8* %202, %stat* %204)
  %206 = sext i32 %205 to i64
  store i64 %206, i64* %rax
  %207 = sext i32 %205 to i64
  store i64 %207, i64* %rax
  store volatile i64 63772, i64* @assembly_address
  %208 = load i64* %rax
  %209 = trunc i64 %208 to i32
  %210 = load i64* %rax
  %211 = trunc i64 %210 to i32
  %212 = and i32 %209, %211
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %213 = icmp eq i32 %212, 0
  store i1 %213, i1* %zf
  %214 = icmp slt i32 %212, 0
  store i1 %214, i1* %sf
  %215 = trunc i32 %212 to i8
  %216 = call i8 @llvm.ctpop.i8(i8 %215)
  %217 = and i8 %216, 1
  %218 = icmp eq i8 %217, 0
  store i1 %218, i1* %pf
  store volatile i64 63774, i64* @assembly_address
  %219 = load i1* %zf
  %220 = icmp eq i1 %219, false
  %221 = zext i1 %220 to i8
  %222 = zext i8 %221 to i64
  %223 = load i64* %rax
  %224 = and i64 %223, -256
  %225 = or i64 %224, %222
  store i64 %225, i64* %rax
  store volatile i64 63777, i64* @assembly_address
  br label %block_f93f

block_f923:                                       ; preds = %block_f8fa
  store volatile i64 63779, i64* @assembly_address
  %226 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %226, i64* %rdx
  store volatile i64 63786, i64* @assembly_address
  %227 = load i32* %stack_var_-348
  %228 = zext i32 %227 to i64
  store i64 %228, i64* %rax
  store volatile i64 63792, i64* @assembly_address
  %229 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %229, i64* %rsi
  store volatile i64 63795, i64* @assembly_address
  %230 = load i64* %rax
  %231 = trunc i64 %230 to i32
  %232 = zext i32 %231 to i64
  store i64 %232, i64* %rdi
  store volatile i64 63797, i64* @assembly_address
  %233 = load i64* %rdi
  %234 = trunc i64 %233 to i32
  %235 = load i64* %rsi
  %236 = inttoptr i64 %235 to %stat*
  %237 = call i32 @fstat(i32 %234, %stat* %236)
  %238 = sext i32 %237 to i64
  store i64 %238, i64* %rax
  %239 = sext i32 %237 to i64
  store i64 %239, i64* %rax
  store volatile i64 63802, i64* @assembly_address
  %240 = load i64* %rax
  %241 = trunc i64 %240 to i32
  %242 = load i64* %rax
  %243 = trunc i64 %242 to i32
  %244 = and i32 %241, %243
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %245 = icmp eq i32 %244, 0
  store i1 %245, i1* %zf
  %246 = icmp slt i32 %244, 0
  store i1 %246, i1* %sf
  %247 = trunc i32 %244 to i8
  %248 = call i8 @llvm.ctpop.i8(i8 %247)
  %249 = and i8 %248, 1
  %250 = icmp eq i8 %249, 0
  store i1 %250, i1* %pf
  store volatile i64 63804, i64* @assembly_address
  %251 = load i1* %zf
  %252 = icmp eq i1 %251, false
  %253 = zext i1 %252 to i8
  %254 = zext i8 %253 to i64
  %255 = load i64* %rax
  %256 = and i64 %255, -256
  %257 = or i64 %256, %254
  store i64 %257, i64* %rax
  br label %block_f93f

block_f93f:                                       ; preds = %block_f923, %block_f903
  store volatile i64 63807, i64* @assembly_address
  %258 = load i64* %rax
  %259 = trunc i64 %258 to i8
  %260 = load i64* %rax
  %261 = trunc i64 %260 to i8
  %262 = and i8 %259, %261
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %263 = icmp eq i8 %262, 0
  store i1 %263, i1* %zf
  %264 = icmp slt i8 %262, 0
  store i1 %264, i1* %sf
  %265 = call i8 @llvm.ctpop.i8(i8 %262)
  %266 = and i8 %265, 1
  %267 = icmp eq i8 %266, 0
  store i1 %267, i1* %pf
  store volatile i64 63809, i64* @assembly_address
  %268 = load i1* %zf
  br i1 %268, label %block_f94d, label %block_f943

block_f943:                                       ; preds = %block_f93f
  store volatile i64 63811, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 63816, i64* @assembly_address
  br label %block_fe1c

block_f94d:                                       ; preds = %block_f93f
  store volatile i64 63821, i64* @assembly_address
  %269 = load i64* %stack_var_-320
  store i64 %269, i64* %rax
  store volatile i64 63828, i64* @assembly_address
  %270 = load i64* %rax
  %271 = add i64 %270, 8
  %272 = inttoptr i64 %271 to i64*
  %273 = load i64* %272
  store i64 %273, i64* %rax
  store volatile i64 63832, i64* @assembly_address
  %274 = load i64* %rax
  %275 = sub i64 %274, 1073741822
  %276 = and i64 %274, 15
  %277 = sub i64 %276, 14
  %278 = icmp ugt i64 %277, 15
  %279 = icmp ult i64 %274, 1073741822
  %280 = xor i64 %274, 1073741822
  %281 = xor i64 %274, %275
  %282 = and i64 %280, %281
  %283 = icmp slt i64 %282, 0
  store i1 %278, i1* %az
  store i1 %279, i1* %cf
  store i1 %283, i1* %of
  %284 = icmp eq i64 %275, 0
  store i1 %284, i1* %zf
  %285 = icmp slt i64 %275, 0
  store i1 %285, i1* %sf
  %286 = trunc i64 %275 to i8
  %287 = call i8 @llvm.ctpop.i8(i8 %286)
  %288 = and i8 %287, 1
  %289 = icmp eq i8 %288, 0
  store i1 %289, i1* %pf
  store volatile i64 63838, i64* @assembly_address
  %290 = load i1* %zf
  %291 = icmp eq i1 %290, false
  br i1 %291, label %block_f97f, label %block_f960

block_f960:                                       ; preds = %block_f94d
  store volatile i64 63840, i64* @assembly_address
  %292 = load i64* %stack_var_-320
  store i64 %292, i64* %rbx
  store volatile i64 63847, i64* @assembly_address
  %293 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %293, i64* %rax
  store volatile i64 63854, i64* @assembly_address
  %294 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %294, i64* %rdi
  store volatile i64 63857, i64* @assembly_address
  %295 = load i64* %rdi
  %296 = call i64 @get_stat_atime(i64 %295)
  store i64 %296, i64* %rax
  store i64 %296, i64* %rax
  store volatile i64 63862, i64* @assembly_address
  %297 = load i64* %rax
  %298 = load i64* %rbx
  %299 = inttoptr i64 %298 to i64*
  store i64 %297, i64* %299
  store volatile i64 63865, i64* @assembly_address
  %300 = load i64* %rbx
  %301 = add i64 %300, 8
  %302 = inttoptr i64 %301 to i64*
  %303 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %303, i64* %302
  store volatile i64 63869, i64* @assembly_address
  br label %block_f9b7

block_f97f:                                       ; preds = %block_f94d
  store volatile i64 63871, i64* @assembly_address
  %304 = load i64* %stack_var_-320
  store i64 %304, i64* %rax
  store volatile i64 63878, i64* @assembly_address
  %305 = load i64* %rax
  %306 = add i64 %305, 16
  %307 = and i64 %305, 15
  %308 = icmp ugt i64 %307, 15
  %309 = icmp ult i64 %306, %305
  %310 = xor i64 %305, %306
  %311 = xor i64 16, %306
  %312 = and i64 %310, %311
  %313 = icmp slt i64 %312, 0
  store i1 %308, i1* %az
  store i1 %309, i1* %cf
  store i1 %313, i1* %of
  %314 = icmp eq i64 %306, 0
  store i1 %314, i1* %zf
  %315 = icmp slt i64 %306, 0
  store i1 %315, i1* %sf
  %316 = trunc i64 %306 to i8
  %317 = call i8 @llvm.ctpop.i8(i8 %316)
  %318 = and i8 %317, 1
  %319 = icmp eq i8 %318, 0
  store i1 %319, i1* %pf
  store i64 %306, i64* %rax
  store volatile i64 63882, i64* @assembly_address
  %320 = load i64* %rax
  %321 = add i64 %320, 8
  %322 = inttoptr i64 %321 to i64*
  %323 = load i64* %322
  store i64 %323, i64* %rax
  store volatile i64 63886, i64* @assembly_address
  %324 = load i64* %rax
  %325 = sub i64 %324, 1073741822
  %326 = and i64 %324, 15
  %327 = sub i64 %326, 14
  %328 = icmp ugt i64 %327, 15
  %329 = icmp ult i64 %324, 1073741822
  %330 = xor i64 %324, 1073741822
  %331 = xor i64 %324, %325
  %332 = and i64 %330, %331
  %333 = icmp slt i64 %332, 0
  store i1 %328, i1* %az
  store i1 %329, i1* %cf
  store i1 %333, i1* %of
  %334 = icmp eq i64 %325, 0
  store i1 %334, i1* %zf
  %335 = icmp slt i64 %325, 0
  store i1 %335, i1* %sf
  %336 = trunc i64 %325 to i8
  %337 = call i8 @llvm.ctpop.i8(i8 %336)
  %338 = and i8 %337, 1
  %339 = icmp eq i8 %338, 0
  store i1 %339, i1* %pf
  store volatile i64 63892, i64* @assembly_address
  %340 = load i1* %zf
  %341 = icmp eq i1 %340, false
  br i1 %341, label %block_f9b7, label %block_f996

block_f996:                                       ; preds = %block_f97f
  store volatile i64 63894, i64* @assembly_address
  %342 = load i64* %stack_var_-320
  store i64 %342, i64* %rax
  store volatile i64 63901, i64* @assembly_address
  %343 = load i64* %rax
  %344 = add i64 %343, 16
  store i64 %344, i64* %rbx
  store volatile i64 63905, i64* @assembly_address
  %345 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %345, i64* %rax
  store volatile i64 63912, i64* @assembly_address
  %346 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %346, i64* %rdi
  store volatile i64 63915, i64* @assembly_address
  %347 = load i64* %rdi
  %348 = inttoptr i64 %347 to i64*
  %349 = call i64 @get_stat_mtime(i64* %348)
  store i64 %349, i64* %rax
  store i64 %349, i64* %rax
  store volatile i64 63920, i64* @assembly_address
  %350 = load i64* %rax
  %351 = load i64* %rbx
  %352 = inttoptr i64 %351 to i64*
  store i64 %350, i64* %352
  store volatile i64 63923, i64* @assembly_address
  %353 = load i64* %rbx
  %354 = add i64 %353, 8
  %355 = inttoptr i64 %354 to i64*
  %356 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %356, i64* %355
  br label %block_f9b7

block_f9b7:                                       ; preds = %block_f996, %block_f97f, %block_f960
  store volatile i64 63927, i64* @assembly_address
  %357 = load i32* %stack_var_-328
  %358 = add i32 %357, 1
  %359 = and i32 %357, 15
  %360 = add i32 %359, 1
  %361 = icmp ugt i32 %360, 15
  %362 = icmp ult i32 %358, %357
  %363 = xor i32 %357, %358
  %364 = xor i32 1, %358
  %365 = and i32 %363, %364
  %366 = icmp slt i32 %365, 0
  store i1 %361, i1* %az
  store i1 %362, i1* %cf
  store i1 %366, i1* %of
  %367 = icmp eq i32 %358, 0
  store i1 %367, i1* %zf
  %368 = icmp slt i32 %358, 0
  store i1 %368, i1* %sf
  %369 = trunc i32 %358 to i8
  %370 = call i8 @llvm.ctpop.i8(i8 %369)
  %371 = and i8 %370, 1
  %372 = icmp eq i8 %371, 0
  store i1 %372, i1* %pf
  store i32 %358, i32* %stack_var_-328
  br label %block_f9be

block_f9be:                                       ; preds = %block_f9b7, %block_f8ed
  store volatile i64 63934, i64* @assembly_address
  %373 = load i32* %stack_var_-348
  %374 = and i32 %373, 15
  %375 = icmp ugt i32 %374, 15
  %376 = icmp ult i32 %373, 0
  %377 = xor i32 %373, 0
  %378 = and i32 %377, 0
  %379 = icmp slt i32 %378, 0
  store i1 %375, i1* %az
  store i1 %376, i1* %cf
  store i1 %379, i1* %of
  %380 = icmp eq i32 %373, 0
  store i1 %380, i1* %zf
  %381 = icmp slt i32 %373, 0
  store i1 %381, i1* %sf
  %382 = trunc i32 %373 to i8
  %383 = call i8 @llvm.ctpop.i8(i8 %382)
  %384 = and i8 %383, 1
  %385 = icmp eq i8 %384, 0
  store i1 %385, i1* %pf
  store volatile i64 63941, i64* @assembly_address
  %386 = load i1* %sf
  %387 = icmp eq i1 %386, false
  br i1 %387, label %block_fa2b, label %block_f9c7

block_f9c7:                                       ; preds = %block_f9be
  store volatile i64 63943, i64* @assembly_address
  %388 = load i64* %stack_var_-320
  store i64 %388, i64* %rdx
  store volatile i64 63950, i64* @assembly_address
  %389 = load i8** %stack_var_-360
  %390 = ptrtoint i8* %389 to i64
  store i64 %390, i64* %rax
  store volatile i64 63957, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 63962, i64* @assembly_address
  %391 = load i64* %rax
  store i64 %391, i64* %rsi
  store volatile i64 63965, i64* @assembly_address
  store i64 4294967196, i64* %rdi
  store volatile i64 63970, i64* @assembly_address
  %392 = load i64* %rdi
  %393 = trunc i64 %392 to i32
  %394 = load i64* %rsi
  %395 = inttoptr i64 %394 to i8*
  %396 = load i64* %rdx
  %397 = insertvalue %timespec undef, i64 %396, 0
  %398 = insertvalue [2 x %timespec] undef, %timespec %397, 0
  %399 = load i64* %rcx
  %400 = trunc i64 %399 to i32
  %401 = call i32 @utimensat(i32 %393, i8* %395, [2 x %timespec] %398, i32 %400)
  %402 = sext i32 %401 to i64
  store i64 %402, i64* %rax
  %403 = sext i32 %401 to i64
  store i64 %403, i64* %rax
  store volatile i64 63975, i64* @assembly_address
  %404 = load i64* %rax
  %405 = trunc i64 %404 to i32
  store i32 %405, i32* %stack_var_-324
  store volatile i64 63981, i64* @assembly_address
  %406 = load i32* %stack_var_-324
  %407 = and i32 %406, 15
  %408 = icmp ugt i32 %407, 15
  %409 = icmp ult i32 %406, 0
  %410 = xor i32 %406, 0
  %411 = and i32 %410, 0
  %412 = icmp slt i32 %411, 0
  store i1 %408, i1* %az
  store i1 %409, i1* %cf
  store i1 %412, i1* %of
  store i32 %406, i32* %7
  store i32 0, i32* %6
  %413 = icmp eq i32 %406, 0
  store i1 %413, i1* %zf
  %414 = icmp slt i32 %406, 0
  store i1 %414, i1* %sf
  %415 = trunc i32 %406 to i8
  %416 = call i8 @llvm.ctpop.i8(i8 %415)
  %417 = and i8 %416, 1
  %418 = icmp eq i8 %417, 0
  store i1 %418, i1* %pf
  store volatile i64 63988, i64* @assembly_address
  %419 = load i32* %7
  %420 = load i32* %6
  %421 = icmp sle i32 %419, %420
  br i1 %421, label %block_fa01, label %block_f9f6

block_f9f6:                                       ; preds = %block_f9c7
  store volatile i64 63990, i64* @assembly_address
  %422 = call i32* @__errno_location()
  %423 = ptrtoint i32* %422 to i64
  store i64 %423, i64* %rax
  %424 = ptrtoint i32* %422 to i64
  store i64 %424, i64* %rax
  %425 = ptrtoint i32* %422 to i64
  store i64 %425, i64* %rax
  store volatile i64 63995, i64* @assembly_address
  %426 = load i64* %rax
  %427 = inttoptr i64 %426 to i32*
  store i32 38, i32* %427
  br label %block_fa01

block_fa01:                                       ; preds = %block_f9f6, %block_f9c7
  store volatile i64 64001, i64* @assembly_address
  %428 = load i32* %stack_var_-324
  %429 = and i32 %428, 15
  %430 = icmp ugt i32 %429, 15
  %431 = icmp ult i32 %428, 0
  %432 = xor i32 %428, 0
  %433 = and i32 %432, 0
  %434 = icmp slt i32 %433, 0
  store i1 %430, i1* %az
  store i1 %431, i1* %cf
  store i1 %434, i1* %of
  %435 = icmp eq i32 %428, 0
  store i1 %435, i1* %zf
  %436 = icmp slt i32 %428, 0
  store i1 %436, i1* %sf
  %437 = trunc i32 %428 to i8
  %438 = call i8 @llvm.ctpop.i8(i8 %437)
  %439 = and i8 %438, 1
  %440 = icmp eq i8 %439, 0
  store i1 %440, i1* %pf
  store volatile i64 64008, i64* @assembly_address
  %441 = load i1* %zf
  br i1 %441, label %block_fa16, label %block_fa0a

block_fa0a:                                       ; preds = %block_fa01
  store volatile i64 64010, i64* @assembly_address
  %442 = call i32* @__errno_location()
  %443 = ptrtoint i32* %442 to i64
  store i64 %443, i64* %rax
  %444 = ptrtoint i32* %442 to i64
  store i64 %444, i64* %rax
  %445 = ptrtoint i32* %442 to i64
  store i64 %445, i64* %rax
  store volatile i64 64015, i64* @assembly_address
  %446 = load i64* %rax
  %447 = inttoptr i64 %446 to i32*
  %448 = load i32* %447
  %449 = zext i32 %448 to i64
  store i64 %449, i64* %rax
  store volatile i64 64017, i64* @assembly_address
  %450 = load i64* %rax
  %451 = trunc i64 %450 to i32
  %452 = sub i32 %451, 38
  %453 = and i32 %451, 15
  %454 = sub i32 %453, 6
  %455 = icmp ugt i32 %454, 15
  %456 = icmp ult i32 %451, 38
  %457 = xor i32 %451, 38
  %458 = xor i32 %451, %452
  %459 = and i32 %457, %458
  %460 = icmp slt i32 %459, 0
  store i1 %455, i1* %az
  store i1 %456, i1* %cf
  store i1 %460, i1* %of
  %461 = icmp eq i32 %452, 0
  store i1 %461, i1* %zf
  %462 = icmp slt i32 %452, 0
  store i1 %462, i1* %sf
  %463 = trunc i32 %452 to i8
  %464 = call i8 @llvm.ctpop.i8(i8 %463)
  %465 = and i8 %464, 1
  %466 = icmp eq i8 %465, 0
  store i1 %466, i1* %pf
  store volatile i64 64020, i64* @assembly_address
  %467 = load i1* %zf
  br i1 %467, label %block_fa2b, label %block_fa16

block_fa16:                                       ; preds = %block_fa0a, %block_fa01
  store volatile i64 64022, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21a410 to i32*)
  store volatile i64 64032, i64* @assembly_address
  %468 = load i32* %stack_var_-324
  %469 = zext i32 %468 to i64
  store i64 %469, i64* %rax
  store volatile i64 64038, i64* @assembly_address
  br label %block_fe1c

block_fa2b:                                       ; preds = %block_fa0a, %block_f9be
  store volatile i64 64043, i64* @assembly_address
  %470 = load i32* %stack_var_-348
  %471 = and i32 %470, 15
  %472 = icmp ugt i32 %471, 15
  %473 = icmp ult i32 %470, 0
  %474 = xor i32 %470, 0
  %475 = and i32 %474, 0
  %476 = icmp slt i32 %475, 0
  store i1 %472, i1* %az
  store i1 %473, i1* %cf
  store i1 %476, i1* %of
  %477 = icmp eq i32 %470, 0
  store i1 %477, i1* %zf
  %478 = icmp slt i32 %470, 0
  store i1 %478, i1* %sf
  %479 = trunc i32 %470 to i8
  %480 = call i8 @llvm.ctpop.i8(i8 %479)
  %481 = and i8 %480, 1
  %482 = icmp eq i8 %481, 0
  store i1 %482, i1* %pf
  store volatile i64 64050, i64* @assembly_address
  %483 = load i1* %sf
  br i1 %483, label %block_fa8f, label %block_fa34

block_fa34:                                       ; preds = %block_fa2b
  store volatile i64 64052, i64* @assembly_address
  %484 = load i64* %stack_var_-320
  store i64 %484, i64* %rdx
  store volatile i64 64059, i64* @assembly_address
  %485 = load i32* %stack_var_-348
  %486 = zext i32 %485 to i64
  store i64 %486, i64* %rax
  store volatile i64 64065, i64* @assembly_address
  %487 = load i64* %rdx
  store i64 %487, i64* %rsi
  store volatile i64 64068, i64* @assembly_address
  %488 = load i64* %rax
  %489 = trunc i64 %488 to i32
  %490 = zext i32 %489 to i64
  store i64 %490, i64* %rdi
  store volatile i64 64070, i64* @assembly_address
  %491 = load i64* %rdi
  %492 = trunc i64 %491 to i32
  %493 = load i64* %rsi
  %494 = insertvalue %timespec undef, i64 %493, 0
  %495 = insertvalue [2 x %timespec] undef, %timespec %494, 0
  %496 = call i32 @futimens(i32 %492, [2 x %timespec] %495)
  %497 = sext i32 %496 to i64
  store i64 %497, i64* %rax
  %498 = sext i32 %496 to i64
  store i64 %498, i64* %rax
  store volatile i64 64075, i64* @assembly_address
  %499 = load i64* %rax
  %500 = trunc i64 %499 to i32
  store i32 %500, i32* %stack_var_-324
  store volatile i64 64081, i64* @assembly_address
  %501 = load i32* %stack_var_-324
  %502 = and i32 %501, 15
  %503 = icmp ugt i32 %502, 15
  %504 = icmp ult i32 %501, 0
  %505 = xor i32 %501, 0
  %506 = and i32 %505, 0
  %507 = icmp slt i32 %506, 0
  store i1 %503, i1* %az
  store i1 %504, i1* %cf
  store i1 %507, i1* %of
  store i32 %501, i32* %5
  store i32 0, i32* %4
  %508 = icmp eq i32 %501, 0
  store i1 %508, i1* %zf
  %509 = icmp slt i32 %501, 0
  store i1 %509, i1* %sf
  %510 = trunc i32 %501 to i8
  %511 = call i8 @llvm.ctpop.i8(i8 %510)
  %512 = and i8 %511, 1
  %513 = icmp eq i8 %512, 0
  store i1 %513, i1* %pf
  store volatile i64 64088, i64* @assembly_address
  %514 = load i32* %5
  %515 = load i32* %4
  %516 = icmp sle i32 %514, %515
  br i1 %516, label %block_fa65, label %block_fa5a

block_fa5a:                                       ; preds = %block_fa34
  store volatile i64 64090, i64* @assembly_address
  %517 = call i32* @__errno_location()
  %518 = ptrtoint i32* %517 to i64
  store i64 %518, i64* %rax
  %519 = ptrtoint i32* %517 to i64
  store i64 %519, i64* %rax
  %520 = ptrtoint i32* %517 to i64
  store i64 %520, i64* %rax
  store volatile i64 64095, i64* @assembly_address
  %521 = load i64* %rax
  %522 = inttoptr i64 %521 to i32*
  store i32 38, i32* %522
  br label %block_fa65

block_fa65:                                       ; preds = %block_fa5a, %block_fa34
  store volatile i64 64101, i64* @assembly_address
  %523 = load i32* %stack_var_-324
  %524 = and i32 %523, 15
  %525 = icmp ugt i32 %524, 15
  %526 = icmp ult i32 %523, 0
  %527 = xor i32 %523, 0
  %528 = and i32 %527, 0
  %529 = icmp slt i32 %528, 0
  store i1 %525, i1* %az
  store i1 %526, i1* %cf
  store i1 %529, i1* %of
  %530 = icmp eq i32 %523, 0
  store i1 %530, i1* %zf
  %531 = icmp slt i32 %523, 0
  store i1 %531, i1* %sf
  %532 = trunc i32 %523 to i8
  %533 = call i8 @llvm.ctpop.i8(i8 %532)
  %534 = and i8 %533, 1
  %535 = icmp eq i8 %534, 0
  store i1 %535, i1* %pf
  store volatile i64 64108, i64* @assembly_address
  %536 = load i1* %zf
  br i1 %536, label %block_fa7a, label %block_fa6e

block_fa6e:                                       ; preds = %block_fa65
  store volatile i64 64110, i64* @assembly_address
  %537 = call i32* @__errno_location()
  %538 = ptrtoint i32* %537 to i64
  store i64 %538, i64* %rax
  %539 = ptrtoint i32* %537 to i64
  store i64 %539, i64* %rax
  %540 = ptrtoint i32* %537 to i64
  store i64 %540, i64* %rax
  store volatile i64 64115, i64* @assembly_address
  %541 = load i64* %rax
  %542 = inttoptr i64 %541 to i32*
  %543 = load i32* %542
  %544 = zext i32 %543 to i64
  store i64 %544, i64* %rax
  store volatile i64 64117, i64* @assembly_address
  %545 = load i64* %rax
  %546 = trunc i64 %545 to i32
  %547 = sub i32 %546, 38
  %548 = and i32 %546, 15
  %549 = sub i32 %548, 6
  %550 = icmp ugt i32 %549, 15
  %551 = icmp ult i32 %546, 38
  %552 = xor i32 %546, 38
  %553 = xor i32 %546, %547
  %554 = and i32 %552, %553
  %555 = icmp slt i32 %554, 0
  store i1 %550, i1* %az
  store i1 %551, i1* %cf
  store i1 %555, i1* %of
  %556 = icmp eq i32 %547, 0
  store i1 %556, i1* %zf
  %557 = icmp slt i32 %547, 0
  store i1 %557, i1* %sf
  %558 = trunc i32 %547 to i8
  %559 = call i8 @llvm.ctpop.i8(i8 %558)
  %560 = and i8 %559, 1
  %561 = icmp eq i8 %560, 0
  store i1 %561, i1* %pf
  store volatile i64 64120, i64* @assembly_address
  %562 = load i1* %zf
  br i1 %562, label %block_fa8f, label %block_fa7a

block_fa7a:                                       ; preds = %block_fa6e, %block_fa65
  store volatile i64 64122, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21a410 to i32*)
  store volatile i64 64132, i64* @assembly_address
  %563 = load i32* %stack_var_-324
  %564 = zext i32 %563 to i64
  store i64 %564, i64* %rax
  store volatile i64 64138, i64* @assembly_address
  br label %block_fe1c

block_fa8f:                                       ; preds = %block_fa6e, %block_fa2b, %block_f8df
  store volatile i64 64143, i64* @assembly_address
  store i32 -1, i32* bitcast (i64* @global_var_21a410 to i32*)
  store volatile i64 64153, i64* @assembly_address
  store i32 -1, i32* bitcast (i64* @global_var_21a414 to i32*)
  store volatile i64 64163, i64* @assembly_address
  %565 = load i32* %stack_var_-328
  %566 = and i32 %565, 15
  %567 = icmp ugt i32 %566, 15
  %568 = icmp ult i32 %565, 0
  %569 = xor i32 %565, 0
  %570 = and i32 %569, 0
  %571 = icmp slt i32 %570, 0
  store i1 %567, i1* %az
  store i1 %568, i1* %cf
  store i1 %571, i1* %of
  %572 = icmp eq i32 %565, 0
  store i1 %572, i1* %zf
  %573 = icmp slt i32 %565, 0
  store i1 %573, i1* %sf
  %574 = trunc i32 %565 to i8
  %575 = call i8 @llvm.ctpop.i8(i8 %574)
  %576 = and i8 %575, 1
  %577 = icmp eq i8 %576, 0
  store i1 %577, i1* %pf
  store volatile i64 64170, i64* @assembly_address
  %578 = load i1* %zf
  br i1 %578, label %block_fb3f, label %block_fab0

block_fab0:                                       ; preds = %block_fa8f
  store volatile i64 64176, i64* @assembly_address
  %579 = load i32* %stack_var_-328
  %580 = sub i32 %579, 3
  %581 = and i32 %579, 15
  %582 = sub i32 %581, 3
  %583 = icmp ugt i32 %582, 15
  %584 = icmp ult i32 %579, 3
  %585 = xor i32 %579, 3
  %586 = xor i32 %579, %580
  %587 = and i32 %585, %586
  %588 = icmp slt i32 %587, 0
  store i1 %583, i1* %az
  store i1 %584, i1* %cf
  store i1 %588, i1* %of
  %589 = icmp eq i32 %580, 0
  store i1 %589, i1* %zf
  %590 = icmp slt i32 %580, 0
  store i1 %590, i1* %sf
  %591 = trunc i32 %580 to i8
  %592 = call i8 @llvm.ctpop.i8(i8 %591)
  %593 = and i8 %592, 1
  %594 = icmp eq i8 %593, 0
  store i1 %594, i1* %pf
  store volatile i64 64183, i64* @assembly_address
  %595 = load i1* %zf
  br i1 %595, label %block_fb0c, label %block_fab9

block_fab9:                                       ; preds = %block_fab0
  store volatile i64 64185, i64* @assembly_address
  %596 = load i32* %stack_var_-348
  %597 = and i32 %596, 15
  %598 = icmp ugt i32 %597, 15
  %599 = icmp ult i32 %596, 0
  %600 = xor i32 %596, 0
  %601 = and i32 %600, 0
  %602 = icmp slt i32 %601, 0
  store i1 %598, i1* %az
  store i1 %599, i1* %cf
  store i1 %602, i1* %of
  %603 = icmp eq i32 %596, 0
  store i1 %603, i1* %zf
  %604 = icmp slt i32 %596, 0
  store i1 %604, i1* %sf
  %605 = trunc i32 %596 to i8
  %606 = call i8 @llvm.ctpop.i8(i8 %605)
  %607 = and i8 %606, 1
  %608 = icmp eq i8 %607, 0
  store i1 %608, i1* %pf
  store volatile i64 64192, i64* @assembly_address
  %609 = load i1* %sf
  %610 = icmp eq i1 %609, false
  br i1 %610, label %block_fae2, label %block_fac2

block_fac2:                                       ; preds = %block_fab9
  store volatile i64 64194, i64* @assembly_address
  %611 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %611, i64* %rdx
  store volatile i64 64201, i64* @assembly_address
  %612 = load i8** %stack_var_-360
  %613 = ptrtoint i8* %612 to i64
  store i64 %613, i64* %rax
  store volatile i64 64208, i64* @assembly_address
  %614 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %614, i64* %rsi
  store volatile i64 64211, i64* @assembly_address
  %615 = load i64* %rax
  store i64 %615, i64* %rdi
  store volatile i64 64214, i64* @assembly_address
  %616 = load i64* %rdi
  %617 = inttoptr i64 %616 to i8*
  %618 = load i64* %rsi
  %619 = inttoptr i64 %618 to %stat*
  %620 = call i32 @stat(i8* %617, %stat* %619)
  %621 = sext i32 %620 to i64
  store i64 %621, i64* %rax
  %622 = sext i32 %620 to i64
  store i64 %622, i64* %rax
  store volatile i64 64219, i64* @assembly_address
  %623 = load i64* %rax
  %624 = trunc i64 %623 to i32
  %625 = load i64* %rax
  %626 = trunc i64 %625 to i32
  %627 = and i32 %624, %626
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %628 = icmp eq i32 %627, 0
  store i1 %628, i1* %zf
  %629 = icmp slt i32 %627, 0
  store i1 %629, i1* %sf
  %630 = trunc i32 %627 to i8
  %631 = call i8 @llvm.ctpop.i8(i8 %630)
  %632 = and i8 %631, 1
  %633 = icmp eq i8 %632, 0
  store i1 %633, i1* %pf
  store volatile i64 64221, i64* @assembly_address
  %634 = load i1* %zf
  %635 = icmp eq i1 %634, false
  %636 = zext i1 %635 to i8
  %637 = zext i8 %636 to i64
  %638 = load i64* %rax
  %639 = and i64 %638, -256
  %640 = or i64 %639, %637
  store i64 %640, i64* %rax
  store volatile i64 64224, i64* @assembly_address
  br label %block_fafe

block_fae2:                                       ; preds = %block_fab9
  store volatile i64 64226, i64* @assembly_address
  %641 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %641, i64* %rdx
  store volatile i64 64233, i64* @assembly_address
  %642 = load i32* %stack_var_-348
  %643 = zext i32 %642 to i64
  store i64 %643, i64* %rax
  store volatile i64 64239, i64* @assembly_address
  %644 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %644, i64* %rsi
  store volatile i64 64242, i64* @assembly_address
  %645 = load i64* %rax
  %646 = trunc i64 %645 to i32
  %647 = zext i32 %646 to i64
  store i64 %647, i64* %rdi
  store volatile i64 64244, i64* @assembly_address
  %648 = load i64* %rdi
  %649 = trunc i64 %648 to i32
  %650 = load i64* %rsi
  %651 = inttoptr i64 %650 to %stat*
  %652 = call i32 @fstat(i32 %649, %stat* %651)
  %653 = sext i32 %652 to i64
  store i64 %653, i64* %rax
  %654 = sext i32 %652 to i64
  store i64 %654, i64* %rax
  store volatile i64 64249, i64* @assembly_address
  %655 = load i64* %rax
  %656 = trunc i64 %655 to i32
  %657 = load i64* %rax
  %658 = trunc i64 %657 to i32
  %659 = and i32 %656, %658
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %660 = icmp eq i32 %659, 0
  store i1 %660, i1* %zf
  %661 = icmp slt i32 %659, 0
  store i1 %661, i1* %sf
  %662 = trunc i32 %659 to i8
  %663 = call i8 @llvm.ctpop.i8(i8 %662)
  %664 = and i8 %663, 1
  %665 = icmp eq i8 %664, 0
  store i1 %665, i1* %pf
  store volatile i64 64251, i64* @assembly_address
  %666 = load i1* %zf
  %667 = icmp eq i1 %666, false
  %668 = zext i1 %667 to i8
  %669 = zext i8 %668 to i64
  %670 = load i64* %rax
  %671 = and i64 %670, -256
  %672 = or i64 %671, %669
  store i64 %672, i64* %rax
  br label %block_fafe

block_fafe:                                       ; preds = %block_fae2, %block_fac2
  store volatile i64 64254, i64* @assembly_address
  %673 = load i64* %rax
  %674 = trunc i64 %673 to i8
  %675 = load i64* %rax
  %676 = trunc i64 %675 to i8
  %677 = and i8 %674, %676
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %678 = icmp eq i8 %677, 0
  store i1 %678, i1* %zf
  %679 = icmp slt i8 %677, 0
  store i1 %679, i1* %sf
  %680 = call i8 @llvm.ctpop.i8(i8 %677)
  %681 = and i8 %680, 1
  %682 = icmp eq i8 %681, 0
  store i1 %682, i1* %pf
  store volatile i64 64256, i64* @assembly_address
  %683 = load i1* %zf
  br i1 %683, label %block_fb0c, label %block_fb02

block_fb02:                                       ; preds = %block_fafe
  store volatile i64 64258, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 64263, i64* @assembly_address
  br label %block_fe1c

block_fb0c:                                       ; preds = %block_fafe, %block_fab0
  store volatile i64 64268, i64* @assembly_address
  %684 = load i64* %stack_var_-320
  store i64 %684, i64* %rax
  store volatile i64 64275, i64* @assembly_address
  %685 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %686 = icmp eq i64 %685, 0
  store i1 %686, i1* %zf
  %687 = icmp slt i64 %685, 0
  store i1 %687, i1* %sf
  %688 = trunc i64 %685 to i8
  %689 = call i8 @llvm.ctpop.i8(i8 %688)
  %690 = and i8 %689, 1
  %691 = icmp eq i8 %690, 0
  store i1 %691, i1* %pf
  store volatile i64 64278, i64* @assembly_address
  %692 = load i1* %zf
  br i1 %692, label %block_fb3f, label %block_fb18

block_fb18:                                       ; preds = %block_fb0c
  store volatile i64 64280, i64* @assembly_address
  %693 = ptrtoint i64* %stack_var_-320 to i64
  store i64 %693, i64* %rdx
  store volatile i64 64287, i64* @assembly_address
  %694 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %694, i64* %rax
  store volatile i64 64294, i64* @assembly_address
  %695 = ptrtoint i64* %stack_var_-320 to i64
  store i64 %695, i64* %rsi
  store volatile i64 64297, i64* @assembly_address
  %696 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %696, i64* %rdi
  store volatile i64 64300, i64* @assembly_address
  %697 = load i64* %rdi
  %698 = inttoptr i64 %697 to i64*
  %699 = load i64* %rsi
  %700 = inttoptr i64 %699 to i64*
  %701 = load i64* %rdx
  %702 = inttoptr i64 %701 to i64*
  %703 = call i64 @update_timespec(i64* %698, i64* %700, i64* %702)
  store i64 %703, i64* %rax
  store i64 %703, i64* %rax
  store volatile i64 64305, i64* @assembly_address
  %704 = load i64* %rax
  %705 = trunc i64 %704 to i8
  %706 = load i64* %rax
  %707 = trunc i64 %706 to i8
  %708 = and i8 %705, %707
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %709 = icmp eq i8 %708, 0
  store i1 %709, i1* %zf
  %710 = icmp slt i8 %708, 0
  store i1 %710, i1* %sf
  %711 = call i8 @llvm.ctpop.i8(i8 %708)
  %712 = and i8 %711, 1
  %713 = icmp eq i8 %712, 0
  store i1 %713, i1* %pf
  store volatile i64 64307, i64* @assembly_address
  %714 = load i1* %zf
  br i1 %714, label %block_fb3f, label %block_fb35

block_fb35:                                       ; preds = %block_fb18
  store volatile i64 64309, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 64314, i64* @assembly_address
  br label %block_fe1c

block_fb3f:                                       ; preds = %block_fb18, %block_fb0c, %block_fa8f
  store volatile i64 64319, i64* @assembly_address
  %715 = load i64* %stack_var_-320
  store i64 %715, i64* %rax
  store volatile i64 64326, i64* @assembly_address
  %716 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %717 = icmp eq i64 %716, 0
  store i1 %717, i1* %zf
  %718 = icmp slt i64 %716, 0
  store i1 %718, i1* %sf
  %719 = trunc i64 %716 to i8
  %720 = call i8 @llvm.ctpop.i8(i8 %719)
  %721 = and i8 %720, 1
  %722 = icmp eq i8 %721, 0
  store i1 %722, i1* %pf
  store volatile i64 64329, i64* @assembly_address
  %723 = load i1* %zf
  br i1 %723, label %block_fbef, label %block_fb4f

block_fb4f:                                       ; preds = %block_fb3f
  store volatile i64 64335, i64* @assembly_address
  %724 = load i64* %stack_var_-320
  store i64 %724, i64* %rax
  store volatile i64 64342, i64* @assembly_address
  %725 = load i64* %rax
  %726 = inttoptr i64 %725 to i64*
  %727 = load i64* %726
  store i64 %727, i64* %rax
  store volatile i64 64345, i64* @assembly_address
  %728 = load i64* %rax
  store i64 %728, i64* %stack_var_-248
  store volatile i64 64352, i64* @assembly_address
  %729 = load i64* %stack_var_-320
  store i64 %729, i64* %rax
  store volatile i64 64359, i64* @assembly_address
  %730 = load i64* %rax
  %731 = add i64 %730, 8
  %732 = inttoptr i64 %731 to i64*
  %733 = load i64* %732
  store i64 %733, i64* %rcx
  store volatile i64 64363, i64* @assembly_address
  store i64 2361183241434822607, i64* %rdx
  store volatile i64 64373, i64* @assembly_address
  %734 = load i64* %rcx
  store i64 %734, i64* %rax
  store volatile i64 64376, i64* @assembly_address
  %735 = load i64* %rdx
  %736 = load i64* %rax
  %737 = sext i64 %735 to i128
  %738 = sext i64 %736 to i128
  %739 = mul i128 %737, %738
  %740 = trunc i128 %739 to i64
  %741 = lshr i128 %739, 64
  %742 = trunc i128 %741 to i64
  %743 = icmp ne i64 %742, 0
  store i64 %740, i64* %rax
  store i64 %742, i64* %rdx
  %744 = icmp ne i64 %742, -1
  %745 = icmp eq i1 %743, %744
  store i1 %745, i1* %of
  store i1 %745, i1* %cf
  store volatile i64 64379, i64* @assembly_address
  %746 = load i64* %rdx
  %747 = load i1* %of
  %748 = ashr i64 %746, 7
  %749 = icmp eq i64 %748, 0
  store i1 %749, i1* %zf
  %750 = icmp slt i64 %748, 0
  store i1 %750, i1* %sf
  %751 = trunc i64 %748 to i8
  %752 = call i8 @llvm.ctpop.i8(i8 %751)
  %753 = and i8 %752, 1
  %754 = icmp eq i8 %753, 0
  store i1 %754, i1* %pf
  store i64 %748, i64* %rdx
  %755 = and i64 64, %746
  %756 = icmp ne i64 %755, 0
  store i1 %756, i1* %cf
  %757 = select i1 false, i1 false, i1 %747
  store i1 %757, i1* %of
  store volatile i64 64383, i64* @assembly_address
  %758 = load i64* %rcx
  store i64 %758, i64* %rax
  store volatile i64 64386, i64* @assembly_address
  %759 = load i64* %rax
  %760 = load i1* %of
  %761 = ashr i64 %759, 63
  %762 = icmp eq i64 %761, 0
  store i1 %762, i1* %zf
  %763 = icmp slt i64 %761, 0
  store i1 %763, i1* %sf
  %764 = trunc i64 %761 to i8
  %765 = call i8 @llvm.ctpop.i8(i8 %764)
  %766 = and i8 %765, 1
  %767 = icmp eq i8 %766, 0
  store i1 %767, i1* %pf
  store i64 %761, i64* %rax
  %768 = and i64 4611686018427387904, %759
  %769 = icmp ne i64 %768, 0
  store i1 %769, i1* %cf
  %770 = select i1 false, i1 false, i1 %760
  store i1 %770, i1* %of
  store volatile i64 64390, i64* @assembly_address
  %771 = load i64* %rdx
  %772 = load i64* %rax
  %773 = sub i64 %771, %772
  %774 = and i64 %771, 15
  %775 = and i64 %772, 15
  %776 = sub i64 %774, %775
  %777 = icmp ugt i64 %776, 15
  %778 = icmp ult i64 %771, %772
  %779 = xor i64 %771, %772
  %780 = xor i64 %771, %773
  %781 = and i64 %779, %780
  %782 = icmp slt i64 %781, 0
  store i1 %777, i1* %az
  store i1 %778, i1* %cf
  store i1 %782, i1* %of
  %783 = icmp eq i64 %773, 0
  store i1 %783, i1* %zf
  %784 = icmp slt i64 %773, 0
  store i1 %784, i1* %sf
  %785 = trunc i64 %773 to i8
  %786 = call i8 @llvm.ctpop.i8(i8 %785)
  %787 = and i8 %786, 1
  %788 = icmp eq i8 %787, 0
  store i1 %788, i1* %pf
  store i64 %773, i64* %rdx
  store volatile i64 64393, i64* @assembly_address
  %789 = load i64* %rdx
  store i64 %789, i64* %rax
  store volatile i64 64396, i64* @assembly_address
  %790 = load i64* %rax
  store i64 %790, i64* %stack_var_-240
  store volatile i64 64403, i64* @assembly_address
  %791 = load i64* %stack_var_-320
  store i64 %791, i64* %rax
  store volatile i64 64410, i64* @assembly_address
  %792 = load i64* %rax
  %793 = add i64 %792, 16
  %794 = and i64 %792, 15
  %795 = icmp ugt i64 %794, 15
  %796 = icmp ult i64 %793, %792
  %797 = xor i64 %792, %793
  %798 = xor i64 16, %793
  %799 = and i64 %797, %798
  %800 = icmp slt i64 %799, 0
  store i1 %795, i1* %az
  store i1 %796, i1* %cf
  store i1 %800, i1* %of
  %801 = icmp eq i64 %793, 0
  store i1 %801, i1* %zf
  %802 = icmp slt i64 %793, 0
  store i1 %802, i1* %sf
  %803 = trunc i64 %793 to i8
  %804 = call i8 @llvm.ctpop.i8(i8 %803)
  %805 = and i8 %804, 1
  %806 = icmp eq i8 %805, 0
  store i1 %806, i1* %pf
  store i64 %793, i64* %rax
  store volatile i64 64414, i64* @assembly_address
  %807 = load i64* %rax
  %808 = inttoptr i64 %807 to i64*
  %809 = load i64* %808
  store i64 %809, i64* %rax
  store volatile i64 64417, i64* @assembly_address
  %810 = load i64* %rax
  store i64 %810, i64* %stack_var_-232
  store volatile i64 64424, i64* @assembly_address
  %811 = load i64* %stack_var_-320
  store i64 %811, i64* %rax
  store volatile i64 64431, i64* @assembly_address
  %812 = load i64* %rax
  %813 = add i64 %812, 16
  %814 = and i64 %812, 15
  %815 = icmp ugt i64 %814, 15
  %816 = icmp ult i64 %813, %812
  %817 = xor i64 %812, %813
  %818 = xor i64 16, %813
  %819 = and i64 %817, %818
  %820 = icmp slt i64 %819, 0
  store i1 %815, i1* %az
  store i1 %816, i1* %cf
  store i1 %820, i1* %of
  %821 = icmp eq i64 %813, 0
  store i1 %821, i1* %zf
  %822 = icmp slt i64 %813, 0
  store i1 %822, i1* %sf
  %823 = trunc i64 %813 to i8
  %824 = call i8 @llvm.ctpop.i8(i8 %823)
  %825 = and i8 %824, 1
  %826 = icmp eq i8 %825, 0
  store i1 %826, i1* %pf
  store i64 %813, i64* %rax
  store volatile i64 64435, i64* @assembly_address
  %827 = load i64* %rax
  %828 = add i64 %827, 8
  %829 = inttoptr i64 %828 to i64*
  %830 = load i64* %829
  store i64 %830, i64* %rcx
  store volatile i64 64439, i64* @assembly_address
  store i64 2361183241434822607, i64* %rdx
  store volatile i64 64449, i64* @assembly_address
  %831 = load i64* %rcx
  store i64 %831, i64* %rax
  store volatile i64 64452, i64* @assembly_address
  %832 = load i64* %rdx
  %833 = load i64* %rax
  %834 = sext i64 %832 to i128
  %835 = sext i64 %833 to i128
  %836 = mul i128 %834, %835
  %837 = trunc i128 %836 to i64
  %838 = lshr i128 %836, 64
  %839 = trunc i128 %838 to i64
  %840 = icmp ne i64 %839, 0
  store i64 %837, i64* %rax
  store i64 %839, i64* %rdx
  %841 = icmp ne i64 %839, -1
  %842 = icmp eq i1 %840, %841
  store i1 %842, i1* %of
  store i1 %842, i1* %cf
  store volatile i64 64455, i64* @assembly_address
  %843 = load i64* %rdx
  %844 = load i1* %of
  %845 = ashr i64 %843, 7
  %846 = icmp eq i64 %845, 0
  store i1 %846, i1* %zf
  %847 = icmp slt i64 %845, 0
  store i1 %847, i1* %sf
  %848 = trunc i64 %845 to i8
  %849 = call i8 @llvm.ctpop.i8(i8 %848)
  %850 = and i8 %849, 1
  %851 = icmp eq i8 %850, 0
  store i1 %851, i1* %pf
  store i64 %845, i64* %rdx
  %852 = and i64 64, %843
  %853 = icmp ne i64 %852, 0
  store i1 %853, i1* %cf
  %854 = select i1 false, i1 false, i1 %844
  store i1 %854, i1* %of
  store volatile i64 64459, i64* @assembly_address
  %855 = load i64* %rcx
  store i64 %855, i64* %rax
  store volatile i64 64462, i64* @assembly_address
  %856 = load i64* %rax
  %857 = load i1* %of
  %858 = ashr i64 %856, 63
  %859 = icmp eq i64 %858, 0
  store i1 %859, i1* %zf
  %860 = icmp slt i64 %858, 0
  store i1 %860, i1* %sf
  %861 = trunc i64 %858 to i8
  %862 = call i8 @llvm.ctpop.i8(i8 %861)
  %863 = and i8 %862, 1
  %864 = icmp eq i8 %863, 0
  store i1 %864, i1* %pf
  store i64 %858, i64* %rax
  %865 = and i64 4611686018427387904, %856
  %866 = icmp ne i64 %865, 0
  store i1 %866, i1* %cf
  %867 = select i1 false, i1 false, i1 %857
  store i1 %867, i1* %of
  store volatile i64 64466, i64* @assembly_address
  %868 = load i64* %rdx
  %869 = load i64* %rax
  %870 = sub i64 %868, %869
  %871 = and i64 %868, 15
  %872 = and i64 %869, 15
  %873 = sub i64 %871, %872
  %874 = icmp ugt i64 %873, 15
  %875 = icmp ult i64 %868, %869
  %876 = xor i64 %868, %869
  %877 = xor i64 %868, %870
  %878 = and i64 %876, %877
  %879 = icmp slt i64 %878, 0
  store i1 %874, i1* %az
  store i1 %875, i1* %cf
  store i1 %879, i1* %of
  %880 = icmp eq i64 %870, 0
  store i1 %880, i1* %zf
  %881 = icmp slt i64 %870, 0
  store i1 %881, i1* %sf
  %882 = trunc i64 %870 to i8
  %883 = call i8 @llvm.ctpop.i8(i8 %882)
  %884 = and i8 %883, 1
  %885 = icmp eq i8 %884, 0
  store i1 %885, i1* %pf
  store i64 %870, i64* %rdx
  store volatile i64 64469, i64* @assembly_address
  %886 = load i64* %rdx
  store i64 %886, i64* %rax
  store volatile i64 64472, i64* @assembly_address
  %887 = load i64* %rax
  store i64 %887, i64* %stack_var_-224
  store volatile i64 64479, i64* @assembly_address
  %888 = ptrtoint i64* %stack_var_-248 to i64
  store i64 %888, i64* %rax
  store volatile i64 64486, i64* @assembly_address
  %889 = ptrtoint i64* %stack_var_-248 to i64
  store i64 %889, i64* %stack_var_-312
  store volatile i64 64493, i64* @assembly_address
  br label %block_fbfa

block_fbef:                                       ; preds = %block_fb3f
  store volatile i64 64495, i64* @assembly_address
  store i64 0, i64* %stack_var_-312
  br label %block_fbfa

block_fbfa:                                       ; preds = %block_fbef, %block_fb4f
  store volatile i64 64506, i64* @assembly_address
  %890 = load i32* %stack_var_-348
  %891 = and i32 %890, 15
  %892 = icmp ugt i32 %891, 15
  %893 = icmp ult i32 %890, 0
  %894 = xor i32 %890, 0
  %895 = and i32 %894, 0
  %896 = icmp slt i32 %895, 0
  store i1 %892, i1* %az
  store i1 %893, i1* %cf
  store i1 %896, i1* %of
  %897 = icmp eq i32 %890, 0
  store i1 %897, i1* %zf
  %898 = icmp slt i32 %890, 0
  store i1 %898, i1* %sf
  %899 = trunc i32 %890 to i8
  %900 = call i8 @llvm.ctpop.i8(i8 %899)
  %901 = and i8 %900, 1
  %902 = icmp eq i8 %901, 0
  store i1 %902, i1* %pf
  store volatile i64 64513, i64* @assembly_address
  %903 = load i1* %sf
  %904 = icmp eq i1 %903, false
  br i1 %904, label %block_fc23, label %block_fc03

block_fc03:                                       ; preds = %block_fbfa
  store volatile i64 64515, i64* @assembly_address
  %905 = load i64* %stack_var_-312
  store i64 %905, i64* %rdx
  store volatile i64 64522, i64* @assembly_address
  %906 = load i8** %stack_var_-360
  %907 = ptrtoint i8* %906 to i64
  store i64 %907, i64* %rax
  store volatile i64 64529, i64* @assembly_address
  %908 = load i64* %rax
  store i64 %908, i64* %rsi
  store volatile i64 64532, i64* @assembly_address
  store i64 4294967196, i64* %rdi
  store volatile i64 64537, i64* @assembly_address
  %909 = load i64* %rdi
  %910 = trunc i64 %909 to i32
  %911 = load i64* %rsi
  %912 = inttoptr i64 %911 to i8*
  %913 = load i64* %rdx
  %914 = insertvalue %timeval undef, i64 %913, 0
  %915 = insertvalue [2 x %timeval] undef, %timeval %914, 0
  %916 = call i32 @futimesat(i32 %910, i8* %912, [2 x %timeval] %915)
  %917 = sext i32 %916 to i64
  store i64 %917, i64* %rax
  %918 = sext i32 %916 to i64
  store i64 %918, i64* %rax
  store volatile i64 64542, i64* @assembly_address
  br label %block_fe1c

block_fc23:                                       ; preds = %block_fbfa
  store volatile i64 64547, i64* @assembly_address
  %919 = load i64* %stack_var_-312
  store i64 %919, i64* %rdx
  store volatile i64 64554, i64* @assembly_address
  %920 = load i32* %stack_var_-348
  %921 = zext i32 %920 to i64
  store i64 %921, i64* %rax
  store volatile i64 64560, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 64565, i64* @assembly_address
  %922 = load i64* %rax
  %923 = trunc i64 %922 to i32
  %924 = zext i32 %923 to i64
  store i64 %924, i64* %rdi
  store volatile i64 64567, i64* @assembly_address
  %925 = load i64* %rdi
  %926 = trunc i64 %925 to i32
  %927 = load i64* %rsi
  %928 = inttoptr i64 %927 to i8*
  %929 = load i64* %rdx
  %930 = insertvalue %timeval undef, i64 %929, 0
  %931 = insertvalue [2 x %timeval] undef, %timeval %930, 0
  %932 = call i32 @futimesat(i32 %926, i8* %928, [2 x %timeval] %931)
  %933 = sext i32 %932 to i64
  store i64 %933, i64* %rax
  %934 = sext i32 %932 to i64
  store i64 %934, i64* %rax
  store volatile i64 64572, i64* @assembly_address
  %935 = load i64* %rax
  %936 = trunc i64 %935 to i32
  %937 = load i64* %rax
  %938 = trunc i64 %937 to i32
  %939 = and i32 %936, %938
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %940 = icmp eq i32 %939, 0
  store i1 %940, i1* %zf
  %941 = icmp slt i32 %939, 0
  store i1 %941, i1* %sf
  %942 = trunc i32 %939 to i8
  %943 = call i8 @llvm.ctpop.i8(i8 %942)
  %944 = and i8 %943, 1
  %945 = icmp eq i8 %944, 0
  store i1 %945, i1* %pf
  store volatile i64 64574, i64* @assembly_address
  %946 = load i1* %zf
  %947 = icmp eq i1 %946, false
  br i1 %947, label %block_fdf2, label %block_fc44

block_fc44:                                       ; preds = %block_fc23
  store volatile i64 64580, i64* @assembly_address
  %948 = load i64* %stack_var_-312
  %949 = and i64 %948, 15
  %950 = icmp ugt i64 %949, 15
  %951 = icmp ult i64 %948, 0
  %952 = xor i64 %948, 0
  %953 = and i64 %952, 0
  %954 = icmp slt i64 %953, 0
  store i1 %950, i1* %az
  store i1 %951, i1* %cf
  store i1 %954, i1* %of
  %955 = icmp eq i64 %948, 0
  store i1 %955, i1* %zf
  %956 = icmp slt i64 %948, 0
  store i1 %956, i1* %sf
  %957 = trunc i64 %948 to i8
  %958 = call i8 @llvm.ctpop.i8(i8 %957)
  %959 = and i8 %958, 1
  %960 = icmp eq i8 %959, 0
  store i1 %960, i1* %pf
  store volatile i64 64588, i64* @assembly_address
  %961 = load i1* %zf
  br i1 %961, label %block_fdeb, label %block_fc52

block_fc52:                                       ; preds = %block_fc44
  store volatile i64 64594, i64* @assembly_address
  %962 = load i64* %stack_var_-312
  store i64 %962, i64* %rax
  store volatile i64 64601, i64* @assembly_address
  %963 = load i64* %rax
  %964 = add i64 %963, 8
  %965 = inttoptr i64 %964 to i64*
  %966 = load i64* %965
  store i64 %966, i64* %rax
  store volatile i64 64605, i64* @assembly_address
  %967 = load i64* %rax
  %968 = sub i64 %967, 499999
  %969 = and i64 %967, 15
  %970 = sub i64 %969, 15
  %971 = icmp ugt i64 %970, 15
  %972 = icmp ult i64 %967, 499999
  %973 = xor i64 %967, 499999
  %974 = xor i64 %967, %968
  %975 = and i64 %973, %974
  %976 = icmp slt i64 %975, 0
  store i1 %971, i1* %az
  store i1 %972, i1* %cf
  store i1 %976, i1* %of
  %977 = icmp eq i64 %968, 0
  store i1 %977, i1* %zf
  %978 = icmp slt i64 %968, 0
  store i1 %978, i1* %sf
  %979 = trunc i64 %968 to i8
  %980 = call i8 @llvm.ctpop.i8(i8 %979)
  %981 = and i8 %980, 1
  %982 = icmp eq i8 %981, 0
  store i1 %982, i1* %pf
  store volatile i64 64611, i64* @assembly_address
  %983 = load i1* %zf
  %984 = load i1* %sf
  %985 = load i1* %of
  %986 = icmp eq i1 %984, %985
  %987 = icmp eq i1 %983, false
  %988 = icmp eq i1 %986, %987
  %989 = zext i1 %988 to i8
  %990 = zext i8 %989 to i64
  %991 = load i64* %rax
  %992 = and i64 %991, -256
  %993 = or i64 %992, %990
  store i64 %993, i64* %rax
  store volatile i64 64614, i64* @assembly_address
  %994 = load i64* %rax
  %995 = trunc i64 %994 to i8
  store i8 %995, i8* %stack_var_-330
  store volatile i64 64620, i64* @assembly_address
  %996 = load i64* %stack_var_-312
  store i64 %996, i64* %rax
  store volatile i64 64627, i64* @assembly_address
  %997 = load i64* %rax
  %998 = add i64 %997, 16
  %999 = and i64 %997, 15
  %1000 = icmp ugt i64 %999, 15
  %1001 = icmp ult i64 %998, %997
  %1002 = xor i64 %997, %998
  %1003 = xor i64 16, %998
  %1004 = and i64 %1002, %1003
  %1005 = icmp slt i64 %1004, 0
  store i1 %1000, i1* %az
  store i1 %1001, i1* %cf
  store i1 %1005, i1* %of
  %1006 = icmp eq i64 %998, 0
  store i1 %1006, i1* %zf
  %1007 = icmp slt i64 %998, 0
  store i1 %1007, i1* %sf
  %1008 = trunc i64 %998 to i8
  %1009 = call i8 @llvm.ctpop.i8(i8 %1008)
  %1010 = and i8 %1009, 1
  %1011 = icmp eq i8 %1010, 0
  store i1 %1011, i1* %pf
  store i64 %998, i64* %rax
  store volatile i64 64631, i64* @assembly_address
  %1012 = load i64* %rax
  %1013 = add i64 %1012, 8
  %1014 = inttoptr i64 %1013 to i64*
  %1015 = load i64* %1014
  store i64 %1015, i64* %rax
  store volatile i64 64635, i64* @assembly_address
  %1016 = load i64* %rax
  %1017 = sub i64 %1016, 499999
  %1018 = and i64 %1016, 15
  %1019 = sub i64 %1018, 15
  %1020 = icmp ugt i64 %1019, 15
  %1021 = icmp ult i64 %1016, 499999
  %1022 = xor i64 %1016, 499999
  %1023 = xor i64 %1016, %1017
  %1024 = and i64 %1022, %1023
  %1025 = icmp slt i64 %1024, 0
  store i1 %1020, i1* %az
  store i1 %1021, i1* %cf
  store i1 %1025, i1* %of
  %1026 = icmp eq i64 %1017, 0
  store i1 %1026, i1* %zf
  %1027 = icmp slt i64 %1017, 0
  store i1 %1027, i1* %sf
  %1028 = trunc i64 %1017 to i8
  %1029 = call i8 @llvm.ctpop.i8(i8 %1028)
  %1030 = and i8 %1029, 1
  %1031 = icmp eq i8 %1030, 0
  store i1 %1031, i1* %pf
  store volatile i64 64641, i64* @assembly_address
  %1032 = load i1* %zf
  %1033 = load i1* %sf
  %1034 = load i1* %of
  %1035 = icmp eq i1 %1033, %1034
  %1036 = icmp eq i1 %1032, false
  %1037 = icmp eq i1 %1035, %1036
  %1038 = zext i1 %1037 to i8
  %1039 = zext i8 %1038 to i64
  %1040 = load i64* %rax
  %1041 = and i64 %1040, -256
  %1042 = or i64 %1041, %1039
  store i64 %1042, i64* %rax
  store volatile i64 64644, i64* @assembly_address
  %1043 = load i64* %rax
  %1044 = trunc i64 %1043 to i8
  store i8 %1044, i8* %stack_var_-329
  store volatile i64 64650, i64* @assembly_address
  %1045 = load i8* %stack_var_-330
  %1046 = zext i8 %1045 to i64
  store i64 %1046, i64* %rax
  store volatile i64 64657, i64* @assembly_address
  %1047 = load i64* %rax
  %1048 = trunc i64 %1047 to i8
  %1049 = load i8* %stack_var_-329
  %1050 = or i8 %1048, %1049
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1051 = icmp eq i8 %1050, 0
  store i1 %1051, i1* %zf
  %1052 = icmp slt i8 %1050, 0
  store i1 %1052, i1* %sf
  %1053 = call i8 @llvm.ctpop.i8(i8 %1050)
  %1054 = and i8 %1053, 1
  %1055 = icmp eq i8 %1054, 0
  store i1 %1055, i1* %pf
  %1056 = zext i8 %1050 to i64
  %1057 = load i64* %rax
  %1058 = and i64 %1057, -256
  %1059 = or i64 %1058, %1056
  store i64 %1059, i64* %rax
  store volatile i64 64663, i64* @assembly_address
  %1060 = load i64* %rax
  %1061 = trunc i64 %1060 to i8
  %1062 = load i64* %rax
  %1063 = trunc i64 %1062 to i8
  %1064 = and i8 %1061, %1063
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1065 = icmp eq i8 %1064, 0
  store i1 %1065, i1* %zf
  %1066 = icmp slt i8 %1064, 0
  store i1 %1066, i1* %sf
  %1067 = call i8 @llvm.ctpop.i8(i8 %1064)
  %1068 = and i8 %1067, 1
  %1069 = icmp eq i8 %1068, 0
  store i1 %1069, i1* %pf
  store volatile i64 64665, i64* @assembly_address
  %1070 = load i1* %zf
  br i1 %1070, label %block_fdeb, label %block_fc9f

block_fc9f:                                       ; preds = %block_fc52
  store volatile i64 64671, i64* @assembly_address
  %1071 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %1071, i64* %rdx
  store volatile i64 64678, i64* @assembly_address
  %1072 = load i32* %stack_var_-348
  %1073 = zext i32 %1072 to i64
  store i64 %1073, i64* %rax
  store volatile i64 64684, i64* @assembly_address
  %1074 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %1074, i64* %rsi
  store volatile i64 64687, i64* @assembly_address
  %1075 = load i64* %rax
  %1076 = trunc i64 %1075 to i32
  %1077 = zext i32 %1076 to i64
  store i64 %1077, i64* %rdi
  store volatile i64 64689, i64* @assembly_address
  %1078 = load i64* %rdi
  %1079 = trunc i64 %1078 to i32
  %1080 = load i64* %rsi
  %1081 = inttoptr i64 %1080 to %stat*
  %1082 = call i32 @fstat(i32 %1079, %stat* %1081)
  %1083 = sext i32 %1082 to i64
  store i64 %1083, i64* %rax
  %1084 = sext i32 %1082 to i64
  store i64 %1084, i64* %rax
  store volatile i64 64694, i64* @assembly_address
  %1085 = load i64* %rax
  %1086 = trunc i64 %1085 to i32
  %1087 = load i64* %rax
  %1088 = trunc i64 %1087 to i32
  %1089 = and i32 %1086, %1088
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1090 = icmp eq i32 %1089, 0
  store i1 %1090, i1* %zf
  %1091 = icmp slt i32 %1089, 0
  store i1 %1091, i1* %sf
  %1092 = trunc i32 %1089 to i8
  %1093 = call i8 @llvm.ctpop.i8(i8 %1092)
  %1094 = and i8 %1093, 1
  %1095 = icmp eq i8 %1094, 0
  store i1 %1095, i1* %pf
  store volatile i64 64696, i64* @assembly_address
  %1096 = load i1* %zf
  %1097 = icmp eq i1 %1096, false
  br i1 %1097, label %block_fdeb, label %block_fcbe

block_fcbe:                                       ; preds = %block_fc9f
  store volatile i64 64702, i64* @assembly_address
  %1098 = load i64* %stack_var_-112
  store i64 %1098, i64* %rdx
  store volatile i64 64706, i64* @assembly_address
  %1099 = load i64* %stack_var_-312
  store i64 %1099, i64* %rax
  store volatile i64 64713, i64* @assembly_address
  %1100 = load i64* %rax
  %1101 = inttoptr i64 %1100 to i64*
  %1102 = load i64* %1101
  store i64 %1102, i64* %rax
  store volatile i64 64716, i64* @assembly_address
  %1103 = load i64* %rdx
  %1104 = load i64* %rax
  %1105 = sub i64 %1103, %1104
  %1106 = and i64 %1103, 15
  %1107 = and i64 %1104, 15
  %1108 = sub i64 %1106, %1107
  %1109 = icmp ugt i64 %1108, 15
  %1110 = icmp ult i64 %1103, %1104
  %1111 = xor i64 %1103, %1104
  %1112 = xor i64 %1103, %1105
  %1113 = and i64 %1111, %1112
  %1114 = icmp slt i64 %1113, 0
  store i1 %1109, i1* %az
  store i1 %1110, i1* %cf
  store i1 %1114, i1* %of
  %1115 = icmp eq i64 %1105, 0
  store i1 %1115, i1* %zf
  %1116 = icmp slt i64 %1105, 0
  store i1 %1116, i1* %sf
  %1117 = trunc i64 %1105 to i8
  %1118 = call i8 @llvm.ctpop.i8(i8 %1117)
  %1119 = and i8 %1118, 1
  %1120 = icmp eq i8 %1119, 0
  store i1 %1120, i1* %pf
  store i64 %1105, i64* %rdx
  store volatile i64 64719, i64* @assembly_address
  %1121 = load i64* %rdx
  store i64 %1121, i64* %rax
  store volatile i64 64722, i64* @assembly_address
  %1122 = load i64* %rax
  store i64 %1122, i64* %stack_var_-296
  store volatile i64 64729, i64* @assembly_address
  %1123 = load i64* %stack_var_-96
  store i64 %1123, i64* %rdx
  store volatile i64 64733, i64* @assembly_address
  %1124 = load i64* %stack_var_-312
  store i64 %1124, i64* %rax
  store volatile i64 64740, i64* @assembly_address
  %1125 = load i64* %rax
  %1126 = add i64 %1125, 16
  %1127 = and i64 %1125, 15
  %1128 = icmp ugt i64 %1127, 15
  %1129 = icmp ult i64 %1126, %1125
  %1130 = xor i64 %1125, %1126
  %1131 = xor i64 16, %1126
  %1132 = and i64 %1130, %1131
  %1133 = icmp slt i64 %1132, 0
  store i1 %1128, i1* %az
  store i1 %1129, i1* %cf
  store i1 %1133, i1* %of
  %1134 = icmp eq i64 %1126, 0
  store i1 %1134, i1* %zf
  %1135 = icmp slt i64 %1126, 0
  store i1 %1135, i1* %sf
  %1136 = trunc i64 %1126 to i8
  %1137 = call i8 @llvm.ctpop.i8(i8 %1136)
  %1138 = and i8 %1137, 1
  %1139 = icmp eq i8 %1138, 0
  store i1 %1139, i1* %pf
  store i64 %1126, i64* %rax
  store volatile i64 64744, i64* @assembly_address
  %1140 = load i64* %rax
  %1141 = inttoptr i64 %1140 to i64*
  %1142 = load i64* %1141
  store i64 %1142, i64* %rax
  store volatile i64 64747, i64* @assembly_address
  %1143 = load i64* %rdx
  %1144 = load i64* %rax
  %1145 = sub i64 %1143, %1144
  %1146 = and i64 %1143, 15
  %1147 = and i64 %1144, 15
  %1148 = sub i64 %1146, %1147
  %1149 = icmp ugt i64 %1148, 15
  %1150 = icmp ult i64 %1143, %1144
  %1151 = xor i64 %1143, %1144
  %1152 = xor i64 %1143, %1145
  %1153 = and i64 %1151, %1152
  %1154 = icmp slt i64 %1153, 0
  store i1 %1149, i1* %az
  store i1 %1150, i1* %cf
  store i1 %1154, i1* %of
  %1155 = icmp eq i64 %1145, 0
  store i1 %1155, i1* %zf
  %1156 = icmp slt i64 %1145, 0
  store i1 %1156, i1* %sf
  %1157 = trunc i64 %1145 to i8
  %1158 = call i8 @llvm.ctpop.i8(i8 %1157)
  %1159 = and i8 %1158, 1
  %1160 = icmp eq i8 %1159, 0
  store i1 %1160, i1* %pf
  store i64 %1145, i64* %rdx
  store volatile i64 64750, i64* @assembly_address
  %1161 = load i64* %rdx
  store i64 %1161, i64* %rax
  store volatile i64 64753, i64* @assembly_address
  %1162 = load i64* %rax
  store i64 %1162, i64* %stack_var_-288
  store volatile i64 64760, i64* @assembly_address
  store i64 0, i64* %stack_var_-304
  store volatile i64 64771, i64* @assembly_address
  %1163 = load i64* %stack_var_-312
  store i64 %1163, i64* %rax
  store volatile i64 64778, i64* @assembly_address
  %1164 = load i64* %rax
  %1165 = add i64 %1164, 8
  %1166 = inttoptr i64 %1165 to i64*
  %1167 = load i64* %1166
  store i64 %1167, i64* %rdx
  store volatile i64 64782, i64* @assembly_address
  %1168 = load i64* %rax
  %1169 = inttoptr i64 %1168 to i64*
  %1170 = load i64* %1169
  store i64 %1170, i64* %rax
  store volatile i64 64785, i64* @assembly_address
  %1171 = load i64* %rax
  store i64 %1171, i64* %stack_var_-216
  store volatile i64 64792, i64* @assembly_address
  %1172 = load i64* %rdx
  store i64 %1172, i64* %stack_var_-208
  store volatile i64 64799, i64* @assembly_address
  %1173 = load i64* %stack_var_-312
  store i64 %1173, i64* %rax
  store volatile i64 64806, i64* @assembly_address
  %1174 = load i64* %rax
  %1175 = add i64 %1174, 24
  %1176 = inttoptr i64 %1175 to i64*
  %1177 = load i64* %1176
  store i64 %1177, i64* %rdx
  store volatile i64 64810, i64* @assembly_address
  %1178 = load i64* %rax
  %1179 = add i64 %1178, 16
  %1180 = inttoptr i64 %1179 to i64*
  %1181 = load i64* %1180
  store i64 %1181, i64* %rax
  store volatile i64 64814, i64* @assembly_address
  %1182 = load i64* %rax
  store i64 %1182, i64* %stack_var_-200
  store volatile i64 64821, i64* @assembly_address
  %1183 = load i64* %rdx
  store i64 %1183, i64* %stack_var_-192
  store volatile i64 64828, i64* @assembly_address
  %1184 = load i8* %stack_var_-330
  %1185 = and i8 %1184, 15
  %1186 = icmp ugt i8 %1185, 15
  %1187 = icmp ult i8 %1184, 0
  %1188 = xor i8 %1184, 0
  %1189 = and i8 %1188, 0
  %1190 = icmp slt i8 %1189, 0
  store i1 %1186, i1* %az
  store i1 %1187, i1* %cf
  store i1 %1190, i1* %of
  %1191 = icmp eq i8 %1184, 0
  store i1 %1191, i1* %zf
  %1192 = icmp slt i8 %1184, 0
  store i1 %1192, i1* %sf
  %1193 = call i8 @llvm.ctpop.i8(i8 %1184)
  %1194 = and i8 %1193, 1
  %1195 = icmp eq i8 %1194, 0
  store i1 %1195, i1* %pf
  store volatile i64 64835, i64* @assembly_address
  %1196 = load i1* %zf
  br i1 %1196, label %block_fd80, label %block_fd45

block_fd45:                                       ; preds = %block_fcbe
  store volatile i64 64837, i64* @assembly_address
  %1197 = load i64* %stack_var_-296
  %1198 = sub i64 %1197, 1
  %1199 = and i64 %1197, 15
  %1200 = sub i64 %1199, 1
  %1201 = icmp ugt i64 %1200, 15
  %1202 = icmp ult i64 %1197, 1
  %1203 = xor i64 %1197, 1
  %1204 = xor i64 %1197, %1198
  %1205 = and i64 %1203, %1204
  %1206 = icmp slt i64 %1205, 0
  store i1 %1201, i1* %az
  store i1 %1202, i1* %cf
  store i1 %1206, i1* %of
  %1207 = icmp eq i64 %1198, 0
  store i1 %1207, i1* %zf
  %1208 = icmp slt i64 %1198, 0
  store i1 %1208, i1* %sf
  %1209 = trunc i64 %1198 to i8
  %1210 = call i8 @llvm.ctpop.i8(i8 %1209)
  %1211 = and i8 %1210, 1
  %1212 = icmp eq i8 %1211, 0
  store i1 %1212, i1* %pf
  store volatile i64 64845, i64* @assembly_address
  %1213 = load i1* %zf
  %1214 = icmp eq i1 %1213, false
  br i1 %1214, label %block_fd80, label %block_fd4f

block_fd4f:                                       ; preds = %block_fd45
  store volatile i64 64847, i64* @assembly_address
  %1215 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %1215, i64* %rax
  store volatile i64 64854, i64* @assembly_address
  %1216 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %1216, i64* %rdi
  store volatile i64 64857, i64* @assembly_address
  %1217 = load i64* %rdi
  %1218 = inttoptr i64 %1217 to i64*
  %1219 = call i64 @get_stat_atime_ns(i64* %1218)
  store i64 %1219, i64* %rax
  store i64 %1219, i64* %rax
  store volatile i64 64862, i64* @assembly_address
  %1220 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1221 = icmp eq i64 %1220, 0
  store i1 %1221, i1* %zf
  %1222 = icmp slt i64 %1220, 0
  store i1 %1222, i1* %sf
  %1223 = trunc i64 %1220 to i8
  %1224 = call i8 @llvm.ctpop.i8(i8 %1223)
  %1225 = and i8 %1224, 1
  %1226 = icmp eq i8 %1225, 0
  store i1 %1226, i1* %pf
  store volatile i64 64865, i64* @assembly_address
  %1227 = load i1* %zf
  %1228 = icmp eq i1 %1227, false
  br i1 %1228, label %block_fd80, label %block_fd63

block_fd63:                                       ; preds = %block_fd4f
  store volatile i64 64867, i64* @assembly_address
  %1229 = ptrtoint i64* %stack_var_-216 to i64
  store i64 %1229, i64* %rax
  store volatile i64 64874, i64* @assembly_address
  %1230 = ptrtoint i64* %stack_var_-216 to i64
  store i64 %1230, i64* %stack_var_-304
  store volatile i64 64881, i64* @assembly_address
  %1231 = load i64* %stack_var_-304
  store i64 %1231, i64* %rax
  store volatile i64 64888, i64* @assembly_address
  %1232 = load i64* %rax
  %1233 = add i64 %1232, 8
  %1234 = inttoptr i64 %1233 to i64*
  store i64 0, i64* %1234
  br label %block_fd80

block_fd80:                                       ; preds = %block_fd63, %block_fd4f, %block_fd45, %block_fcbe
  store volatile i64 64896, i64* @assembly_address
  %1235 = load i8* %stack_var_-329
  %1236 = and i8 %1235, 15
  %1237 = icmp ugt i8 %1236, 15
  %1238 = icmp ult i8 %1235, 0
  %1239 = xor i8 %1235, 0
  %1240 = and i8 %1239, 0
  %1241 = icmp slt i8 %1240, 0
  store i1 %1237, i1* %az
  store i1 %1238, i1* %cf
  store i1 %1241, i1* %of
  %1242 = icmp eq i8 %1235, 0
  store i1 %1242, i1* %zf
  %1243 = icmp slt i8 %1235, 0
  store i1 %1243, i1* %sf
  %1244 = call i8 @llvm.ctpop.i8(i8 %1235)
  %1245 = and i8 %1244, 1
  %1246 = icmp eq i8 %1245, 0
  store i1 %1246, i1* %pf
  store volatile i64 64903, i64* @assembly_address
  %1247 = load i1* %zf
  br i1 %1247, label %block_fdc8, label %block_fd89

block_fd89:                                       ; preds = %block_fd80
  store volatile i64 64905, i64* @assembly_address
  %1248 = load i64* %stack_var_-288
  %1249 = sub i64 %1248, 1
  %1250 = and i64 %1248, 15
  %1251 = sub i64 %1250, 1
  %1252 = icmp ugt i64 %1251, 15
  %1253 = icmp ult i64 %1248, 1
  %1254 = xor i64 %1248, 1
  %1255 = xor i64 %1248, %1249
  %1256 = and i64 %1254, %1255
  %1257 = icmp slt i64 %1256, 0
  store i1 %1252, i1* %az
  store i1 %1253, i1* %cf
  store i1 %1257, i1* %of
  %1258 = icmp eq i64 %1249, 0
  store i1 %1258, i1* %zf
  %1259 = icmp slt i64 %1249, 0
  store i1 %1259, i1* %sf
  %1260 = trunc i64 %1249 to i8
  %1261 = call i8 @llvm.ctpop.i8(i8 %1260)
  %1262 = and i8 %1261, 1
  %1263 = icmp eq i8 %1262, 0
  store i1 %1263, i1* %pf
  store volatile i64 64913, i64* @assembly_address
  %1264 = load i1* %zf
  %1265 = icmp eq i1 %1264, false
  br i1 %1265, label %block_fdc8, label %block_fd93

block_fd93:                                       ; preds = %block_fd89
  store volatile i64 64915, i64* @assembly_address
  %1266 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %1266, i64* %rax
  store volatile i64 64922, i64* @assembly_address
  %1267 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %1267, i64* %rdi
  store volatile i64 64925, i64* @assembly_address
  %1268 = load i64* %rdi
  %1269 = inttoptr i64 %1268 to i64*
  %1270 = call i64 @get_stat_mtime_ns(i64* %1269)
  store i64 %1270, i64* %rax
  store i64 %1270, i64* %rax
  store volatile i64 64930, i64* @assembly_address
  %1271 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1272 = icmp eq i64 %1271, 0
  store i1 %1272, i1* %zf
  %1273 = icmp slt i64 %1271, 0
  store i1 %1273, i1* %sf
  %1274 = trunc i64 %1271 to i8
  %1275 = call i8 @llvm.ctpop.i8(i8 %1274)
  %1276 = and i8 %1275, 1
  %1277 = icmp eq i8 %1276, 0
  store i1 %1277, i1* %pf
  store volatile i64 64933, i64* @assembly_address
  %1278 = load i1* %zf
  %1279 = icmp eq i1 %1278, false
  br i1 %1279, label %block_fdc8, label %block_fda7

block_fda7:                                       ; preds = %block_fd93
  store volatile i64 64935, i64* @assembly_address
  %1280 = ptrtoint i64* %stack_var_-216 to i64
  store i64 %1280, i64* %rax
  store volatile i64 64942, i64* @assembly_address
  %1281 = ptrtoint i64* %stack_var_-216 to i64
  store i64 %1281, i64* %stack_var_-304
  store volatile i64 64949, i64* @assembly_address
  %1282 = load i64* %stack_var_-304
  store i64 %1282, i64* %rax
  store volatile i64 64956, i64* @assembly_address
  %1283 = load i64* %rax
  %1284 = add i64 %1283, 16
  %1285 = and i64 %1283, 15
  %1286 = icmp ugt i64 %1285, 15
  %1287 = icmp ult i64 %1284, %1283
  %1288 = xor i64 %1283, %1284
  %1289 = xor i64 16, %1284
  %1290 = and i64 %1288, %1289
  %1291 = icmp slt i64 %1290, 0
  store i1 %1286, i1* %az
  store i1 %1287, i1* %cf
  store i1 %1291, i1* %of
  %1292 = icmp eq i64 %1284, 0
  store i1 %1292, i1* %zf
  %1293 = icmp slt i64 %1284, 0
  store i1 %1293, i1* %sf
  %1294 = trunc i64 %1284 to i8
  %1295 = call i8 @llvm.ctpop.i8(i8 %1294)
  %1296 = and i8 %1295, 1
  %1297 = icmp eq i8 %1296, 0
  store i1 %1297, i1* %pf
  store i64 %1284, i64* %rax
  store volatile i64 64960, i64* @assembly_address
  %1298 = load i64* %rax
  %1299 = add i64 %1298, 8
  %1300 = inttoptr i64 %1299 to i64*
  store i64 0, i64* %1300
  br label %block_fdc8

block_fdc8:                                       ; preds = %block_fda7, %block_fd93, %block_fd89, %block_fd80
  store volatile i64 64968, i64* @assembly_address
  %1301 = load i64* %stack_var_-304
  %1302 = and i64 %1301, 15
  %1303 = icmp ugt i64 %1302, 15
  %1304 = icmp ult i64 %1301, 0
  %1305 = xor i64 %1301, 0
  %1306 = and i64 %1305, 0
  %1307 = icmp slt i64 %1306, 0
  store i1 %1303, i1* %az
  store i1 %1304, i1* %cf
  store i1 %1307, i1* %of
  %1308 = icmp eq i64 %1301, 0
  store i1 %1308, i1* %zf
  %1309 = icmp slt i64 %1301, 0
  store i1 %1309, i1* %sf
  %1310 = trunc i64 %1301 to i8
  %1311 = call i8 @llvm.ctpop.i8(i8 %1310)
  %1312 = and i8 %1311, 1
  %1313 = icmp eq i8 %1312, 0
  store i1 %1313, i1* %pf
  store volatile i64 64976, i64* @assembly_address
  %1314 = load i1* %zf
  br i1 %1314, label %block_fdeb, label %block_fdd2

block_fdd2:                                       ; preds = %block_fdc8
  store volatile i64 64978, i64* @assembly_address
  %1315 = load i64* %stack_var_-304
  store i64 %1315, i64* %rdx
  store volatile i64 64985, i64* @assembly_address
  %1316 = load i32* %stack_var_-348
  %1317 = zext i32 %1316 to i64
  store i64 %1317, i64* %rax
  store volatile i64 64991, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 64996, i64* @assembly_address
  %1318 = load i64* %rax
  %1319 = trunc i64 %1318 to i32
  %1320 = zext i32 %1319 to i64
  store i64 %1320, i64* %rdi
  store volatile i64 64998, i64* @assembly_address
  %1321 = load i64* %rdi
  %1322 = trunc i64 %1321 to i32
  %1323 = load i64* %rsi
  %1324 = inttoptr i64 %1323 to i8*
  %1325 = load i64* %rdx
  %1326 = insertvalue %timeval undef, i64 %1325, 0
  %1327 = insertvalue [2 x %timeval] undef, %timeval %1326, 0
  %1328 = call i32 @futimesat(i32 %1322, i8* %1324, [2 x %timeval] %1327)
  %1329 = sext i32 %1328 to i64
  store i64 %1329, i64* %rax
  %1330 = sext i32 %1328 to i64
  store i64 %1330, i64* %rax
  br label %block_fdeb

block_fdeb:                                       ; preds = %block_fdd2, %block_fdc8, %block_fc9f, %block_fc52, %block_fc44
  store volatile i64 65003, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 65008, i64* @assembly_address
  br label %block_fe1c

block_fdf2:                                       ; preds = %block_fc23
  store volatile i64 65010, i64* @assembly_address
  %1331 = load i8** %stack_var_-360
  %1332 = ptrtoint i8* %1331 to i64
  %1333 = and i64 %1332, 15
  %1334 = icmp ugt i64 %1333, 15
  %1335 = icmp ult i64 %1332, 0
  %1336 = xor i64 %1332, 0
  %1337 = and i64 %1336, 0
  %1338 = icmp slt i64 %1337, 0
  store i1 %1334, i1* %az
  store i1 %1335, i1* %cf
  store i1 %1338, i1* %of
  %1339 = icmp eq i64 %1332, 0
  store i1 %1339, i1* %zf
  %1340 = icmp slt i64 %1332, 0
  store i1 %1340, i1* %sf
  %1341 = trunc i64 %1332 to i8
  %1342 = call i8 @llvm.ctpop.i8(i8 %1341)
  %1343 = and i8 %1342, 1
  %1344 = icmp eq i8 %1343, 0
  store i1 %1344, i1* %pf
  store volatile i64 65018, i64* @assembly_address
  %1345 = load i1* %zf
  %1346 = icmp eq i1 %1345, false
  br i1 %1346, label %block_fe03, label %block_fdfc

block_fdfc:                                       ; preds = %block_fdf2
  store volatile i64 65020, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 65025, i64* @assembly_address
  br label %block_fe1c

block_fe03:                                       ; preds = %block_fdf2
  store volatile i64 65027, i64* @assembly_address
  %1347 = load i64* %stack_var_-312
  store i64 %1347, i64* %rdx
  store volatile i64 65034, i64* @assembly_address
  %1348 = load i8** %stack_var_-360
  %1349 = ptrtoint i8* %1348 to i64
  store i64 %1349, i64* %rax
  store volatile i64 65041, i64* @assembly_address
  %1350 = load i64* %rdx
  store i64 %1350, i64* %rsi
  store volatile i64 65044, i64* @assembly_address
  %1351 = load i64* %rax
  store i64 %1351, i64* %rdi
  store volatile i64 65047, i64* @assembly_address
  %1352 = load i64* %rdi
  %1353 = inttoptr i64 %1352 to i8*
  %1354 = load i64* %rsi
  %1355 = insertvalue %timeval undef, i64 %1354, 0
  %1356 = insertvalue [2 x %timeval] undef, %timeval %1355, 0
  %1357 = call i32 @utimes(i8* %1353, [2 x %timeval] %1356)
  %1358 = sext i32 %1357 to i64
  store i64 %1358, i64* %rax
  %1359 = sext i32 %1357 to i64
  store i64 %1359, i64* %rax
  br label %block_fe1c

block_fe1c:                                       ; preds = %block_fe03, %block_fdfc, %block_fdeb, %block_fc03, %block_fb35, %block_fb02, %block_fa7a, %block_fa16, %block_f943, %block_f8ca, %block_f8ad
  store volatile i64 65052, i64* @assembly_address
  %1360 = load i64* %stack_var_-32
  store i64 %1360, i64* %rbx
  store volatile i64 65056, i64* @assembly_address
  %1361 = load i64* %rbx
  %1362 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %1363 = xor i64 %1361, %1362
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %1364 = icmp eq i64 %1363, 0
  store i1 %1364, i1* %zf
  %1365 = icmp slt i64 %1363, 0
  store i1 %1365, i1* %sf
  %1366 = trunc i64 %1363 to i8
  %1367 = call i8 @llvm.ctpop.i8(i8 %1366)
  %1368 = and i8 %1367, 1
  %1369 = icmp eq i8 %1368, 0
  store i1 %1369, i1* %pf
  store i64 %1363, i64* %rbx
  store volatile i64 65065, i64* @assembly_address
  %1370 = load i1* %zf
  br i1 %1370, label %block_fe30, label %block_fe2b

block_fe2b:                                       ; preds = %block_fe1c
  store volatile i64 65067, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_fe30:                                       ; preds = %block_fe1c
  store volatile i64 65072, i64* @assembly_address
  %1371 = load i64* %rsp
  %1372 = add i64 %1371, 360
  %1373 = and i64 %1371, 15
  %1374 = add i64 %1373, 8
  %1375 = icmp ugt i64 %1374, 15
  %1376 = icmp ult i64 %1372, %1371
  %1377 = xor i64 %1371, %1372
  %1378 = xor i64 360, %1372
  %1379 = and i64 %1377, %1378
  %1380 = icmp slt i64 %1379, 0
  store i1 %1375, i1* %az
  store i1 %1376, i1* %cf
  store i1 %1380, i1* %of
  %1381 = icmp eq i64 %1372, 0
  store i1 %1381, i1* %zf
  %1382 = icmp slt i64 %1372, 0
  store i1 %1382, i1* %sf
  %1383 = trunc i64 %1372 to i8
  %1384 = call i8 @llvm.ctpop.i8(i8 %1383)
  %1385 = and i8 %1384, 1
  %1386 = icmp eq i8 %1385, 0
  store i1 %1386, i1* %pf
  %1387 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %1387, i64* %rsp
  store volatile i64 65079, i64* @assembly_address
  %1388 = load i64* %stack_var_-16
  store i64 %1388, i64* %rbx
  %1389 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1389, i64* %rsp
  store volatile i64 65080, i64* @assembly_address
  %1390 = load i64* %stack_var_-8
  store i64 %1390, i64* %rbp
  %1391 = ptrtoint i64* %stack_var_0 to i64
  store i64 %1391, i64* %rsp
  store volatile i64 65081, i64* @assembly_address
  %1392 = load i64* %rax
  ret i64 %1392
}

declare i64 @277(i64, i8*, i64*)

declare i64 @278(i64, i64*, i64*)

define i64 @utimens(i64* %arg1, i64 %arg2) {
block_fe3a:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 65082, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 65083, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 65086, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 65090, i64* @assembly_address
  %20 = load i64* %rdi
  store i64 %20, i64* %stack_var_-16
  store volatile i64 65094, i64* @assembly_address
  %21 = load i64* %rsi
  store i64 %21, i64* %stack_var_-24
  store volatile i64 65098, i64* @assembly_address
  %22 = load i64* %stack_var_-24
  store i64 %22, i64* %rdx
  store volatile i64 65102, i64* @assembly_address
  %23 = load i64* %stack_var_-16
  store i64 %23, i64* %rax
  store volatile i64 65106, i64* @assembly_address
  %24 = load i64* %rax
  store i64 %24, i64* %rsi
  store volatile i64 65109, i64* @assembly_address
  store i64 4294967295, i64* %rdi
  store volatile i64 65114, i64* @assembly_address
  %25 = load i64* %rdi
  %26 = load i64* %rsi
  %27 = inttoptr i64 %26 to i64*
  %28 = load i64* %rdx
  %29 = inttoptr i64 %28 to i64*
  %30 = trunc i64 %25 to i32
  %31 = call i64 @fdutimens(i32 %30, i64* %27, i64* %29)
  store i64 %31, i64* %rax
  store i64 %31, i64* %rax
  store volatile i64 65119, i64* @assembly_address
  %32 = load i64* %stack_var_-8
  store i64 %32, i64* %rbp
  %33 = ptrtoint i64* %stack_var_0 to i64
  store i64 %33, i64* %rsp
  store volatile i64 65120, i64* @assembly_address
  %34 = load i64* %rax
  ret i64 %34
}

declare i64 @279(i64, i64*)

declare i64 @280(i64, i64)

define i64 @lutimens(i8* %arg1, i64 %arg2) {
block_fe61:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-160 = alloca i32
  %stack_var_-228 = alloca i32
  %stack_var_-184 = alloca i64
  %stack_var_-192 = alloca i64
  %stack_var_-200 = alloca i64
  %stack_var_-208 = alloca i64
  %stack_var_-232 = alloca i32
  %stack_var_-224 = alloca i64
  %stack_var_-216 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-240 = alloca i8*
  %1 = alloca i64
  %stack_var_-248 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i32
  store volatile i64 65121, i64* @assembly_address
  %4 = load i64* %rbp
  store i64 %4, i64* %stack_var_-8
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rsp
  store volatile i64 65122, i64* @assembly_address
  %6 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %6, i64* %rbp
  store volatile i64 65125, i64* @assembly_address
  %7 = load i64* %rbx
  store i64 %7, i64* %stack_var_-16
  %8 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %8, i64* %rsp
  store volatile i64 65126, i64* @assembly_address
  %9 = load i64* %rsp
  %10 = sub i64 %9, 232
  %11 = and i64 %9, 15
  %12 = sub i64 %11, 8
  %13 = icmp ugt i64 %12, 15
  %14 = icmp ult i64 %9, 232
  %15 = xor i64 %9, 232
  %16 = xor i64 %9, %10
  %17 = and i64 %15, %16
  %18 = icmp slt i64 %17, 0
  store i1 %13, i1* %az
  store i1 %14, i1* %cf
  store i1 %18, i1* %of
  %19 = icmp eq i64 %10, 0
  store i1 %19, i1* %zf
  %20 = icmp slt i64 %10, 0
  store i1 %20, i1* %sf
  %21 = trunc i64 %10 to i8
  %22 = call i8 @llvm.ctpop.i8(i8 %21)
  %23 = and i8 %22, 1
  %24 = icmp eq i8 %23, 0
  store i1 %24, i1* %pf
  %25 = ptrtoint i64* %stack_var_-248 to i64
  store i64 %25, i64* %rsp
  store volatile i64 65133, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to i8*
  store i8* %27, i8** %stack_var_-240
  store volatile i64 65140, i64* @assembly_address
  %28 = load i64* %rsi
  store i64 %28, i64* %stack_var_-248
  store volatile i64 65147, i64* @assembly_address
  %29 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %29, i64* %rax
  store volatile i64 65156, i64* @assembly_address
  %30 = load i64* %rax
  store i64 %30, i64* %stack_var_-32
  store volatile i64 65160, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %31 = icmp eq i32 0, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i32 0, 0
  store i1 %32, i1* %sf
  %33 = trunc i32 0 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  %37 = zext i32 0 to i64
  store i64 %37, i64* %rax
  store volatile i64 65162, i64* @assembly_address
  %38 = load i64* %stack_var_-248
  %39 = and i64 %38, 15
  %40 = icmp ugt i64 %39, 15
  %41 = icmp ult i64 %38, 0
  %42 = xor i64 %38, 0
  %43 = and i64 %42, 0
  %44 = icmp slt i64 %43, 0
  store i1 %40, i1* %az
  store i1 %41, i1* %cf
  store i1 %44, i1* %of
  %45 = icmp eq i64 %38, 0
  store i1 %45, i1* %zf
  %46 = icmp slt i64 %38, 0
  store i1 %46, i1* %sf
  %47 = trunc i64 %38 to i8
  %48 = call i8 @llvm.ctpop.i8(i8 %47)
  %49 = and i8 %48, 1
  %50 = icmp eq i8 %49, 0
  store i1 %50, i1* %pf
  store volatile i64 65170, i64* @assembly_address
  %51 = load i1* %zf
  br i1 %51, label %block_fe9d, label %block_fe94

block_fe94:                                       ; preds = %block_fe61
  store volatile i64 65172, i64* @assembly_address
  %52 = ptrtoint i64* %stack_var_-216 to i64
  store i64 %52, i64* %rax
  store volatile i64 65179, i64* @assembly_address
  br label %block_fea2

block_fe9d:                                       ; preds = %block_fe61
  store volatile i64 65181, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_fea2

block_fea2:                                       ; preds = %block_fe9d, %block_fe94
  store volatile i64 65186, i64* @assembly_address
  %53 = load i64* %rax
  store i64 %53, i64* %stack_var_-224
  store volatile i64 65193, i64* @assembly_address
  store i32 0, i32* %stack_var_-232
  store volatile i64 65203, i64* @assembly_address
  %54 = load i64* %stack_var_-224
  store i64 %54, i64* %rax
  store volatile i64 65210, i64* @assembly_address
  %55 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %56 = icmp eq i64 %55, 0
  store i1 %56, i1* %zf
  %57 = icmp slt i64 %55, 0
  store i1 %57, i1* %sf
  %58 = trunc i64 %55 to i8
  %59 = call i8 @llvm.ctpop.i8(i8 %58)
  %60 = and i8 %59, 1
  %61 = icmp eq i8 %60, 0
  store i1 %61, i1* %pf
  store volatile i64 65213, i64* @assembly_address
  %62 = load i1* %zf
  br i1 %62, label %block_ff0d, label %block_febf

block_febf:                                       ; preds = %block_fea2
  store volatile i64 65215, i64* @assembly_address
  %63 = load i64* %stack_var_-248
  store i64 %63, i64* %rax
  store volatile i64 65222, i64* @assembly_address
  %64 = load i64* %rax
  %65 = add i64 %64, 8
  %66 = inttoptr i64 %65 to i64*
  %67 = load i64* %66
  store i64 %67, i64* %rdx
  store volatile i64 65226, i64* @assembly_address
  %68 = load i64* %rax
  %69 = inttoptr i64 %68 to i64*
  %70 = load i64* %69
  store i64 %70, i64* %rax
  store volatile i64 65229, i64* @assembly_address
  %71 = load i64* %rax
  store i64 %71, i64* %stack_var_-216
  store volatile i64 65236, i64* @assembly_address
  %72 = load i64* %rdx
  store i64 %72, i64* %stack_var_-208
  store volatile i64 65243, i64* @assembly_address
  %73 = load i64* %stack_var_-248
  store i64 %73, i64* %rax
  store volatile i64 65250, i64* @assembly_address
  %74 = load i64* %rax
  %75 = add i64 %74, 24
  %76 = inttoptr i64 %75 to i64*
  %77 = load i64* %76
  store i64 %77, i64* %rdx
  store volatile i64 65254, i64* @assembly_address
  %78 = load i64* %rax
  %79 = add i64 %78, 16
  %80 = inttoptr i64 %79 to i64*
  %81 = load i64* %80
  store i64 %81, i64* %rax
  store volatile i64 65258, i64* @assembly_address
  %82 = load i64* %rax
  store i64 %82, i64* %stack_var_-200
  store volatile i64 65265, i64* @assembly_address
  %83 = load i64* %rdx
  store i64 %83, i64* %stack_var_-192
  store volatile i64 65272, i64* @assembly_address
  %84 = load i64* %stack_var_-224
  store i64 %84, i64* %rax
  store volatile i64 65279, i64* @assembly_address
  %85 = load i64* %rax
  store i64 %85, i64* %rdi
  store volatile i64 65282, i64* @assembly_address
  %86 = load i64* %rdi
  %87 = inttoptr i64 %86 to i64*
  %88 = call i64 @validate_timespec(i64* %87)
  store i64 %88, i64* %rax
  store i64 %88, i64* %rax
  store volatile i64 65287, i64* @assembly_address
  %89 = load i64* %rax
  %90 = trunc i64 %89 to i32
  store i32 %90, i32* %stack_var_-232
  br label %block_ff0d

block_ff0d:                                       ; preds = %block_febf, %block_fea2
  store volatile i64 65293, i64* @assembly_address
  %91 = load i32* %stack_var_-232
  %92 = and i32 %91, 15
  %93 = icmp ugt i32 %92, 15
  %94 = icmp ult i32 %91, 0
  %95 = xor i32 %91, 0
  %96 = and i32 %95, 0
  %97 = icmp slt i32 %96, 0
  store i1 %93, i1* %az
  store i1 %94, i1* %cf
  store i1 %97, i1* %of
  %98 = icmp eq i32 %91, 0
  store i1 %98, i1* %zf
  %99 = icmp slt i32 %91, 0
  store i1 %99, i1* %sf
  %100 = trunc i32 %91 to i8
  %101 = call i8 @llvm.ctpop.i8(i8 %100)
  %102 = and i8 %101, 1
  %103 = icmp eq i8 %102, 0
  store i1 %103, i1* %pf
  store volatile i64 65300, i64* @assembly_address
  %104 = load i1* %sf
  %105 = icmp eq i1 %104, false
  br i1 %105, label %block_ff20, label %block_ff16

block_ff16:                                       ; preds = %block_ff0d
  store volatile i64 65302, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 65307, i64* @assembly_address
  br label %block_10120

block_ff20:                                       ; preds = %block_ff0d
  store volatile i64 65312, i64* @assembly_address
  %106 = load i32* bitcast (i64* @global_var_21a414 to i32*)
  %107 = zext i32 %106 to i64
  store i64 %107, i64* %rax
  store volatile i64 65318, i64* @assembly_address
  %108 = load i64* %rax
  %109 = trunc i64 %108 to i32
  %110 = load i64* %rax
  %111 = trunc i64 %110 to i32
  %112 = and i32 %109, %111
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %113 = icmp eq i32 %112, 0
  store i1 %113, i1* %zf
  %114 = icmp slt i32 %112, 0
  store i1 %114, i1* %sf
  %115 = trunc i32 %112 to i8
  %116 = call i8 @llvm.ctpop.i8(i8 %115)
  %117 = and i8 %116, 1
  %118 = icmp eq i8 %117, 0
  store i1 %118, i1* %pf
  store volatile i64 65320, i64* @assembly_address
  %119 = load i1* %sf
  br i1 %119, label %block_10041, label %block_ff2e

block_ff2e:                                       ; preds = %block_ff20
  store volatile i64 65326, i64* @assembly_address
  %120 = load i32* %stack_var_-232
  %121 = sub i32 %120, 2
  %122 = and i32 %120, 15
  %123 = sub i32 %122, 2
  %124 = icmp ugt i32 %123, 15
  %125 = icmp ult i32 %120, 2
  %126 = xor i32 %120, 2
  %127 = xor i32 %120, %121
  %128 = and i32 %126, %127
  %129 = icmp slt i32 %128, 0
  store i1 %124, i1* %az
  store i1 %125, i1* %cf
  store i1 %129, i1* %of
  %130 = icmp eq i32 %121, 0
  store i1 %130, i1* %zf
  %131 = icmp slt i32 %121, 0
  store i1 %131, i1* %sf
  %132 = trunc i32 %121 to i8
  %133 = call i8 @llvm.ctpop.i8(i8 %132)
  %134 = and i8 %133, 1
  %135 = icmp eq i8 %134, 0
  store i1 %135, i1* %pf
  store volatile i64 65333, i64* @assembly_address
  %136 = load i1* %zf
  %137 = icmp eq i1 %136, false
  br i1 %137, label %block_ffd3, label %block_ff3b

block_ff3b:                                       ; preds = %block_ff2e
  store volatile i64 65339, i64* @assembly_address
  %138 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %138, i64* %rdx
  store volatile i64 65346, i64* @assembly_address
  %139 = load i8** %stack_var_-240
  %140 = ptrtoint i8* %139 to i64
  store i64 %140, i64* %rax
  store volatile i64 65353, i64* @assembly_address
  %141 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %141, i64* %rsi
  store volatile i64 65356, i64* @assembly_address
  %142 = load i64* %rax
  store i64 %142, i64* %rdi
  store volatile i64 65359, i64* @assembly_address
  %143 = load i64* %rdi
  %144 = inttoptr i64 %143 to i8*
  %145 = load i64* %rsi
  %146 = inttoptr i64 %145 to %stat*
  %147 = call i32 @lstat(i8* %144, %stat* %146)
  %148 = sext i32 %147 to i64
  store i64 %148, i64* %rax
  %149 = sext i32 %147 to i64
  store i64 %149, i64* %rax
  store volatile i64 65364, i64* @assembly_address
  %150 = load i64* %rax
  %151 = trunc i64 %150 to i32
  %152 = load i64* %rax
  %153 = trunc i64 %152 to i32
  %154 = and i32 %151, %153
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %155 = icmp eq i32 %154, 0
  store i1 %155, i1* %zf
  %156 = icmp slt i32 %154, 0
  store i1 %156, i1* %sf
  %157 = trunc i32 %154 to i8
  %158 = call i8 @llvm.ctpop.i8(i8 %157)
  %159 = and i8 %158, 1
  %160 = icmp eq i8 %159, 0
  store i1 %160, i1* %pf
  store volatile i64 65366, i64* @assembly_address
  %161 = load i1* %zf
  br i1 %161, label %block_ff62, label %block_ff58

block_ff58:                                       ; preds = %block_ff3b
  store volatile i64 65368, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 65373, i64* @assembly_address
  br label %block_10120

block_ff62:                                       ; preds = %block_ff3b
  store volatile i64 65378, i64* @assembly_address
  %162 = load i64* %stack_var_-224
  store i64 %162, i64* %rax
  store volatile i64 65385, i64* @assembly_address
  %163 = load i64* %rax
  %164 = add i64 %163, 8
  %165 = inttoptr i64 %164 to i64*
  %166 = load i64* %165
  store i64 %166, i64* %rax
  store volatile i64 65389, i64* @assembly_address
  %167 = load i64* %rax
  %168 = sub i64 %167, 1073741822
  %169 = and i64 %167, 15
  %170 = sub i64 %169, 14
  %171 = icmp ugt i64 %170, 15
  %172 = icmp ult i64 %167, 1073741822
  %173 = xor i64 %167, 1073741822
  %174 = xor i64 %167, %168
  %175 = and i64 %173, %174
  %176 = icmp slt i64 %175, 0
  store i1 %171, i1* %az
  store i1 %172, i1* %cf
  store i1 %176, i1* %of
  %177 = icmp eq i64 %168, 0
  store i1 %177, i1* %zf
  %178 = icmp slt i64 %168, 0
  store i1 %178, i1* %sf
  %179 = trunc i64 %168 to i8
  %180 = call i8 @llvm.ctpop.i8(i8 %179)
  %181 = and i8 %180, 1
  %182 = icmp eq i8 %181, 0
  store i1 %182, i1* %pf
  store volatile i64 65395, i64* @assembly_address
  %183 = load i1* %zf
  %184 = icmp eq i1 %183, false
  br i1 %184, label %block_ff94, label %block_ff75

block_ff75:                                       ; preds = %block_ff62
  store volatile i64 65397, i64* @assembly_address
  %185 = load i64* %stack_var_-224
  store i64 %185, i64* %rbx
  store volatile i64 65404, i64* @assembly_address
  %186 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %186, i64* %rax
  store volatile i64 65411, i64* @assembly_address
  %187 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %187, i64* %rdi
  store volatile i64 65414, i64* @assembly_address
  %188 = load i64* %rdi
  %189 = call i64 @get_stat_atime(i64 %188)
  store i64 %189, i64* %rax
  store i64 %189, i64* %rax
  store volatile i64 65419, i64* @assembly_address
  %190 = load i64* %rax
  %191 = load i64* %rbx
  %192 = inttoptr i64 %191 to i64*
  store i64 %190, i64* %192
  store volatile i64 65422, i64* @assembly_address
  %193 = load i64* %rbx
  %194 = add i64 %193, 8
  %195 = inttoptr i64 %194 to i64*
  %196 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %196, i64* %195
  store volatile i64 65426, i64* @assembly_address
  br label %block_ffcc

block_ff94:                                       ; preds = %block_ff62
  store volatile i64 65428, i64* @assembly_address
  %197 = load i64* %stack_var_-224
  store i64 %197, i64* %rax
  store volatile i64 65435, i64* @assembly_address
  %198 = load i64* %rax
  %199 = add i64 %198, 16
  %200 = and i64 %198, 15
  %201 = icmp ugt i64 %200, 15
  %202 = icmp ult i64 %199, %198
  %203 = xor i64 %198, %199
  %204 = xor i64 16, %199
  %205 = and i64 %203, %204
  %206 = icmp slt i64 %205, 0
  store i1 %201, i1* %az
  store i1 %202, i1* %cf
  store i1 %206, i1* %of
  %207 = icmp eq i64 %199, 0
  store i1 %207, i1* %zf
  %208 = icmp slt i64 %199, 0
  store i1 %208, i1* %sf
  %209 = trunc i64 %199 to i8
  %210 = call i8 @llvm.ctpop.i8(i8 %209)
  %211 = and i8 %210, 1
  %212 = icmp eq i8 %211, 0
  store i1 %212, i1* %pf
  store i64 %199, i64* %rax
  store volatile i64 65439, i64* @assembly_address
  %213 = load i64* %rax
  %214 = add i64 %213, 8
  %215 = inttoptr i64 %214 to i64*
  %216 = load i64* %215
  store i64 %216, i64* %rax
  store volatile i64 65443, i64* @assembly_address
  %217 = load i64* %rax
  %218 = sub i64 %217, 1073741822
  %219 = and i64 %217, 15
  %220 = sub i64 %219, 14
  %221 = icmp ugt i64 %220, 15
  %222 = icmp ult i64 %217, 1073741822
  %223 = xor i64 %217, 1073741822
  %224 = xor i64 %217, %218
  %225 = and i64 %223, %224
  %226 = icmp slt i64 %225, 0
  store i1 %221, i1* %az
  store i1 %222, i1* %cf
  store i1 %226, i1* %of
  %227 = icmp eq i64 %218, 0
  store i1 %227, i1* %zf
  %228 = icmp slt i64 %218, 0
  store i1 %228, i1* %sf
  %229 = trunc i64 %218 to i8
  %230 = call i8 @llvm.ctpop.i8(i8 %229)
  %231 = and i8 %230, 1
  %232 = icmp eq i8 %231, 0
  store i1 %232, i1* %pf
  store volatile i64 65449, i64* @assembly_address
  %233 = load i1* %zf
  %234 = icmp eq i1 %233, false
  br i1 %234, label %block_ffcc, label %block_ffab

block_ffab:                                       ; preds = %block_ff94
  store volatile i64 65451, i64* @assembly_address
  %235 = load i64* %stack_var_-224
  store i64 %235, i64* %rax
  store volatile i64 65458, i64* @assembly_address
  %236 = load i64* %rax
  %237 = add i64 %236, 16
  store i64 %237, i64* %rbx
  store volatile i64 65462, i64* @assembly_address
  %238 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %238, i64* %rax
  store volatile i64 65469, i64* @assembly_address
  %239 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %239, i64* %rdi
  store volatile i64 65472, i64* @assembly_address
  %240 = load i64* %rdi
  %241 = inttoptr i64 %240 to i64*
  %242 = call i64 @get_stat_mtime(i64* %241)
  store i64 %242, i64* %rax
  store i64 %242, i64* %rax
  store volatile i64 65477, i64* @assembly_address
  %243 = load i64* %rax
  %244 = load i64* %rbx
  %245 = inttoptr i64 %244 to i64*
  store i64 %243, i64* %245
  store volatile i64 65480, i64* @assembly_address
  %246 = load i64* %rbx
  %247 = add i64 %246, 8
  %248 = inttoptr i64 %247 to i64*
  %249 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %249, i64* %248
  br label %block_ffcc

block_ffcc:                                       ; preds = %block_ffab, %block_ff94, %block_ff75
  store volatile i64 65484, i64* @assembly_address
  %250 = load i32* %stack_var_-232
  %251 = add i32 %250, 1
  %252 = and i32 %250, 15
  %253 = add i32 %252, 1
  %254 = icmp ugt i32 %253, 15
  %255 = icmp ult i32 %251, %250
  %256 = xor i32 %250, %251
  %257 = xor i32 1, %251
  %258 = and i32 %256, %257
  %259 = icmp slt i32 %258, 0
  store i1 %254, i1* %az
  store i1 %255, i1* %cf
  store i1 %259, i1* %of
  %260 = icmp eq i32 %251, 0
  store i1 %260, i1* %zf
  %261 = icmp slt i32 %251, 0
  store i1 %261, i1* %sf
  %262 = trunc i32 %251 to i8
  %263 = call i8 @llvm.ctpop.i8(i8 %262)
  %264 = and i8 %263, 1
  %265 = icmp eq i8 %264, 0
  store i1 %265, i1* %pf
  store i32 %251, i32* %stack_var_-232
  br label %block_ffd3

block_ffd3:                                       ; preds = %block_ffcc, %block_ff2e
  store volatile i64 65491, i64* @assembly_address
  %266 = load i64* %stack_var_-224
  store i64 %266, i64* %rdx
  store volatile i64 65498, i64* @assembly_address
  %267 = load i8** %stack_var_-240
  %268 = ptrtoint i8* %267 to i64
  store i64 %268, i64* %rax
  store volatile i64 65505, i64* @assembly_address
  store i64 256, i64* %rcx
  store volatile i64 65510, i64* @assembly_address
  %269 = load i64* %rax
  store i64 %269, i64* %rsi
  store volatile i64 65513, i64* @assembly_address
  store i64 4294967196, i64* %rdi
  store volatile i64 65518, i64* @assembly_address
  %270 = load i64* %rdi
  %271 = trunc i64 %270 to i32
  %272 = load i64* %rsi
  %273 = inttoptr i64 %272 to i8*
  %274 = load i64* %rdx
  %275 = insertvalue %timespec undef, i64 %274, 0
  %276 = insertvalue [2 x %timespec] undef, %timespec %275, 0
  %277 = load i64* %rcx
  %278 = trunc i64 %277 to i32
  %279 = call i32 @utimensat(i32 %271, i8* %273, [2 x %timespec] %276, i32 %278)
  %280 = sext i32 %279 to i64
  store i64 %280, i64* %rax
  %281 = sext i32 %279 to i64
  store i64 %281, i64* %rax
  store volatile i64 65523, i64* @assembly_address
  %282 = load i64* %rax
  %283 = trunc i64 %282 to i32
  store i32 %283, i32* %stack_var_-228
  store volatile i64 65529, i64* @assembly_address
  %284 = load i32* %stack_var_-228
  %285 = and i32 %284, 15
  %286 = icmp ugt i32 %285, 15
  %287 = icmp ult i32 %284, 0
  %288 = xor i32 %284, 0
  %289 = and i32 %288, 0
  %290 = icmp slt i32 %289, 0
  store i1 %286, i1* %az
  store i1 %287, i1* %cf
  store i1 %290, i1* %of
  store i32 %284, i32* %3
  store i32 0, i32* %2
  %291 = icmp eq i32 %284, 0
  store i1 %291, i1* %zf
  %292 = icmp slt i32 %284, 0
  store i1 %292, i1* %sf
  %293 = trunc i32 %284 to i8
  %294 = call i8 @llvm.ctpop.i8(i8 %293)
  %295 = and i8 %294, 1
  %296 = icmp eq i8 %295, 0
  store i1 %296, i1* %pf
  store volatile i64 65536, i64* @assembly_address
  %297 = load i32* %3
  %298 = load i32* %2
  %299 = icmp sle i32 %297, %298
  br i1 %299, label %block_1000d, label %block_10002

block_10002:                                      ; preds = %block_ffd3
  store volatile i64 65538, i64* @assembly_address
  %300 = call i32* @__errno_location()
  %301 = ptrtoint i32* %300 to i64
  store i64 %301, i64* %rax
  %302 = ptrtoint i32* %300 to i64
  store i64 %302, i64* %rax
  %303 = ptrtoint i32* %300 to i64
  store i64 %303, i64* %rax
  store volatile i64 65543, i64* @assembly_address
  %304 = load i64* %rax
  %305 = inttoptr i64 %304 to i32*
  store i32 38, i32* %305
  br label %block_1000d

block_1000d:                                      ; preds = %block_10002, %block_ffd3
  store volatile i64 65549, i64* @assembly_address
  %306 = load i32* %stack_var_-228
  %307 = and i32 %306, 15
  %308 = icmp ugt i32 %307, 15
  %309 = icmp ult i32 %306, 0
  %310 = xor i32 %306, 0
  %311 = and i32 %310, 0
  %312 = icmp slt i32 %311, 0
  store i1 %308, i1* %az
  store i1 %309, i1* %cf
  store i1 %312, i1* %of
  %313 = icmp eq i32 %306, 0
  store i1 %313, i1* %zf
  %314 = icmp slt i32 %306, 0
  store i1 %314, i1* %sf
  %315 = trunc i32 %306 to i8
  %316 = call i8 @llvm.ctpop.i8(i8 %315)
  %317 = and i8 %316, 1
  %318 = icmp eq i8 %317, 0
  store i1 %318, i1* %pf
  store volatile i64 65556, i64* @assembly_address
  %319 = load i1* %zf
  br i1 %319, label %block_10022, label %block_10016

block_10016:                                      ; preds = %block_1000d
  store volatile i64 65558, i64* @assembly_address
  %320 = call i32* @__errno_location()
  %321 = ptrtoint i32* %320 to i64
  store i64 %321, i64* %rax
  %322 = ptrtoint i32* %320 to i64
  store i64 %322, i64* %rax
  %323 = ptrtoint i32* %320 to i64
  store i64 %323, i64* %rax
  store volatile i64 65563, i64* @assembly_address
  %324 = load i64* %rax
  %325 = inttoptr i64 %324 to i32*
  %326 = load i32* %325
  %327 = zext i32 %326 to i64
  store i64 %327, i64* %rax
  store volatile i64 65565, i64* @assembly_address
  %328 = load i64* %rax
  %329 = trunc i64 %328 to i32
  %330 = sub i32 %329, 38
  %331 = and i32 %329, 15
  %332 = sub i32 %331, 6
  %333 = icmp ugt i32 %332, 15
  %334 = icmp ult i32 %329, 38
  %335 = xor i32 %329, 38
  %336 = xor i32 %329, %330
  %337 = and i32 %335, %336
  %338 = icmp slt i32 %337, 0
  store i1 %333, i1* %az
  store i1 %334, i1* %cf
  store i1 %338, i1* %of
  %339 = icmp eq i32 %330, 0
  store i1 %339, i1* %zf
  %340 = icmp slt i32 %330, 0
  store i1 %340, i1* %sf
  %341 = trunc i32 %330 to i8
  %342 = call i8 @llvm.ctpop.i8(i8 %341)
  %343 = and i8 %342, 1
  %344 = icmp eq i8 %343, 0
  store i1 %344, i1* %pf
  store volatile i64 65568, i64* @assembly_address
  %345 = load i1* %zf
  br i1 %345, label %block_10041, label %block_10022

block_10022:                                      ; preds = %block_10016, %block_1000d
  store volatile i64 65570, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21a410 to i32*)
  store volatile i64 65580, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21a414 to i32*)
  store volatile i64 65590, i64* @assembly_address
  %346 = load i32* %stack_var_-228
  %347 = zext i32 %346 to i64
  store i64 %347, i64* %rax
  store volatile i64 65596, i64* @assembly_address
  br label %block_10120

block_10041:                                      ; preds = %block_10016, %block_ff20
  store volatile i64 65601, i64* @assembly_address
  store i32 -1, i32* bitcast (i64* @global_var_21a414 to i32*)
  store volatile i64 65611, i64* @assembly_address
  %348 = load i32* %stack_var_-232
  %349 = and i32 %348, 15
  %350 = icmp ugt i32 %349, 15
  %351 = icmp ult i32 %348, 0
  %352 = xor i32 %348, 0
  %353 = and i32 %352, 0
  %354 = icmp slt i32 %353, 0
  store i1 %350, i1* %az
  store i1 %351, i1* %cf
  store i1 %354, i1* %of
  %355 = icmp eq i32 %348, 0
  store i1 %355, i1* %zf
  %356 = icmp slt i32 %348, 0
  store i1 %356, i1* %sf
  %357 = trunc i32 %348 to i8
  %358 = call i8 @llvm.ctpop.i8(i8 %357)
  %359 = and i8 %358, 1
  %360 = icmp eq i8 %359, 0
  store i1 %360, i1* %pf
  store volatile i64 65618, i64* @assembly_address
  %361 = load i1* %zf
  br i1 %361, label %block_100b4, label %block_10054

block_10054:                                      ; preds = %block_10041
  store volatile i64 65620, i64* @assembly_address
  %362 = load i32* %stack_var_-232
  %363 = sub i32 %362, 3
  %364 = and i32 %362, 15
  %365 = sub i32 %364, 3
  %366 = icmp ugt i32 %365, 15
  %367 = icmp ult i32 %362, 3
  %368 = xor i32 %362, 3
  %369 = xor i32 %362, %363
  %370 = and i32 %368, %369
  %371 = icmp slt i32 %370, 0
  store i1 %366, i1* %az
  store i1 %367, i1* %cf
  store i1 %371, i1* %of
  %372 = icmp eq i32 %363, 0
  store i1 %372, i1* %zf
  %373 = icmp slt i32 %363, 0
  store i1 %373, i1* %sf
  %374 = trunc i32 %363 to i8
  %375 = call i8 @llvm.ctpop.i8(i8 %374)
  %376 = and i8 %375, 1
  %377 = icmp eq i8 %376, 0
  store i1 %377, i1* %pf
  store volatile i64 65627, i64* @assembly_address
  %378 = load i1* %zf
  br i1 %378, label %block_10084, label %block_1005d

block_1005d:                                      ; preds = %block_10054
  store volatile i64 65629, i64* @assembly_address
  %379 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %379, i64* %rdx
  store volatile i64 65636, i64* @assembly_address
  %380 = load i8** %stack_var_-240
  %381 = ptrtoint i8* %380 to i64
  store i64 %381, i64* %rax
  store volatile i64 65643, i64* @assembly_address
  %382 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %382, i64* %rsi
  store volatile i64 65646, i64* @assembly_address
  %383 = load i64* %rax
  store i64 %383, i64* %rdi
  store volatile i64 65649, i64* @assembly_address
  %384 = load i64* %rdi
  %385 = inttoptr i64 %384 to i8*
  %386 = load i64* %rsi
  %387 = inttoptr i64 %386 to %stat*
  %388 = call i32 @lstat(i8* %385, %stat* %387)
  %389 = sext i32 %388 to i64
  store i64 %389, i64* %rax
  %390 = sext i32 %388 to i64
  store i64 %390, i64* %rax
  store volatile i64 65654, i64* @assembly_address
  %391 = load i64* %rax
  %392 = trunc i64 %391 to i32
  %393 = load i64* %rax
  %394 = trunc i64 %393 to i32
  %395 = and i32 %392, %394
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %396 = icmp eq i32 %395, 0
  store i1 %396, i1* %zf
  %397 = icmp slt i32 %395, 0
  store i1 %397, i1* %sf
  %398 = trunc i32 %395 to i8
  %399 = call i8 @llvm.ctpop.i8(i8 %398)
  %400 = and i8 %399, 1
  %401 = icmp eq i8 %400, 0
  store i1 %401, i1* %pf
  store volatile i64 65656, i64* @assembly_address
  %402 = load i1* %zf
  br i1 %402, label %block_10084, label %block_1007a

block_1007a:                                      ; preds = %block_1005d
  store volatile i64 65658, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 65663, i64* @assembly_address
  br label %block_10120

block_10084:                                      ; preds = %block_1005d, %block_10054
  store volatile i64 65668, i64* @assembly_address
  %403 = load i64* %stack_var_-224
  store i64 %403, i64* %rax
  store volatile i64 65675, i64* @assembly_address
  %404 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %405 = icmp eq i64 %404, 0
  store i1 %405, i1* %zf
  %406 = icmp slt i64 %404, 0
  store i1 %406, i1* %sf
  %407 = trunc i64 %404 to i8
  %408 = call i8 @llvm.ctpop.i8(i8 %407)
  %409 = and i8 %408, 1
  %410 = icmp eq i8 %409, 0
  store i1 %410, i1* %pf
  store volatile i64 65678, i64* @assembly_address
  %411 = load i1* %zf
  br i1 %411, label %block_100b4, label %block_10090

block_10090:                                      ; preds = %block_10084
  store volatile i64 65680, i64* @assembly_address
  %412 = ptrtoint i64* %stack_var_-224 to i64
  store i64 %412, i64* %rdx
  store volatile i64 65687, i64* @assembly_address
  %413 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %413, i64* %rax
  store volatile i64 65694, i64* @assembly_address
  %414 = ptrtoint i64* %stack_var_-224 to i64
  store i64 %414, i64* %rsi
  store volatile i64 65697, i64* @assembly_address
  %415 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %415, i64* %rdi
  store volatile i64 65700, i64* @assembly_address
  %416 = load i64* %rdi
  %417 = inttoptr i64 %416 to i64*
  %418 = load i64* %rsi
  %419 = inttoptr i64 %418 to i64*
  %420 = load i64* %rdx
  %421 = inttoptr i64 %420 to i64*
  %422 = call i64 @update_timespec(i64* %417, i64* %419, i64* %421)
  store i64 %422, i64* %rax
  store i64 %422, i64* %rax
  store volatile i64 65705, i64* @assembly_address
  %423 = load i64* %rax
  %424 = trunc i64 %423 to i8
  %425 = load i64* %rax
  %426 = trunc i64 %425 to i8
  %427 = and i8 %424, %426
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %428 = icmp eq i8 %427, 0
  store i1 %428, i1* %zf
  %429 = icmp slt i8 %427, 0
  store i1 %429, i1* %sf
  %430 = call i8 @llvm.ctpop.i8(i8 %427)
  %431 = and i8 %430, 1
  %432 = icmp eq i8 %431, 0
  store i1 %432, i1* %pf
  store volatile i64 65707, i64* @assembly_address
  %433 = load i1* %zf
  br i1 %433, label %block_100b4, label %block_100ad

block_100ad:                                      ; preds = %block_10090
  store volatile i64 65709, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 65714, i64* @assembly_address
  br label %block_10120

block_100b4:                                      ; preds = %block_10090, %block_10084, %block_10041
  store volatile i64 65716, i64* @assembly_address
  %434 = load i32* %stack_var_-232
  %435 = and i32 %434, 15
  %436 = icmp ugt i32 %435, 15
  %437 = icmp ult i32 %434, 0
  %438 = xor i32 %434, 0
  %439 = and i32 %438, 0
  %440 = icmp slt i32 %439, 0
  store i1 %436, i1* %az
  store i1 %437, i1* %cf
  store i1 %440, i1* %of
  %441 = icmp eq i32 %434, 0
  store i1 %441, i1* %zf
  %442 = icmp slt i32 %434, 0
  store i1 %442, i1* %sf
  %443 = trunc i32 %434 to i8
  %444 = call i8 @llvm.ctpop.i8(i8 %443)
  %445 = and i8 %444, 1
  %446 = icmp eq i8 %445, 0
  store i1 %446, i1* %pf
  store volatile i64 65723, i64* @assembly_address
  %447 = load i1* %zf
  %448 = icmp eq i1 %447, false
  br i1 %448, label %block_100e1, label %block_100bd

block_100bd:                                      ; preds = %block_100b4
  store volatile i64 65725, i64* @assembly_address
  %449 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %449, i64* %rdx
  store volatile i64 65732, i64* @assembly_address
  %450 = load i8** %stack_var_-240
  %451 = ptrtoint i8* %450 to i64
  store i64 %451, i64* %rax
  store volatile i64 65739, i64* @assembly_address
  %452 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %452, i64* %rsi
  store volatile i64 65742, i64* @assembly_address
  %453 = load i64* %rax
  store i64 %453, i64* %rdi
  store volatile i64 65745, i64* @assembly_address
  %454 = load i64* %rdi
  %455 = inttoptr i64 %454 to i8*
  %456 = load i64* %rsi
  %457 = inttoptr i64 %456 to %stat*
  %458 = call i32 @lstat(i8* %455, %stat* %457)
  %459 = sext i32 %458 to i64
  store i64 %459, i64* %rax
  %460 = sext i32 %458 to i64
  store i64 %460, i64* %rax
  store volatile i64 65750, i64* @assembly_address
  %461 = load i64* %rax
  %462 = trunc i64 %461 to i32
  %463 = load i64* %rax
  %464 = trunc i64 %463 to i32
  %465 = and i32 %462, %464
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %466 = icmp eq i32 %465, 0
  store i1 %466, i1* %zf
  %467 = icmp slt i32 %465, 0
  store i1 %467, i1* %sf
  %468 = trunc i32 %465 to i8
  %469 = call i8 @llvm.ctpop.i8(i8 %468)
  %470 = and i8 %469, 1
  %471 = icmp eq i8 %470, 0
  store i1 %471, i1* %pf
  store volatile i64 65752, i64* @assembly_address
  %472 = load i1* %zf
  br i1 %472, label %block_100e1, label %block_100da

block_100da:                                      ; preds = %block_100bd
  store volatile i64 65754, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 65759, i64* @assembly_address
  br label %block_10120

block_100e1:                                      ; preds = %block_100bd, %block_100b4
  store volatile i64 65761, i64* @assembly_address
  %473 = load i32* %stack_var_-160
  %474 = zext i32 %473 to i64
  store i64 %474, i64* %rax
  store volatile i64 65767, i64* @assembly_address
  %475 = load i64* %rax
  %476 = trunc i64 %475 to i32
  %477 = and i32 %476, 61440
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %478 = icmp eq i32 %477, 0
  store i1 %478, i1* %zf
  %479 = icmp slt i32 %477, 0
  store i1 %479, i1* %sf
  %480 = trunc i32 %477 to i8
  %481 = call i8 @llvm.ctpop.i8(i8 %480)
  %482 = and i8 %481, 1
  %483 = icmp eq i8 %482, 0
  store i1 %483, i1* %pf
  %484 = zext i32 %477 to i64
  store i64 %484, i64* %rax
  store volatile i64 65772, i64* @assembly_address
  %485 = load i64* %rax
  %486 = trunc i64 %485 to i32
  %487 = sub i32 %486, 40960
  %488 = and i32 %486, 15
  %489 = icmp ugt i32 %488, 15
  %490 = icmp ult i32 %486, 40960
  %491 = xor i32 %486, 40960
  %492 = xor i32 %486, %487
  %493 = and i32 %491, %492
  %494 = icmp slt i32 %493, 0
  store i1 %489, i1* %az
  store i1 %490, i1* %cf
  store i1 %494, i1* %of
  %495 = icmp eq i32 %487, 0
  store i1 %495, i1* %zf
  %496 = icmp slt i32 %487, 0
  store i1 %496, i1* %sf
  %497 = trunc i32 %487 to i8
  %498 = call i8 @llvm.ctpop.i8(i8 %497)
  %499 = and i8 %498, 1
  %500 = icmp eq i8 %499, 0
  store i1 %500, i1* %pf
  store volatile i64 65777, i64* @assembly_address
  %501 = load i1* %zf
  br i1 %501, label %block_10110, label %block_100f3

block_100f3:                                      ; preds = %block_100e1
  store volatile i64 65779, i64* @assembly_address
  %502 = load i64* %stack_var_-224
  store i64 %502, i64* %rdx
  store volatile i64 65786, i64* @assembly_address
  %503 = load i8** %stack_var_-240
  %504 = ptrtoint i8* %503 to i64
  store i64 %504, i64* %rax
  store volatile i64 65793, i64* @assembly_address
  %505 = load i64* %rax
  store i64 %505, i64* %rsi
  store volatile i64 65796, i64* @assembly_address
  store i64 4294967295, i64* %rdi
  store volatile i64 65801, i64* @assembly_address
  %506 = load i64* %rdi
  %507 = load i64* %rsi
  %508 = inttoptr i64 %507 to i64*
  %509 = load i64* %rdx
  %510 = inttoptr i64 %509 to i64*
  %511 = trunc i64 %506 to i32
  %512 = call i64 @fdutimens(i32 %511, i64* %508, i64* %510)
  store i64 %512, i64* %rax
  store i64 %512, i64* %rax
  store volatile i64 65806, i64* @assembly_address
  br label %block_10120

block_10110:                                      ; preds = %block_100e1
  store volatile i64 65808, i64* @assembly_address
  %513 = call i32* @__errno_location()
  %514 = ptrtoint i32* %513 to i64
  store i64 %514, i64* %rax
  %515 = ptrtoint i32* %513 to i64
  store i64 %515, i64* %rax
  %516 = ptrtoint i32* %513 to i64
  store i64 %516, i64* %rax
  store volatile i64 65813, i64* @assembly_address
  %517 = load i64* %rax
  %518 = inttoptr i64 %517 to i32*
  store i32 38, i32* %518
  store volatile i64 65819, i64* @assembly_address
  store i64 4294967295, i64* %rax
  br label %block_10120

block_10120:                                      ; preds = %block_10110, %block_100f3, %block_100da, %block_100ad, %block_1007a, %block_10022, %block_ff58, %block_ff16
  store volatile i64 65824, i64* @assembly_address
  %519 = load i64* %stack_var_-32
  store i64 %519, i64* %rcx
  store volatile i64 65828, i64* @assembly_address
  %520 = load i64* %rcx
  %521 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %522 = xor i64 %520, %521
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %523 = icmp eq i64 %522, 0
  store i1 %523, i1* %zf
  %524 = icmp slt i64 %522, 0
  store i1 %524, i1* %sf
  %525 = trunc i64 %522 to i8
  %526 = call i8 @llvm.ctpop.i8(i8 %525)
  %527 = and i8 %526, 1
  %528 = icmp eq i8 %527, 0
  store i1 %528, i1* %pf
  store i64 %522, i64* %rcx
  store volatile i64 65837, i64* @assembly_address
  %529 = load i1* %zf
  br i1 %529, label %block_10134, label %block_1012f

block_1012f:                                      ; preds = %block_10120
  store volatile i64 65839, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_10134:                                      ; preds = %block_10120
  store volatile i64 65844, i64* @assembly_address
  %530 = load i64* %rsp
  %531 = add i64 %530, 232
  %532 = and i64 %530, 15
  %533 = add i64 %532, 8
  %534 = icmp ugt i64 %533, 15
  %535 = icmp ult i64 %531, %530
  %536 = xor i64 %530, %531
  %537 = xor i64 232, %531
  %538 = and i64 %536, %537
  %539 = icmp slt i64 %538, 0
  store i1 %534, i1* %az
  store i1 %535, i1* %cf
  store i1 %539, i1* %of
  %540 = icmp eq i64 %531, 0
  store i1 %540, i1* %zf
  %541 = icmp slt i64 %531, 0
  store i1 %541, i1* %sf
  %542 = trunc i64 %531 to i8
  %543 = call i8 @llvm.ctpop.i8(i8 %542)
  %544 = and i8 %543, 1
  %545 = icmp eq i8 %544, 0
  store i1 %545, i1* %pf
  %546 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %546, i64* %rsp
  store volatile i64 65851, i64* @assembly_address
  %547 = load i64* %stack_var_-16
  store i64 %547, i64* %rbx
  %548 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %548, i64* %rsp
  store volatile i64 65852, i64* @assembly_address
  %549 = load i64* %stack_var_-8
  store i64 %549, i64* %rbp
  %550 = ptrtoint i64* %stack_var_0 to i64
  store i64 %550, i64* %rsp
  store volatile i64 65853, i64* @assembly_address
  %551 = load i64* %rax
  ret i64 %551
}

declare i64 @281(i64, i64*)

declare i64 @282(i64, i64)

define i64 @xnmalloc(i128 %arg1, i64 %arg2) {
block_1013e:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = trunc i128 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i128
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 65854, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 65855, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 65858, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 65862, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = sext i64 %21 to i128
  store i128 %22, i128* %stack_var_-16
  store volatile i64 65866, i64* @assembly_address
  %23 = load i64* %rsi
  store i64 %23, i64* %stack_var_-24
  store volatile i64 65870, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 65875, i64* @assembly_address
  %24 = load i128* %stack_var_-16
  %25 = trunc i128 %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 65879, i64* @assembly_address
  %26 = load i64* %stack_var_-24
  %27 = load i64* %rax
  %28 = zext i64 %26 to i128
  %29 = zext i64 %27 to i128
  %30 = mul i128 %28, %29
  %31 = trunc i128 %30 to i64
  %32 = lshr i128 %30, 64
  %33 = trunc i128 %32 to i64
  %34 = icmp ne i64 %33, 0
  store i64 %31, i64* %rax
  store i64 %33, i64* %rdx
  store i1 %34, i1* %of
  store i1 %34, i1* %cf
  store volatile i64 65883, i64* @assembly_address
  %35 = load i1* %of
  %36 = icmp eq i1 %35, false
  br i1 %36, label %block_10162, label %block_1015d

block_1015d:                                      ; preds = %block_1013e
  store volatile i64 65885, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_10162

block_10162:                                      ; preds = %block_1015d, %block_1013e
  store volatile i64 65890, i64* @assembly_address
  %37 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %38 = icmp eq i64 %37, 0
  store i1 %38, i1* %zf
  %39 = icmp slt i64 %37, 0
  store i1 %39, i1* %sf
  %40 = trunc i64 %37 to i8
  %41 = call i8 @llvm.ctpop.i8(i8 %40)
  %42 = and i8 %41, 1
  %43 = icmp eq i8 %42, 0
  store i1 %43, i1* %pf
  store volatile i64 65893, i64* @assembly_address
  %44 = load i1* %sf
  %45 = icmp eq i1 %44, false
  br i1 %45, label %block_1016c, label %block_10167

block_10167:                                      ; preds = %block_10162
  store volatile i64 65895, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_1016c

block_1016c:                                      ; preds = %block_10167, %block_10162
  store volatile i64 65900, i64* @assembly_address
  %46 = load i64* %rcx
  store i64 %46, i64* %rax
  store volatile i64 65903, i64* @assembly_address
  %47 = load i64* %rax
  %48 = trunc i64 %47 to i32
  %49 = and i32 %48, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %50 = icmp eq i32 %49, 0
  store i1 %50, i1* %zf
  %51 = icmp slt i32 %49, 0
  store i1 %51, i1* %sf
  %52 = trunc i32 %49 to i8
  %53 = call i8 @llvm.ctpop.i8(i8 %52)
  %54 = and i8 %53, 1
  %55 = icmp eq i8 %54, 0
  store i1 %55, i1* %pf
  %56 = zext i32 %49 to i64
  store i64 %56, i64* %rax
  store volatile i64 65906, i64* @assembly_address
  %57 = load i64* %rax
  %58 = trunc i64 %57 to i8
  %59 = load i64* %rax
  %60 = trunc i64 %59 to i8
  %61 = and i8 %58, %60
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %62 = icmp eq i8 %61, 0
  store i1 %62, i1* %zf
  %63 = icmp slt i8 %61, 0
  store i1 %63, i1* %sf
  %64 = call i8 @llvm.ctpop.i8(i8 %61)
  %65 = and i8 %64, 1
  %66 = icmp eq i8 %65, 0
  store i1 %66, i1* %pf
  store volatile i64 65908, i64* @assembly_address
  %67 = load i1* %zf
  br i1 %67, label %block_1017b, label %block_10176

block_10176:                                      ; preds = %block_1016c
  store volatile i64 65910, i64* @assembly_address
  %68 = call i64 @xalloc_die()
  store i64 %68, i64* %rax
  store i64 %68, i64* %rax
  store i64 %68, i64* %rax
  unreachable

block_1017b:                                      ; preds = %block_1016c
  store volatile i64 65915, i64* @assembly_address
  %69 = load i128* %stack_var_-16
  %70 = trunc i128 %69 to i64
  store i64 %70, i64* %rax
  store volatile i64 65919, i64* @assembly_address
  %71 = load i64* %rax
  %72 = load i64* %stack_var_-24
  %73 = sext i64 %71 to i128
  %74 = sext i64 %72 to i128
  %75 = mul i128 %73, %74
  %76 = trunc i128 %75 to i64
  store i64 %76, i64* %rax
  %77 = trunc i128 %75 to i64
  %78 = sext i64 %77 to i128
  %79 = icmp ne i128 %75, %78
  store i1 %79, i1* %of
  store i1 %79, i1* %cf
  store volatile i64 65924, i64* @assembly_address
  %80 = load i64* %rax
  store i64 %80, i64* %rdi
  store volatile i64 65927, i64* @assembly_address
  %81 = load i64* %rdi
  %82 = trunc i64 %81 to i32
  %83 = call i64 @xmalloc(i32 %82)
  store i64 %83, i64* %rax
  store i64 %83, i64* %rax
  store volatile i64 65932, i64* @assembly_address
  %84 = load i64* %stack_var_-8
  store i64 %84, i64* %rbp
  %85 = ptrtoint i64* %stack_var_0 to i64
  store i64 %85, i64* %rsp
  store volatile i64 65933, i64* @assembly_address
  %86 = load i64* %rax
  ret i64 %86
}

declare i64 @283(i64, i64)

define i64 @xnrealloc(i64 %arg1, i128 %arg2, i64 %arg3) {
block_1018e:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  %0 = trunc i128 %arg2 to i64
  store i64 %0, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-24 = alloca i128
  %1 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 65934, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 65935, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 65938, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 65942, i64* @assembly_address
  %21 = load i64* %rdi
  store i64 %21, i64* %stack_var_-16
  store volatile i64 65946, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = sext i64 %22 to i128
  store i128 %23, i128* %stack_var_-24
  store volatile i64 65950, i64* @assembly_address
  %24 = load i64* %rdx
  store i64 %24, i64* %stack_var_-32
  store volatile i64 65954, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 65959, i64* @assembly_address
  %25 = load i128* %stack_var_-24
  %26 = trunc i128 %25 to i64
  store i64 %26, i64* %rax
  store volatile i64 65963, i64* @assembly_address
  %27 = load i64* %stack_var_-32
  %28 = load i64* %rax
  %29 = zext i64 %27 to i128
  %30 = zext i64 %28 to i128
  %31 = mul i128 %29, %30
  %32 = trunc i128 %31 to i64
  %33 = lshr i128 %31, 64
  %34 = trunc i128 %33 to i64
  %35 = icmp ne i64 %34, 0
  store i64 %32, i64* %rax
  store i64 %34, i64* %rdx
  store i1 %35, i1* %of
  store i1 %35, i1* %cf
  store volatile i64 65967, i64* @assembly_address
  %36 = load i1* %of
  %37 = icmp eq i1 %36, false
  br i1 %37, label %block_101b6, label %block_101b1

block_101b1:                                      ; preds = %block_1018e
  store volatile i64 65969, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_101b6

block_101b6:                                      ; preds = %block_101b1, %block_1018e
  store volatile i64 65974, i64* @assembly_address
  %38 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %39 = icmp eq i64 %38, 0
  store i1 %39, i1* %zf
  %40 = icmp slt i64 %38, 0
  store i1 %40, i1* %sf
  %41 = trunc i64 %38 to i8
  %42 = call i8 @llvm.ctpop.i8(i8 %41)
  %43 = and i8 %42, 1
  %44 = icmp eq i8 %43, 0
  store i1 %44, i1* %pf
  store volatile i64 65977, i64* @assembly_address
  %45 = load i1* %sf
  %46 = icmp eq i1 %45, false
  br i1 %46, label %block_101c0, label %block_101bb

block_101bb:                                      ; preds = %block_101b6
  store volatile i64 65979, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_101c0

block_101c0:                                      ; preds = %block_101bb, %block_101b6
  store volatile i64 65984, i64* @assembly_address
  %47 = load i64* %rcx
  store i64 %47, i64* %rax
  store volatile i64 65987, i64* @assembly_address
  %48 = load i64* %rax
  %49 = trunc i64 %48 to i32
  %50 = and i32 %49, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %51 = icmp eq i32 %50, 0
  store i1 %51, i1* %zf
  %52 = icmp slt i32 %50, 0
  store i1 %52, i1* %sf
  %53 = trunc i32 %50 to i8
  %54 = call i8 @llvm.ctpop.i8(i8 %53)
  %55 = and i8 %54, 1
  %56 = icmp eq i8 %55, 0
  store i1 %56, i1* %pf
  %57 = zext i32 %50 to i64
  store i64 %57, i64* %rax
  store volatile i64 65990, i64* @assembly_address
  %58 = load i64* %rax
  %59 = trunc i64 %58 to i8
  %60 = load i64* %rax
  %61 = trunc i64 %60 to i8
  %62 = and i8 %59, %61
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %63 = icmp eq i8 %62, 0
  store i1 %63, i1* %zf
  %64 = icmp slt i8 %62, 0
  store i1 %64, i1* %sf
  %65 = call i8 @llvm.ctpop.i8(i8 %62)
  %66 = and i8 %65, 1
  %67 = icmp eq i8 %66, 0
  store i1 %67, i1* %pf
  store volatile i64 65992, i64* @assembly_address
  %68 = load i1* %zf
  br i1 %68, label %block_101cf, label %block_101ca

block_101ca:                                      ; preds = %block_101c0
  store volatile i64 65994, i64* @assembly_address
  %69 = call i64 @xalloc_die()
  store i64 %69, i64* %rax
  store i64 %69, i64* %rax
  store i64 %69, i64* %rax
  unreachable

block_101cf:                                      ; preds = %block_101c0
  store volatile i64 65999, i64* @assembly_address
  %70 = load i128* %stack_var_-24
  %71 = trunc i128 %70 to i64
  store i64 %71, i64* %rax
  store volatile i64 66003, i64* @assembly_address
  %72 = load i64* %rax
  %73 = load i64* %stack_var_-32
  %74 = sext i64 %72 to i128
  %75 = sext i64 %73 to i128
  %76 = mul i128 %74, %75
  %77 = trunc i128 %76 to i64
  store i64 %77, i64* %rax
  %78 = trunc i128 %76 to i64
  %79 = sext i64 %78 to i128
  %80 = icmp ne i128 %76, %79
  store i1 %80, i1* %of
  store i1 %80, i1* %cf
  store volatile i64 66008, i64* @assembly_address
  %81 = load i64* %rax
  store i64 %81, i64* %rdx
  store volatile i64 66011, i64* @assembly_address
  %82 = load i64* %stack_var_-16
  store i64 %82, i64* %rax
  store volatile i64 66015, i64* @assembly_address
  %83 = load i64* %rdx
  store i64 %83, i64* %rsi
  store volatile i64 66018, i64* @assembly_address
  %84 = load i64* %rax
  store i64 %84, i64* %rdi
  store volatile i64 66021, i64* @assembly_address
  %85 = load i64* %rdi
  %86 = load i64* %rsi
  %87 = inttoptr i64 %85 to i64*
  %88 = call i64 @xrealloc(i64* %87, i64 %86)
  store i64 %88, i64* %rax
  store i64 %88, i64* %rax
  store volatile i64 66026, i64* @assembly_address
  %89 = load i64* %stack_var_-8
  store i64 %89, i64* %rbp
  %90 = ptrtoint i64* %stack_var_0 to i64
  store i64 %90, i64* %rsp
  store volatile i64 66027, i64* @assembly_address
  %91 = load i64* %rax
  ret i64 %91
}

declare i64 @284(i64, i64, i64)

define i64 @x2nrealloc(i64 %arg1, i64* %arg2, i128 %arg3) {
block_101ec:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = trunc i128 %arg3 to i64
  store i64 %0, i64* %rdx
  %1 = ptrtoint i64* %arg2 to i64
  store i64 %1, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-48 = alloca i128
  %2 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66028, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 66029, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 66032, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 48
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 48
  %11 = xor i64 %6, 48
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %21, i64* %rsp
  store volatile i64 66036, i64* @assembly_address
  %22 = load i64* %rdi
  store i64 %22, i64* %stack_var_-32
  store volatile i64 66040, i64* @assembly_address
  %23 = load i64* %rsi
  store i64 %23, i64* %stack_var_-40
  store volatile i64 66044, i64* @assembly_address
  %24 = load i64* %rdx
  %25 = sext i64 %24 to i128
  store i128 %25, i128* %stack_var_-48
  store volatile i64 66048, i64* @assembly_address
  %26 = load i64* %stack_var_-40
  store i64 %26, i64* %rax
  store volatile i64 66052, i64* @assembly_address
  %27 = load i64* %rax
  %28 = inttoptr i64 %27 to i64*
  %29 = load i64* %28
  store i64 %29, i64* %rax
  store volatile i64 66055, i64* @assembly_address
  %30 = load i64* %rax
  store i64 %30, i64* %stack_var_-16
  store volatile i64 66059, i64* @assembly_address
  %31 = load i64* %stack_var_-32
  %32 = and i64 %31, 15
  %33 = icmp ugt i64 %32, 15
  %34 = icmp ult i64 %31, 0
  %35 = xor i64 %31, 0
  %36 = and i64 %35, 0
  %37 = icmp slt i64 %36, 0
  store i1 %33, i1* %az
  store i1 %34, i1* %cf
  store i1 %37, i1* %of
  %38 = icmp eq i64 %31, 0
  store i1 %38, i1* %zf
  %39 = icmp slt i64 %31, 0
  store i1 %39, i1* %sf
  %40 = trunc i64 %31 to i8
  %41 = call i8 @llvm.ctpop.i8(i8 %40)
  %42 = and i8 %41, 1
  %43 = icmp eq i8 %42, 0
  store i1 %43, i1* %pf
  store volatile i64 66064, i64* @assembly_address
  %44 = load i1* %zf
  %45 = icmp eq i1 %44, false
  br i1 %45, label %block_10267, label %block_10212

block_10212:                                      ; preds = %block_101ec
  store volatile i64 66066, i64* @assembly_address
  %46 = load i64* %stack_var_-16
  %47 = and i64 %46, 15
  %48 = icmp ugt i64 %47, 15
  %49 = icmp ult i64 %46, 0
  %50 = xor i64 %46, 0
  %51 = and i64 %50, 0
  %52 = icmp slt i64 %51, 0
  store i1 %48, i1* %az
  store i1 %49, i1* %cf
  store i1 %52, i1* %of
  %53 = icmp eq i64 %46, 0
  store i1 %53, i1* %zf
  %54 = icmp slt i64 %46, 0
  store i1 %54, i1* %sf
  %55 = trunc i64 %46 to i8
  %56 = call i8 @llvm.ctpop.i8(i8 %55)
  %57 = and i8 %56, 1
  %58 = icmp eq i8 %57, 0
  store i1 %58, i1* %pf
  store volatile i64 66071, i64* @assembly_address
  %59 = load i1* %zf
  %60 = icmp eq i1 %59, false
  br i1 %60, label %block_1023a, label %block_10219

block_10219:                                      ; preds = %block_10212
  store volatile i64 66073, i64* @assembly_address
  store i64 128, i64* %rax
  store volatile i64 66078, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 66083, i64* @assembly_address
  %61 = load i128* %stack_var_-48
  %62 = trunc i128 %61 to i64
  %63 = load i64* %rax
  %64 = zext i64 %63 to i128
  %65 = load i64* %rdx
  %66 = zext i64 %65 to i128
  %67 = shl i128 %66, 64
  %68 = or i128 %67, %64
  %69 = zext i64 %62 to i128
  %70 = udiv i128 %68, %69
  %71 = trunc i128 %70 to i64
  store i64 %71, i64* %rax
  %72 = urem i128 %68, %69
  %73 = trunc i128 %72 to i64
  store i64 %73, i64* %rdx
  store volatile i64 66087, i64* @assembly_address
  %74 = load i64* %rax
  store i64 %74, i64* %stack_var_-16
  store volatile i64 66091, i64* @assembly_address
  %75 = load i64* %stack_var_-16
  %76 = and i64 %75, 15
  %77 = icmp ugt i64 %76, 15
  %78 = icmp ult i64 %75, 0
  %79 = xor i64 %75, 0
  %80 = and i64 %79, 0
  %81 = icmp slt i64 %80, 0
  store i1 %77, i1* %az
  store i1 %78, i1* %cf
  store i1 %81, i1* %of
  %82 = icmp eq i64 %75, 0
  store i1 %82, i1* %zf
  %83 = icmp slt i64 %75, 0
  store i1 %83, i1* %sf
  %84 = trunc i64 %75 to i8
  %85 = call i8 @llvm.ctpop.i8(i8 %84)
  %86 = and i8 %85, 1
  %87 = icmp eq i8 %86, 0
  store i1 %87, i1* %pf
  store volatile i64 66096, i64* @assembly_address
  %88 = load i1* %zf
  %89 = zext i1 %88 to i8
  %90 = zext i8 %89 to i64
  %91 = load i64* %rax
  %92 = and i64 %91, -256
  %93 = or i64 %92, %90
  store i64 %93, i64* %rax
  store volatile i64 66099, i64* @assembly_address
  %94 = load i64* %rax
  %95 = trunc i64 %94 to i8
  %96 = zext i8 %95 to i64
  store i64 %96, i64* %rax
  store volatile i64 66102, i64* @assembly_address
  %97 = load i64* %stack_var_-16
  %98 = load i64* %rax
  %99 = add i64 %97, %98
  %100 = and i64 %97, 15
  %101 = and i64 %98, 15
  %102 = add i64 %100, %101
  %103 = icmp ugt i64 %102, 15
  %104 = icmp ult i64 %99, %97
  %105 = xor i64 %97, %99
  %106 = xor i64 %98, %99
  %107 = and i64 %105, %106
  %108 = icmp slt i64 %107, 0
  store i1 %103, i1* %az
  store i1 %104, i1* %cf
  store i1 %108, i1* %of
  %109 = icmp eq i64 %99, 0
  store i1 %109, i1* %zf
  %110 = icmp slt i64 %99, 0
  store i1 %110, i1* %sf
  %111 = trunc i64 %99 to i8
  %112 = call i8 @llvm.ctpop.i8(i8 %111)
  %113 = and i8 %112, 1
  %114 = icmp eq i8 %113, 0
  store i1 %114, i1* %pf
  store i64 %99, i64* %stack_var_-16
  br label %block_1023a

block_1023a:                                      ; preds = %block_10219, %block_10212
  store volatile i64 66106, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 66111, i64* @assembly_address
  %115 = load i64* %stack_var_-16
  store i64 %115, i64* %rax
  store volatile i64 66115, i64* @assembly_address
  %116 = load i128* %stack_var_-48
  %117 = trunc i128 %116 to i64
  %118 = load i64* %rax
  %119 = zext i64 %117 to i128
  %120 = zext i64 %118 to i128
  %121 = mul i128 %119, %120
  %122 = trunc i128 %121 to i64
  %123 = lshr i128 %121, 64
  %124 = trunc i128 %123 to i64
  %125 = icmp ne i64 %124, 0
  store i64 %122, i64* %rax
  store i64 %124, i64* %rdx
  store i1 %125, i1* %of
  store i1 %125, i1* %cf
  store volatile i64 66119, i64* @assembly_address
  %126 = load i1* %of
  %127 = icmp eq i1 %126, false
  br i1 %127, label %block_1024e, label %block_10249

block_10249:                                      ; preds = %block_1023a
  store volatile i64 66121, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_1024e

block_1024e:                                      ; preds = %block_10249, %block_1023a
  store volatile i64 66126, i64* @assembly_address
  %128 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %129 = icmp eq i64 %128, 0
  store i1 %129, i1* %zf
  %130 = icmp slt i64 %128, 0
  store i1 %130, i1* %sf
  %131 = trunc i64 %128 to i8
  %132 = call i8 @llvm.ctpop.i8(i8 %131)
  %133 = and i8 %132, 1
  %134 = icmp eq i8 %133, 0
  store i1 %134, i1* %pf
  store volatile i64 66129, i64* @assembly_address
  %135 = load i1* %sf
  %136 = icmp eq i1 %135, false
  br i1 %136, label %block_10258, label %block_10253

block_10253:                                      ; preds = %block_1024e
  store volatile i64 66131, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_10258

block_10258:                                      ; preds = %block_10253, %block_1024e
  store volatile i64 66136, i64* @assembly_address
  %137 = load i64* %rcx
  store i64 %137, i64* %rax
  store volatile i64 66139, i64* @assembly_address
  %138 = load i64* %rax
  %139 = trunc i64 %138 to i32
  %140 = and i32 %139, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %141 = icmp eq i32 %140, 0
  store i1 %141, i1* %zf
  %142 = icmp slt i32 %140, 0
  store i1 %142, i1* %sf
  %143 = trunc i32 %140 to i8
  %144 = call i8 @llvm.ctpop.i8(i8 %143)
  %145 = and i8 %144, 1
  %146 = icmp eq i8 %145, 0
  store i1 %146, i1* %pf
  %147 = zext i32 %140 to i64
  store i64 %147, i64* %rax
  store volatile i64 66142, i64* @assembly_address
  %148 = load i64* %rax
  %149 = trunc i64 %148 to i8
  %150 = load i64* %rax
  %151 = trunc i64 %150 to i8
  %152 = and i8 %149, %151
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %153 = icmp eq i8 %152, 0
  store i1 %153, i1* %zf
  %154 = icmp slt i8 %152, 0
  store i1 %154, i1* %sf
  %155 = call i8 @llvm.ctpop.i8(i8 %152)
  %156 = and i8 %155, 1
  %157 = icmp eq i8 %156, 0
  store i1 %157, i1* %pf
  store volatile i64 66144, i64* @assembly_address
  %158 = load i1* %zf
  br i1 %158, label %block_1029e, label %block_10262

block_10262:                                      ; preds = %block_10258
  store volatile i64 66146, i64* @assembly_address
  %159 = call i64 @xalloc_die()
  store i64 %159, i64* %rax
  store i64 %159, i64* %rax
  store i64 %159, i64* %rax
  unreachable

block_10267:                                      ; preds = %block_101ec
  store volatile i64 66151, i64* @assembly_address
  store i64 6148914691236517204, i64* %rax
  store volatile i64 66161, i64* @assembly_address
  store i64 0, i64* %rdx
  store volatile i64 66166, i64* @assembly_address
  %160 = load i128* %stack_var_-48
  %161 = trunc i128 %160 to i64
  %162 = load i64* %rax
  %163 = zext i64 %162 to i128
  %164 = load i64* %rdx
  %165 = zext i64 %164 to i128
  %166 = shl i128 %165, 64
  %167 = or i128 %166, %163
  %168 = zext i64 %161 to i128
  %169 = udiv i128 %167, %168
  %170 = trunc i128 %169 to i64
  store i64 %170, i64* %rax
  %171 = urem i128 %167, %168
  %172 = trunc i128 %171 to i64
  store i64 %172, i64* %rdx
  store volatile i64 66170, i64* @assembly_address
  %173 = load i64* %stack_var_-16
  %174 = load i64* %rax
  %175 = sub i64 %173, %174
  %176 = and i64 %173, 15
  %177 = and i64 %174, 15
  %178 = sub i64 %176, %177
  %179 = icmp ugt i64 %178, 15
  %180 = icmp ult i64 %173, %174
  %181 = xor i64 %173, %174
  %182 = xor i64 %173, %175
  %183 = and i64 %181, %182
  %184 = icmp slt i64 %183, 0
  store i1 %179, i1* %az
  store i1 %180, i1* %cf
  store i1 %184, i1* %of
  %185 = icmp eq i64 %175, 0
  store i1 %185, i1* %zf
  %186 = icmp slt i64 %175, 0
  store i1 %186, i1* %sf
  %187 = trunc i64 %175 to i8
  %188 = call i8 @llvm.ctpop.i8(i8 %187)
  %189 = and i8 %188, 1
  %190 = icmp eq i8 %189, 0
  store i1 %190, i1* %pf
  store volatile i64 66174, i64* @assembly_address
  %191 = load i1* %cf
  br i1 %191, label %block_10285, label %block_10280

block_10280:                                      ; preds = %block_10267
  store volatile i64 66176, i64* @assembly_address
  %192 = call i64 @xalloc_die()
  store i64 %192, i64* %rax
  store i64 %192, i64* %rax
  store i64 %192, i64* %rax
  unreachable

block_10285:                                      ; preds = %block_10267
  store volatile i64 66181, i64* @assembly_address
  %193 = load i64* %stack_var_-16
  store i64 %193, i64* %rax
  store volatile i64 66185, i64* @assembly_address
  %194 = load i64* %rax
  %195 = load i1* %of
  %196 = lshr i64 %194, 1
  %197 = icmp eq i64 %196, 0
  store i1 %197, i1* %zf
  %198 = icmp slt i64 %196, 0
  store i1 %198, i1* %sf
  %199 = trunc i64 %196 to i8
  %200 = call i8 @llvm.ctpop.i8(i8 %199)
  %201 = and i8 %200, 1
  %202 = icmp eq i8 %201, 0
  store i1 %202, i1* %pf
  store i64 %196, i64* %rax
  %203 = and i64 1, %194
  %204 = icmp ne i64 %203, 0
  store i1 %204, i1* %cf
  %205 = icmp slt i64 %194, 0
  %206 = select i1 true, i1 %205, i1 %195
  store i1 %206, i1* %of
  store volatile i64 66188, i64* @assembly_address
  %207 = load i64* %rax
  store i64 %207, i64* %rdx
  store volatile i64 66191, i64* @assembly_address
  %208 = load i64* %stack_var_-16
  store i64 %208, i64* %rax
  store volatile i64 66195, i64* @assembly_address
  %209 = load i64* %rax
  %210 = load i64* %rdx
  %211 = add i64 %209, %210
  %212 = and i64 %209, 15
  %213 = and i64 %210, 15
  %214 = add i64 %212, %213
  %215 = icmp ugt i64 %214, 15
  %216 = icmp ult i64 %211, %209
  %217 = xor i64 %209, %211
  %218 = xor i64 %210, %211
  %219 = and i64 %217, %218
  %220 = icmp slt i64 %219, 0
  store i1 %215, i1* %az
  store i1 %216, i1* %cf
  store i1 %220, i1* %of
  %221 = icmp eq i64 %211, 0
  store i1 %221, i1* %zf
  %222 = icmp slt i64 %211, 0
  store i1 %222, i1* %sf
  %223 = trunc i64 %211 to i8
  %224 = call i8 @llvm.ctpop.i8(i8 %223)
  %225 = and i8 %224, 1
  %226 = icmp eq i8 %225, 0
  store i1 %226, i1* %pf
  store i64 %211, i64* %rax
  store volatile i64 66198, i64* @assembly_address
  %227 = load i64* %rax
  %228 = add i64 %227, 1
  %229 = and i64 %227, 15
  %230 = add i64 %229, 1
  %231 = icmp ugt i64 %230, 15
  %232 = icmp ult i64 %228, %227
  %233 = xor i64 %227, %228
  %234 = xor i64 1, %228
  %235 = and i64 %233, %234
  %236 = icmp slt i64 %235, 0
  store i1 %231, i1* %az
  store i1 %232, i1* %cf
  store i1 %236, i1* %of
  %237 = icmp eq i64 %228, 0
  store i1 %237, i1* %zf
  %238 = icmp slt i64 %228, 0
  store i1 %238, i1* %sf
  %239 = trunc i64 %228 to i8
  %240 = call i8 @llvm.ctpop.i8(i8 %239)
  %241 = and i8 %240, 1
  %242 = icmp eq i8 %241, 0
  store i1 %242, i1* %pf
  store i64 %228, i64* %rax
  store volatile i64 66202, i64* @assembly_address
  %243 = load i64* %rax
  store i64 %243, i64* %stack_var_-16
  br label %block_1029e

block_1029e:                                      ; preds = %block_10285, %block_10258
  store volatile i64 66206, i64* @assembly_address
  %244 = load i64* %stack_var_-40
  store i64 %244, i64* %rax
  store volatile i64 66210, i64* @assembly_address
  %245 = load i64* %stack_var_-16
  store i64 %245, i64* %rdx
  store volatile i64 66214, i64* @assembly_address
  %246 = load i64* %rdx
  %247 = load i64* %rax
  %248 = inttoptr i64 %247 to i64*
  store i64 %246, i64* %248
  store volatile i64 66217, i64* @assembly_address
  %249 = load i64* %stack_var_-16
  store i64 %249, i64* %rax
  store volatile i64 66221, i64* @assembly_address
  %250 = load i64* %rax
  %251 = load i128* %stack_var_-48
  %252 = trunc i128 %251 to i64
  %253 = sext i64 %250 to i128
  %254 = sext i64 %252 to i128
  %255 = mul i128 %253, %254
  %256 = trunc i128 %255 to i64
  store i64 %256, i64* %rax
  %257 = trunc i128 %255 to i64
  %258 = sext i64 %257 to i128
  %259 = icmp ne i128 %255, %258
  store i1 %259, i1* %of
  store i1 %259, i1* %cf
  store volatile i64 66226, i64* @assembly_address
  %260 = load i64* %rax
  store i64 %260, i64* %rdx
  store volatile i64 66229, i64* @assembly_address
  %261 = load i64* %stack_var_-32
  store i64 %261, i64* %rax
  store volatile i64 66233, i64* @assembly_address
  %262 = load i64* %rdx
  store i64 %262, i64* %rsi
  store volatile i64 66236, i64* @assembly_address
  %263 = load i64* %rax
  store i64 %263, i64* %rdi
  store volatile i64 66239, i64* @assembly_address
  %264 = load i64* %rdi
  %265 = load i64* %rsi
  %266 = inttoptr i64 %264 to i64*
  %267 = call i64 @xrealloc(i64* %266, i64 %265)
  store i64 %267, i64* %rax
  store i64 %267, i64* %rax
  store volatile i64 66244, i64* @assembly_address
  %268 = load i64* %stack_var_-8
  store i64 %268, i64* %rbp
  %269 = ptrtoint i64* %stack_var_0 to i64
  store i64 %269, i64* %rsp
  store volatile i64 66245, i64* @assembly_address
  %270 = load i64* %rax
  ret i64 %270
}

declare i64 @285(i64, i64*, i64)

define i64 @xcharalloc(i64 %arg1) {
block_102c6:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66246, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 66247, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 66250, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 16
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 16
  %8 = xor i64 %3, 16
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %18, i64* %rsp
  store volatile i64 66254, i64* @assembly_address
  %19 = load i64* %rdi
  store i64 %19, i64* %stack_var_-16
  store volatile i64 66258, i64* @assembly_address
  %20 = load i64* %stack_var_-16
  store i64 %20, i64* %rax
  store volatile i64 66262, i64* @assembly_address
  %21 = load i64* %rax
  store i64 %21, i64* %rdi
  store volatile i64 66265, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = trunc i64 %22 to i32
  %24 = call i64 @xmalloc(i32 %23)
  store i64 %24, i64* %rax
  store i64 %24, i64* %rax
  store volatile i64 66270, i64* @assembly_address
  %25 = load i64* %stack_var_-8
  store i64 %25, i64* %rbp
  %26 = ptrtoint i64* %stack_var_0 to i64
  store i64 %26, i64* %rsp
  store volatile i64 66271, i64* @assembly_address
  %27 = load i64* %rax
  ret i64 %27
}

define i64 @xmalloc(i32 %arg1) {
block_102e0:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-32 = alloca i32
  %1 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66272, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 66273, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 66276, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 66280, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = trunc i64 %21 to i32
  store i32 %22, i32* %stack_var_-32
  store volatile i64 66284, i64* @assembly_address
  %23 = load i32* %stack_var_-32
  %24 = sext i32 %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 66288, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rdi
  store volatile i64 66291, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = trunc i64 %26 to i32
  %28 = call i64* @malloc(i32 %27)
  %29 = ptrtoint i64* %28 to i64
  store i64 %29, i64* %rax
  %30 = ptrtoint i64* %28 to i64
  store i64 %30, i64* %rax
  store volatile i64 66296, i64* @assembly_address
  %31 = load i64* %rax
  store i64 %31, i64* %stack_var_-16
  store volatile i64 66300, i64* @assembly_address
  %32 = load i64* %stack_var_-16
  %33 = and i64 %32, 15
  %34 = icmp ugt i64 %33, 15
  %35 = icmp ult i64 %32, 0
  %36 = xor i64 %32, 0
  %37 = and i64 %36, 0
  %38 = icmp slt i64 %37, 0
  store i1 %34, i1* %az
  store i1 %35, i1* %cf
  store i1 %38, i1* %of
  %39 = icmp eq i64 %32, 0
  store i1 %39, i1* %zf
  %40 = icmp slt i64 %32, 0
  store i1 %40, i1* %sf
  %41 = trunc i64 %32 to i8
  %42 = call i8 @llvm.ctpop.i8(i8 %41)
  %43 = and i8 %42, 1
  %44 = icmp eq i8 %43, 0
  store i1 %44, i1* %pf
  store volatile i64 66305, i64* @assembly_address
  %45 = load i1* %zf
  %46 = icmp eq i1 %45, false
  br i1 %46, label %block_1030f, label %block_10303

block_10303:                                      ; preds = %block_102e0
  store volatile i64 66307, i64* @assembly_address
  %47 = load i32* %stack_var_-32
  %48 = sext i32 %47 to i64
  %49 = and i64 %48, 15
  %50 = icmp ugt i64 %49, 15
  %51 = icmp ult i64 %48, 0
  %52 = xor i64 %48, 0
  %53 = and i64 %52, 0
  %54 = icmp slt i64 %53, 0
  store i1 %50, i1* %az
  store i1 %51, i1* %cf
  store i1 %54, i1* %of
  %55 = icmp eq i64 %48, 0
  store i1 %55, i1* %zf
  %56 = icmp slt i64 %48, 0
  store i1 %56, i1* %sf
  %57 = trunc i64 %48 to i8
  %58 = call i8 @llvm.ctpop.i8(i8 %57)
  %59 = and i8 %58, 1
  %60 = icmp eq i8 %59, 0
  store i1 %60, i1* %pf
  store volatile i64 66312, i64* @assembly_address
  %61 = load i1* %zf
  br i1 %61, label %block_1030f, label %block_1030a

block_1030a:                                      ; preds = %block_10303
  store volatile i64 66314, i64* @assembly_address
  %62 = call i64 @xalloc_die()
  store i64 %62, i64* %rax
  store i64 %62, i64* %rax
  store i64 %62, i64* %rax
  unreachable

block_1030f:                                      ; preds = %block_10303, %block_102e0
  store volatile i64 66319, i64* @assembly_address
  %63 = load i64* %stack_var_-16
  store i64 %63, i64* %rax
  store volatile i64 66323, i64* @assembly_address
  %64 = load i64* %stack_var_-8
  store i64 %64, i64* %rbp
  %65 = ptrtoint i64* %stack_var_0 to i64
  store i64 %65, i64* %rsp
  store volatile i64 66324, i64* @assembly_address
  %66 = load i64* %rax
  ret i64 %66
}

declare i64 @286(i64)

define i64 @xrealloc(i64* %arg1, i64 %arg2) {
block_10315:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i32
  %1 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66325, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 66326, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 66329, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 66333, i64* @assembly_address
  %21 = load i64* %rdi
  store i64 %21, i64* %stack_var_-16
  store volatile i64 66337, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-24
  store volatile i64 66341, i64* @assembly_address
  %24 = load i32* %stack_var_-24
  %25 = sext i32 %24 to i64
  %26 = and i64 %25, 15
  %27 = icmp ugt i64 %26, 15
  %28 = icmp ult i64 %25, 0
  %29 = xor i64 %25, 0
  %30 = and i64 %29, 0
  %31 = icmp slt i64 %30, 0
  store i1 %27, i1* %az
  store i1 %28, i1* %cf
  store i1 %31, i1* %of
  %32 = icmp eq i64 %25, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i64 %25, 0
  store i1 %33, i1* %sf
  %34 = trunc i64 %25 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  store volatile i64 66346, i64* @assembly_address
  %38 = load i1* %zf
  %39 = icmp eq i1 %38, false
  br i1 %39, label %block_10346, label %block_1032c

block_1032c:                                      ; preds = %block_10315
  store volatile i64 66348, i64* @assembly_address
  %40 = load i64* %stack_var_-16
  %41 = and i64 %40, 15
  %42 = icmp ugt i64 %41, 15
  %43 = icmp ult i64 %40, 0
  %44 = xor i64 %40, 0
  %45 = and i64 %44, 0
  %46 = icmp slt i64 %45, 0
  store i1 %42, i1* %az
  store i1 %43, i1* %cf
  store i1 %46, i1* %of
  %47 = icmp eq i64 %40, 0
  store i1 %47, i1* %zf
  %48 = icmp slt i64 %40, 0
  store i1 %48, i1* %sf
  %49 = trunc i64 %40 to i8
  %50 = call i8 @llvm.ctpop.i8(i8 %49)
  %51 = and i8 %50, 1
  %52 = icmp eq i8 %51, 0
  store i1 %52, i1* %pf
  store volatile i64 66353, i64* @assembly_address
  %53 = load i1* %zf
  br i1 %53, label %block_10346, label %block_10333

block_10333:                                      ; preds = %block_1032c
  store volatile i64 66355, i64* @assembly_address
  %54 = load i64* %stack_var_-16
  store i64 %54, i64* %rax
  store volatile i64 66359, i64* @assembly_address
  %55 = load i64* %rax
  store i64 %55, i64* %rdi
  store volatile i64 66362, i64* @assembly_address
  %56 = load i64* %rdi
  %57 = inttoptr i64 %56 to i64*
  call void @free(i64* %57)
  store volatile i64 66367, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 66372, i64* @assembly_address
  br label %block_10374

block_10346:                                      ; preds = %block_1032c, %block_10315
  store volatile i64 66374, i64* @assembly_address
  %58 = load i32* %stack_var_-24
  %59 = sext i32 %58 to i64
  store i64 %59, i64* %rdx
  store volatile i64 66378, i64* @assembly_address
  %60 = load i64* %stack_var_-16
  store i64 %60, i64* %rax
  store volatile i64 66382, i64* @assembly_address
  %61 = load i64* %rdx
  store i64 %61, i64* %rsi
  store volatile i64 66385, i64* @assembly_address
  %62 = load i64* %rax
  store i64 %62, i64* %rdi
  store volatile i64 66388, i64* @assembly_address
  %63 = load i64* %rdi
  %64 = inttoptr i64 %63 to i64*
  %65 = load i64* %rsi
  %66 = trunc i64 %65 to i32
  %67 = call i64* @realloc(i64* %64, i32 %66)
  %68 = ptrtoint i64* %67 to i64
  store i64 %68, i64* %rax
  %69 = ptrtoint i64* %67 to i64
  store i64 %69, i64* %rax
  store volatile i64 66393, i64* @assembly_address
  %70 = load i64* %rax
  store i64 %70, i64* %stack_var_-16
  store volatile i64 66397, i64* @assembly_address
  %71 = load i64* %stack_var_-16
  %72 = and i64 %71, 15
  %73 = icmp ugt i64 %72, 15
  %74 = icmp ult i64 %71, 0
  %75 = xor i64 %71, 0
  %76 = and i64 %75, 0
  %77 = icmp slt i64 %76, 0
  store i1 %73, i1* %az
  store i1 %74, i1* %cf
  store i1 %77, i1* %of
  %78 = icmp eq i64 %71, 0
  store i1 %78, i1* %zf
  %79 = icmp slt i64 %71, 0
  store i1 %79, i1* %sf
  %80 = trunc i64 %71 to i8
  %81 = call i8 @llvm.ctpop.i8(i8 %80)
  %82 = and i8 %81, 1
  %83 = icmp eq i8 %82, 0
  store i1 %83, i1* %pf
  store volatile i64 66402, i64* @assembly_address
  %84 = load i1* %zf
  %85 = icmp eq i1 %84, false
  br i1 %85, label %block_10370, label %block_10364

block_10364:                                      ; preds = %block_10346
  store volatile i64 66404, i64* @assembly_address
  %86 = load i32* %stack_var_-24
  %87 = sext i32 %86 to i64
  %88 = and i64 %87, 15
  %89 = icmp ugt i64 %88, 15
  %90 = icmp ult i64 %87, 0
  %91 = xor i64 %87, 0
  %92 = and i64 %91, 0
  %93 = icmp slt i64 %92, 0
  store i1 %89, i1* %az
  store i1 %90, i1* %cf
  store i1 %93, i1* %of
  %94 = icmp eq i64 %87, 0
  store i1 %94, i1* %zf
  %95 = icmp slt i64 %87, 0
  store i1 %95, i1* %sf
  %96 = trunc i64 %87 to i8
  %97 = call i8 @llvm.ctpop.i8(i8 %96)
  %98 = and i8 %97, 1
  %99 = icmp eq i8 %98, 0
  store i1 %99, i1* %pf
  store volatile i64 66409, i64* @assembly_address
  %100 = load i1* %zf
  br i1 %100, label %block_10370, label %block_1036b

block_1036b:                                      ; preds = %block_10364
  store volatile i64 66411, i64* @assembly_address
  %101 = call i64 @xalloc_die()
  store i64 %101, i64* %rax
  store i64 %101, i64* %rax
  store i64 %101, i64* %rax
  unreachable

block_10370:                                      ; preds = %block_10364, %block_10346
  store volatile i64 66416, i64* @assembly_address
  %102 = load i64* %stack_var_-16
  store i64 %102, i64* %rax
  br label %block_10374

block_10374:                                      ; preds = %block_10370, %block_10333
  store volatile i64 66420, i64* @assembly_address
  %103 = load i64* %stack_var_-8
  store i64 %103, i64* %rbp
  %104 = ptrtoint i64* %stack_var_0 to i64
  store i64 %104, i64* %rsp
  store volatile i64 66421, i64* @assembly_address
  %105 = load i64* %rax
  ret i64 %105
}

declare i64 @287(i64, i32)

declare i64 @288(i64, i64)

define i64 @x2realloc(i64 %arg1, i64* %arg2) {
block_10376:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i64* %arg2 to i64
  store i64 %0, i64* %rsi
  store i64 %arg1, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66422, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 66423, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 66426, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 66430, i64* @assembly_address
  %20 = load i64* %rdi
  store i64 %20, i64* %stack_var_-16
  store volatile i64 66434, i64* @assembly_address
  %21 = load i64* %rsi
  store i64 %21, i64* %stack_var_-24
  store volatile i64 66438, i64* @assembly_address
  %22 = load i64* %stack_var_-24
  store i64 %22, i64* %rcx
  store volatile i64 66442, i64* @assembly_address
  %23 = load i64* %stack_var_-16
  store i64 %23, i64* %rax
  store volatile i64 66446, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 66451, i64* @assembly_address
  %24 = load i64* %rcx
  store i64 %24, i64* %rsi
  store volatile i64 66454, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rdi
  store volatile i64 66457, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = load i64* %rsi
  %28 = inttoptr i64 %27 to i64*
  %29 = load i64* %rdx
  %30 = sext i64 %29 to i128
  %31 = call i64 @x2nrealloc(i64 %26, i64* %28, i128 %30)
  store i64 %31, i64* %rax
  store i64 %31, i64* %rax
  store volatile i64 66462, i64* @assembly_address
  %32 = load i64* %stack_var_-8
  store i64 %32, i64* %rbp
  %33 = ptrtoint i64* %stack_var_0 to i64
  store i64 %33, i64* %rsp
  store volatile i64 66463, i64* @assembly_address
  %34 = load i64* %rax
  ret i64 %34
}

declare i64 @289(i64, i64)

define i64 @xzalloc(i32 %arg1) {
block_103a0:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i32
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66464, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 66465, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 66468, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 66472, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = trunc i64 %21 to i32
  store i32 %22, i32* %stack_var_-16
  store volatile i64 66476, i64* @assembly_address
  %23 = load i32* %stack_var_-16
  %24 = sext i32 %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 66480, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rdi
  store volatile i64 66483, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = trunc i64 %26 to i32
  %28 = call i64 @xmalloc(i32 %27)
  store i64 %28, i64* %rax
  store i64 %28, i64* %rax
  store volatile i64 66488, i64* @assembly_address
  %29 = load i64* %rax
  store i64 %29, i64* %rcx
  store volatile i64 66491, i64* @assembly_address
  %30 = load i32* %stack_var_-16
  %31 = sext i32 %30 to i64
  store i64 %31, i64* %rax
  store volatile i64 66495, i64* @assembly_address
  %32 = load i64* %rax
  store i64 %32, i64* %rdx
  store volatile i64 66498, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 66503, i64* @assembly_address
  %33 = load i64* %rcx
  store i64 %33, i64* %rdi
  store volatile i64 66506, i64* @assembly_address
  %34 = load i64* %rdi
  %35 = inttoptr i64 %34 to i64*
  %36 = load i64* %rsi
  %37 = trunc i64 %36 to i32
  %38 = load i64* %rdx
  %39 = trunc i64 %38 to i32
  %40 = call i64* @memset(i64* %35, i32 %37, i32 %39)
  %41 = ptrtoint i64* %40 to i64
  store i64 %41, i64* %rax
  %42 = ptrtoint i64* %40 to i64
  store i64 %42, i64* %rax
  store volatile i64 66511, i64* @assembly_address
  %43 = load i64* %stack_var_-8
  store i64 %43, i64* %rbp
  %44 = ptrtoint i64* %stack_var_0 to i64
  store i64 %44, i64* %rsp
  store volatile i64 66512, i64* @assembly_address
  %45 = load i64* %rax
  ret i64 %45
}

declare i64 @290(i64)

define i64 @xcalloc(i32 %arg1, i64 %arg2) {
block_103d1:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-32 = alloca i32
  %1 = alloca i64
  %stack_var_-40 = alloca i32
  %2 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66513, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 66514, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 66517, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 32
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 32
  %11 = xor i64 %6, 32
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i32* %stack_var_-40 to i64
  store i64 %21, i64* %rsp
  store volatile i64 66521, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-32
  store volatile i64 66525, i64* @assembly_address
  %24 = load i64* %rsi
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-40
  store volatile i64 66529, i64* @assembly_address
  store i64 0, i64* %rcx
  store volatile i64 66534, i64* @assembly_address
  %26 = load i32* %stack_var_-32
  %27 = sext i32 %26 to i64
  store i64 %27, i64* %rax
  store volatile i64 66538, i64* @assembly_address
  %28 = load i32* %stack_var_-40
  %29 = sext i32 %28 to i64
  %30 = load i64* %rax
  %31 = zext i64 %29 to i128
  %32 = zext i64 %30 to i128
  %33 = mul i128 %31, %32
  %34 = trunc i128 %33 to i64
  %35 = lshr i128 %33, 64
  %36 = trunc i128 %35 to i64
  %37 = icmp ne i64 %36, 0
  store i64 %34, i64* %rax
  store i64 %36, i64* %rdx
  store i1 %37, i1* %of
  store i1 %37, i1* %cf
  store volatile i64 66542, i64* @assembly_address
  %38 = load i1* %of
  %39 = icmp eq i1 %38, false
  br i1 %39, label %block_103f5, label %block_103f0

block_103f0:                                      ; preds = %block_103d1
  store volatile i64 66544, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_103f5

block_103f5:                                      ; preds = %block_103f0, %block_103d1
  store volatile i64 66549, i64* @assembly_address
  %40 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %41 = icmp eq i64 %40, 0
  store i1 %41, i1* %zf
  %42 = icmp slt i64 %40, 0
  store i1 %42, i1* %sf
  %43 = trunc i64 %40 to i8
  %44 = call i8 @llvm.ctpop.i8(i8 %43)
  %45 = and i8 %44, 1
  %46 = icmp eq i8 %45, 0
  store i1 %46, i1* %pf
  store volatile i64 66552, i64* @assembly_address
  %47 = load i1* %sf
  %48 = icmp eq i1 %47, false
  br i1 %48, label %block_103ff, label %block_103fa

block_103fa:                                      ; preds = %block_103f5
  store volatile i64 66554, i64* @assembly_address
  store i64 1, i64* %rcx
  br label %block_103ff

block_103ff:                                      ; preds = %block_103fa, %block_103f5
  store volatile i64 66559, i64* @assembly_address
  %49 = load i64* %rcx
  store i64 %49, i64* %rax
  store volatile i64 66562, i64* @assembly_address
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i32
  %52 = and i32 %51, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %53 = icmp eq i32 %52, 0
  store i1 %53, i1* %zf
  %54 = icmp slt i32 %52, 0
  store i1 %54, i1* %sf
  %55 = trunc i32 %52 to i8
  %56 = call i8 @llvm.ctpop.i8(i8 %55)
  %57 = and i8 %56, 1
  %58 = icmp eq i8 %57, 0
  store i1 %58, i1* %pf
  %59 = zext i32 %52 to i64
  store i64 %59, i64* %rax
  store volatile i64 66565, i64* @assembly_address
  %60 = load i64* %rax
  %61 = trunc i64 %60 to i8
  %62 = load i64* %rax
  %63 = trunc i64 %62 to i8
  %64 = and i8 %61, %63
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %65 = icmp eq i8 %64, 0
  store i1 %65, i1* %zf
  %66 = icmp slt i8 %64, 0
  store i1 %66, i1* %sf
  %67 = call i8 @llvm.ctpop.i8(i8 %64)
  %68 = and i8 %67, 1
  %69 = icmp eq i8 %68, 0
  store i1 %69, i1* %pf
  store volatile i64 66567, i64* @assembly_address
  %70 = load i1* %zf
  %71 = icmp eq i1 %70, false
  br i1 %71, label %block_10427, label %block_10409

block_10409:                                      ; preds = %block_103ff
  store volatile i64 66569, i64* @assembly_address
  %72 = load i32* %stack_var_-40
  %73 = sext i32 %72 to i64
  store i64 %73, i64* %rdx
  store volatile i64 66573, i64* @assembly_address
  %74 = load i32* %stack_var_-32
  %75 = sext i32 %74 to i64
  store i64 %75, i64* %rax
  store volatile i64 66577, i64* @assembly_address
  %76 = load i64* %rdx
  store i64 %76, i64* %rsi
  store volatile i64 66580, i64* @assembly_address
  %77 = load i64* %rax
  store i64 %77, i64* %rdi
  store volatile i64 66583, i64* @assembly_address
  %78 = load i64* %rdi
  %79 = trunc i64 %78 to i32
  %80 = load i64* %rsi
  %81 = trunc i64 %80 to i32
  %82 = call i64* @calloc(i32 %79, i32 %81)
  %83 = ptrtoint i64* %82 to i64
  store i64 %83, i64* %rax
  %84 = ptrtoint i64* %82 to i64
  store i64 %84, i64* %rax
  store volatile i64 66588, i64* @assembly_address
  %85 = load i64* %rax
  store i64 %85, i64* %stack_var_-16
  store volatile i64 66592, i64* @assembly_address
  %86 = load i64* %stack_var_-16
  %87 = and i64 %86, 15
  %88 = icmp ugt i64 %87, 15
  %89 = icmp ult i64 %86, 0
  %90 = xor i64 %86, 0
  %91 = and i64 %90, 0
  %92 = icmp slt i64 %91, 0
  store i1 %88, i1* %az
  store i1 %89, i1* %cf
  store i1 %92, i1* %of
  %93 = icmp eq i64 %86, 0
  store i1 %93, i1* %zf
  %94 = icmp slt i64 %86, 0
  store i1 %94, i1* %sf
  %95 = trunc i64 %86 to i8
  %96 = call i8 @llvm.ctpop.i8(i8 %95)
  %97 = and i8 %96, 1
  %98 = icmp eq i8 %97, 0
  store i1 %98, i1* %pf
  store volatile i64 66597, i64* @assembly_address
  %99 = load i1* %zf
  %100 = icmp eq i1 %99, false
  br i1 %100, label %block_1042c, label %block_10427

block_10427:                                      ; preds = %block_10409, %block_103ff
  store volatile i64 66599, i64* @assembly_address
  %101 = call i64 @xalloc_die()
  store i64 %101, i64* %rax
  store i64 %101, i64* %rax
  store i64 %101, i64* %rax
  unreachable

block_1042c:                                      ; preds = %block_10409
  store volatile i64 66604, i64* @assembly_address
  %102 = load i64* %stack_var_-16
  store i64 %102, i64* %rax
  store volatile i64 66608, i64* @assembly_address
  %103 = load i64* %stack_var_-8
  store i64 %103, i64* %rbp
  %104 = ptrtoint i64* %stack_var_0 to i64
  store i64 %104, i64* %rsp
  store volatile i64 66609, i64* @assembly_address
  %105 = load i64* %rax
  ret i64 %105
}

declare i64 @291(i64, i32)

declare i64 @292(i64, i64)

define i64 @xmemdup(i64* %arg1, i64 %arg2) {
block_10432:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint i64* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-24 = alloca i32
  %1 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66610, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 66611, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 66614, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i32* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 66618, i64* @assembly_address
  %21 = load i64* %rdi
  store i64 %21, i64* %stack_var_-16
  store volatile i64 66622, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-24
  store volatile i64 66626, i64* @assembly_address
  %24 = load i32* %stack_var_-24
  %25 = sext i32 %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 66630, i64* @assembly_address
  %26 = load i64* %rax
  store i64 %26, i64* %rdi
  store volatile i64 66633, i64* @assembly_address
  %27 = load i64* %rdi
  %28 = trunc i64 %27 to i32
  %29 = call i64 @xmalloc(i32 %28)
  store i64 %29, i64* %rax
  store i64 %29, i64* %rax
  store volatile i64 66638, i64* @assembly_address
  %30 = load i64* %rax
  store i64 %30, i64* %rcx
  store volatile i64 66641, i64* @assembly_address
  %31 = load i32* %stack_var_-24
  %32 = sext i32 %31 to i64
  store i64 %32, i64* %rdx
  store volatile i64 66645, i64* @assembly_address
  %33 = load i64* %stack_var_-16
  store i64 %33, i64* %rax
  store volatile i64 66649, i64* @assembly_address
  %34 = load i64* %rax
  store i64 %34, i64* %rsi
  store volatile i64 66652, i64* @assembly_address
  %35 = load i64* %rcx
  store i64 %35, i64* %rdi
  store volatile i64 66655, i64* @assembly_address
  %36 = load i64* %rdi
  %37 = inttoptr i64 %36 to i64*
  %38 = load i64* %rsi
  %39 = inttoptr i64 %38 to i64*
  %40 = load i64* %rdx
  %41 = trunc i64 %40 to i32
  %42 = call i64* @memcpy(i64* %37, i64* %39, i32 %41)
  %43 = ptrtoint i64* %42 to i64
  store i64 %43, i64* %rax
  %44 = ptrtoint i64* %42 to i64
  store i64 %44, i64* %rax
  store volatile i64 66660, i64* @assembly_address
  %45 = load i64* %stack_var_-8
  store i64 %45, i64* %rbp
  %46 = ptrtoint i64* %stack_var_0 to i64
  store i64 %46, i64* %rsp
  store volatile i64 66661, i64* @assembly_address
  %47 = load i64* %rax
  ret i64 %47
}

declare i64 @293(i64, i32)

declare i64 @294(i64, i64)

define i64 @xstrdup(i8* %arg1) {
block_10466:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i8*
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66662, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 66663, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 66666, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 66670, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to i8*
  store i8* %22, i8** %stack_var_-16
  store volatile i64 66674, i64* @assembly_address
  %23 = load i8** %stack_var_-16
  %24 = ptrtoint i8* %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 66678, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rdi
  store volatile i64 66681, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to i8*
  %28 = call i32 @strlen(i8* %27)
  %29 = sext i32 %28 to i64
  store i64 %29, i64* %rax
  %30 = sext i32 %28 to i64
  store i64 %30, i64* %rax
  store volatile i64 66686, i64* @assembly_address
  %31 = load i64* %rax
  %32 = add i64 %31, 1
  store i64 %32, i64* %rdx
  store volatile i64 66690, i64* @assembly_address
  %33 = load i8** %stack_var_-16
  %34 = ptrtoint i8* %33 to i64
  store i64 %34, i64* %rax
  store volatile i64 66694, i64* @assembly_address
  %35 = load i64* %rdx
  store i64 %35, i64* %rsi
  store volatile i64 66697, i64* @assembly_address
  %36 = load i64* %rax
  store i64 %36, i64* %rdi
  store volatile i64 66700, i64* @assembly_address
  %37 = load i64* %rdi
  %38 = load i64* %rsi
  %39 = inttoptr i64 %37 to i64*
  %40 = call i64 @xmemdup(i64* %39, i64 %38)
  store i64 %40, i64* %rax
  store i64 %40, i64* %rax
  store volatile i64 66705, i64* @assembly_address
  %41 = load i64* %stack_var_-8
  store i64 %41, i64* %rbp
  %42 = ptrtoint i64* %stack_var_0 to i64
  store i64 %42, i64* %rsp
  store volatile i64 66706, i64* @assembly_address
  %43 = load i64* %rax
  ret i64 %43
}

declare i64 @295(i64)

define i64 @yesno() {
block_10493:
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-13 = alloca i8
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66707, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 66708, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 66711, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 16
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 16
  %8 = xor i64 %3, 16
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %18, i64* %rsp
  store volatile i64 66715, i64* @assembly_address
  %19 = call i32 @getchar()
  %20 = sext i32 %19 to i64
  store i64 %20, i64* %rax
  %21 = sext i32 %19 to i64
  store i64 %21, i64* %rax
  %22 = sext i32 %19 to i64
  store i64 %22, i64* %rax
  store volatile i64 66720, i64* @assembly_address
  %23 = load i64* %rax
  %24 = trunc i64 %23 to i32
  store i32 %24, i32* %stack_var_-12
  store volatile i64 66723, i64* @assembly_address
  %25 = load i32* %stack_var_-12
  %26 = sub i32 %25, 121
  %27 = and i32 %25, 15
  %28 = sub i32 %27, 9
  %29 = icmp ugt i32 %28, 15
  %30 = icmp ult i32 %25, 121
  %31 = xor i32 %25, 121
  %32 = xor i32 %25, %26
  %33 = and i32 %31, %32
  %34 = icmp slt i32 %33, 0
  store i1 %29, i1* %az
  store i1 %30, i1* %cf
  store i1 %34, i1* %of
  %35 = icmp eq i32 %26, 0
  store i1 %35, i1* %zf
  %36 = icmp slt i32 %26, 0
  store i1 %36, i1* %sf
  %37 = trunc i32 %26 to i8
  %38 = call i8 @llvm.ctpop.i8(i8 %37)
  %39 = and i8 %38, 1
  %40 = icmp eq i8 %39, 0
  store i1 %40, i1* %pf
  store volatile i64 66727, i64* @assembly_address
  %41 = load i1* %zf
  br i1 %41, label %block_104af, label %block_104a9

block_104a9:                                      ; preds = %block_10493
  store volatile i64 66729, i64* @assembly_address
  %42 = load i32* %stack_var_-12
  %43 = sub i32 %42, 89
  %44 = and i32 %42, 15
  %45 = sub i32 %44, 9
  %46 = icmp ugt i32 %45, 15
  %47 = icmp ult i32 %42, 89
  %48 = xor i32 %42, 89
  %49 = xor i32 %42, %43
  %50 = and i32 %48, %49
  %51 = icmp slt i32 %50, 0
  store i1 %46, i1* %az
  store i1 %47, i1* %cf
  store i1 %51, i1* %of
  %52 = icmp eq i32 %43, 0
  store i1 %52, i1* %zf
  %53 = icmp slt i32 %43, 0
  store i1 %53, i1* %sf
  %54 = trunc i32 %43 to i8
  %55 = call i8 @llvm.ctpop.i8(i8 %54)
  %56 = and i8 %55, 1
  %57 = icmp eq i8 %56, 0
  store i1 %57, i1* %pf
  store volatile i64 66733, i64* @assembly_address
  %58 = load i1* %zf
  %59 = icmp eq i1 %58, false
  br i1 %59, label %block_104b6, label %block_104af

block_104af:                                      ; preds = %block_104a9, %block_10493
  store volatile i64 66735, i64* @assembly_address
  store i64 1, i64* %rax
  store volatile i64 66740, i64* @assembly_address
  br label %block_104bb

block_104b6:                                      ; preds = %block_104a9
  store volatile i64 66742, i64* @assembly_address
  store i64 0, i64* %rax
  br label %block_104bb

block_104bb:                                      ; preds = %block_104b6, %block_104af
  store volatile i64 66747, i64* @assembly_address
  %60 = load i64* %rax
  %61 = trunc i64 %60 to i8
  store i8 %61, i8* %stack_var_-13
  store volatile i64 66750, i64* @assembly_address
  %62 = load i8* %stack_var_-13
  %63 = and i8 %62, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %64 = icmp eq i8 %63, 0
  store i1 %64, i1* %zf
  %65 = icmp slt i8 %63, 0
  store i1 %65, i1* %sf
  %66 = call i8 @llvm.ctpop.i8(i8 %63)
  %67 = and i8 %66, 1
  %68 = icmp eq i8 %67, 0
  store i1 %68, i1* %pf
  store i8 %63, i8* %stack_var_-13
  store volatile i64 66754, i64* @assembly_address
  br label %block_104cc

block_104c4:                                      ; preds = %block_104d2
  store volatile i64 66756, i64* @assembly_address
  %69 = call i32 @getchar()
  %70 = sext i32 %69 to i64
  store i64 %70, i64* %rax
  %71 = sext i32 %69 to i64
  store i64 %71, i64* %rax
  %72 = sext i32 %69 to i64
  store i64 %72, i64* %rax
  store volatile i64 66761, i64* @assembly_address
  %73 = load i64* %rax
  %74 = trunc i64 %73 to i32
  store i32 %74, i32* %stack_var_-12
  br label %block_104cc

block_104cc:                                      ; preds = %block_104c4, %block_104bb
  store volatile i64 66764, i64* @assembly_address
  %75 = load i32* %stack_var_-12
  %76 = sub i32 %75, 10
  %77 = and i32 %75, 15
  %78 = sub i32 %77, 10
  %79 = icmp ugt i32 %78, 15
  %80 = icmp ult i32 %75, 10
  %81 = xor i32 %75, 10
  %82 = xor i32 %75, %76
  %83 = and i32 %81, %82
  %84 = icmp slt i32 %83, 0
  store i1 %79, i1* %az
  store i1 %80, i1* %cf
  store i1 %84, i1* %of
  %85 = icmp eq i32 %76, 0
  store i1 %85, i1* %zf
  %86 = icmp slt i32 %76, 0
  store i1 %86, i1* %sf
  %87 = trunc i32 %76 to i8
  %88 = call i8 @llvm.ctpop.i8(i8 %87)
  %89 = and i8 %88, 1
  %90 = icmp eq i8 %89, 0
  store i1 %90, i1* %pf
  store volatile i64 66768, i64* @assembly_address
  %91 = load i1* %zf
  br i1 %91, label %block_104d8, label %block_104d2

block_104d2:                                      ; preds = %block_104cc
  store volatile i64 66770, i64* @assembly_address
  %92 = load i32* %stack_var_-12
  %93 = sub i32 %92, -1
  %94 = and i32 %92, 15
  %95 = sub i32 %94, 15
  %96 = icmp ugt i32 %95, 15
  %97 = icmp ult i32 %92, -1
  %98 = xor i32 %92, -1
  %99 = xor i32 %92, %93
  %100 = and i32 %98, %99
  %101 = icmp slt i32 %100, 0
  store i1 %96, i1* %az
  store i1 %97, i1* %cf
  store i1 %101, i1* %of
  %102 = icmp eq i32 %93, 0
  store i1 %102, i1* %zf
  %103 = icmp slt i32 %93, 0
  store i1 %103, i1* %sf
  %104 = trunc i32 %93 to i8
  %105 = call i8 @llvm.ctpop.i8(i8 %104)
  %106 = and i8 %105, 1
  %107 = icmp eq i8 %106, 0
  store i1 %107, i1* %pf
  store volatile i64 66774, i64* @assembly_address
  %108 = load i1* %zf
  %109 = icmp eq i1 %108, false
  br i1 %109, label %block_104c4, label %block_104d8

block_104d8:                                      ; preds = %block_104d2, %block_104cc
  store volatile i64 66776, i64* @assembly_address
  %110 = load i8* %stack_var_-13
  %111 = zext i8 %110 to i64
  store i64 %111, i64* %rax
  store volatile i64 66780, i64* @assembly_address
  %112 = load i64* %stack_var_-8
  store i64 %112, i64* %rbp
  %113 = ptrtoint i64* %stack_var_0 to i64
  store i64 %113, i64* %rsp
  store volatile i64 66781, i64* @assembly_address
  %114 = load i64* %rax
  %115 = load i64* %rax
  ret i64 %115
}

define i64 @rpl_fclose(%_IO_FILE* %arg1) {
block_104de:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint %_IO_FILE* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-32 = alloca %_IO_FILE*
  %1 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 66782, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 66783, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 66786, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 32
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 32
  %10 = xor i64 %5, 32
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %20, i64* %rsp
  store volatile i64 66790, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to %_IO_FILE*
  store %_IO_FILE* %22, %_IO_FILE** %stack_var_-32
  store volatile i64 66794, i64* @assembly_address
  store i32 0, i32* %stack_var_-20
  store volatile i64 66801, i64* @assembly_address
  store i32 0, i32* %stack_var_-16
  store volatile i64 66808, i64* @assembly_address
  %23 = load %_IO_FILE** %stack_var_-32
  %24 = ptrtoint %_IO_FILE* %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 66812, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rdi
  store volatile i64 66815, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = inttoptr i64 %26 to %_IO_FILE*
  %28 = call i32 @fileno(%_IO_FILE* %27)
  %29 = sext i32 %28 to i64
  store i64 %29, i64* %rax
  %30 = sext i32 %28 to i64
  store i64 %30, i64* %rax
  store volatile i64 66820, i64* @assembly_address
  %31 = load i64* %rax
  %32 = trunc i64 %31 to i32
  store i32 %32, i32* %stack_var_-12
  store volatile i64 66823, i64* @assembly_address
  %33 = load i32* %stack_var_-12
  %34 = and i32 %33, 15
  %35 = icmp ugt i32 %34, 15
  %36 = icmp ult i32 %33, 0
  %37 = xor i32 %33, 0
  %38 = and i32 %37, 0
  %39 = icmp slt i32 %38, 0
  store i1 %35, i1* %az
  store i1 %36, i1* %cf
  store i1 %39, i1* %of
  %40 = icmp eq i32 %33, 0
  store i1 %40, i1* %zf
  %41 = icmp slt i32 %33, 0
  store i1 %41, i1* %sf
  %42 = trunc i32 %33 to i8
  %43 = call i8 @llvm.ctpop.i8(i8 %42)
  %44 = and i8 %43, 1
  %45 = icmp eq i8 %44, 0
  store i1 %45, i1* %pf
  store volatile i64 66827, i64* @assembly_address
  %46 = load i1* %sf
  %47 = icmp eq i1 %46, false
  br i1 %47, label %block_1051b, label %block_1050d

block_1050d:                                      ; preds = %block_104de
  store volatile i64 66829, i64* @assembly_address
  %48 = load %_IO_FILE** %stack_var_-32
  %49 = ptrtoint %_IO_FILE* %48 to i64
  store i64 %49, i64* %rax
  store volatile i64 66833, i64* @assembly_address
  %50 = load i64* %rax
  store i64 %50, i64* %rdi
  store volatile i64 66836, i64* @assembly_address
  %51 = load i64* %rdi
  %52 = inttoptr i64 %51 to %_IO_FILE*
  %53 = call i32 @fclose(%_IO_FILE* %52)
  %54 = sext i32 %53 to i64
  store i64 %54, i64* %rax
  %55 = sext i32 %53 to i64
  store i64 %55, i64* %rax
  store volatile i64 66841, i64* @assembly_address
  br label %block_10594

block_1051b:                                      ; preds = %block_104de
  store volatile i64 66843, i64* @assembly_address
  %56 = load %_IO_FILE** %stack_var_-32
  %57 = ptrtoint %_IO_FILE* %56 to i64
  store i64 %57, i64* %rax
  store volatile i64 66847, i64* @assembly_address
  %58 = load i64* %rax
  store i64 %58, i64* %rdi
  store volatile i64 66850, i64* @assembly_address
  %59 = load i64* %rdi
  %60 = inttoptr i64 %59 to %_IO_FILE*
  %61 = call i32 @__freading(%_IO_FILE* %60)
  %62 = sext i32 %61 to i64
  store i64 %62, i64* %rax
  %63 = sext i32 %61 to i64
  store i64 %63, i64* %rax
  store volatile i64 66855, i64* @assembly_address
  %64 = load i64* %rax
  %65 = trunc i64 %64 to i32
  %66 = load i64* %rax
  %67 = trunc i64 %66 to i32
  %68 = and i32 %65, %67
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %69 = icmp eq i32 %68, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 %68, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 %68 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  store volatile i64 66857, i64* @assembly_address
  %75 = load i1* %zf
  br i1 %75, label %block_1054e, label %block_1052b

block_1052b:                                      ; preds = %block_1051b
  store volatile i64 66859, i64* @assembly_address
  %76 = load %_IO_FILE** %stack_var_-32
  %77 = ptrtoint %_IO_FILE* %76 to i64
  store i64 %77, i64* %rax
  store volatile i64 66863, i64* @assembly_address
  %78 = load i64* %rax
  store i64 %78, i64* %rdi
  store volatile i64 66866, i64* @assembly_address
  %79 = load i64* %rdi
  %80 = inttoptr i64 %79 to %_IO_FILE*
  %81 = call i32 @fileno(%_IO_FILE* %80)
  %82 = sext i32 %81 to i64
  store i64 %82, i64* %rax
  %83 = sext i32 %81 to i64
  store i64 %83, i64* %rax
  store volatile i64 66871, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 66876, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 66881, i64* @assembly_address
  %84 = load i64* %rax
  %85 = trunc i64 %84 to i32
  %86 = zext i32 %85 to i64
  store i64 %86, i64* %rdi
  store volatile i64 66883, i64* @assembly_address
  %87 = load i64* %rdi
  %88 = trunc i64 %87 to i32
  %89 = load i64* %rsi
  %90 = trunc i64 %89 to i32
  %91 = load i64* %rdx
  %92 = trunc i64 %91 to i32
  %93 = call i32 @lseek(i32 %88, i32 %90, i32 %92)
  %94 = sext i32 %93 to i64
  store i64 %94, i64* %rax
  %95 = sext i32 %93 to i64
  store i64 %95, i64* %rax
  store volatile i64 66888, i64* @assembly_address
  %96 = load i64* %rax
  %97 = sub i64 %96, -1
  %98 = and i64 %96, 15
  %99 = sub i64 %98, 15
  %100 = icmp ugt i64 %99, 15
  %101 = icmp ult i64 %96, -1
  %102 = xor i64 %96, -1
  %103 = xor i64 %96, %97
  %104 = and i64 %102, %103
  %105 = icmp slt i64 %104, 0
  store i1 %100, i1* %az
  store i1 %101, i1* %cf
  store i1 %105, i1* %of
  %106 = icmp eq i64 %97, 0
  store i1 %106, i1* %zf
  %107 = icmp slt i64 %97, 0
  store i1 %107, i1* %sf
  %108 = trunc i64 %97 to i8
  %109 = call i8 @llvm.ctpop.i8(i8 %108)
  %110 = and i8 %109, 1
  %111 = icmp eq i8 %110, 0
  store i1 %111, i1* %pf
  store volatile i64 66892, i64* @assembly_address
  %112 = load i1* %zf
  br i1 %112, label %block_10568, label %block_1054e

block_1054e:                                      ; preds = %block_1052b, %block_1051b
  store volatile i64 66894, i64* @assembly_address
  %113 = load %_IO_FILE** %stack_var_-32
  %114 = ptrtoint %_IO_FILE* %113 to i64
  store i64 %114, i64* %rax
  store volatile i64 66898, i64* @assembly_address
  %115 = load i64* %rax
  store i64 %115, i64* %rdi
  store volatile i64 66901, i64* @assembly_address
  %116 = load i64* %rdi
  %117 = inttoptr i64 %116 to %_IO_FILE*
  %118 = call i64 @rpl_fflush(%_IO_FILE* %117)
  store i64 %118, i64* %rax
  store i64 %118, i64* %rax
  store volatile i64 66906, i64* @assembly_address
  %119 = load i64* %rax
  %120 = trunc i64 %119 to i32
  %121 = load i64* %rax
  %122 = trunc i64 %121 to i32
  %123 = and i32 %120, %122
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %124 = icmp eq i32 %123, 0
  store i1 %124, i1* %zf
  %125 = icmp slt i32 %123, 0
  store i1 %125, i1* %sf
  %126 = trunc i32 %123 to i8
  %127 = call i8 @llvm.ctpop.i8(i8 %126)
  %128 = and i8 %127, 1
  %129 = icmp eq i8 %128, 0
  store i1 %129, i1* %pf
  store volatile i64 66908, i64* @assembly_address
  %130 = load i1* %zf
  br i1 %130, label %block_10568, label %block_1055e

block_1055e:                                      ; preds = %block_1054e
  store volatile i64 66910, i64* @assembly_address
  %131 = call i32* @__errno_location()
  %132 = ptrtoint i32* %131 to i64
  store i64 %132, i64* %rax
  %133 = ptrtoint i32* %131 to i64
  store i64 %133, i64* %rax
  %134 = ptrtoint i32* %131 to i64
  store i64 %134, i64* %rax
  store volatile i64 66915, i64* @assembly_address
  %135 = load i64* %rax
  %136 = inttoptr i64 %135 to i32*
  %137 = load i32* %136
  %138 = zext i32 %137 to i64
  store i64 %138, i64* %rax
  store volatile i64 66917, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  store i32 %140, i32* %stack_var_-20
  br label %block_10568

block_10568:                                      ; preds = %block_1055e, %block_1054e, %block_1052b
  store volatile i64 66920, i64* @assembly_address
  %141 = load %_IO_FILE** %stack_var_-32
  %142 = ptrtoint %_IO_FILE* %141 to i64
  store i64 %142, i64* %rax
  store volatile i64 66924, i64* @assembly_address
  %143 = load i64* %rax
  store i64 %143, i64* %rdi
  store volatile i64 66927, i64* @assembly_address
  %144 = load i64* %rdi
  %145 = inttoptr i64 %144 to %_IO_FILE*
  %146 = call i32 @fclose(%_IO_FILE* %145)
  %147 = sext i32 %146 to i64
  store i64 %147, i64* %rax
  %148 = sext i32 %146 to i64
  store i64 %148, i64* %rax
  store volatile i64 66932, i64* @assembly_address
  %149 = load i64* %rax
  %150 = trunc i64 %149 to i32
  store i32 %150, i32* %stack_var_-16
  store volatile i64 66935, i64* @assembly_address
  %151 = load i32* %stack_var_-20
  %152 = and i32 %151, 15
  %153 = icmp ugt i32 %152, 15
  %154 = icmp ult i32 %151, 0
  %155 = xor i32 %151, 0
  %156 = and i32 %155, 0
  %157 = icmp slt i32 %156, 0
  store i1 %153, i1* %az
  store i1 %154, i1* %cf
  store i1 %157, i1* %of
  %158 = icmp eq i32 %151, 0
  store i1 %158, i1* %zf
  %159 = icmp slt i32 %151, 0
  store i1 %159, i1* %sf
  %160 = trunc i32 %151 to i8
  %161 = call i8 @llvm.ctpop.i8(i8 %160)
  %162 = and i8 %161, 1
  %163 = icmp eq i8 %162, 0
  store i1 %163, i1* %pf
  store volatile i64 66939, i64* @assembly_address
  %164 = load i1* %zf
  br i1 %164, label %block_10591, label %block_1057d

block_1057d:                                      ; preds = %block_10568
  store volatile i64 66941, i64* @assembly_address
  %165 = call i32* @__errno_location()
  %166 = ptrtoint i32* %165 to i64
  store i64 %166, i64* %rax
  %167 = ptrtoint i32* %165 to i64
  store i64 %167, i64* %rax
  %168 = ptrtoint i32* %165 to i64
  store i64 %168, i64* %rax
  store volatile i64 66946, i64* @assembly_address
  %169 = load i64* %rax
  store i64 %169, i64* %rdx
  store volatile i64 66949, i64* @assembly_address
  %170 = load i32* %stack_var_-20
  %171 = zext i32 %170 to i64
  store i64 %171, i64* %rax
  store volatile i64 66952, i64* @assembly_address
  %172 = load i64* %rax
  %173 = trunc i64 %172 to i32
  %174 = load i64* %rdx
  %175 = inttoptr i64 %174 to i32*
  store i32 %173, i32* %175
  store volatile i64 66954, i64* @assembly_address
  store i32 -1, i32* %stack_var_-16
  br label %block_10591

block_10591:                                      ; preds = %block_1057d, %block_10568
  store volatile i64 66961, i64* @assembly_address
  %176 = load i32* %stack_var_-16
  %177 = zext i32 %176 to i64
  store i64 %177, i64* %rax
  br label %block_10594

block_10594:                                      ; preds = %block_10591, %block_1050d
  store volatile i64 66964, i64* @assembly_address
  %178 = load i64* %stack_var_-8
  store i64 %178, i64* %rbp
  %179 = ptrtoint i64* %stack_var_0 to i64
  store i64 %179, i64* %rsp
  store volatile i64 66965, i64* @assembly_address
  %180 = load i64* %rax
  ret i64 %180
}

declare i64 @296(i64)

define i64 @rpl_fcntl(i32 %arg1, i64 %arg2, i64 %arg3, i64 %arg4, i64 %arg5, i64 %arg6) {
block_10596:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %xmm7 = alloca i128
  %xmm6 = alloca i128
  %xmm5 = alloca i128
  %xmm4 = alloca i128
  %xmm3 = alloca i128
  %xmm2 = alloca i128
  %xmm1 = alloca i128
  %xmm0 = alloca i128
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg6, i64* %r9
  store i64 %arg5, i64* %r8
  store i64 %arg4, i64* %rcx
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-224 = alloca i64
  %stack_var_-228 = alloca i32
  %stack_var_-236 = alloca i32
  %stack_var_-232 = alloca i32
  %stack_var_-200 = alloca i32*
  %1 = alloca i64
  %stack_var_-184 = alloca i64
  %stack_var_-208 = alloca i64
  %stack_var_8 = alloca i64
  %stack_var_-212 = alloca i32
  %stack_var_-216 = alloca i32
  %stack_var_-240 = alloca i32
  %stack_var_-192 = alloca i64
  %stack_var_-24 = alloca i128
  %stack_var_-40 = alloca i128
  %stack_var_-56 = alloca i128
  %stack_var_-72 = alloca i128
  %stack_var_-88 = alloca i128
  %stack_var_-104 = alloca i128
  %stack_var_-120 = alloca i128
  %stack_var_-136 = alloca i128
  %stack_var_-144 = alloca i64
  %stack_var_-152 = alloca i64
  %stack_var_-160 = alloca i64
  %stack_var_-168 = alloca i64
  %stack_var_-256 = alloca i32
  %stack_var_-252 = alloca i32
  %stack_var_-264 = alloca i64
  %stack_var_-8 = alloca i64
  %2 = alloca i32
  %3 = alloca i32
  %4 = alloca i64
  %5 = alloca i32
  %6 = alloca i32
  %7 = alloca i64
  %8 = alloca i32
  %9 = alloca i32
  %10 = alloca i64
  %11 = alloca i32
  %12 = alloca i32
  %13 = alloca i64
  %14 = alloca i32
  %15 = alloca i32
  %16 = alloca i64
  %17 = alloca i32
  %18 = alloca i32
  %19 = alloca i64
  %20 = alloca i32
  %21 = alloca i32
  %22 = alloca i64
  store volatile i64 66966, i64* @assembly_address
  %23 = load i64* %rbp
  store i64 %23, i64* %stack_var_-8
  %24 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %24, i64* %rsp
  store volatile i64 66967, i64* @assembly_address
  %25 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %25, i64* %rbp
  store volatile i64 66970, i64* @assembly_address
  %26 = load i64* %rsp
  %27 = sub i64 %26, 256
  %28 = and i64 %26, 15
  %29 = icmp ugt i64 %28, 15
  %30 = icmp ult i64 %26, 256
  %31 = xor i64 %26, 256
  %32 = xor i64 %26, %27
  %33 = and i64 %31, %32
  %34 = icmp slt i64 %33, 0
  store i1 %29, i1* %az
  store i1 %30, i1* %cf
  store i1 %34, i1* %of
  %35 = icmp eq i64 %27, 0
  store i1 %35, i1* %zf
  %36 = icmp slt i64 %27, 0
  store i1 %36, i1* %sf
  %37 = trunc i64 %27 to i8
  %38 = call i8 @llvm.ctpop.i8(i8 %37)
  %39 = and i8 %38, 1
  %40 = icmp eq i8 %39, 0
  store i1 %40, i1* %pf
  %41 = ptrtoint i64* %stack_var_-264 to i64
  store i64 %41, i64* %rsp
  store volatile i64 66977, i64* @assembly_address
  %42 = load i64* %rdi
  %43 = trunc i64 %42 to i32
  store i32 %43, i32* %stack_var_-252
  store volatile i64 66983, i64* @assembly_address
  %44 = load i64* %rsi
  %45 = trunc i64 %44 to i32
  store i32 %45, i32* %stack_var_-256
  store volatile i64 66989, i64* @assembly_address
  %46 = load i64* %rdx
  store i64 %46, i64* %stack_var_-168
  store volatile i64 66996, i64* @assembly_address
  %47 = load i64* %rcx
  store i64 %47, i64* %stack_var_-160
  store volatile i64 67003, i64* @assembly_address
  %48 = load i64* %r8
  store i64 %48, i64* %stack_var_-152
  store volatile i64 67010, i64* @assembly_address
  %49 = load i64* %r9
  store i64 %49, i64* %stack_var_-144
  store volatile i64 67017, i64* @assembly_address
  %50 = load i64* %rax
  %51 = trunc i64 %50 to i8
  %52 = load i64* %rax
  %53 = trunc i64 %52 to i8
  %54 = and i8 %51, %53
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %55 = icmp eq i8 %54, 0
  store i1 %55, i1* %zf
  %56 = icmp slt i8 %54, 0
  store i1 %56, i1* %sf
  %57 = call i8 @llvm.ctpop.i8(i8 %54)
  %58 = and i8 %57, 1
  %59 = icmp eq i8 %58, 0
  store i1 %59, i1* %pf
  store volatile i64 67019, i64* @assembly_address
  %60 = load i1* %zf
  br i1 %60, label %block_105ed, label %block_105cd

block_105cd:                                      ; preds = %block_10596
  store volatile i64 67021, i64* @assembly_address
  %61 = load i128* %xmm0
  %62 = call i64 @__asm_movaps(i128 %61)
  %63 = sext i64 %62 to i128
  store i128 %63, i128* %stack_var_-136
  store volatile i64 67025, i64* @assembly_address
  %64 = load i128* %xmm1
  %65 = call i64 @__asm_movaps(i128 %64)
  %66 = sext i64 %65 to i128
  store i128 %66, i128* %stack_var_-120
  store volatile i64 67029, i64* @assembly_address
  %67 = load i128* %xmm2
  %68 = call i64 @__asm_movaps(i128 %67)
  %69 = sext i64 %68 to i128
  store i128 %69, i128* %stack_var_-104
  store volatile i64 67033, i64* @assembly_address
  %70 = load i128* %xmm3
  %71 = call i64 @__asm_movaps(i128 %70)
  %72 = sext i64 %71 to i128
  store i128 %72, i128* %stack_var_-88
  store volatile i64 67037, i64* @assembly_address
  %73 = load i128* %xmm4
  %74 = call i64 @__asm_movaps(i128 %73)
  %75 = sext i64 %74 to i128
  store i128 %75, i128* %stack_var_-72
  store volatile i64 67041, i64* @assembly_address
  %76 = load i128* %xmm5
  %77 = call i64 @__asm_movaps(i128 %76)
  %78 = sext i64 %77 to i128
  store i128 %78, i128* %stack_var_-56
  store volatile i64 67045, i64* @assembly_address
  %79 = load i128* %xmm6
  %80 = call i64 @__asm_movaps(i128 %79)
  %81 = sext i64 %80 to i128
  store i128 %81, i128* %stack_var_-40
  store volatile i64 67049, i64* @assembly_address
  %82 = load i128* %xmm7
  %83 = call i64 @__asm_movaps(i128 %82)
  %84 = sext i64 %83 to i128
  store i128 %84, i128* %stack_var_-24
  br label %block_105ed

block_105ed:                                      ; preds = %block_105cd, %block_10596
  store volatile i64 67053, i64* @assembly_address
  %85 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %85, i64* %rax
  store volatile i64 67062, i64* @assembly_address
  %86 = load i64* %rax
  store i64 %86, i64* %stack_var_-192
  store volatile i64 67069, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %87 = icmp eq i32 0, 0
  store i1 %87, i1* %zf
  %88 = icmp slt i32 0, 0
  store i1 %88, i1* %sf
  %89 = trunc i32 0 to i8
  %90 = call i8 @llvm.ctpop.i8(i8 %89)
  %91 = and i8 %90, 1
  %92 = icmp eq i8 %91, 0
  store i1 %92, i1* %pf
  %93 = zext i32 0 to i64
  store i64 %93, i64* %rax
  store volatile i64 67071, i64* @assembly_address
  store i32 -1, i32* %stack_var_-240
  store volatile i64 67081, i64* @assembly_address
  store i32 16, i32* %stack_var_-216
  store volatile i64 67091, i64* @assembly_address
  store i32 48, i32* %stack_var_-212
  store volatile i64 67101, i64* @assembly_address
  %94 = ptrtoint i64* %stack_var_8 to i64
  store i64 %94, i64* %rax
  store volatile i64 67105, i64* @assembly_address
  %95 = ptrtoint i64* %stack_var_8 to i64
  store i64 %95, i64* %stack_var_-208
  store volatile i64 67112, i64* @assembly_address
  %96 = ptrtoint i64* %stack_var_-184 to i64
  store i64 %96, i64* %rax
  store volatile i64 67119, i64* @assembly_address
  %97 = bitcast i64* %stack_var_-184 to i32*
  store i32* %97, i32** %stack_var_-200
  store volatile i64 67126, i64* @assembly_address
  %98 = load i32* %stack_var_-256
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rax
  store volatile i64 67132, i64* @assembly_address
  %100 = load i64* %rax
  %101 = trunc i64 %100 to i32
  %102 = load i64* %rax
  %103 = trunc i64 %102 to i32
  %104 = and i32 %101, %103
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %105 = icmp eq i32 %104, 0
  store i1 %105, i1* %zf
  %106 = icmp slt i32 %104, 0
  store i1 %106, i1* %sf
  %107 = trunc i32 %104 to i8
  %108 = call i8 @llvm.ctpop.i8(i8 %107)
  %109 = and i8 %108, 1
  %110 = icmp eq i8 %109, 0
  store i1 %110, i1* %pf
  store volatile i64 67134, i64* @assembly_address
  %111 = load i1* %zf
  br i1 %111, label %block_1064c, label %block_10640

block_10640:                                      ; preds = %block_105ed
  store volatile i64 67136, i64* @assembly_address
  %112 = load i64* %rax
  %113 = trunc i64 %112 to i32
  %114 = sub i32 %113, 1030
  %115 = and i32 %113, 15
  %116 = sub i32 %115, 6
  %117 = icmp ugt i32 %116, 15
  %118 = icmp ult i32 %113, 1030
  %119 = xor i32 %113, 1030
  %120 = xor i32 %113, %114
  %121 = and i32 %119, %120
  %122 = icmp slt i32 %121, 0
  store i1 %117, i1* %az
  store i1 %118, i1* %cf
  store i1 %122, i1* %of
  %123 = icmp eq i32 %114, 0
  store i1 %123, i1* %zf
  %124 = icmp slt i32 %114, 0
  store i1 %124, i1* %sf
  %125 = trunc i32 %114 to i8
  %126 = call i8 @llvm.ctpop.i8(i8 %125)
  %127 = and i8 %126, 1
  %128 = icmp eq i8 %127, 0
  store i1 %128, i1* %pf
  store volatile i64 67141, i64* @assembly_address
  %129 = load i1* %zf
  br i1 %129, label %block_106b4, label %block_10647

block_10647:                                      ; preds = %block_10640
  store volatile i64 67143, i64* @assembly_address
  br label %block_1071c

block_1064c:                                      ; preds = %block_105ed
  store volatile i64 67148, i64* @assembly_address
  %130 = load i32* %stack_var_-216
  %131 = zext i32 %130 to i64
  store i64 %131, i64* %rax
  store volatile i64 67154, i64* @assembly_address
  %132 = load i64* %rax
  %133 = trunc i64 %132 to i32
  %134 = sub i32 %133, 47
  %135 = and i32 %133, 15
  %136 = sub i32 %135, 15
  %137 = icmp ugt i32 %136, 15
  %138 = icmp ult i32 %133, 47
  %139 = xor i32 %133, 47
  %140 = xor i32 %133, %134
  %141 = and i32 %139, %140
  %142 = icmp slt i32 %141, 0
  store i1 %137, i1* %az
  store i1 %138, i1* %cf
  store i1 %142, i1* %of
  %143 = icmp eq i32 %134, 0
  store i1 %143, i1* %zf
  %144 = icmp slt i32 %134, 0
  store i1 %144, i1* %sf
  %145 = trunc i32 %134 to i8
  %146 = call i8 @llvm.ctpop.i8(i8 %145)
  %147 = and i8 %146, 1
  %148 = icmp eq i8 %147, 0
  store i1 %148, i1* %pf
  store volatile i64 67157, i64* @assembly_address
  %149 = load i1* %cf
  %150 = load i1* %zf
  %151 = or i1 %149, %150
  %152 = icmp ne i1 %151, true
  br i1 %152, label %block_1067a, label %block_10657

block_10657:                                      ; preds = %block_1064c
  store volatile i64 67159, i64* @assembly_address
  %153 = load i32** %stack_var_-200
  %154 = ptrtoint i32* %153 to i64
  store i64 %154, i64* %rax
  store volatile i64 67166, i64* @assembly_address
  %155 = load i32* %stack_var_-216
  %156 = zext i32 %155 to i64
  store i64 %156, i64* %rdx
  store volatile i64 67172, i64* @assembly_address
  %157 = load i64* %rdx
  %158 = trunc i64 %157 to i32
  %159 = zext i32 %158 to i64
  store i64 %159, i64* %rdx
  store volatile i64 67174, i64* @assembly_address
  %160 = load i64* %rax
  %161 = load i64* %rdx
  %162 = add i64 %160, %161
  %163 = and i64 %160, 15
  %164 = and i64 %161, 15
  %165 = add i64 %163, %164
  %166 = icmp ugt i64 %165, 15
  %167 = icmp ult i64 %162, %160
  %168 = xor i64 %160, %162
  %169 = xor i64 %161, %162
  %170 = and i64 %168, %169
  %171 = icmp slt i64 %170, 0
  store i1 %166, i1* %az
  store i1 %167, i1* %cf
  store i1 %171, i1* %of
  %172 = icmp eq i64 %162, 0
  store i1 %172, i1* %zf
  %173 = icmp slt i64 %162, 0
  store i1 %173, i1* %sf
  %174 = trunc i64 %162 to i8
  %175 = call i8 @llvm.ctpop.i8(i8 %174)
  %176 = and i8 %175, 1
  %177 = icmp eq i8 %176, 0
  store i1 %177, i1* %pf
  store i64 %162, i64* %rax
  store volatile i64 67177, i64* @assembly_address
  %178 = load i32* %stack_var_-216
  %179 = zext i32 %178 to i64
  store i64 %179, i64* %rdx
  store volatile i64 67183, i64* @assembly_address
  %180 = load i64* %rdx
  %181 = trunc i64 %180 to i32
  %182 = add i32 %181, 8
  %183 = and i32 %181, 15
  %184 = add i32 %183, 8
  %185 = icmp ugt i32 %184, 15
  %186 = icmp ult i32 %182, %181
  %187 = xor i32 %181, %182
  %188 = xor i32 8, %182
  %189 = and i32 %187, %188
  %190 = icmp slt i32 %189, 0
  store i1 %185, i1* %az
  store i1 %186, i1* %cf
  store i1 %190, i1* %of
  %191 = icmp eq i32 %182, 0
  store i1 %191, i1* %zf
  %192 = icmp slt i32 %182, 0
  store i1 %192, i1* %sf
  %193 = trunc i32 %182 to i8
  %194 = call i8 @llvm.ctpop.i8(i8 %193)
  %195 = and i8 %194, 1
  %196 = icmp eq i8 %195, 0
  store i1 %196, i1* %pf
  %197 = zext i32 %182 to i64
  store i64 %197, i64* %rdx
  store volatile i64 67186, i64* @assembly_address
  %198 = load i64* %rdx
  %199 = trunc i64 %198 to i32
  store i32 %199, i32* %stack_var_-216
  store volatile i64 67192, i64* @assembly_address
  br label %block_1068c

block_1067a:                                      ; preds = %block_1064c
  store volatile i64 67194, i64* @assembly_address
  %200 = load i64* %stack_var_-208
  store i64 %200, i64* %rax
  store volatile i64 67201, i64* @assembly_address
  %201 = load i64* %rax
  %202 = add i64 %201, 8
  store i64 %202, i64* %rdx
  store volatile i64 67205, i64* @assembly_address
  %203 = load i64* %rdx
  store i64 %203, i64* %stack_var_-208
  br label %block_1068c

block_1068c:                                      ; preds = %block_1067a, %block_10657
  store volatile i64 67212, i64* @assembly_address
  %204 = load i64* %rax
  %205 = inttoptr i64 %204 to i32*
  %206 = load i32* %205
  %207 = zext i32 %206 to i64
  store i64 %207, i64* %rax
  store volatile i64 67214, i64* @assembly_address
  %208 = load i64* %rax
  %209 = trunc i64 %208 to i32
  store i32 %209, i32* %stack_var_-232
  store volatile i64 67220, i64* @assembly_address
  %210 = load i32* %stack_var_-232
  %211 = zext i32 %210 to i64
  store i64 %211, i64* %rdx
  store volatile i64 67226, i64* @assembly_address
  %212 = load i32* %stack_var_-252
  %213 = zext i32 %212 to i64
  store i64 %213, i64* %rax
  store volatile i64 67232, i64* @assembly_address
  %214 = load i64* %rdx
  %215 = trunc i64 %214 to i32
  %216 = zext i32 %215 to i64
  store i64 %216, i64* %rsi
  store volatile i64 67234, i64* @assembly_address
  %217 = load i64* %rax
  %218 = trunc i64 %217 to i32
  %219 = zext i32 %218 to i64
  store i64 %219, i64* %rdi
  store volatile i64 67236, i64* @assembly_address
  %220 = load i64* %rdi
  %221 = load i64* %rsi
  %222 = trunc i64 %220 to i32
  %223 = call i64 @rpl_fcntl_DUPFD(i32 %222, i64 %221)
  store i64 %223, i64* %rax
  store i64 %223, i64* %rax
  store volatile i64 67241, i64* @assembly_address
  %224 = load i64* %rax
  %225 = trunc i64 %224 to i32
  store i32 %225, i32* %stack_var_-240
  store volatile i64 67247, i64* @assembly_address
  br label %block_108c9

block_106b4:                                      ; preds = %block_10640
  store volatile i64 67252, i64* @assembly_address
  %226 = load i32* %stack_var_-216
  %227 = zext i32 %226 to i64
  store i64 %227, i64* %rax
  store volatile i64 67258, i64* @assembly_address
  %228 = load i64* %rax
  %229 = trunc i64 %228 to i32
  %230 = sub i32 %229, 47
  %231 = and i32 %229, 15
  %232 = sub i32 %231, 15
  %233 = icmp ugt i32 %232, 15
  %234 = icmp ult i32 %229, 47
  %235 = xor i32 %229, 47
  %236 = xor i32 %229, %230
  %237 = and i32 %235, %236
  %238 = icmp slt i32 %237, 0
  store i1 %233, i1* %az
  store i1 %234, i1* %cf
  store i1 %238, i1* %of
  %239 = icmp eq i32 %230, 0
  store i1 %239, i1* %zf
  %240 = icmp slt i32 %230, 0
  store i1 %240, i1* %sf
  %241 = trunc i32 %230 to i8
  %242 = call i8 @llvm.ctpop.i8(i8 %241)
  %243 = and i8 %242, 1
  %244 = icmp eq i8 %243, 0
  store i1 %244, i1* %pf
  store volatile i64 67261, i64* @assembly_address
  %245 = load i1* %cf
  %246 = load i1* %zf
  %247 = or i1 %245, %246
  %248 = icmp ne i1 %247, true
  br i1 %248, label %block_106e2, label %block_106bf

block_106bf:                                      ; preds = %block_106b4
  store volatile i64 67263, i64* @assembly_address
  %249 = load i32** %stack_var_-200
  %250 = ptrtoint i32* %249 to i64
  store i64 %250, i64* %rax
  store volatile i64 67270, i64* @assembly_address
  %251 = load i32* %stack_var_-216
  %252 = zext i32 %251 to i64
  store i64 %252, i64* %rdx
  store volatile i64 67276, i64* @assembly_address
  %253 = load i64* %rdx
  %254 = trunc i64 %253 to i32
  %255 = zext i32 %254 to i64
  store i64 %255, i64* %rdx
  store volatile i64 67278, i64* @assembly_address
  %256 = load i64* %rax
  %257 = load i64* %rdx
  %258 = add i64 %256, %257
  %259 = and i64 %256, 15
  %260 = and i64 %257, 15
  %261 = add i64 %259, %260
  %262 = icmp ugt i64 %261, 15
  %263 = icmp ult i64 %258, %256
  %264 = xor i64 %256, %258
  %265 = xor i64 %257, %258
  %266 = and i64 %264, %265
  %267 = icmp slt i64 %266, 0
  store i1 %262, i1* %az
  store i1 %263, i1* %cf
  store i1 %267, i1* %of
  %268 = icmp eq i64 %258, 0
  store i1 %268, i1* %zf
  %269 = icmp slt i64 %258, 0
  store i1 %269, i1* %sf
  %270 = trunc i64 %258 to i8
  %271 = call i8 @llvm.ctpop.i8(i8 %270)
  %272 = and i8 %271, 1
  %273 = icmp eq i8 %272, 0
  store i1 %273, i1* %pf
  store i64 %258, i64* %rax
  store volatile i64 67281, i64* @assembly_address
  %274 = load i32* %stack_var_-216
  %275 = zext i32 %274 to i64
  store i64 %275, i64* %rdx
  store volatile i64 67287, i64* @assembly_address
  %276 = load i64* %rdx
  %277 = trunc i64 %276 to i32
  %278 = add i32 %277, 8
  %279 = and i32 %277, 15
  %280 = add i32 %279, 8
  %281 = icmp ugt i32 %280, 15
  %282 = icmp ult i32 %278, %277
  %283 = xor i32 %277, %278
  %284 = xor i32 8, %278
  %285 = and i32 %283, %284
  %286 = icmp slt i32 %285, 0
  store i1 %281, i1* %az
  store i1 %282, i1* %cf
  store i1 %286, i1* %of
  %287 = icmp eq i32 %278, 0
  store i1 %287, i1* %zf
  %288 = icmp slt i32 %278, 0
  store i1 %288, i1* %sf
  %289 = trunc i32 %278 to i8
  %290 = call i8 @llvm.ctpop.i8(i8 %289)
  %291 = and i8 %290, 1
  %292 = icmp eq i8 %291, 0
  store i1 %292, i1* %pf
  %293 = zext i32 %278 to i64
  store i64 %293, i64* %rdx
  store volatile i64 67290, i64* @assembly_address
  %294 = load i64* %rdx
  %295 = trunc i64 %294 to i32
  store i32 %295, i32* %stack_var_-216
  store volatile i64 67296, i64* @assembly_address
  br label %block_106f4

block_106e2:                                      ; preds = %block_106b4
  store volatile i64 67298, i64* @assembly_address
  %296 = load i64* %stack_var_-208
  store i64 %296, i64* %rax
  store volatile i64 67305, i64* @assembly_address
  %297 = load i64* %rax
  %298 = add i64 %297, 8
  store i64 %298, i64* %rdx
  store volatile i64 67309, i64* @assembly_address
  %299 = load i64* %rdx
  store i64 %299, i64* %stack_var_-208
  br label %block_106f4

block_106f4:                                      ; preds = %block_106e2, %block_106bf
  store volatile i64 67316, i64* @assembly_address
  %300 = load i64* %rax
  %301 = inttoptr i64 %300 to i32*
  %302 = load i32* %301
  %303 = zext i32 %302 to i64
  store i64 %303, i64* %rax
  store volatile i64 67318, i64* @assembly_address
  %304 = load i64* %rax
  %305 = trunc i64 %304 to i32
  store i32 %305, i32* %stack_var_-236
  store volatile i64 67324, i64* @assembly_address
  %306 = load i32* %stack_var_-236
  %307 = zext i32 %306 to i64
  store i64 %307, i64* %rdx
  store volatile i64 67330, i64* @assembly_address
  %308 = load i32* %stack_var_-252
  %309 = zext i32 %308 to i64
  store i64 %309, i64* %rax
  store volatile i64 67336, i64* @assembly_address
  %310 = load i64* %rdx
  %311 = trunc i64 %310 to i32
  %312 = zext i32 %311 to i64
  store i64 %312, i64* %rsi
  store volatile i64 67338, i64* @assembly_address
  %313 = load i64* %rax
  %314 = trunc i64 %313 to i32
  %315 = zext i32 %314 to i64
  store i64 %315, i64* %rdi
  store volatile i64 67340, i64* @assembly_address
  %316 = load i64* %rdi
  %317 = load i64* %rsi
  %318 = trunc i64 %316 to i32
  %319 = call i64 @rpl_fcntl_DUPFD_CLOEXEC(i32 %318, i64 %317)
  store i64 %319, i64* %rax
  store i64 %319, i64* %rax
  store volatile i64 67345, i64* @assembly_address
  %320 = load i64* %rax
  %321 = trunc i64 %320 to i32
  store i32 %321, i32* %stack_var_-240
  store volatile i64 67351, i64* @assembly_address
  br label %block_108c9

block_1071c:                                      ; preds = %block_10647
  store volatile i64 67356, i64* @assembly_address
  %322 = load i32* %stack_var_-256
  %323 = zext i32 %322 to i64
  store i64 %323, i64* %rax
  store volatile i64 67362, i64* @assembly_address
  %324 = load i64* %rax
  %325 = trunc i64 %324 to i32
  %326 = sub i32 %325, 11
  %327 = and i32 %325, 15
  %328 = sub i32 %327, 11
  %329 = icmp ugt i32 %328, 15
  %330 = icmp ult i32 %325, 11
  %331 = xor i32 %325, 11
  %332 = xor i32 %325, %326
  %333 = and i32 %331, %332
  %334 = icmp slt i32 %333, 0
  store i1 %329, i1* %az
  store i1 %330, i1* %cf
  store i1 %334, i1* %of
  %335 = icmp eq i32 %326, 0
  store i1 %335, i1* %zf
  %336 = icmp slt i32 %326, 0
  store i1 %336, i1* %sf
  %337 = trunc i32 %326 to i8
  %338 = call i8 @llvm.ctpop.i8(i8 %337)
  %339 = and i8 %338, 1
  %340 = icmp eq i8 %339, 0
  store i1 %340, i1* %pf
  store volatile i64 67365, i64* @assembly_address
  %341 = load i1* %zf
  br i1 %341, label %block_107c1, label %block_1072b

block_1072b:                                      ; preds = %block_1071c
  store volatile i64 67371, i64* @assembly_address
  %342 = load i64* %rax
  %343 = trunc i64 %342 to i32
  %344 = trunc i64 %342 to i32
  store i32 %344, i32* %21
  store i32 11, i32* %20
  %345 = sub i32 %343, 11
  %346 = and i32 %343, 15
  %347 = sub i32 %346, 11
  %348 = icmp ugt i32 %347, 15
  %349 = icmp ult i32 %343, 11
  %350 = xor i32 %343, 11
  %351 = xor i32 %343, %345
  %352 = and i32 %350, %351
  %353 = icmp slt i32 %352, 0
  store i1 %348, i1* %az
  store i1 %349, i1* %cf
  store i1 %353, i1* %of
  %354 = icmp eq i32 %345, 0
  store i1 %354, i1* %zf
  %355 = icmp slt i32 %345, 0
  store i1 %355, i1* %sf
  %356 = trunc i32 %345 to i8
  %357 = call i8 @llvm.ctpop.i8(i8 %356)
  %358 = and i8 %357, 1
  %359 = icmp eq i8 %358, 0
  store i1 %359, i1* %pf
  store volatile i64 67374, i64* @assembly_address
  %360 = load i32* %21
  %361 = sext i32 %360 to i64
  %362 = load i32* %20
  %363 = trunc i64 %361 to i32
  %364 = icmp sgt i32 %363, %362
  br i1 %364, label %block_10780, label %block_10730

block_10730:                                      ; preds = %block_1072b
  store volatile i64 67376, i64* @assembly_address
  %365 = load i64* %rax
  %366 = trunc i64 %365 to i32
  %367 = sub i32 %366, 3
  %368 = and i32 %366, 15
  %369 = sub i32 %368, 3
  %370 = icmp ugt i32 %369, 15
  %371 = icmp ult i32 %366, 3
  %372 = xor i32 %366, 3
  %373 = xor i32 %366, %367
  %374 = and i32 %372, %373
  %375 = icmp slt i32 %374, 0
  store i1 %370, i1* %az
  store i1 %371, i1* %cf
  store i1 %375, i1* %of
  %376 = icmp eq i32 %367, 0
  store i1 %376, i1* %zf
  %377 = icmp slt i32 %367, 0
  store i1 %377, i1* %sf
  %378 = trunc i32 %367 to i8
  %379 = call i8 @llvm.ctpop.i8(i8 %378)
  %380 = and i8 %379, 1
  %381 = icmp eq i8 %380, 0
  store i1 %381, i1* %pf
  store volatile i64 67379, i64* @assembly_address
  %382 = load i1* %zf
  br i1 %382, label %block_107c1, label %block_10739

block_10739:                                      ; preds = %block_10730
  store volatile i64 67385, i64* @assembly_address
  %383 = load i64* %rax
  %384 = trunc i64 %383 to i32
  %385 = trunc i64 %383 to i32
  store i32 %385, i32* %18
  store i32 3, i32* %17
  %386 = sub i32 %384, 3
  %387 = and i32 %384, 15
  %388 = sub i32 %387, 3
  %389 = icmp ugt i32 %388, 15
  %390 = icmp ult i32 %384, 3
  %391 = xor i32 %384, 3
  %392 = xor i32 %384, %386
  %393 = and i32 %391, %392
  %394 = icmp slt i32 %393, 0
  store i1 %389, i1* %az
  store i1 %390, i1* %cf
  store i1 %394, i1* %of
  %395 = icmp eq i32 %386, 0
  store i1 %395, i1* %zf
  %396 = icmp slt i32 %386, 0
  store i1 %396, i1* %sf
  %397 = trunc i32 %386 to i8
  %398 = call i8 @llvm.ctpop.i8(i8 %397)
  %399 = and i8 %398, 1
  %400 = icmp eq i8 %399, 0
  store i1 %400, i1* %pf
  store volatile i64 67388, i64* @assembly_address
  %401 = load i32* %18
  %402 = sext i32 %401 to i64
  %403 = load i32* %17
  %404 = trunc i64 %402 to i32
  %405 = icmp sgt i32 %404, %403
  br i1 %405, label %block_10759, label %block_1073e

block_1073e:                                      ; preds = %block_10739
  store volatile i64 67390, i64* @assembly_address
  %406 = load i64* %rax
  %407 = trunc i64 %406 to i32
  %408 = sub i32 %407, 1
  %409 = and i32 %407, 15
  %410 = sub i32 %409, 1
  %411 = icmp ugt i32 %410, 15
  %412 = icmp ult i32 %407, 1
  %413 = xor i32 %407, 1
  %414 = xor i32 %407, %408
  %415 = and i32 %413, %414
  %416 = icmp slt i32 %415, 0
  store i1 %411, i1* %az
  store i1 %412, i1* %cf
  store i1 %416, i1* %of
  %417 = icmp eq i32 %408, 0
  store i1 %417, i1* %zf
  %418 = icmp slt i32 %408, 0
  store i1 %418, i1* %sf
  %419 = trunc i32 %408 to i8
  %420 = call i8 @llvm.ctpop.i8(i8 %419)
  %421 = and i8 %420, 1
  %422 = icmp eq i8 %421, 0
  store i1 %422, i1* %pf
  store volatile i64 67393, i64* @assembly_address
  %423 = load i1* %zf
  br i1 %423, label %block_107c1, label %block_10743

block_10743:                                      ; preds = %block_1073e
  store volatile i64 67395, i64* @assembly_address
  %424 = load i64* %rax
  %425 = trunc i64 %424 to i32
  %426 = trunc i64 %424 to i32
  store i32 %426, i32* %15
  store i32 1, i32* %14
  %427 = sub i32 %425, 1
  %428 = and i32 %425, 15
  %429 = sub i32 %428, 1
  %430 = icmp ugt i32 %429, 15
  %431 = icmp ult i32 %425, 1
  %432 = xor i32 %425, 1
  %433 = xor i32 %425, %427
  %434 = and i32 %432, %433
  %435 = icmp slt i32 %434, 0
  store i1 %430, i1* %az
  store i1 %431, i1* %cf
  store i1 %435, i1* %of
  %436 = icmp eq i32 %427, 0
  store i1 %436, i1* %zf
  %437 = icmp slt i32 %427, 0
  store i1 %437, i1* %sf
  %438 = trunc i32 %427 to i8
  %439 = call i8 @llvm.ctpop.i8(i8 %438)
  %440 = and i8 %439, 1
  %441 = icmp eq i8 %440, 0
  store i1 %441, i1* %pf
  store volatile i64 67398, i64* @assembly_address
  %442 = load i32* %15
  %443 = sext i32 %442 to i64
  %444 = load i32* %14
  %445 = trunc i64 %443 to i32
  %446 = icmp sgt i32 %445, %444
  br i1 %446, label %block_107e6, label %block_1074c

block_1074c:                                      ; preds = %block_10743
  store volatile i64 67404, i64* @assembly_address
  %447 = load i64* %rax
  %448 = trunc i64 %447 to i32
  %449 = load i64* %rax
  %450 = trunc i64 %449 to i32
  %451 = and i32 %448, %450
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %452 = icmp eq i32 %451, 0
  store i1 %452, i1* %zf
  %453 = icmp slt i32 %451, 0
  store i1 %453, i1* %sf
  %454 = trunc i32 %451 to i8
  %455 = call i8 @llvm.ctpop.i8(i8 %454)
  %456 = and i8 %455, 1
  %457 = icmp eq i8 %456, 0
  store i1 %457, i1* %pf
  store volatile i64 67406, i64* @assembly_address
  %458 = load i1* %zf
  br i1 %458, label %block_107e6, label %block_10754

block_10754:                                      ; preds = %block_1074c
  store volatile i64 67412, i64* @assembly_address
  br label %block_10856

block_10759:                                      ; preds = %block_10739
  store volatile i64 67417, i64* @assembly_address
  %459 = load i64* %rax
  %460 = trunc i64 %459 to i32
  %461 = sub i32 %460, 8
  %462 = and i32 %460, 15
  %463 = sub i32 %462, 8
  %464 = icmp ugt i32 %463, 15
  %465 = icmp ult i32 %460, 8
  %466 = xor i32 %460, 8
  %467 = xor i32 %460, %461
  %468 = and i32 %466, %467
  %469 = icmp slt i32 %468, 0
  store i1 %464, i1* %az
  store i1 %465, i1* %cf
  store i1 %469, i1* %of
  %470 = icmp eq i32 %461, 0
  store i1 %470, i1* %zf
  %471 = icmp slt i32 %461, 0
  store i1 %471, i1* %sf
  %472 = trunc i32 %461 to i8
  %473 = call i8 @llvm.ctpop.i8(i8 %472)
  %474 = and i8 %473, 1
  %475 = icmp eq i8 %474, 0
  store i1 %475, i1* %pf
  store volatile i64 67420, i64* @assembly_address
  %476 = load i1* %zf
  br i1 %476, label %block_107e6, label %block_10762

block_10762:                                      ; preds = %block_10759
  store volatile i64 67426, i64* @assembly_address
  %477 = load i64* %rax
  %478 = trunc i64 %477 to i32
  %479 = trunc i64 %477 to i32
  store i32 %479, i32* %12
  store i32 8, i32* %11
  %480 = sub i32 %478, 8
  %481 = and i32 %478, 15
  %482 = sub i32 %481, 8
  %483 = icmp ugt i32 %482, 15
  %484 = icmp ult i32 %478, 8
  %485 = xor i32 %478, 8
  %486 = xor i32 %478, %480
  %487 = and i32 %485, %486
  %488 = icmp slt i32 %487, 0
  store i1 %483, i1* %az
  store i1 %484, i1* %cf
  store i1 %488, i1* %of
  %489 = icmp eq i32 %480, 0
  store i1 %489, i1* %zf
  %490 = icmp slt i32 %480, 0
  store i1 %490, i1* %sf
  %491 = trunc i32 %480 to i8
  %492 = call i8 @llvm.ctpop.i8(i8 %491)
  %493 = and i8 %492, 1
  %494 = icmp eq i8 %493, 0
  store i1 %494, i1* %pf
  store volatile i64 67429, i64* @assembly_address
  %495 = load i32* %12
  %496 = sext i32 %495 to i64
  %497 = load i32* %11
  %498 = trunc i64 %496 to i32
  %499 = icmp sgt i32 %498, %497
  br i1 %499, label %block_10771, label %block_10767

block_10767:                                      ; preds = %block_10762
  store volatile i64 67431, i64* @assembly_address
  %500 = load i64* %rax
  %501 = trunc i64 %500 to i32
  %502 = sub i32 %501, 4
  %503 = and i32 %501, 15
  %504 = sub i32 %503, 4
  %505 = icmp ugt i32 %504, 15
  %506 = icmp ult i32 %501, 4
  %507 = xor i32 %501, 4
  %508 = xor i32 %501, %502
  %509 = and i32 %507, %508
  %510 = icmp slt i32 %509, 0
  store i1 %505, i1* %az
  store i1 %506, i1* %cf
  store i1 %510, i1* %of
  %511 = icmp eq i32 %502, 0
  store i1 %511, i1* %zf
  %512 = icmp slt i32 %502, 0
  store i1 %512, i1* %sf
  %513 = trunc i32 %502 to i8
  %514 = call i8 @llvm.ctpop.i8(i8 %513)
  %515 = and i8 %514, 1
  %516 = icmp eq i8 %515, 0
  store i1 %516, i1* %pf
  store volatile i64 67434, i64* @assembly_address
  %517 = load i1* %zf
  br i1 %517, label %block_107e6, label %block_1076c

block_1076c:                                      ; preds = %block_10767
  store volatile i64 67436, i64* @assembly_address
  br label %block_10856

block_10771:                                      ; preds = %block_10762
  store volatile i64 67441, i64* @assembly_address
  %518 = load i64* %rax
  %519 = trunc i64 %518 to i32
  %520 = sub i32 %519, 9
  %521 = and i32 %519, 15
  %522 = sub i32 %521, 9
  %523 = icmp ugt i32 %522, 15
  %524 = icmp ult i32 %519, 9
  %525 = xor i32 %519, 9
  %526 = xor i32 %519, %520
  %527 = and i32 %525, %526
  %528 = icmp slt i32 %527, 0
  store i1 %523, i1* %az
  store i1 %524, i1* %cf
  store i1 %528, i1* %of
  %529 = icmp eq i32 %520, 0
  store i1 %529, i1* %zf
  %530 = icmp slt i32 %520, 0
  store i1 %530, i1* %sf
  %531 = trunc i32 %520 to i8
  %532 = call i8 @llvm.ctpop.i8(i8 %531)
  %533 = and i8 %532, 1
  %534 = icmp eq i8 %533, 0
  store i1 %534, i1* %pf
  store volatile i64 67444, i64* @assembly_address
  %535 = load i1* %zf
  br i1 %535, label %block_107c1, label %block_10776

block_10776:                                      ; preds = %block_10771
  store volatile i64 67446, i64* @assembly_address
  %536 = load i64* %rax
  %537 = trunc i64 %536 to i32
  %538 = sub i32 %537, 10
  %539 = and i32 %537, 15
  %540 = sub i32 %539, 10
  %541 = icmp ugt i32 %540, 15
  %542 = icmp ult i32 %537, 10
  %543 = xor i32 %537, 10
  %544 = xor i32 %537, %538
  %545 = and i32 %543, %544
  %546 = icmp slt i32 %545, 0
  store i1 %541, i1* %az
  store i1 %542, i1* %cf
  store i1 %546, i1* %of
  %547 = icmp eq i32 %538, 0
  store i1 %547, i1* %zf
  %548 = icmp slt i32 %538, 0
  store i1 %548, i1* %sf
  %549 = trunc i32 %538 to i8
  %550 = call i8 @llvm.ctpop.i8(i8 %549)
  %551 = and i8 %550, 1
  %552 = icmp eq i8 %551, 0
  store i1 %552, i1* %pf
  store volatile i64 67449, i64* @assembly_address
  %553 = load i1* %zf
  br i1 %553, label %block_107e6, label %block_1077b

block_1077b:                                      ; preds = %block_10776
  store volatile i64 67451, i64* @assembly_address
  br label %block_10856

block_10780:                                      ; preds = %block_1072b
  store volatile i64 67456, i64* @assembly_address
  %554 = load i64* %rax
  %555 = trunc i64 %554 to i32
  %556 = trunc i64 %554 to i32
  store i32 %556, i32* %9
  store i32 ptrtoint (i64* @global_var_407 to i32), i32* %8
  %557 = sub i32 %555, 1031
  %558 = and i32 %555, 15
  %559 = sub i32 %558, 7
  %560 = icmp ugt i32 %559, 15
  %561 = icmp ult i32 %555, 1031
  %562 = xor i32 %555, 1031
  %563 = xor i32 %555, %557
  %564 = and i32 %562, %563
  %565 = icmp slt i32 %564, 0
  store i1 %560, i1* %az
  store i1 %561, i1* %cf
  store i1 %565, i1* %of
  %566 = icmp eq i32 %557, 0
  store i1 %566, i1* %zf
  %567 = icmp slt i32 %557, 0
  store i1 %567, i1* %sf
  %568 = trunc i32 %557 to i8
  %569 = call i8 @llvm.ctpop.i8(i8 %568)
  %570 = and i8 %569, 1
  %571 = icmp eq i8 %570, 0
  store i1 %571, i1* %pf
  store volatile i64 67461, i64* @assembly_address
  %572 = load i32* %9
  %573 = sext i32 %572 to i64
  %574 = load i32* %8
  %575 = trunc i64 %573 to i32
  %576 = icmp sgt i32 %575, %574
  br i1 %576, label %block_107a8, label %block_10787

block_10787:                                      ; preds = %block_10780
  store volatile i64 67463, i64* @assembly_address
  %577 = load i64* %rax
  %578 = trunc i64 %577 to i32
  %579 = trunc i64 %577 to i32
  store i32 %579, i32* %6
  store i32 ptrtoint (i64* @global_var_406 to i32), i32* %5
  %580 = sub i32 %578, 1030
  %581 = and i32 %578, 15
  %582 = sub i32 %581, 6
  %583 = icmp ugt i32 %582, 15
  %584 = icmp ult i32 %578, 1030
  %585 = xor i32 %578, 1030
  %586 = xor i32 %578, %580
  %587 = and i32 %585, %586
  %588 = icmp slt i32 %587, 0
  store i1 %583, i1* %az
  store i1 %584, i1* %cf
  store i1 %588, i1* %of
  %589 = icmp eq i32 %580, 0
  store i1 %589, i1* %zf
  %590 = icmp slt i32 %580, 0
  store i1 %590, i1* %sf
  %591 = trunc i32 %580 to i8
  %592 = call i8 @llvm.ctpop.i8(i8 %591)
  %593 = and i8 %592, 1
  %594 = icmp eq i8 %593, 0
  store i1 %594, i1* %pf
  store volatile i64 67468, i64* @assembly_address
  %595 = load i32* %6
  %596 = sext i32 %595 to i64
  %597 = load i32* %5
  %598 = trunc i64 %596 to i32
  %599 = icmp sge i32 %598, %597
  br i1 %599, label %block_107e6, label %block_1078e

block_1078e:                                      ; preds = %block_10787
  store volatile i64 67470, i64* @assembly_address
  %600 = load i64* %rax
  %601 = trunc i64 %600 to i32
  %602 = sub i32 %601, 1025
  %603 = and i32 %601, 15
  %604 = sub i32 %603, 1
  %605 = icmp ugt i32 %604, 15
  %606 = icmp ult i32 %601, 1025
  %607 = xor i32 %601, 1025
  %608 = xor i32 %601, %602
  %609 = and i32 %607, %608
  %610 = icmp slt i32 %609, 0
  store i1 %605, i1* %az
  store i1 %606, i1* %cf
  store i1 %610, i1* %of
  %611 = icmp eq i32 %602, 0
  store i1 %611, i1* %zf
  %612 = icmp slt i32 %602, 0
  store i1 %612, i1* %sf
  %613 = trunc i32 %602 to i8
  %614 = call i8 @llvm.ctpop.i8(i8 %613)
  %615 = and i8 %614, 1
  %616 = icmp eq i8 %615, 0
  store i1 %616, i1* %pf
  store volatile i64 67475, i64* @assembly_address
  %617 = load i1* %zf
  br i1 %617, label %block_107c1, label %block_10795

block_10795:                                      ; preds = %block_1078e
  store volatile i64 67477, i64* @assembly_address
  %618 = load i64* %rax
  %619 = trunc i64 %618 to i32
  %620 = sub i32 %619, 1026
  %621 = and i32 %619, 15
  %622 = sub i32 %621, 2
  %623 = icmp ugt i32 %622, 15
  %624 = icmp ult i32 %619, 1026
  %625 = xor i32 %619, 1026
  %626 = xor i32 %619, %620
  %627 = and i32 %625, %626
  %628 = icmp slt i32 %627, 0
  store i1 %623, i1* %az
  store i1 %624, i1* %cf
  store i1 %628, i1* %of
  %629 = icmp eq i32 %620, 0
  store i1 %629, i1* %zf
  %630 = icmp slt i32 %620, 0
  store i1 %630, i1* %sf
  %631 = trunc i32 %620 to i8
  %632 = call i8 @llvm.ctpop.i8(i8 %631)
  %633 = and i8 %632, 1
  %634 = icmp eq i8 %633, 0
  store i1 %634, i1* %pf
  store volatile i64 67482, i64* @assembly_address
  %635 = load i1* %zf
  br i1 %635, label %block_107e6, label %block_1079c

block_1079c:                                      ; preds = %block_10795
  store volatile i64 67484, i64* @assembly_address
  %636 = load i64* %rax
  %637 = trunc i64 %636 to i32
  %638 = sub i32 %637, 1024
  %639 = and i32 %637, 15
  %640 = icmp ugt i32 %639, 15
  %641 = icmp ult i32 %637, 1024
  %642 = xor i32 %637, 1024
  %643 = xor i32 %637, %638
  %644 = and i32 %642, %643
  %645 = icmp slt i32 %644, 0
  store i1 %640, i1* %az
  store i1 %641, i1* %cf
  store i1 %645, i1* %of
  %646 = icmp eq i32 %638, 0
  store i1 %646, i1* %zf
  %647 = icmp slt i32 %638, 0
  store i1 %647, i1* %sf
  %648 = trunc i32 %638 to i8
  %649 = call i8 @llvm.ctpop.i8(i8 %648)
  %650 = and i8 %649, 1
  %651 = icmp eq i8 %650, 0
  store i1 %651, i1* %pf
  store volatile i64 67489, i64* @assembly_address
  %652 = load i1* %zf
  br i1 %652, label %block_107e6, label %block_107a3

block_107a3:                                      ; preds = %block_1079c
  store volatile i64 67491, i64* @assembly_address
  br label %block_10856

block_107a8:                                      ; preds = %block_10780
  store volatile i64 67496, i64* @assembly_address
  %653 = load i64* %rax
  %654 = trunc i64 %653 to i32
  %655 = sub i32 %654, 1033
  %656 = and i32 %654, 15
  %657 = sub i32 %656, 9
  %658 = icmp ugt i32 %657, 15
  %659 = icmp ult i32 %654, 1033
  %660 = xor i32 %654, 1033
  %661 = xor i32 %654, %655
  %662 = and i32 %660, %661
  %663 = icmp slt i32 %662, 0
  store i1 %658, i1* %az
  store i1 %659, i1* %cf
  store i1 %663, i1* %of
  %664 = icmp eq i32 %655, 0
  store i1 %664, i1* %zf
  %665 = icmp slt i32 %655, 0
  store i1 %665, i1* %sf
  %666 = trunc i32 %655 to i8
  %667 = call i8 @llvm.ctpop.i8(i8 %666)
  %668 = and i8 %667, 1
  %669 = icmp eq i8 %668, 0
  store i1 %669, i1* %pf
  store volatile i64 67501, i64* @assembly_address
  %670 = load i1* %zf
  br i1 %670, label %block_107e6, label %block_107af

block_107af:                                      ; preds = %block_107a8
  store volatile i64 67503, i64* @assembly_address
  %671 = load i64* %rax
  %672 = trunc i64 %671 to i32
  %673 = trunc i64 %671 to i32
  store i32 %673, i32* %3
  store i32 ptrtoint (i64* @global_var_409 to i32), i32* %2
  %674 = sub i32 %672, 1033
  %675 = and i32 %672, 15
  %676 = sub i32 %675, 9
  %677 = icmp ugt i32 %676, 15
  %678 = icmp ult i32 %672, 1033
  %679 = xor i32 %672, 1033
  %680 = xor i32 %672, %674
  %681 = and i32 %679, %680
  %682 = icmp slt i32 %681, 0
  store i1 %677, i1* %az
  store i1 %678, i1* %cf
  store i1 %682, i1* %of
  %683 = icmp eq i32 %674, 0
  store i1 %683, i1* %zf
  %684 = icmp slt i32 %674, 0
  store i1 %684, i1* %sf
  %685 = trunc i32 %674 to i8
  %686 = call i8 @llvm.ctpop.i8(i8 %685)
  %687 = and i8 %686, 1
  %688 = icmp eq i8 %687, 0
  store i1 %688, i1* %pf
  store volatile i64 67508, i64* @assembly_address
  %689 = load i32* %3
  %690 = sext i32 %689 to i64
  %691 = load i32* %2
  %692 = trunc i64 %690 to i32
  %693 = icmp slt i32 %692, %691
  br i1 %693, label %block_107c1, label %block_107b6

block_107b6:                                      ; preds = %block_107af
  store volatile i64 67510, i64* @assembly_address
  %694 = load i64* %rax
  %695 = trunc i64 %694 to i32
  %696 = sub i32 %695, 1034
  %697 = and i32 %695, 15
  %698 = sub i32 %697, 10
  %699 = icmp ugt i32 %698, 15
  %700 = icmp ult i32 %695, 1034
  %701 = xor i32 %695, 1034
  %702 = xor i32 %695, %696
  %703 = and i32 %701, %702
  %704 = icmp slt i32 %703, 0
  store i1 %699, i1* %az
  store i1 %700, i1* %cf
  store i1 %704, i1* %of
  %705 = icmp eq i32 %696, 0
  store i1 %705, i1* %zf
  %706 = icmp slt i32 %696, 0
  store i1 %706, i1* %sf
  %707 = trunc i32 %696 to i8
  %708 = call i8 @llvm.ctpop.i8(i8 %707)
  %709 = and i8 %708, 1
  %710 = icmp eq i8 %709, 0
  store i1 %710, i1* %pf
  store volatile i64 67515, i64* @assembly_address
  %711 = load i1* %zf
  %712 = icmp eq i1 %711, false
  br i1 %712, label %block_10856, label %block_107c1

block_107c1:                                      ; preds = %block_107b6, %block_107af, %block_1078e, %block_10771, %block_1073e, %block_10730, %block_1071c
  store volatile i64 67521, i64* @assembly_address
  %713 = load i32* %stack_var_-256
  %714 = zext i32 %713 to i64
  store i64 %714, i64* %rdx
  store volatile i64 67527, i64* @assembly_address
  %715 = load i32* %stack_var_-252
  %716 = zext i32 %715 to i64
  store i64 %716, i64* %rax
  store volatile i64 67533, i64* @assembly_address
  %717 = load i64* %rdx
  %718 = trunc i64 %717 to i32
  %719 = zext i32 %718 to i64
  store i64 %719, i64* %rsi
  store volatile i64 67535, i64* @assembly_address
  %720 = load i64* %rax
  %721 = trunc i64 %720 to i32
  %722 = zext i32 %721 to i64
  store i64 %722, i64* %rdi
  store volatile i64 67537, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 67542, i64* @assembly_address
  %723 = load i64* %rdi
  %724 = trunc i64 %723 to i32
  %725 = load i64* %rsi
  %726 = trunc i64 %725 to i32
  %727 = call i32 (i32, i32, ...)* @fcntl(i32 %724, i32 %726)
  %728 = sext i32 %727 to i64
  store i64 %728, i64* %rax
  %729 = sext i32 %727 to i64
  store i64 %729, i64* %rax
  store volatile i64 67547, i64* @assembly_address
  %730 = load i64* %rax
  %731 = trunc i64 %730 to i32
  store i32 %731, i32* %stack_var_-240
  store volatile i64 67553, i64* @assembly_address
  br label %block_108c8

block_107e6:                                      ; preds = %block_107a8, %block_1079c, %block_10795, %block_10787, %block_10776, %block_10767, %block_10759, %block_1074c, %block_10743
  store volatile i64 67558, i64* @assembly_address
  %732 = load i32* %stack_var_-216
  %733 = zext i32 %732 to i64
  store i64 %733, i64* %rax
  store volatile i64 67564, i64* @assembly_address
  %734 = load i64* %rax
  %735 = trunc i64 %734 to i32
  %736 = sub i32 %735, 47
  %737 = and i32 %735, 15
  %738 = sub i32 %737, 15
  %739 = icmp ugt i32 %738, 15
  %740 = icmp ult i32 %735, 47
  %741 = xor i32 %735, 47
  %742 = xor i32 %735, %736
  %743 = and i32 %741, %742
  %744 = icmp slt i32 %743, 0
  store i1 %739, i1* %az
  store i1 %740, i1* %cf
  store i1 %744, i1* %of
  %745 = icmp eq i32 %736, 0
  store i1 %745, i1* %zf
  %746 = icmp slt i32 %736, 0
  store i1 %746, i1* %sf
  %747 = trunc i32 %736 to i8
  %748 = call i8 @llvm.ctpop.i8(i8 %747)
  %749 = and i8 %748, 1
  %750 = icmp eq i8 %749, 0
  store i1 %750, i1* %pf
  store volatile i64 67567, i64* @assembly_address
  %751 = load i1* %cf
  %752 = load i1* %zf
  %753 = or i1 %751, %752
  %754 = icmp ne i1 %753, true
  br i1 %754, label %block_10814, label %block_107f1

block_107f1:                                      ; preds = %block_107e6
  store volatile i64 67569, i64* @assembly_address
  %755 = load i32** %stack_var_-200
  %756 = ptrtoint i32* %755 to i64
  store i64 %756, i64* %rax
  store volatile i64 67576, i64* @assembly_address
  %757 = load i32* %stack_var_-216
  %758 = zext i32 %757 to i64
  store i64 %758, i64* %rdx
  store volatile i64 67582, i64* @assembly_address
  %759 = load i64* %rdx
  %760 = trunc i64 %759 to i32
  %761 = zext i32 %760 to i64
  store i64 %761, i64* %rdx
  store volatile i64 67584, i64* @assembly_address
  %762 = load i64* %rax
  %763 = load i64* %rdx
  %764 = add i64 %762, %763
  %765 = and i64 %762, 15
  %766 = and i64 %763, 15
  %767 = add i64 %765, %766
  %768 = icmp ugt i64 %767, 15
  %769 = icmp ult i64 %764, %762
  %770 = xor i64 %762, %764
  %771 = xor i64 %763, %764
  %772 = and i64 %770, %771
  %773 = icmp slt i64 %772, 0
  store i1 %768, i1* %az
  store i1 %769, i1* %cf
  store i1 %773, i1* %of
  %774 = icmp eq i64 %764, 0
  store i1 %774, i1* %zf
  %775 = icmp slt i64 %764, 0
  store i1 %775, i1* %sf
  %776 = trunc i64 %764 to i8
  %777 = call i8 @llvm.ctpop.i8(i8 %776)
  %778 = and i8 %777, 1
  %779 = icmp eq i8 %778, 0
  store i1 %779, i1* %pf
  store i64 %764, i64* %rax
  store volatile i64 67587, i64* @assembly_address
  %780 = load i32* %stack_var_-216
  %781 = zext i32 %780 to i64
  store i64 %781, i64* %rdx
  store volatile i64 67593, i64* @assembly_address
  %782 = load i64* %rdx
  %783 = trunc i64 %782 to i32
  %784 = add i32 %783, 8
  %785 = and i32 %783, 15
  %786 = add i32 %785, 8
  %787 = icmp ugt i32 %786, 15
  %788 = icmp ult i32 %784, %783
  %789 = xor i32 %783, %784
  %790 = xor i32 8, %784
  %791 = and i32 %789, %790
  %792 = icmp slt i32 %791, 0
  store i1 %787, i1* %az
  store i1 %788, i1* %cf
  store i1 %792, i1* %of
  %793 = icmp eq i32 %784, 0
  store i1 %793, i1* %zf
  %794 = icmp slt i32 %784, 0
  store i1 %794, i1* %sf
  %795 = trunc i32 %784 to i8
  %796 = call i8 @llvm.ctpop.i8(i8 %795)
  %797 = and i8 %796, 1
  %798 = icmp eq i8 %797, 0
  store i1 %798, i1* %pf
  %799 = zext i32 %784 to i64
  store i64 %799, i64* %rdx
  store volatile i64 67596, i64* @assembly_address
  %800 = load i64* %rdx
  %801 = trunc i64 %800 to i32
  store i32 %801, i32* %stack_var_-216
  store volatile i64 67602, i64* @assembly_address
  br label %block_10826

block_10814:                                      ; preds = %block_107e6
  store volatile i64 67604, i64* @assembly_address
  %802 = load i64* %stack_var_-208
  store i64 %802, i64* %rax
  store volatile i64 67611, i64* @assembly_address
  %803 = load i64* %rax
  %804 = add i64 %803, 8
  store i64 %804, i64* %rdx
  store volatile i64 67615, i64* @assembly_address
  %805 = load i64* %rdx
  store i64 %805, i64* %stack_var_-208
  br label %block_10826

block_10826:                                      ; preds = %block_10814, %block_107f1
  store volatile i64 67622, i64* @assembly_address
  %806 = load i64* %rax
  %807 = inttoptr i64 %806 to i32*
  %808 = load i32* %807
  %809 = zext i32 %808 to i64
  store i64 %809, i64* %rax
  store volatile i64 67624, i64* @assembly_address
  %810 = load i64* %rax
  %811 = trunc i64 %810 to i32
  store i32 %811, i32* %stack_var_-228
  store volatile i64 67630, i64* @assembly_address
  %812 = load i32* %stack_var_-228
  %813 = zext i32 %812 to i64
  store i64 %813, i64* %rdx
  store volatile i64 67636, i64* @assembly_address
  %814 = load i32* %stack_var_-256
  %815 = zext i32 %814 to i64
  store i64 %815, i64* %rcx
  store volatile i64 67642, i64* @assembly_address
  %816 = load i32* %stack_var_-252
  %817 = zext i32 %816 to i64
  store i64 %817, i64* %rax
  store volatile i64 67648, i64* @assembly_address
  %818 = load i64* %rcx
  %819 = trunc i64 %818 to i32
  %820 = zext i32 %819 to i64
  store i64 %820, i64* %rsi
  store volatile i64 67650, i64* @assembly_address
  %821 = load i64* %rax
  %822 = trunc i64 %821 to i32
  %823 = zext i32 %822 to i64
  store i64 %823, i64* %rdi
  store volatile i64 67652, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 67657, i64* @assembly_address
  %824 = load i64* %rdi
  %825 = trunc i64 %824 to i32
  %826 = load i64* %rsi
  %827 = trunc i64 %826 to i32
  %828 = call i32 (i32, i32, ...)* @fcntl(i32 %825, i32 %827)
  %829 = sext i32 %828 to i64
  store i64 %829, i64* %rax
  %830 = sext i32 %828 to i64
  store i64 %830, i64* %rax
  store volatile i64 67662, i64* @assembly_address
  %831 = load i64* %rax
  %832 = trunc i64 %831 to i32
  store i32 %832, i32* %stack_var_-240
  store volatile i64 67668, i64* @assembly_address
  br label %block_108c8

block_10856:                                      ; preds = %block_107b6, %block_107a3, %block_1077b, %block_1076c, %block_10754
  store volatile i64 67670, i64* @assembly_address
  %833 = load i32* %stack_var_-216
  %834 = zext i32 %833 to i64
  store i64 %834, i64* %rax
  store volatile i64 67676, i64* @assembly_address
  %835 = load i64* %rax
  %836 = trunc i64 %835 to i32
  %837 = sub i32 %836, 47
  %838 = and i32 %836, 15
  %839 = sub i32 %838, 15
  %840 = icmp ugt i32 %839, 15
  %841 = icmp ult i32 %836, 47
  %842 = xor i32 %836, 47
  %843 = xor i32 %836, %837
  %844 = and i32 %842, %843
  %845 = icmp slt i32 %844, 0
  store i1 %840, i1* %az
  store i1 %841, i1* %cf
  store i1 %845, i1* %of
  %846 = icmp eq i32 %837, 0
  store i1 %846, i1* %zf
  %847 = icmp slt i32 %837, 0
  store i1 %847, i1* %sf
  %848 = trunc i32 %837 to i8
  %849 = call i8 @llvm.ctpop.i8(i8 %848)
  %850 = and i8 %849, 1
  %851 = icmp eq i8 %850, 0
  store i1 %851, i1* %pf
  store volatile i64 67679, i64* @assembly_address
  %852 = load i1* %cf
  %853 = load i1* %zf
  %854 = or i1 %852, %853
  %855 = icmp ne i1 %854, true
  br i1 %855, label %block_10884, label %block_10861

block_10861:                                      ; preds = %block_10856
  store volatile i64 67681, i64* @assembly_address
  %856 = load i32** %stack_var_-200
  %857 = ptrtoint i32* %856 to i64
  store i64 %857, i64* %rax
  store volatile i64 67688, i64* @assembly_address
  %858 = load i32* %stack_var_-216
  %859 = zext i32 %858 to i64
  store i64 %859, i64* %rdx
  store volatile i64 67694, i64* @assembly_address
  %860 = load i64* %rdx
  %861 = trunc i64 %860 to i32
  %862 = zext i32 %861 to i64
  store i64 %862, i64* %rdx
  store volatile i64 67696, i64* @assembly_address
  %863 = load i64* %rax
  %864 = load i64* %rdx
  %865 = add i64 %863, %864
  %866 = and i64 %863, 15
  %867 = and i64 %864, 15
  %868 = add i64 %866, %867
  %869 = icmp ugt i64 %868, 15
  %870 = icmp ult i64 %865, %863
  %871 = xor i64 %863, %865
  %872 = xor i64 %864, %865
  %873 = and i64 %871, %872
  %874 = icmp slt i64 %873, 0
  store i1 %869, i1* %az
  store i1 %870, i1* %cf
  store i1 %874, i1* %of
  %875 = icmp eq i64 %865, 0
  store i1 %875, i1* %zf
  %876 = icmp slt i64 %865, 0
  store i1 %876, i1* %sf
  %877 = trunc i64 %865 to i8
  %878 = call i8 @llvm.ctpop.i8(i8 %877)
  %879 = and i8 %878, 1
  %880 = icmp eq i8 %879, 0
  store i1 %880, i1* %pf
  store i64 %865, i64* %rax
  store volatile i64 67699, i64* @assembly_address
  %881 = load i32* %stack_var_-216
  %882 = zext i32 %881 to i64
  store i64 %882, i64* %rdx
  store volatile i64 67705, i64* @assembly_address
  %883 = load i64* %rdx
  %884 = trunc i64 %883 to i32
  %885 = add i32 %884, 8
  %886 = and i32 %884, 15
  %887 = add i32 %886, 8
  %888 = icmp ugt i32 %887, 15
  %889 = icmp ult i32 %885, %884
  %890 = xor i32 %884, %885
  %891 = xor i32 8, %885
  %892 = and i32 %890, %891
  %893 = icmp slt i32 %892, 0
  store i1 %888, i1* %az
  store i1 %889, i1* %cf
  store i1 %893, i1* %of
  %894 = icmp eq i32 %885, 0
  store i1 %894, i1* %zf
  %895 = icmp slt i32 %885, 0
  store i1 %895, i1* %sf
  %896 = trunc i32 %885 to i8
  %897 = call i8 @llvm.ctpop.i8(i8 %896)
  %898 = and i8 %897, 1
  %899 = icmp eq i8 %898, 0
  store i1 %899, i1* %pf
  %900 = zext i32 %885 to i64
  store i64 %900, i64* %rdx
  store volatile i64 67708, i64* @assembly_address
  %901 = load i64* %rdx
  %902 = trunc i64 %901 to i32
  store i32 %902, i32* %stack_var_-216
  store volatile i64 67714, i64* @assembly_address
  br label %block_10896

block_10884:                                      ; preds = %block_10856
  store volatile i64 67716, i64* @assembly_address
  %903 = load i64* %stack_var_-208
  store i64 %903, i64* %rax
  store volatile i64 67723, i64* @assembly_address
  %904 = load i64* %rax
  %905 = add i64 %904, 8
  store i64 %905, i64* %rdx
  store volatile i64 67727, i64* @assembly_address
  %906 = load i64* %rdx
  store i64 %906, i64* %stack_var_-208
  br label %block_10896

block_10896:                                      ; preds = %block_10884, %block_10861
  store volatile i64 67734, i64* @assembly_address
  %907 = load i64* %rax
  %908 = inttoptr i64 %907 to i64*
  %909 = load i64* %908
  store i64 %909, i64* %rax
  store volatile i64 67737, i64* @assembly_address
  %910 = load i64* %rax
  store i64 %910, i64* %stack_var_-224
  store volatile i64 67744, i64* @assembly_address
  %911 = load i64* %stack_var_-224
  store i64 %911, i64* %rdx
  store volatile i64 67751, i64* @assembly_address
  %912 = load i32* %stack_var_-256
  %913 = zext i32 %912 to i64
  store i64 %913, i64* %rcx
  store volatile i64 67757, i64* @assembly_address
  %914 = load i32* %stack_var_-252
  %915 = zext i32 %914 to i64
  store i64 %915, i64* %rax
  store volatile i64 67763, i64* @assembly_address
  %916 = load i64* %rcx
  %917 = trunc i64 %916 to i32
  %918 = zext i32 %917 to i64
  store i64 %918, i64* %rsi
  store volatile i64 67765, i64* @assembly_address
  %919 = load i64* %rax
  %920 = trunc i64 %919 to i32
  %921 = zext i32 %920 to i64
  store i64 %921, i64* %rdi
  store volatile i64 67767, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 67772, i64* @assembly_address
  %922 = load i64* %rdi
  %923 = trunc i64 %922 to i32
  %924 = load i64* %rsi
  %925 = trunc i64 %924 to i32
  %926 = call i32 (i32, i32, ...)* @fcntl(i32 %923, i32 %925)
  %927 = sext i32 %926 to i64
  store i64 %927, i64* %rax
  %928 = sext i32 %926 to i64
  store i64 %928, i64* %rax
  store volatile i64 67777, i64* @assembly_address
  %929 = load i64* %rax
  %930 = trunc i64 %929 to i32
  store i32 %930, i32* %stack_var_-240
  store volatile i64 67783, i64* @assembly_address
  br label %block_108c8

block_108c8:                                      ; preds = %block_10896, %block_10826, %block_107c1
  store volatile i64 67784, i64* @assembly_address
  br label %block_108c9

block_108c9:                                      ; preds = %block_108c8, %block_106f4, %block_1068c
  store volatile i64 67785, i64* @assembly_address
  %931 = load i32* %stack_var_-240
  %932 = zext i32 %931 to i64
  store i64 %932, i64* %rax
  store volatile i64 67791, i64* @assembly_address
  %933 = load i64* %stack_var_-192
  store i64 %933, i64* %rcx
  store volatile i64 67798, i64* @assembly_address
  %934 = load i64* %rcx
  %935 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %936 = xor i64 %934, %935
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %937 = icmp eq i64 %936, 0
  store i1 %937, i1* %zf
  %938 = icmp slt i64 %936, 0
  store i1 %938, i1* %sf
  %939 = trunc i64 %936 to i8
  %940 = call i8 @llvm.ctpop.i8(i8 %939)
  %941 = and i8 %940, 1
  %942 = icmp eq i8 %941, 0
  store i1 %942, i1* %pf
  store i64 %936, i64* %rcx
  store volatile i64 67807, i64* @assembly_address
  %943 = load i1* %zf
  br i1 %943, label %block_108e6, label %block_108e1

block_108e1:                                      ; preds = %block_108c9
  store volatile i64 67809, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_108e6:                                      ; preds = %block_108c9
  store volatile i64 67814, i64* @assembly_address
  %944 = load i64* %stack_var_-8
  store i64 %944, i64* %rbp
  %945 = ptrtoint i64* %stack_var_0 to i64
  store i64 %945, i64* %rsp
  store volatile i64 67815, i64* @assembly_address
  %946 = load i64* %rax
  ret i64 %946
}

declare i64 @297(i64, i32, i64, i64, i64, i64)

declare i64 @298(i64, i64, i64, i64, i64, i64)

define i64 @rpl_fcntl_DUPFD(i32 %arg1, i64 %arg2) {
block_108e8:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 67816, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 67817, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 67820, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 67824, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-28
  store volatile i64 67827, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-32
  store volatile i64 67830, i64* @assembly_address
  %24 = load i32* %stack_var_-32
  %25 = zext i32 %24 to i64
  store i64 %25, i64* %rdx
  store volatile i64 67833, i64* @assembly_address
  %26 = load i32* %stack_var_-28
  %27 = zext i32 %26 to i64
  store i64 %27, i64* %rax
  store volatile i64 67836, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 67841, i64* @assembly_address
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  %30 = zext i32 %29 to i64
  store i64 %30, i64* %rdi
  store volatile i64 67843, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 67848, i64* @assembly_address
  %31 = load i64* %rdi
  %32 = trunc i64 %31 to i32
  %33 = load i64* %rsi
  %34 = trunc i64 %33 to i32
  %35 = call i32 (i32, i32, ...)* @fcntl(i32 %32, i32 %34)
  %36 = sext i32 %35 to i64
  store i64 %36, i64* %rax
  %37 = sext i32 %35 to i64
  store i64 %37, i64* %rax
  store volatile i64 67853, i64* @assembly_address
  %38 = load i64* %rax
  %39 = trunc i64 %38 to i32
  store i32 %39, i32* %stack_var_-12
  store volatile i64 67856, i64* @assembly_address
  %40 = load i32* %stack_var_-12
  %41 = zext i32 %40 to i64
  store i64 %41, i64* %rax
  store volatile i64 67859, i64* @assembly_address
  %42 = load i64* %stack_var_-8
  store i64 %42, i64* %rbp
  %43 = ptrtoint i64* %stack_var_0 to i64
  store i64 %43, i64* %rsp
  store volatile i64 67860, i64* @assembly_address
  %44 = load i64* %rax
  ret i64 %44
}

declare i64 @299(i64, i32)

declare i64 @300(i64, i64)

define i64 @rpl_fcntl_DUPFD_CLOEXEC(i32 %arg1, i64 %arg2) {
block_10915:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-16 = alloca i32
  %stack_var_-20 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 67861, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 67862, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 67865, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 32
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 32
  %9 = xor i64 %4, 32
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %19, i64* %rsp
  store volatile i64 67869, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-28
  store volatile i64 67872, i64* @assembly_address
  %22 = load i64* %rsi
  %23 = trunc i64 %22 to i32
  store i32 %23, i32* %stack_var_-32
  store volatile i64 67875, i64* @assembly_address
  %24 = load i32* bitcast (i64* @global_var_21a418 to i32*)
  %25 = zext i32 %24 to i64
  store i64 %25, i64* %rax
  store volatile i64 67881, i64* @assembly_address
  %26 = load i64* %rax
  %27 = trunc i64 %26 to i32
  %28 = load i64* %rax
  %29 = trunc i64 %28 to i32
  %30 = and i32 %27, %29
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %31 = icmp eq i32 %30, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i32 %30, 0
  store i1 %32, i1* %sf
  %33 = trunc i32 %30 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  store volatile i64 67883, i64* @assembly_address
  %37 = load i1* %sf
  br i1 %37, label %block_10989, label %block_1092d

block_1092d:                                      ; preds = %block_10915
  store volatile i64 67885, i64* @assembly_address
  %38 = load i32* %stack_var_-32
  %39 = zext i32 %38 to i64
  store i64 %39, i64* %rdx
  store volatile i64 67888, i64* @assembly_address
  %40 = load i32* %stack_var_-28
  %41 = zext i32 %40 to i64
  store i64 %41, i64* %rax
  store volatile i64 67891, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_406 to i64), i64* %rsi
  store volatile i64 67896, i64* @assembly_address
  %42 = load i64* %rax
  %43 = trunc i64 %42 to i32
  %44 = zext i32 %43 to i64
  store i64 %44, i64* %rdi
  store volatile i64 67898, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 67903, i64* @assembly_address
  %45 = load i64* %rdi
  %46 = trunc i64 %45 to i32
  %47 = load i64* %rsi
  %48 = trunc i64 %47 to i32
  %49 = call i32 (i32, i32, ...)* @fcntl(i32 %46, i32 %48)
  %50 = sext i32 %49 to i64
  store i64 %50, i64* %rax
  %51 = sext i32 %49 to i64
  store i64 %51, i64* %rax
  store volatile i64 67908, i64* @assembly_address
  %52 = load i64* %rax
  %53 = trunc i64 %52 to i32
  store i32 %53, i32* %stack_var_-20
  store volatile i64 67911, i64* @assembly_address
  %54 = load i32* %stack_var_-20
  %55 = and i32 %54, 15
  %56 = icmp ugt i32 %55, 15
  %57 = icmp ult i32 %54, 0
  %58 = xor i32 %54, 0
  %59 = and i32 %58, 0
  %60 = icmp slt i32 %59, 0
  store i1 %56, i1* %az
  store i1 %57, i1* %cf
  store i1 %60, i1* %of
  %61 = icmp eq i32 %54, 0
  store i1 %61, i1* %zf
  %62 = icmp slt i32 %54, 0
  store i1 %62, i1* %sf
  %63 = trunc i32 %54 to i8
  %64 = call i8 @llvm.ctpop.i8(i8 %63)
  %65 = and i8 %64, 1
  %66 = icmp eq i8 %65, 0
  store i1 %66, i1* %pf
  store volatile i64 67915, i64* @assembly_address
  %67 = load i1* %sf
  %68 = icmp eq i1 %67, false
  br i1 %68, label %block_10959, label %block_1094d

block_1094d:                                      ; preds = %block_1092d
  store volatile i64 67917, i64* @assembly_address
  %69 = call i32* @__errno_location()
  %70 = ptrtoint i32* %69 to i64
  store i64 %70, i64* %rax
  %71 = ptrtoint i32* %69 to i64
  store i64 %71, i64* %rax
  %72 = ptrtoint i32* %69 to i64
  store i64 %72, i64* %rax
  store volatile i64 67922, i64* @assembly_address
  %73 = load i64* %rax
  %74 = inttoptr i64 %73 to i32*
  %75 = load i32* %74
  %76 = zext i32 %75 to i64
  store i64 %76, i64* %rax
  store volatile i64 67924, i64* @assembly_address
  %77 = load i64* %rax
  %78 = trunc i64 %77 to i32
  %79 = sub i32 %78, 22
  %80 = and i32 %78, 15
  %81 = sub i32 %80, 6
  %82 = icmp ugt i32 %81, 15
  %83 = icmp ult i32 %78, 22
  %84 = xor i32 %78, 22
  %85 = xor i32 %78, %79
  %86 = and i32 %84, %85
  %87 = icmp slt i32 %86, 0
  store i1 %82, i1* %az
  store i1 %83, i1* %cf
  store i1 %87, i1* %of
  %88 = icmp eq i32 %79, 0
  store i1 %88, i1* %zf
  %89 = icmp slt i32 %79, 0
  store i1 %89, i1* %sf
  %90 = trunc i32 %79 to i8
  %91 = call i8 @llvm.ctpop.i8(i8 %90)
  %92 = and i8 %91, 1
  %93 = icmp eq i8 %92, 0
  store i1 %93, i1* %pf
  store volatile i64 67927, i64* @assembly_address
  %94 = load i1* %zf
  br i1 %94, label %block_10965, label %block_10959

block_10959:                                      ; preds = %block_1094d, %block_1092d
  store volatile i64 67929, i64* @assembly_address
  store i32 1, i32* bitcast (i64* @global_var_21a418 to i32*)
  store volatile i64 67939, i64* @assembly_address
  br label %block_1099b

block_10965:                                      ; preds = %block_1094d
  store volatile i64 67941, i64* @assembly_address
  %95 = load i32* %stack_var_-32
  %96 = zext i32 %95 to i64
  store i64 %96, i64* %rdx
  store volatile i64 67944, i64* @assembly_address
  %97 = load i32* %stack_var_-28
  %98 = zext i32 %97 to i64
  store i64 %98, i64* %rax
  store volatile i64 67947, i64* @assembly_address
  %99 = load i64* %rdx
  %100 = trunc i64 %99 to i32
  %101 = zext i32 %100 to i64
  store i64 %101, i64* %rsi
  store volatile i64 67949, i64* @assembly_address
  %102 = load i64* %rax
  %103 = trunc i64 %102 to i32
  %104 = zext i32 %103 to i64
  store i64 %104, i64* %rdi
  store volatile i64 67951, i64* @assembly_address
  %105 = load i64* %rdi
  %106 = load i64* %rsi
  %107 = trunc i64 %105 to i32
  %108 = call i64 @rpl_fcntl_DUPFD(i32 %107, i64 %106)
  store i64 %108, i64* %rax
  store i64 %108, i64* %rax
  store volatile i64 67956, i64* @assembly_address
  %109 = load i64* %rax
  %110 = trunc i64 %109 to i32
  store i32 %110, i32* %stack_var_-20
  store volatile i64 67959, i64* @assembly_address
  %111 = load i32* %stack_var_-20
  %112 = and i32 %111, 15
  %113 = icmp ugt i32 %112, 15
  %114 = icmp ult i32 %111, 0
  %115 = xor i32 %111, 0
  %116 = and i32 %115, 0
  %117 = icmp slt i32 %116, 0
  store i1 %113, i1* %az
  store i1 %114, i1* %cf
  store i1 %117, i1* %of
  %118 = icmp eq i32 %111, 0
  store i1 %118, i1* %zf
  %119 = icmp slt i32 %111, 0
  store i1 %119, i1* %sf
  %120 = trunc i32 %111 to i8
  %121 = call i8 @llvm.ctpop.i8(i8 %120)
  %122 = and i8 %121, 1
  %123 = icmp eq i8 %122, 0
  store i1 %123, i1* %pf
  store volatile i64 67963, i64* @assembly_address
  %124 = load i1* %sf
  br i1 %124, label %block_1099b, label %block_1097d

block_1097d:                                      ; preds = %block_10965
  store volatile i64 67965, i64* @assembly_address
  store i32 -1, i32* bitcast (i64* @global_var_21a418 to i32*)
  store volatile i64 67975, i64* @assembly_address
  br label %block_1099b

block_10989:                                      ; preds = %block_10915
  store volatile i64 67977, i64* @assembly_address
  %125 = load i32* %stack_var_-32
  %126 = zext i32 %125 to i64
  store i64 %126, i64* %rdx
  store volatile i64 67980, i64* @assembly_address
  %127 = load i32* %stack_var_-28
  %128 = zext i32 %127 to i64
  store i64 %128, i64* %rax
  store volatile i64 67983, i64* @assembly_address
  %129 = load i64* %rdx
  %130 = trunc i64 %129 to i32
  %131 = zext i32 %130 to i64
  store i64 %131, i64* %rsi
  store volatile i64 67985, i64* @assembly_address
  %132 = load i64* %rax
  %133 = trunc i64 %132 to i32
  %134 = zext i32 %133 to i64
  store i64 %134, i64* %rdi
  store volatile i64 67987, i64* @assembly_address
  %135 = load i64* %rdi
  %136 = load i64* %rsi
  %137 = trunc i64 %135 to i32
  %138 = call i64 @rpl_fcntl_DUPFD(i32 %137, i64 %136)
  store i64 %138, i64* %rax
  store i64 %138, i64* %rax
  store volatile i64 67992, i64* @assembly_address
  %139 = load i64* %rax
  %140 = trunc i64 %139 to i32
  store i32 %140, i32* %stack_var_-20
  br label %block_1099b

block_1099b:                                      ; preds = %block_10989, %block_1097d, %block_10965, %block_10959
  store volatile i64 67995, i64* @assembly_address
  %141 = load i32* %stack_var_-20
  %142 = and i32 %141, 15
  %143 = icmp ugt i32 %142, 15
  %144 = icmp ult i32 %141, 0
  %145 = xor i32 %141, 0
  %146 = and i32 %145, 0
  %147 = icmp slt i32 %146, 0
  store i1 %143, i1* %az
  store i1 %144, i1* %cf
  store i1 %147, i1* %of
  %148 = icmp eq i32 %141, 0
  store i1 %148, i1* %zf
  %149 = icmp slt i32 %141, 0
  store i1 %149, i1* %sf
  %150 = trunc i32 %141 to i8
  %151 = call i8 @llvm.ctpop.i8(i8 %150)
  %152 = and i8 %151, 1
  %153 = icmp eq i8 %152, 0
  store i1 %153, i1* %pf
  store volatile i64 67999, i64* @assembly_address
  %154 = load i1* %sf
  br i1 %154, label %block_10a12, label %block_109a1

block_109a1:                                      ; preds = %block_1099b
  store volatile i64 68001, i64* @assembly_address
  %155 = load i32* bitcast (i64* @global_var_21a418 to i32*)
  %156 = zext i32 %155 to i64
  store i64 %156, i64* %rax
  store volatile i64 68007, i64* @assembly_address
  %157 = load i64* %rax
  %158 = trunc i64 %157 to i32
  %159 = sub i32 %158, -1
  %160 = and i32 %158, 15
  %161 = sub i32 %160, 15
  %162 = icmp ugt i32 %161, 15
  %163 = icmp ult i32 %158, -1
  %164 = xor i32 %158, -1
  %165 = xor i32 %158, %159
  %166 = and i32 %164, %165
  %167 = icmp slt i32 %166, 0
  store i1 %162, i1* %az
  store i1 %163, i1* %cf
  store i1 %167, i1* %of
  %168 = icmp eq i32 %159, 0
  store i1 %168, i1* %zf
  %169 = icmp slt i32 %159, 0
  store i1 %169, i1* %sf
  %170 = trunc i32 %159 to i8
  %171 = call i8 @llvm.ctpop.i8(i8 %170)
  %172 = and i8 %171, 1
  %173 = icmp eq i8 %172, 0
  store i1 %173, i1* %pf
  store volatile i64 68010, i64* @assembly_address
  %174 = load i1* %zf
  %175 = icmp eq i1 %174, false
  br i1 %175, label %block_10a12, label %block_109ac

block_109ac:                                      ; preds = %block_109a1
  store volatile i64 68012, i64* @assembly_address
  %176 = load i32* %stack_var_-20
  %177 = zext i32 %176 to i64
  store i64 %177, i64* %rax
  store volatile i64 68015, i64* @assembly_address
  store i64 1, i64* %rsi
  store volatile i64 68020, i64* @assembly_address
  %178 = load i64* %rax
  %179 = trunc i64 %178 to i32
  %180 = zext i32 %179 to i64
  store i64 %180, i64* %rdi
  store volatile i64 68022, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 68027, i64* @assembly_address
  %181 = load i64* %rdi
  %182 = trunc i64 %181 to i32
  %183 = load i64* %rsi
  %184 = trunc i64 %183 to i32
  %185 = call i32 (i32, i32, ...)* @fcntl(i32 %182, i32 %184)
  %186 = sext i32 %185 to i64
  store i64 %186, i64* %rax
  %187 = sext i32 %185 to i64
  store i64 %187, i64* %rax
  store volatile i64 68032, i64* @assembly_address
  %188 = load i64* %rax
  %189 = trunc i64 %188 to i32
  store i32 %189, i32* %stack_var_-16
  store volatile i64 68035, i64* @assembly_address
  %190 = load i32* %stack_var_-16
  %191 = and i32 %190, 15
  %192 = icmp ugt i32 %191, 15
  %193 = icmp ult i32 %190, 0
  %194 = xor i32 %190, 0
  %195 = and i32 %194, 0
  %196 = icmp slt i32 %195, 0
  store i1 %192, i1* %az
  store i1 %193, i1* %cf
  store i1 %196, i1* %of
  %197 = icmp eq i32 %190, 0
  store i1 %197, i1* %zf
  %198 = icmp slt i32 %190, 0
  store i1 %198, i1* %sf
  %199 = trunc i32 %190 to i8
  %200 = call i8 @llvm.ctpop.i8(i8 %199)
  %201 = and i8 %200, 1
  %202 = icmp eq i8 %201, 0
  store i1 %202, i1* %pf
  store volatile i64 68039, i64* @assembly_address
  %203 = load i1* %sf
  br i1 %203, label %block_109ea, label %block_109c9

block_109c9:                                      ; preds = %block_109ac
  store volatile i64 68041, i64* @assembly_address
  %204 = load i32* %stack_var_-16
  %205 = zext i32 %204 to i64
  store i64 %205, i64* %rax
  store volatile i64 68044, i64* @assembly_address
  %206 = load i64* %rax
  %207 = trunc i64 %206 to i32
  %208 = or i32 %207, 1
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %209 = icmp eq i32 %208, 0
  store i1 %209, i1* %zf
  %210 = icmp slt i32 %208, 0
  store i1 %210, i1* %sf
  %211 = trunc i32 %208 to i8
  %212 = call i8 @llvm.ctpop.i8(i8 %211)
  %213 = and i8 %212, 1
  %214 = icmp eq i8 %213, 0
  store i1 %214, i1* %pf
  %215 = zext i32 %208 to i64
  store i64 %215, i64* %rax
  store volatile i64 68047, i64* @assembly_address
  %216 = load i64* %rax
  %217 = trunc i64 %216 to i32
  %218 = zext i32 %217 to i64
  store i64 %218, i64* %rdx
  store volatile i64 68049, i64* @assembly_address
  %219 = load i32* %stack_var_-20
  %220 = zext i32 %219 to i64
  store i64 %220, i64* %rax
  store volatile i64 68052, i64* @assembly_address
  store i64 2, i64* %rsi
  store volatile i64 68057, i64* @assembly_address
  %221 = load i64* %rax
  %222 = trunc i64 %221 to i32
  %223 = zext i32 %222 to i64
  store i64 %223, i64* %rdi
  store volatile i64 68059, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 68064, i64* @assembly_address
  %224 = load i64* %rdi
  %225 = trunc i64 %224 to i32
  %226 = load i64* %rsi
  %227 = trunc i64 %226 to i32
  %228 = call i32 (i32, i32, ...)* @fcntl(i32 %225, i32 %227)
  %229 = sext i32 %228 to i64
  store i64 %229, i64* %rax
  %230 = sext i32 %228 to i64
  store i64 %230, i64* %rax
  store volatile i64 68069, i64* @assembly_address
  %231 = load i64* %rax
  %232 = trunc i64 %231 to i32
  %233 = sub i32 %232, -1
  %234 = and i32 %232, 15
  %235 = sub i32 %234, 15
  %236 = icmp ugt i32 %235, 15
  %237 = icmp ult i32 %232, -1
  %238 = xor i32 %232, -1
  %239 = xor i32 %232, %233
  %240 = and i32 %238, %239
  %241 = icmp slt i32 %240, 0
  store i1 %236, i1* %az
  store i1 %237, i1* %cf
  store i1 %241, i1* %of
  %242 = icmp eq i32 %233, 0
  store i1 %242, i1* %zf
  %243 = icmp slt i32 %233, 0
  store i1 %243, i1* %sf
  %244 = trunc i32 %233 to i8
  %245 = call i8 @llvm.ctpop.i8(i8 %244)
  %246 = and i8 %245, 1
  %247 = icmp eq i8 %246, 0
  store i1 %247, i1* %pf
  store volatile i64 68072, i64* @assembly_address
  %248 = load i1* %zf
  %249 = icmp eq i1 %248, false
  br i1 %249, label %block_10a12, label %block_109ea

block_109ea:                                      ; preds = %block_109c9, %block_109ac
  store volatile i64 68074, i64* @assembly_address
  %250 = call i32* @__errno_location()
  %251 = ptrtoint i32* %250 to i64
  store i64 %251, i64* %rax
  %252 = ptrtoint i32* %250 to i64
  store i64 %252, i64* %rax
  %253 = ptrtoint i32* %250 to i64
  store i64 %253, i64* %rax
  store volatile i64 68079, i64* @assembly_address
  %254 = load i64* %rax
  %255 = inttoptr i64 %254 to i32*
  %256 = load i32* %255
  %257 = zext i32 %256 to i64
  store i64 %257, i64* %rax
  store volatile i64 68081, i64* @assembly_address
  %258 = load i64* %rax
  %259 = trunc i64 %258 to i32
  store i32 %259, i32* %stack_var_-12
  store volatile i64 68084, i64* @assembly_address
  %260 = load i32* %stack_var_-20
  %261 = zext i32 %260 to i64
  store i64 %261, i64* %rax
  store volatile i64 68087, i64* @assembly_address
  %262 = load i64* %rax
  %263 = trunc i64 %262 to i32
  %264 = zext i32 %263 to i64
  store i64 %264, i64* %rdi
  store volatile i64 68089, i64* @assembly_address
  %265 = load i64* %rdi
  %266 = trunc i64 %265 to i32
  %267 = call i32 @close(i32 %266)
  %268 = sext i32 %267 to i64
  store i64 %268, i64* %rax
  %269 = sext i32 %267 to i64
  store i64 %269, i64* %rax
  store volatile i64 68094, i64* @assembly_address
  %270 = call i32* @__errno_location()
  %271 = ptrtoint i32* %270 to i64
  store i64 %271, i64* %rax
  %272 = ptrtoint i32* %270 to i64
  store i64 %272, i64* %rax
  %273 = ptrtoint i32* %270 to i64
  store i64 %273, i64* %rax
  store volatile i64 68099, i64* @assembly_address
  %274 = load i64* %rax
  store i64 %274, i64* %rdx
  store volatile i64 68102, i64* @assembly_address
  %275 = load i32* %stack_var_-12
  %276 = zext i32 %275 to i64
  store i64 %276, i64* %rax
  store volatile i64 68105, i64* @assembly_address
  %277 = load i64* %rax
  %278 = trunc i64 %277 to i32
  %279 = load i64* %rdx
  %280 = inttoptr i64 %279 to i32*
  store i32 %278, i32* %280
  store volatile i64 68107, i64* @assembly_address
  store i32 -1, i32* %stack_var_-20
  br label %block_10a12

block_10a12:                                      ; preds = %block_109ea, %block_109c9, %block_109a1, %block_1099b
  store volatile i64 68114, i64* @assembly_address
  %281 = load i32* %stack_var_-20
  %282 = zext i32 %281 to i64
  store i64 %282, i64* %rax
  store volatile i64 68117, i64* @assembly_address
  %283 = load i64* %stack_var_-8
  store i64 %283, i64* %rbp
  %284 = ptrtoint i64* %stack_var_0 to i64
  store i64 %284, i64* %rsp
  store volatile i64 68118, i64* @assembly_address
  %285 = load i64* %rax
  ret i64 %285
}

declare i64 @301(i64, i32)

declare i64 @302(i64, i64)

define i64 @clear_ungetc_buffer_preserving_position(i32* %arg1) {
block_10a17:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i32* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i32*
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68119, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 68120, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 68123, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 68127, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to i32*
  store i32* %22, i32** %stack_var_-16
  store volatile i64 68131, i64* @assembly_address
  %23 = load i32** %stack_var_-16
  %24 = ptrtoint i32* %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 68135, i64* @assembly_address
  %25 = load i64* %rax
  %26 = inttoptr i64 %25 to i32*
  %27 = load i32* %26
  %28 = zext i32 %27 to i64
  store i64 %28, i64* %rax
  store volatile i64 68137, i64* @assembly_address
  %29 = load i64* %rax
  %30 = trunc i64 %29 to i32
  %31 = and i32 %30, 256
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %32 = icmp eq i32 %31, 0
  store i1 %32, i1* %zf
  %33 = icmp slt i32 %31, 0
  store i1 %33, i1* %sf
  %34 = trunc i32 %31 to i8
  %35 = call i8 @llvm.ctpop.i8(i8 %34)
  %36 = and i8 %35, 1
  %37 = icmp eq i8 %36, 0
  store i1 %37, i1* %pf
  %38 = zext i32 %31 to i64
  store i64 %38, i64* %rax
  store volatile i64 68142, i64* @assembly_address
  %39 = load i64* %rax
  %40 = trunc i64 %39 to i32
  %41 = load i64* %rax
  %42 = trunc i64 %41 to i32
  %43 = and i32 %40, %42
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %44 = icmp eq i32 %43, 0
  store i1 %44, i1* %zf
  %45 = icmp slt i32 %43, 0
  store i1 %45, i1* %sf
  %46 = trunc i32 %43 to i8
  %47 = call i8 @llvm.ctpop.i8(i8 %46)
  %48 = and i8 %47, 1
  %49 = icmp eq i8 %48, 0
  store i1 %49, i1* %pf
  store volatile i64 68144, i64* @assembly_address
  %50 = load i1* %zf
  br i1 %50, label %block_10a48, label %block_10a32

block_10a32:                                      ; preds = %block_10a17
  store volatile i64 68146, i64* @assembly_address
  %51 = load i32** %stack_var_-16
  %52 = ptrtoint i32* %51 to i64
  store i64 %52, i64* %rax
  store volatile i64 68150, i64* @assembly_address
  store i64 1, i64* %rdx
  store volatile i64 68155, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 68160, i64* @assembly_address
  %53 = load i64* %rax
  store i64 %53, i64* %rdi
  store volatile i64 68163, i64* @assembly_address
  %54 = load i64* %rdi
  %55 = load i64* %rsi
  %56 = load i64* %rdx
  %57 = inttoptr i64 %54 to %_IO_FILE*
  %58 = call i64 @rpl_fseeko(%_IO_FILE* %57, i64 %55, i64 %56)
  store i64 %58, i64* %rax
  store i64 %58, i64* %rax
  br label %block_10a48

block_10a48:                                      ; preds = %block_10a32, %block_10a17
  store volatile i64 68168, i64* @assembly_address
  store volatile i64 68169, i64* @assembly_address
  %59 = load i64* %stack_var_-8
  store i64 %59, i64* %rbp
  %60 = ptrtoint i64* %stack_var_0 to i64
  store i64 %60, i64* %rsp
  store volatile i64 68170, i64* @assembly_address
  %61 = load i64* %rax
  ret i64 %61
}

declare i64 @303(i64)

define i64 @rpl_fflush(%_IO_FILE* %arg1) {
block_10a4b:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint %_IO_FILE* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca %_IO_FILE*
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68171, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 68172, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 68175, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 68179, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to %_IO_FILE*
  store %_IO_FILE* %22, %_IO_FILE** %stack_var_-16
  store volatile i64 68183, i64* @assembly_address
  %23 = load %_IO_FILE** %stack_var_-16
  %24 = ptrtoint %_IO_FILE* %23 to i64
  %25 = and i64 %24, 15
  %26 = icmp ugt i64 %25, 15
  %27 = icmp ult i64 %24, 0
  %28 = xor i64 %24, 0
  %29 = and i64 %28, 0
  %30 = icmp slt i64 %29, 0
  store i1 %26, i1* %az
  store i1 %27, i1* %cf
  store i1 %30, i1* %of
  %31 = icmp eq i64 %24, 0
  store i1 %31, i1* %zf
  %32 = icmp slt i64 %24, 0
  store i1 %32, i1* %sf
  %33 = trunc i64 %24 to i8
  %34 = call i8 @llvm.ctpop.i8(i8 %33)
  %35 = and i8 %34, 1
  %36 = icmp eq i8 %35, 0
  store i1 %36, i1* %pf
  store volatile i64 68188, i64* @assembly_address
  %37 = load i1* %zf
  br i1 %37, label %block_10a6e, label %block_10a5e

block_10a5e:                                      ; preds = %block_10a4b
  store volatile i64 68190, i64* @assembly_address
  %38 = load %_IO_FILE** %stack_var_-16
  %39 = ptrtoint %_IO_FILE* %38 to i64
  store i64 %39, i64* %rax
  store volatile i64 68194, i64* @assembly_address
  %40 = load i64* %rax
  store i64 %40, i64* %rdi
  store volatile i64 68197, i64* @assembly_address
  %41 = load i64* %rdi
  %42 = inttoptr i64 %41 to %_IO_FILE*
  %43 = call i32 @__freading(%_IO_FILE* %42)
  %44 = sext i32 %43 to i64
  store i64 %44, i64* %rax
  %45 = sext i32 %43 to i64
  store i64 %45, i64* %rax
  store volatile i64 68202, i64* @assembly_address
  %46 = load i64* %rax
  %47 = trunc i64 %46 to i32
  %48 = load i64* %rax
  %49 = trunc i64 %48 to i32
  %50 = and i32 %47, %49
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %51 = icmp eq i32 %50, 0
  store i1 %51, i1* %zf
  %52 = icmp slt i32 %50, 0
  store i1 %52, i1* %sf
  %53 = trunc i32 %50 to i8
  %54 = call i8 @llvm.ctpop.i8(i8 %53)
  %55 = and i8 %54, 1
  %56 = icmp eq i8 %55, 0
  store i1 %56, i1* %pf
  store volatile i64 68204, i64* @assembly_address
  %57 = load i1* %zf
  %58 = icmp eq i1 %57, false
  br i1 %58, label %block_10a7c, label %block_10a6e

block_10a6e:                                      ; preds = %block_10a5e, %block_10a4b
  store volatile i64 68206, i64* @assembly_address
  %59 = load %_IO_FILE** %stack_var_-16
  %60 = ptrtoint %_IO_FILE* %59 to i64
  store i64 %60, i64* %rax
  store volatile i64 68210, i64* @assembly_address
  %61 = load i64* %rax
  store i64 %61, i64* %rdi
  store volatile i64 68213, i64* @assembly_address
  %62 = load i64* %rdi
  %63 = inttoptr i64 %62 to %_IO_FILE*
  %64 = call i32 @fflush(%_IO_FILE* %63)
  %65 = sext i32 %64 to i64
  store i64 %65, i64* %rax
  %66 = sext i32 %64 to i64
  store i64 %66, i64* %rax
  store volatile i64 68218, i64* @assembly_address
  br label %block_10a94

block_10a7c:                                      ; preds = %block_10a5e
  store volatile i64 68220, i64* @assembly_address
  %67 = load %_IO_FILE** %stack_var_-16
  %68 = ptrtoint %_IO_FILE* %67 to i64
  store i64 %68, i64* %rax
  store volatile i64 68224, i64* @assembly_address
  %69 = load i64* %rax
  store i64 %69, i64* %rdi
  store volatile i64 68227, i64* @assembly_address
  %70 = load i64* %rdi
  %71 = inttoptr i64 %70 to i32*
  %72 = call i64 @clear_ungetc_buffer_preserving_position(i32* %71)
  store i64 %72, i64* %rax
  store i64 %72, i64* %rax
  store volatile i64 68232, i64* @assembly_address
  %73 = load %_IO_FILE** %stack_var_-16
  %74 = ptrtoint %_IO_FILE* %73 to i64
  store i64 %74, i64* %rax
  store volatile i64 68236, i64* @assembly_address
  %75 = load i64* %rax
  store i64 %75, i64* %rdi
  store volatile i64 68239, i64* @assembly_address
  %76 = load i64* %rdi
  %77 = inttoptr i64 %76 to %_IO_FILE*
  %78 = call i32 @fflush(%_IO_FILE* %77)
  %79 = sext i32 %78 to i64
  store i64 %79, i64* %rax
  %80 = sext i32 %78 to i64
  store i64 %80, i64* %rax
  br label %block_10a94

block_10a94:                                      ; preds = %block_10a7c, %block_10a6e
  store volatile i64 68244, i64* @assembly_address
  %81 = load i64* %stack_var_-8
  store i64 %81, i64* %rbp
  %82 = ptrtoint i64* %stack_var_0 to i64
  store i64 %82, i64* %rsp
  store volatile i64 68245, i64* @assembly_address
  %83 = load i64* %rax
  ret i64 %83
}

declare i64 @304(i64)

define i64 @rpl_fseeko(%_IO_FILE* %arg1, i64 %arg2, i64 %arg3) {
block_10a96:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = ptrtoint %_IO_FILE* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-44 = alloca i32
  %stack_var_-40 = alloca i32
  %1 = alloca i64
  %stack_var_-32 = alloca %_IO_FILE*
  %2 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68246, i64* @assembly_address
  %3 = load i64* %rbp
  store i64 %3, i64* %stack_var_-8
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rsp
  store volatile i64 68247, i64* @assembly_address
  %5 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %5, i64* %rbp
  store volatile i64 68250, i64* @assembly_address
  %6 = load i64* %rsp
  %7 = sub i64 %6, 48
  %8 = and i64 %6, 15
  %9 = icmp ugt i64 %8, 15
  %10 = icmp ult i64 %6, 48
  %11 = xor i64 %6, 48
  %12 = xor i64 %6, %7
  %13 = and i64 %11, %12
  %14 = icmp slt i64 %13, 0
  store i1 %9, i1* %az
  store i1 %10, i1* %cf
  store i1 %14, i1* %of
  %15 = icmp eq i64 %7, 0
  store i1 %15, i1* %zf
  %16 = icmp slt i64 %7, 0
  store i1 %16, i1* %sf
  %17 = trunc i64 %7 to i8
  %18 = call i8 @llvm.ctpop.i8(i8 %17)
  %19 = and i8 %18, 1
  %20 = icmp eq i8 %19, 0
  store i1 %20, i1* %pf
  %21 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %21, i64* %rsp
  store volatile i64 68254, i64* @assembly_address
  %22 = load i64* %rdi
  %23 = inttoptr i64 %22 to %_IO_FILE*
  store %_IO_FILE* %23, %_IO_FILE** %stack_var_-32
  store volatile i64 68258, i64* @assembly_address
  %24 = load i64* %rsi
  %25 = trunc i64 %24 to i32
  store i32 %25, i32* %stack_var_-40
  store volatile i64 68262, i64* @assembly_address
  %26 = load i64* %rdx
  %27 = trunc i64 %26 to i32
  store i32 %27, i32* %stack_var_-44
  store volatile i64 68265, i64* @assembly_address
  %28 = load %_IO_FILE** %stack_var_-32
  %29 = ptrtoint %_IO_FILE* %28 to i64
  store i64 %29, i64* %rax
  store volatile i64 68269, i64* @assembly_address
  %30 = load i64* %rax
  %31 = add i64 %30, 16
  %32 = inttoptr i64 %31 to i64*
  %33 = load i64* %32
  store i64 %33, i64* %rdx
  store volatile i64 68273, i64* @assembly_address
  %34 = load %_IO_FILE** %stack_var_-32
  %35 = ptrtoint %_IO_FILE* %34 to i64
  store i64 %35, i64* %rax
  store volatile i64 68277, i64* @assembly_address
  %36 = load i64* %rax
  %37 = add i64 %36, 8
  %38 = inttoptr i64 %37 to i64*
  %39 = load i64* %38
  store i64 %39, i64* %rax
  store volatile i64 68281, i64* @assembly_address
  %40 = load i64* %rdx
  %41 = load i64* %rax
  %42 = sub i64 %40, %41
  %43 = and i64 %40, 15
  %44 = and i64 %41, 15
  %45 = sub i64 %43, %44
  %46 = icmp ugt i64 %45, 15
  %47 = icmp ult i64 %40, %41
  %48 = xor i64 %40, %41
  %49 = xor i64 %40, %42
  %50 = and i64 %48, %49
  %51 = icmp slt i64 %50, 0
  store i1 %46, i1* %az
  store i1 %47, i1* %cf
  store i1 %51, i1* %of
  %52 = icmp eq i64 %42, 0
  store i1 %52, i1* %zf
  %53 = icmp slt i64 %42, 0
  store i1 %53, i1* %sf
  %54 = trunc i64 %42 to i8
  %55 = call i8 @llvm.ctpop.i8(i8 %54)
  %56 = and i8 %55, 1
  %57 = icmp eq i8 %56, 0
  store i1 %57, i1* %pf
  store volatile i64 68284, i64* @assembly_address
  %58 = load i1* %zf
  %59 = icmp eq i1 %58, false
  br i1 %59, label %block_10b38, label %block_10abe

block_10abe:                                      ; preds = %block_10a96
  store volatile i64 68286, i64* @assembly_address
  %60 = load %_IO_FILE** %stack_var_-32
  %61 = ptrtoint %_IO_FILE* %60 to i64
  store i64 %61, i64* %rax
  store volatile i64 68290, i64* @assembly_address
  %62 = load i64* %rax
  %63 = add i64 %62, 40
  %64 = inttoptr i64 %63 to i64*
  %65 = load i64* %64
  store i64 %65, i64* %rdx
  store volatile i64 68294, i64* @assembly_address
  %66 = load %_IO_FILE** %stack_var_-32
  %67 = ptrtoint %_IO_FILE* %66 to i64
  store i64 %67, i64* %rax
  store volatile i64 68298, i64* @assembly_address
  %68 = load i64* %rax
  %69 = add i64 %68, 32
  %70 = inttoptr i64 %69 to i64*
  %71 = load i64* %70
  store i64 %71, i64* %rax
  store volatile i64 68302, i64* @assembly_address
  %72 = load i64* %rdx
  %73 = load i64* %rax
  %74 = sub i64 %72, %73
  %75 = and i64 %72, 15
  %76 = and i64 %73, 15
  %77 = sub i64 %75, %76
  %78 = icmp ugt i64 %77, 15
  %79 = icmp ult i64 %72, %73
  %80 = xor i64 %72, %73
  %81 = xor i64 %72, %74
  %82 = and i64 %80, %81
  %83 = icmp slt i64 %82, 0
  store i1 %78, i1* %az
  store i1 %79, i1* %cf
  store i1 %83, i1* %of
  %84 = icmp eq i64 %74, 0
  store i1 %84, i1* %zf
  %85 = icmp slt i64 %74, 0
  store i1 %85, i1* %sf
  %86 = trunc i64 %74 to i8
  %87 = call i8 @llvm.ctpop.i8(i8 %86)
  %88 = and i8 %87, 1
  %89 = icmp eq i8 %88, 0
  store i1 %89, i1* %pf
  store volatile i64 68305, i64* @assembly_address
  %90 = load i1* %zf
  %91 = icmp eq i1 %90, false
  br i1 %91, label %block_10b38, label %block_10ad3

block_10ad3:                                      ; preds = %block_10abe
  store volatile i64 68307, i64* @assembly_address
  %92 = load %_IO_FILE** %stack_var_-32
  %93 = ptrtoint %_IO_FILE* %92 to i64
  store i64 %93, i64* %rax
  store volatile i64 68311, i64* @assembly_address
  %94 = load i64* %rax
  %95 = add i64 %94, 72
  %96 = inttoptr i64 %95 to i64*
  %97 = load i64* %96
  store i64 %97, i64* %rax
  store volatile i64 68315, i64* @assembly_address
  %98 = load i64* %rax
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %99 = icmp eq i64 %98, 0
  store i1 %99, i1* %zf
  %100 = icmp slt i64 %98, 0
  store i1 %100, i1* %sf
  %101 = trunc i64 %98 to i8
  %102 = call i8 @llvm.ctpop.i8(i8 %101)
  %103 = and i8 %102, 1
  %104 = icmp eq i8 %103, 0
  store i1 %104, i1* %pf
  store volatile i64 68318, i64* @assembly_address
  %105 = load i1* %zf
  %106 = icmp eq i1 %105, false
  br i1 %106, label %block_10b38, label %block_10ae0

block_10ae0:                                      ; preds = %block_10ad3
  store volatile i64 68320, i64* @assembly_address
  %107 = load %_IO_FILE** %stack_var_-32
  %108 = ptrtoint %_IO_FILE* %107 to i64
  store i64 %108, i64* %rax
  store volatile i64 68324, i64* @assembly_address
  %109 = load i64* %rax
  store i64 %109, i64* %rdi
  store volatile i64 68327, i64* @assembly_address
  %110 = load i64* %rdi
  %111 = inttoptr i64 %110 to %_IO_FILE*
  %112 = call i32 @fileno(%_IO_FILE* %111)
  %113 = sext i32 %112 to i64
  store i64 %113, i64* %rax
  %114 = sext i32 %112 to i64
  store i64 %114, i64* %rax
  store volatile i64 68332, i64* @assembly_address
  %115 = load i64* %rax
  %116 = trunc i64 %115 to i32
  %117 = zext i32 %116 to i64
  store i64 %117, i64* %rcx
  store volatile i64 68334, i64* @assembly_address
  %118 = load i32* %stack_var_-44
  %119 = zext i32 %118 to i64
  store i64 %119, i64* %rdx
  store volatile i64 68337, i64* @assembly_address
  %120 = load i32* %stack_var_-40
  %121 = sext i32 %120 to i64
  store i64 %121, i64* %rax
  store volatile i64 68341, i64* @assembly_address
  %122 = load i64* %rax
  store i64 %122, i64* %rsi
  store volatile i64 68344, i64* @assembly_address
  %123 = load i64* %rcx
  %124 = trunc i64 %123 to i32
  %125 = zext i32 %124 to i64
  store i64 %125, i64* %rdi
  store volatile i64 68346, i64* @assembly_address
  %126 = load i64* %rdi
  %127 = trunc i64 %126 to i32
  %128 = load i64* %rsi
  %129 = trunc i64 %128 to i32
  %130 = load i64* %rdx
  %131 = trunc i64 %130 to i32
  %132 = call i32 @lseek(i32 %127, i32 %129, i32 %131)
  %133 = sext i32 %132 to i64
  store i64 %133, i64* %rax
  %134 = sext i32 %132 to i64
  store i64 %134, i64* %rax
  store volatile i64 68351, i64* @assembly_address
  %135 = load i64* %rax
  store i64 %135, i64* %stack_var_-16
  store volatile i64 68355, i64* @assembly_address
  %136 = load i64* %stack_var_-16
  %137 = sub i64 %136, -1
  %138 = and i64 %136, 15
  %139 = sub i64 %138, 15
  %140 = icmp ugt i64 %139, 15
  %141 = icmp ult i64 %136, -1
  %142 = xor i64 %136, -1
  %143 = xor i64 %136, %137
  %144 = and i64 %142, %143
  %145 = icmp slt i64 %144, 0
  store i1 %140, i1* %az
  store i1 %141, i1* %cf
  store i1 %145, i1* %of
  %146 = icmp eq i64 %137, 0
  store i1 %146, i1* %zf
  %147 = icmp slt i64 %137, 0
  store i1 %147, i1* %sf
  %148 = trunc i64 %137 to i8
  %149 = call i8 @llvm.ctpop.i8(i8 %148)
  %150 = and i8 %149, 1
  %151 = icmp eq i8 %150, 0
  store i1 %151, i1* %pf
  store volatile i64 68360, i64* @assembly_address
  %152 = load i1* %zf
  %153 = icmp eq i1 %152, false
  br i1 %153, label %block_10b11, label %block_10b0a

block_10b0a:                                      ; preds = %block_10ae0
  store volatile i64 68362, i64* @assembly_address
  store i64 4294967295, i64* %rax
  store volatile i64 68367, i64* @assembly_address
  br label %block_10b4e

block_10b11:                                      ; preds = %block_10ae0
  store volatile i64 68369, i64* @assembly_address
  %154 = load %_IO_FILE** %stack_var_-32
  %155 = ptrtoint %_IO_FILE* %154 to i64
  store i64 %155, i64* %rax
  store volatile i64 68373, i64* @assembly_address
  %156 = load i64* %rax
  %157 = inttoptr i64 %156 to i32*
  %158 = load i32* %157
  %159 = zext i32 %158 to i64
  store i64 %159, i64* %rax
  store volatile i64 68375, i64* @assembly_address
  %160 = load i64* %rax
  %161 = trunc i64 %160 to i32
  %162 = and i32 %161, -17
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %163 = icmp eq i32 %162, 0
  store i1 %163, i1* %zf
  %164 = icmp slt i32 %162, 0
  store i1 %164, i1* %sf
  %165 = trunc i32 %162 to i8
  %166 = call i8 @llvm.ctpop.i8(i8 %165)
  %167 = and i8 %166, 1
  %168 = icmp eq i8 %167, 0
  store i1 %168, i1* %pf
  %169 = zext i32 %162 to i64
  store i64 %169, i64* %rax
  store volatile i64 68378, i64* @assembly_address
  %170 = load i64* %rax
  %171 = trunc i64 %170 to i32
  %172 = zext i32 %171 to i64
  store i64 %172, i64* %rdx
  store volatile i64 68380, i64* @assembly_address
  %173 = load %_IO_FILE** %stack_var_-32
  %174 = ptrtoint %_IO_FILE* %173 to i64
  store i64 %174, i64* %rax
  store volatile i64 68384, i64* @assembly_address
  %175 = load i64* %rdx
  %176 = trunc i64 %175 to i32
  %177 = load i64* %rax
  %178 = inttoptr i64 %177 to i32*
  store i32 %176, i32* %178
  store volatile i64 68386, i64* @assembly_address
  %179 = load %_IO_FILE** %stack_var_-32
  %180 = ptrtoint %_IO_FILE* %179 to i64
  store i64 %180, i64* %rax
  store volatile i64 68390, i64* @assembly_address
  %181 = load i64* %stack_var_-16
  store i64 %181, i64* %rdx
  store volatile i64 68394, i64* @assembly_address
  %182 = load i64* %rdx
  %183 = load i64* %rax
  %184 = add i64 %183, 144
  %185 = inttoptr i64 %184 to i64*
  store i64 %182, i64* %185
  store volatile i64 68401, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 68406, i64* @assembly_address
  br label %block_10b4e

block_10b38:                                      ; preds = %block_10ad3, %block_10abe, %block_10a96
  store volatile i64 68408, i64* @assembly_address
  %186 = load i32* %stack_var_-44
  %187 = zext i32 %186 to i64
  store i64 %187, i64* %rdx
  store volatile i64 68411, i64* @assembly_address
  %188 = load i32* %stack_var_-40
  %189 = sext i32 %188 to i64
  store i64 %189, i64* %rcx
  store volatile i64 68415, i64* @assembly_address
  %190 = load %_IO_FILE** %stack_var_-32
  %191 = ptrtoint %_IO_FILE* %190 to i64
  store i64 %191, i64* %rax
  store volatile i64 68419, i64* @assembly_address
  %192 = load i64* %rcx
  store i64 %192, i64* %rsi
  store volatile i64 68422, i64* @assembly_address
  %193 = load i64* %rax
  store i64 %193, i64* %rdi
  store volatile i64 68425, i64* @assembly_address
  %194 = load i64* %rdi
  %195 = inttoptr i64 %194 to %_IO_FILE*
  %196 = load i64* %rsi
  %197 = trunc i64 %196 to i32
  %198 = load i64* %rdx
  %199 = trunc i64 %198 to i32
  %200 = call i32 @fseeko(%_IO_FILE* %195, i32 %197, i32 %199)
  %201 = sext i32 %200 to i64
  store i64 %201, i64* %rax
  %202 = sext i32 %200 to i64
  store i64 %202, i64* %rax
  br label %block_10b4e

block_10b4e:                                      ; preds = %block_10b38, %block_10b11, %block_10b0a
  store volatile i64 68430, i64* @assembly_address
  %203 = load i64* %stack_var_-8
  store i64 %203, i64* %rbp
  %204 = ptrtoint i64* %stack_var_0 to i64
  store i64 %204, i64* %rsp
  store volatile i64 68431, i64* @assembly_address
  %205 = load i64* %rax
  ret i64 %205
}

declare i64 @305(i64, i32, i64)

declare i64 @306(i64, i64, i32)

declare i64 @307(i64, i64, i64)

define i64 @opendir_safer(i8* %arg1) {
block_10b50:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint i8* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca %__dirstream*
  %1 = alloca i64
  %stack_var_-36 = alloca i32
  %stack_var_-28 = alloca i32
  %stack_var_-32 = alloca i32
  %stack_var_-24 = alloca %__dirstream*
  %2 = alloca i64
  %stack_var_-48 = alloca i8*
  %3 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-8 = alloca i64
  %4 = alloca i32
  %5 = alloca i32
  store volatile i64 68432, i64* @assembly_address
  %6 = load i64* %rbp
  store i64 %6, i64* %stack_var_-8
  %7 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %7, i64* %rsp
  store volatile i64 68433, i64* @assembly_address
  %8 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %8, i64* %rbp
  store volatile i64 68436, i64* @assembly_address
  %9 = load i64* %rsp
  %10 = sub i64 %9, 48
  %11 = and i64 %9, 15
  %12 = icmp ugt i64 %11, 15
  %13 = icmp ult i64 %9, 48
  %14 = xor i64 %9, 48
  %15 = xor i64 %9, %10
  %16 = and i64 %14, %15
  %17 = icmp slt i64 %16, 0
  store i1 %12, i1* %az
  store i1 %13, i1* %cf
  store i1 %17, i1* %of
  %18 = icmp eq i64 %10, 0
  store i1 %18, i1* %zf
  %19 = icmp slt i64 %10, 0
  store i1 %19, i1* %sf
  %20 = trunc i64 %10 to i8
  %21 = call i8 @llvm.ctpop.i8(i8 %20)
  %22 = and i8 %21, 1
  %23 = icmp eq i8 %22, 0
  store i1 %23, i1* %pf
  %24 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %24, i64* %rsp
  store volatile i64 68440, i64* @assembly_address
  %25 = load i64* %rdi
  %26 = inttoptr i64 %25 to i8*
  store i8* %26, i8** %stack_var_-48
  store volatile i64 68444, i64* @assembly_address
  %27 = load i8** %stack_var_-48
  %28 = ptrtoint i8* %27 to i64
  store i64 %28, i64* %rax
  store volatile i64 68448, i64* @assembly_address
  %29 = load i64* %rax
  store i64 %29, i64* %rdi
  store volatile i64 68451, i64* @assembly_address
  %30 = load i64* %rdi
  %31 = inttoptr i64 %30 to i8*
  %32 = call %__dirstream* @opendir(i8* %31)
  %33 = ptrtoint %__dirstream* %32 to i64
  store i64 %33, i64* %rax
  %34 = ptrtoint %__dirstream* %32 to i64
  store i64 %34, i64* %rax
  store volatile i64 68456, i64* @assembly_address
  %35 = load i64* %rax
  %36 = inttoptr i64 %35 to %__dirstream*
  store %__dirstream* %36, %__dirstream** %stack_var_-24
  store volatile i64 68460, i64* @assembly_address
  %37 = load %__dirstream** %stack_var_-24
  %38 = ptrtoint %__dirstream* %37 to i64
  %39 = and i64 %38, 15
  %40 = icmp ugt i64 %39, 15
  %41 = icmp ult i64 %38, 0
  %42 = xor i64 %38, 0
  %43 = and i64 %42, 0
  %44 = icmp slt i64 %43, 0
  store i1 %40, i1* %az
  store i1 %41, i1* %cf
  store i1 %44, i1* %of
  %45 = icmp eq i64 %38, 0
  store i1 %45, i1* %zf
  %46 = icmp slt i64 %38, 0
  store i1 %46, i1* %sf
  %47 = trunc i64 %38 to i8
  %48 = call i8 @llvm.ctpop.i8(i8 %47)
  %49 = and i8 %48, 1
  %50 = icmp eq i8 %49, 0
  store i1 %50, i1* %pf
  store volatile i64 68465, i64* @assembly_address
  %51 = load i1* %zf
  br i1 %51, label %block_10c1a, label %block_10b77

block_10b77:                                      ; preds = %block_10b50
  store volatile i64 68471, i64* @assembly_address
  %52 = load %__dirstream** %stack_var_-24
  %53 = ptrtoint %__dirstream* %52 to i64
  store i64 %53, i64* %rax
  store volatile i64 68475, i64* @assembly_address
  %54 = load i64* %rax
  store i64 %54, i64* %rdi
  store volatile i64 68478, i64* @assembly_address
  %55 = load i64* %rdi
  %56 = inttoptr i64 %55 to %__dirstream*
  %57 = call i32 @dirfd(%__dirstream* %56)
  %58 = sext i32 %57 to i64
  store i64 %58, i64* %rax
  %59 = sext i32 %57 to i64
  store i64 %59, i64* %rax
  store volatile i64 68483, i64* @assembly_address
  %60 = load i64* %rax
  %61 = trunc i64 %60 to i32
  store i32 %61, i32* %stack_var_-32
  store volatile i64 68486, i64* @assembly_address
  %62 = load i32* %stack_var_-32
  %63 = and i32 %62, 15
  %64 = icmp ugt i32 %63, 15
  %65 = icmp ult i32 %62, 0
  %66 = xor i32 %62, 0
  %67 = and i32 %66, 0
  %68 = icmp slt i32 %67, 0
  store i1 %64, i1* %az
  store i1 %65, i1* %cf
  store i1 %68, i1* %of
  %69 = icmp eq i32 %62, 0
  store i1 %69, i1* %zf
  %70 = icmp slt i32 %62, 0
  store i1 %70, i1* %sf
  %71 = trunc i32 %62 to i8
  %72 = call i8 @llvm.ctpop.i8(i8 %71)
  %73 = and i8 %72, 1
  %74 = icmp eq i8 %73, 0
  store i1 %74, i1* %pf
  store volatile i64 68490, i64* @assembly_address
  %75 = load i1* %sf
  br i1 %75, label %block_10c1a, label %block_10b90

block_10b90:                                      ; preds = %block_10b77
  store volatile i64 68496, i64* @assembly_address
  %76 = load i32* %stack_var_-32
  store i32 %76, i32* %5
  store i32 2, i32* %4
  %77 = sub i32 %76, 2
  %78 = and i32 %76, 15
  %79 = sub i32 %78, 2
  %80 = icmp ugt i32 %79, 15
  %81 = icmp ult i32 %76, 2
  %82 = xor i32 %76, 2
  %83 = xor i32 %76, %77
  %84 = and i32 %82, %83
  %85 = icmp slt i32 %84, 0
  store i1 %80, i1* %az
  store i1 %81, i1* %cf
  store i1 %85, i1* %of
  %86 = icmp eq i32 %77, 0
  store i1 %86, i1* %zf
  %87 = icmp slt i32 %77, 0
  store i1 %87, i1* %sf
  %88 = trunc i32 %77 to i8
  %89 = call i8 @llvm.ctpop.i8(i8 %88)
  %90 = and i8 %89, 1
  %91 = icmp eq i8 %90, 0
  store i1 %91, i1* %pf
  store volatile i64 68500, i64* @assembly_address
  %92 = load i32* %5
  %93 = load i32* %4
  %94 = icmp sgt i32 %92, %93
  br i1 %94, label %block_10c1a, label %block_10b9a

block_10b9a:                                      ; preds = %block_10b90
  store volatile i64 68506, i64* @assembly_address
  %95 = load i32* %stack_var_-32
  %96 = zext i32 %95 to i64
  store i64 %96, i64* %rax
  store volatile i64 68509, i64* @assembly_address
  store i64 3, i64* %rdx
  store volatile i64 68514, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_406 to i64), i64* %rsi
  store volatile i64 68519, i64* @assembly_address
  %97 = load i64* %rax
  %98 = trunc i64 %97 to i32
  %99 = zext i32 %98 to i64
  store i64 %99, i64* %rdi
  store volatile i64 68521, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 68526, i64* @assembly_address
  %100 = load i64* %rdi
  %101 = load i64* %rsi
  %102 = load i64* %rdx
  %103 = load i64* %rcx
  %104 = load i64* %r8
  %105 = load i64* %r9
  %106 = trunc i64 %100 to i32
  %107 = call i64 @rpl_fcntl(i32 %106, i64 %101, i64 %102, i64 %103, i64 %104, i64 %105)
  store i64 %107, i64* %rax
  store i64 %107, i64* %rax
  store volatile i64 68531, i64* @assembly_address
  %108 = load i64* %rax
  %109 = trunc i64 %108 to i32
  store i32 %109, i32* %stack_var_-28
  store volatile i64 68534, i64* @assembly_address
  %110 = load i32* %stack_var_-28
  %111 = and i32 %110, 15
  %112 = icmp ugt i32 %111, 15
  %113 = icmp ult i32 %110, 0
  %114 = xor i32 %110, 0
  %115 = and i32 %114, 0
  %116 = icmp slt i32 %115, 0
  store i1 %112, i1* %az
  store i1 %113, i1* %cf
  store i1 %116, i1* %of
  %117 = icmp eq i32 %110, 0
  store i1 %117, i1* %zf
  %118 = icmp slt i32 %110, 0
  store i1 %118, i1* %sf
  %119 = trunc i32 %110 to i8
  %120 = call i8 @llvm.ctpop.i8(i8 %119)
  %121 = and i8 %120, 1
  %122 = icmp eq i8 %121, 0
  store i1 %122, i1* %pf
  store volatile i64 68538, i64* @assembly_address
  %123 = load i1* %sf
  %124 = icmp eq i1 %123, false
  br i1 %124, label %block_10bd0, label %block_10bbc

block_10bbc:                                      ; preds = %block_10b9a
  store volatile i64 68540, i64* @assembly_address
  %125 = call i32* @__errno_location()
  %126 = ptrtoint i32* %125 to i64
  store i64 %126, i64* %rax
  %127 = ptrtoint i32* %125 to i64
  store i64 %127, i64* %rax
  %128 = ptrtoint i32* %125 to i64
  store i64 %128, i64* %rax
  store volatile i64 68545, i64* @assembly_address
  %129 = load i64* %rax
  %130 = inttoptr i64 %129 to i32*
  %131 = load i32* %130
  %132 = zext i32 %131 to i64
  store i64 %132, i64* %rax
  store volatile i64 68547, i64* @assembly_address
  %133 = load i64* %rax
  %134 = trunc i64 %133 to i32
  store i32 %134, i32* %stack_var_-36
  store volatile i64 68550, i64* @assembly_address
  %135 = inttoptr i64 0 to %__dirstream*
  store %__dirstream* %135, %__dirstream** %stack_var_-16
  store volatile i64 68558, i64* @assembly_address
  br label %block_10bf9

block_10bd0:                                      ; preds = %block_10b9a
  store volatile i64 68560, i64* @assembly_address
  %136 = load i32* %stack_var_-28
  %137 = zext i32 %136 to i64
  store i64 %137, i64* %rax
  store volatile i64 68563, i64* @assembly_address
  %138 = load i64* %rax
  %139 = trunc i64 %138 to i32
  %140 = zext i32 %139 to i64
  store i64 %140, i64* %rdi
  store volatile i64 68565, i64* @assembly_address
  %141 = load i64* %rdi
  %142 = trunc i64 %141 to i32
  %143 = call %__dirstream* @fdopendir(i32 %142)
  %144 = ptrtoint %__dirstream* %143 to i64
  store i64 %144, i64* %rax
  %145 = ptrtoint %__dirstream* %143 to i64
  store i64 %145, i64* %rax
  store volatile i64 68570, i64* @assembly_address
  %146 = load i64* %rax
  %147 = inttoptr i64 %146 to %__dirstream*
  store %__dirstream* %147, %__dirstream** %stack_var_-16
  store volatile i64 68574, i64* @assembly_address
  %148 = call i32* @__errno_location()
  %149 = ptrtoint i32* %148 to i64
  store i64 %149, i64* %rax
  %150 = ptrtoint i32* %148 to i64
  store i64 %150, i64* %rax
  %151 = ptrtoint i32* %148 to i64
  store i64 %151, i64* %rax
  store volatile i64 68579, i64* @assembly_address
  %152 = load i64* %rax
  %153 = inttoptr i64 %152 to i32*
  %154 = load i32* %153
  %155 = zext i32 %154 to i64
  store i64 %155, i64* %rax
  store volatile i64 68581, i64* @assembly_address
  %156 = load i64* %rax
  %157 = trunc i64 %156 to i32
  store i32 %157, i32* %stack_var_-36
  store volatile i64 68584, i64* @assembly_address
  %158 = load %__dirstream** %stack_var_-16
  %159 = ptrtoint %__dirstream* %158 to i64
  %160 = and i64 %159, 15
  %161 = icmp ugt i64 %160, 15
  %162 = icmp ult i64 %159, 0
  %163 = xor i64 %159, 0
  %164 = and i64 %163, 0
  %165 = icmp slt i64 %164, 0
  store i1 %161, i1* %az
  store i1 %162, i1* %cf
  store i1 %165, i1* %of
  %166 = icmp eq i64 %159, 0
  store i1 %166, i1* %zf
  %167 = icmp slt i64 %159, 0
  store i1 %167, i1* %sf
  %168 = trunc i64 %159 to i8
  %169 = call i8 @llvm.ctpop.i8(i8 %168)
  %170 = and i8 %169, 1
  %171 = icmp eq i8 %170, 0
  store i1 %171, i1* %pf
  store volatile i64 68589, i64* @assembly_address
  %172 = load i1* %zf
  %173 = icmp eq i1 %172, false
  br i1 %173, label %block_10bf9, label %block_10bef

block_10bef:                                      ; preds = %block_10bd0
  store volatile i64 68591, i64* @assembly_address
  %174 = load i32* %stack_var_-28
  %175 = zext i32 %174 to i64
  store i64 %175, i64* %rax
  store volatile i64 68594, i64* @assembly_address
  %176 = load i64* %rax
  %177 = trunc i64 %176 to i32
  %178 = zext i32 %177 to i64
  store i64 %178, i64* %rdi
  store volatile i64 68596, i64* @assembly_address
  %179 = load i64* %rdi
  %180 = trunc i64 %179 to i32
  %181 = call i32 @close(i32 %180)
  %182 = sext i32 %181 to i64
  store i64 %182, i64* %rax
  %183 = sext i32 %181 to i64
  store i64 %183, i64* %rax
  br label %block_10bf9

block_10bf9:                                      ; preds = %block_10bef, %block_10bd0, %block_10bbc
  store volatile i64 68601, i64* @assembly_address
  %184 = load %__dirstream** %stack_var_-24
  %185 = ptrtoint %__dirstream* %184 to i64
  store i64 %185, i64* %rax
  store volatile i64 68605, i64* @assembly_address
  %186 = load i64* %rax
  store i64 %186, i64* %rdi
  store volatile i64 68608, i64* @assembly_address
  %187 = load i64* %rdi
  %188 = inttoptr i64 %187 to %__dirstream*
  %189 = call i32 @closedir(%__dirstream* %188)
  %190 = sext i32 %189 to i64
  store i64 %190, i64* %rax
  %191 = sext i32 %189 to i64
  store i64 %191, i64* %rax
  store volatile i64 68613, i64* @assembly_address
  %192 = call i32* @__errno_location()
  %193 = ptrtoint i32* %192 to i64
  store i64 %193, i64* %rax
  %194 = ptrtoint i32* %192 to i64
  store i64 %194, i64* %rax
  %195 = ptrtoint i32* %192 to i64
  store i64 %195, i64* %rax
  store volatile i64 68618, i64* @assembly_address
  %196 = load i64* %rax
  store i64 %196, i64* %rdx
  store volatile i64 68621, i64* @assembly_address
  %197 = load i32* %stack_var_-36
  %198 = zext i32 %197 to i64
  store i64 %198, i64* %rax
  store volatile i64 68624, i64* @assembly_address
  %199 = load i64* %rax
  %200 = trunc i64 %199 to i32
  %201 = load i64* %rdx
  %202 = inttoptr i64 %201 to i32*
  store i32 %200, i32* %202
  store volatile i64 68626, i64* @assembly_address
  %203 = load %__dirstream** %stack_var_-16
  %204 = ptrtoint %__dirstream* %203 to i64
  store i64 %204, i64* %rax
  store volatile i64 68630, i64* @assembly_address
  %205 = load i64* %rax
  %206 = inttoptr i64 %205 to %__dirstream*
  store %__dirstream* %206, %__dirstream** %stack_var_-24
  br label %block_10c1a

block_10c1a:                                      ; preds = %block_10bf9, %block_10b90, %block_10b77, %block_10b50
  store volatile i64 68634, i64* @assembly_address
  %207 = load %__dirstream** %stack_var_-24
  %208 = ptrtoint %__dirstream* %207 to i64
  store i64 %208, i64* %rax
  store volatile i64 68638, i64* @assembly_address
  %209 = load i64* %stack_var_-8
  store i64 %209, i64* %rbp
  %210 = ptrtoint i64* %stack_var_0 to i64
  store i64 %210, i64* %rsp
  store volatile i64 68639, i64* @assembly_address
  %211 = load i64* %rax
  ret i64 %211
}

declare i64 @308(i64)

define i64 @gettime(%timespec* %arg1) {
block_10c20:
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = ptrtoint %timespec* %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-16 = alloca %timespec*
  %1 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68640, i64* @assembly_address
  %2 = load i64* %rbp
  store i64 %2, i64* %stack_var_-8
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rsp
  store volatile i64 68641, i64* @assembly_address
  %4 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %4, i64* %rbp
  store volatile i64 68644, i64* @assembly_address
  %5 = load i64* %rsp
  %6 = sub i64 %5, 16
  %7 = and i64 %5, 15
  %8 = icmp ugt i64 %7, 15
  %9 = icmp ult i64 %5, 16
  %10 = xor i64 %5, 16
  %11 = xor i64 %5, %6
  %12 = and i64 %10, %11
  %13 = icmp slt i64 %12, 0
  store i1 %8, i1* %az
  store i1 %9, i1* %cf
  store i1 %13, i1* %of
  %14 = icmp eq i64 %6, 0
  store i1 %14, i1* %zf
  %15 = icmp slt i64 %6, 0
  store i1 %15, i1* %sf
  %16 = trunc i64 %6 to i8
  %17 = call i8 @llvm.ctpop.i8(i8 %16)
  %18 = and i8 %17, 1
  %19 = icmp eq i8 %18, 0
  store i1 %19, i1* %pf
  %20 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %20, i64* %rsp
  store volatile i64 68648, i64* @assembly_address
  %21 = load i64* %rdi
  %22 = inttoptr i64 %21 to %timespec*
  store %timespec* %22, %timespec** %stack_var_-16
  store volatile i64 68652, i64* @assembly_address
  %23 = load %timespec** %stack_var_-16
  %24 = ptrtoint %timespec* %23 to i64
  store i64 %24, i64* %rax
  store volatile i64 68656, i64* @assembly_address
  %25 = load i64* %rax
  store i64 %25, i64* %rsi
  store volatile i64 68659, i64* @assembly_address
  store i64 0, i64* %rdi
  store volatile i64 68664, i64* @assembly_address
  %26 = load i64* %rdi
  %27 = trunc i64 %26 to i32
  %28 = load i64* %rsi
  %29 = inttoptr i64 %28 to %timespec*
  %30 = call i32 @clock_gettime(i32 %27, %timespec* %29)
  %31 = sext i32 %30 to i64
  store i64 %31, i64* %rax
  %32 = sext i32 %30 to i64
  store i64 %32, i64* %rax
  store volatile i64 68669, i64* @assembly_address
  store volatile i64 68670, i64* @assembly_address
  %33 = load i64* %stack_var_-8
  store i64 %33, i64* %rbp
  %34 = ptrtoint i64* %stack_var_0 to i64
  store i64 %34, i64* %rsp
  store volatile i64 68671, i64* @assembly_address
  %35 = load i64* %rax
  ret i64 %35
}

declare i64 @309(i64)

define i64 @current_timespec() {
block_10c40:
  %rdi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68672, i64* @assembly_address
  %0 = load i64* %rbp
  store i64 %0, i64* %stack_var_-8
  %1 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %1, i64* %rsp
  store volatile i64 68673, i64* @assembly_address
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rbp
  store volatile i64 68676, i64* @assembly_address
  %3 = load i64* %rsp
  %4 = sub i64 %3, 32
  %5 = and i64 %3, 15
  %6 = icmp ugt i64 %5, 15
  %7 = icmp ult i64 %3, 32
  %8 = xor i64 %3, 32
  %9 = xor i64 %3, %4
  %10 = and i64 %8, %9
  %11 = icmp slt i64 %10, 0
  store i1 %6, i1* %az
  store i1 %7, i1* %cf
  store i1 %11, i1* %of
  %12 = icmp eq i64 %4, 0
  store i1 %12, i1* %zf
  %13 = icmp slt i64 %4, 0
  store i1 %13, i1* %sf
  %14 = trunc i64 %4 to i8
  %15 = call i8 @llvm.ctpop.i8(i8 %14)
  %16 = and i8 %15, 1
  %17 = icmp eq i8 %16, 0
  store i1 %17, i1* %pf
  %18 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %18, i64* %rsp
  store volatile i64 68680, i64* @assembly_address
  %19 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  store i64 %19, i64* %rax
  store volatile i64 68689, i64* @assembly_address
  %20 = load i64* %rax
  store i64 %20, i64* %stack_var_-16
  store volatile i64 68693, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %21 = icmp eq i32 0, 0
  store i1 %21, i1* %zf
  %22 = icmp slt i32 0, 0
  store i1 %22, i1* %sf
  %23 = trunc i32 0 to i8
  %24 = call i8 @llvm.ctpop.i8(i8 %23)
  %25 = and i8 %24, 1
  %26 = icmp eq i8 %25, 0
  store i1 %26, i1* %pf
  %27 = zext i32 0 to i64
  store i64 %27, i64* %rax
  store volatile i64 68695, i64* @assembly_address
  %28 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %28, i64* %rax
  store volatile i64 68699, i64* @assembly_address
  %29 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %29, i64* %rdi
  store volatile i64 68702, i64* @assembly_address
  %30 = load i64* %rdi
  %31 = inttoptr i64 %30 to %timespec*
  %32 = call i64 @gettime(%timespec* %31)
  store i64 %32, i64* %rax
  store i64 %32, i64* %rax
  store volatile i64 68707, i64* @assembly_address
  %33 = load i64* %stack_var_-40
  store i64 %33, i64* %rax
  store volatile i64 68711, i64* @assembly_address
  %34 = load i64* %stack_var_-32
  store i64 %34, i64* %rdx
  store volatile i64 68715, i64* @assembly_address
  %35 = load i64* %stack_var_-16
  store i64 %35, i64* %rcx
  store volatile i64 68719, i64* @assembly_address
  %36 = load i64* %rcx
  %37 = load i64 addrspace(257)* inttoptr (i64 40 to i64 addrspace(257)*)
  %38 = xor i64 %36, %37
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %39 = icmp eq i64 %38, 0
  store i1 %39, i1* %zf
  %40 = icmp slt i64 %38, 0
  store i1 %40, i1* %sf
  %41 = trunc i64 %38 to i8
  %42 = call i8 @llvm.ctpop.i8(i8 %41)
  %43 = and i8 %42, 1
  %44 = icmp eq i8 %43, 0
  store i1 %44, i1* %pf
  store i64 %38, i64* %rcx
  store volatile i64 68728, i64* @assembly_address
  %45 = load i1* %zf
  br i1 %45, label %block_10c7f, label %block_10c7a

block_10c7a:                                      ; preds = %block_10c40
  store volatile i64 68730, i64* @assembly_address
  call void @__stack_chk_fail()
  unreachable

block_10c7f:                                      ; preds = %block_10c40
  store volatile i64 68735, i64* @assembly_address
  %46 = load i64* %stack_var_-8
  store i64 %46, i64* %rbp
  %47 = ptrtoint i64* %stack_var_0 to i64
  store i64 %47, i64* %rsp
  store volatile i64 68736, i64* @assembly_address
  %48 = load i64* %rax
  %49 = load i64* %rax
  ret i64 %49
}

define i64 @dup_safer(i32 %arg1) {
block_10c81:
  %r9 = alloca i64
  %r8 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rdx = alloca i64
  %rcx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-12 = alloca i32
  %stack_var_-24 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68737, i64* @assembly_address
  %1 = load i64* %rbp
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 68738, i64* @assembly_address
  %3 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %3, i64* %rbp
  store volatile i64 68741, i64* @assembly_address
  %4 = load i64* %rsp
  %5 = sub i64 %4, 16
  %6 = and i64 %4, 15
  %7 = icmp ugt i64 %6, 15
  %8 = icmp ult i64 %4, 16
  %9 = xor i64 %4, 16
  %10 = xor i64 %4, %5
  %11 = and i64 %9, %10
  %12 = icmp slt i64 %11, 0
  store i1 %7, i1* %az
  store i1 %8, i1* %cf
  store i1 %12, i1* %of
  %13 = icmp eq i64 %5, 0
  store i1 %13, i1* %zf
  %14 = icmp slt i64 %5, 0
  store i1 %14, i1* %sf
  %15 = trunc i64 %5 to i8
  %16 = call i8 @llvm.ctpop.i8(i8 %15)
  %17 = and i8 %16, 1
  %18 = icmp eq i8 %17, 0
  store i1 %18, i1* %pf
  %19 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %19, i64* %rsp
  store volatile i64 68745, i64* @assembly_address
  %20 = load i64* %rdi
  %21 = trunc i64 %20 to i32
  store i32 %21, i32* %stack_var_-12
  store volatile i64 68748, i64* @assembly_address
  %22 = load i32* %stack_var_-12
  %23 = zext i32 %22 to i64
  store i64 %23, i64* %rax
  store volatile i64 68751, i64* @assembly_address
  store i64 3, i64* %rdx
  store volatile i64 68756, i64* @assembly_address
  store i64 0, i64* %rsi
  store volatile i64 68761, i64* @assembly_address
  %24 = load i64* %rax
  %25 = trunc i64 %24 to i32
  %26 = zext i32 %25 to i64
  store i64 %26, i64* %rdi
  store volatile i64 68763, i64* @assembly_address
  store i64 0, i64* %rax
  store volatile i64 68768, i64* @assembly_address
  %27 = load i64* %rdi
  %28 = load i64* %rsi
  %29 = load i64* %rdx
  %30 = load i64* %rcx
  %31 = load i64* %r8
  %32 = load i64* %r9
  %33 = trunc i64 %27 to i32
  %34 = call i64 @rpl_fcntl(i32 %33, i64 %28, i64 %29, i64 %30, i64 %31, i64 %32)
  store i64 %34, i64* %rax
  store i64 %34, i64* %rax
  store volatile i64 68773, i64* @assembly_address
  %35 = load i64* %stack_var_-8
  store i64 %35, i64* %rbp
  %36 = ptrtoint i64* %stack_var_0 to i64
  store i64 %36, i64* %rsp
  store volatile i64 68774, i64* @assembly_address
  %37 = load i64* %rax
  ret i64 %37
}

declare i64 @310(i64)

define i64 @__libc_csu_init(i32 %arg1, i64 %arg2, i64 %arg3) {
block_10cb0:
  %r15 = alloca i64
  %r14 = alloca i64
  %r13 = alloca i64
  %r12 = alloca i64
  %rdi = alloca i64
  %rsi = alloca i64
  %rbp = alloca i64
  %rsp = alloca i64
  %rbx = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  store i64 %arg3, i64* %rdx
  store i64 %arg2, i64* %rsi
  %0 = sext i32 %arg1 to i64
  store i64 %0, i64* %rdi
  %stack_var_0 = alloca i64
  %stack_var_-56 = alloca i64
  %stack_var_-48 = alloca i64
  %stack_var_-40 = alloca i64
  %stack_var_-32 = alloca i64
  %stack_var_-24 = alloca i64
  %stack_var_-16 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68784, i64* @assembly_address
  %1 = load i64* %r15
  store i64 %1, i64* %stack_var_-8
  %2 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %2, i64* %rsp
  store volatile i64 68786, i64* @assembly_address
  %3 = load i64* %r14
  store i64 %3, i64* %stack_var_-16
  %4 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %4, i64* %rsp
  store volatile i64 68788, i64* @assembly_address
  %5 = load i64* %rdx
  store i64 %5, i64* %r15
  store volatile i64 68791, i64* @assembly_address
  %6 = load i64* %r13
  store i64 %6, i64* %stack_var_-24
  %7 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %7, i64* %rsp
  store volatile i64 68793, i64* @assembly_address
  %8 = load i64* %r12
  store i64 %8, i64* %stack_var_-32
  %9 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %9, i64* %rsp
  store volatile i64 68795, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_215670 to i64), i64* %r12
  store volatile i64 68802, i64* @assembly_address
  %10 = load i64* %rbp
  store i64 %10, i64* %stack_var_-40
  %11 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %11, i64* %rsp
  store volatile i64 68803, i64* @assembly_address
  store i64 ptrtoint (i64* @global_var_215678 to i64), i64* %rbp
  store volatile i64 68810, i64* @assembly_address
  %12 = load i64* %rbx
  store i64 %12, i64* %stack_var_-48
  %13 = ptrtoint i64* %stack_var_-48 to i64
  store i64 %13, i64* %rsp
  store volatile i64 68811, i64* @assembly_address
  %14 = load i64* %rdi
  %15 = trunc i64 %14 to i32
  %16 = zext i32 %15 to i64
  store i64 %16, i64* %r13
  store volatile i64 68814, i64* @assembly_address
  %17 = load i64* %rsi
  store i64 %17, i64* %r14
  store volatile i64 68817, i64* @assembly_address
  %18 = load i64* %rbp
  %19 = load i64* %r12
  %20 = sub i64 %18, %19
  %21 = and i64 %18, 15
  %22 = and i64 %19, 15
  %23 = sub i64 %21, %22
  %24 = icmp ugt i64 %23, 15
  %25 = icmp ult i64 %18, %19
  %26 = xor i64 %18, %19
  %27 = xor i64 %18, %20
  %28 = and i64 %26, %27
  %29 = icmp slt i64 %28, 0
  store i1 %24, i1* %az
  store i1 %25, i1* %cf
  store i1 %29, i1* %of
  %30 = icmp eq i64 %20, 0
  store i1 %30, i1* %zf
  %31 = icmp slt i64 %20, 0
  store i1 %31, i1* %sf
  %32 = trunc i64 %20 to i8
  %33 = call i8 @llvm.ctpop.i8(i8 %32)
  %34 = and i8 %33, 1
  %35 = icmp eq i8 %34, 0
  store i1 %35, i1* %pf
  store i64 %20, i64* %rbp
  store volatile i64 68820, i64* @assembly_address
  %36 = load i64* %rsp
  %37 = sub i64 %36, 8
  %38 = and i64 %36, 15
  %39 = sub i64 %38, 8
  %40 = icmp ugt i64 %39, 15
  %41 = icmp ult i64 %36, 8
  %42 = xor i64 %36, 8
  %43 = xor i64 %36, %37
  %44 = and i64 %42, %43
  %45 = icmp slt i64 %44, 0
  store i1 %40, i1* %az
  store i1 %41, i1* %cf
  store i1 %45, i1* %of
  %46 = icmp eq i64 %37, 0
  store i1 %46, i1* %zf
  %47 = icmp slt i64 %37, 0
  store i1 %47, i1* %sf
  %48 = trunc i64 %37 to i8
  %49 = call i8 @llvm.ctpop.i8(i8 %48)
  %50 = and i8 %49, 1
  %51 = icmp eq i8 %50, 0
  store i1 %51, i1* %pf
  %52 = ptrtoint i64* %stack_var_-56 to i64
  store i64 %52, i64* %rsp
  store volatile i64 68824, i64* @assembly_address
  %53 = load i64* %rbp
  %54 = load i1* %of
  %55 = ashr i64 %53, 3
  %56 = icmp eq i64 %55, 0
  store i1 %56, i1* %zf
  %57 = icmp slt i64 %55, 0
  store i1 %57, i1* %sf
  %58 = trunc i64 %55 to i8
  %59 = call i8 @llvm.ctpop.i8(i8 %58)
  %60 = and i8 %59, 1
  %61 = icmp eq i8 %60, 0
  store i1 %61, i1* %pf
  store i64 %55, i64* %rbp
  %62 = and i64 4, %53
  %63 = icmp ne i64 %62, 0
  store i1 %63, i1* %cf
  %64 = select i1 false, i1 false, i1 %54
  store i1 %64, i1* %of
  store volatile i64 68828, i64* @assembly_address
  %65 = load i64* %rdi
  %66 = call i64 @_init(i64 %65)
  store i64 %66, i64* %rax
  store i64 %66, i64* %rax
  store i64 %66, i64* %rax
  store volatile i64 68833, i64* @assembly_address
  %67 = load i64* %rbp
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %68 = icmp eq i64 %67, 0
  store i1 %68, i1* %zf
  %69 = icmp slt i64 %67, 0
  store i1 %69, i1* %sf
  %70 = trunc i64 %67 to i8
  %71 = call i8 @llvm.ctpop.i8(i8 %70)
  %72 = and i8 %71, 1
  %73 = icmp eq i8 %72, 0
  store i1 %73, i1* %pf
  store volatile i64 68836, i64* @assembly_address
  %74 = load i1* %zf
  br i1 %74, label %block_10d06, label %block_10ce6

block_10ce6:                                      ; preds = %block_10cb0
  store volatile i64 68838, i64* @assembly_address
  store i1 false, i1* %az
  store i1 false, i1* %cf
  store i1 false, i1* %of
  %75 = icmp eq i32 0, 0
  store i1 %75, i1* %zf
  %76 = icmp slt i32 0, 0
  store i1 %76, i1* %sf
  %77 = trunc i32 0 to i8
  %78 = call i8 @llvm.ctpop.i8(i8 %77)
  %79 = and i8 %78, 1
  %80 = icmp eq i8 %79, 0
  store i1 %80, i1* %pf
  %81 = zext i32 0 to i64
  store i64 %81, i64* %rbx
  store volatile i64 68840, i64* @assembly_address
  br label %block_10cf0

block_10cf0:                                      ; preds = %block_10cf0, %block_10ce6
  store volatile i64 68848, i64* @assembly_address
  %82 = load i64* %r15
  store i64 %82, i64* %rdx
  store volatile i64 68851, i64* @assembly_address
  %83 = load i64* %r14
  store i64 %83, i64* %rsi
  store volatile i64 68854, i64* @assembly_address
  %84 = load i64* %r13
  %85 = trunc i64 %84 to i32
  %86 = zext i32 %85 to i64
  store i64 %86, i64* %rdi
  store volatile i64 68857, i64* @assembly_address
  %87 = load i64* %r12
  %88 = load i64* %rbx
  %89 = mul i64 %88, 8
  %90 = add i64 %87, %89
  %91 = inttoptr i64 %90 to i64*
  %92 = load i64* %91
  %93 = call i64 @__pseudo_call(i64 %92)
  store i64 %93, i64* %rax
  store volatile i64 68861, i64* @assembly_address
  %94 = load i64* %rbx
  %95 = add i64 %94, 1
  %96 = and i64 %94, 15
  %97 = add i64 %96, 1
  %98 = icmp ugt i64 %97, 15
  %99 = icmp ult i64 %95, %94
  %100 = xor i64 %94, %95
  %101 = xor i64 1, %95
  %102 = and i64 %100, %101
  %103 = icmp slt i64 %102, 0
  store i1 %98, i1* %az
  store i1 %99, i1* %cf
  store i1 %103, i1* %of
  %104 = icmp eq i64 %95, 0
  store i1 %104, i1* %zf
  %105 = icmp slt i64 %95, 0
  store i1 %105, i1* %sf
  %106 = trunc i64 %95 to i8
  %107 = call i8 @llvm.ctpop.i8(i8 %106)
  %108 = and i8 %107, 1
  %109 = icmp eq i8 %108, 0
  store i1 %109, i1* %pf
  store i64 %95, i64* %rbx
  store volatile i64 68865, i64* @assembly_address
  %110 = load i64* %rbp
  %111 = load i64* %rbx
  %112 = sub i64 %110, %111
  %113 = and i64 %110, 15
  %114 = and i64 %111, 15
  %115 = sub i64 %113, %114
  %116 = icmp ugt i64 %115, 15
  %117 = icmp ult i64 %110, %111
  %118 = xor i64 %110, %111
  %119 = xor i64 %110, %112
  %120 = and i64 %118, %119
  %121 = icmp slt i64 %120, 0
  store i1 %116, i1* %az
  store i1 %117, i1* %cf
  store i1 %121, i1* %of
  %122 = icmp eq i64 %112, 0
  store i1 %122, i1* %zf
  %123 = icmp slt i64 %112, 0
  store i1 %123, i1* %sf
  %124 = trunc i64 %112 to i8
  %125 = call i8 @llvm.ctpop.i8(i8 %124)
  %126 = and i8 %125, 1
  %127 = icmp eq i8 %126, 0
  store i1 %127, i1* %pf
  store volatile i64 68868, i64* @assembly_address
  %128 = load i1* %zf
  %129 = icmp eq i1 %128, false
  br i1 %129, label %block_10cf0, label %block_10d06

block_10d06:                                      ; preds = %block_10cf0, %block_10cb0
  store volatile i64 68870, i64* @assembly_address
  %130 = load i64* %rsp
  %131 = add i64 %130, 8
  %132 = and i64 %130, 15
  %133 = add i64 %132, 8
  %134 = icmp ugt i64 %133, 15
  %135 = icmp ult i64 %131, %130
  %136 = xor i64 %130, %131
  %137 = xor i64 8, %131
  %138 = and i64 %136, %137
  %139 = icmp slt i64 %138, 0
  store i1 %134, i1* %az
  store i1 %135, i1* %cf
  store i1 %139, i1* %of
  %140 = icmp eq i64 %131, 0
  store i1 %140, i1* %zf
  %141 = icmp slt i64 %131, 0
  store i1 %141, i1* %sf
  %142 = trunc i64 %131 to i8
  %143 = call i8 @llvm.ctpop.i8(i8 %142)
  %144 = and i8 %143, 1
  %145 = icmp eq i8 %144, 0
  store i1 %145, i1* %pf
  %146 = ptrtoint i64* %stack_var_-48 to i64
  store i64 %146, i64* %rsp
  store volatile i64 68874, i64* @assembly_address
  %147 = load i64* %stack_var_-48
  store i64 %147, i64* %rbx
  %148 = ptrtoint i64* %stack_var_-40 to i64
  store i64 %148, i64* %rsp
  store volatile i64 68875, i64* @assembly_address
  %149 = load i64* %stack_var_-40
  store i64 %149, i64* %rbp
  %150 = ptrtoint i64* %stack_var_-32 to i64
  store i64 %150, i64* %rsp
  store volatile i64 68876, i64* @assembly_address
  %151 = load i64* %stack_var_-32
  store i64 %151, i64* %r12
  %152 = ptrtoint i64* %stack_var_-24 to i64
  store i64 %152, i64* %rsp
  store volatile i64 68878, i64* @assembly_address
  %153 = load i64* %stack_var_-24
  store i64 %153, i64* %r13
  %154 = ptrtoint i64* %stack_var_-16 to i64
  store i64 %154, i64* %rsp
  store volatile i64 68880, i64* @assembly_address
  %155 = load i64* %stack_var_-16
  store i64 %155, i64* %r14
  %156 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %156, i64* %rsp
  store volatile i64 68882, i64* @assembly_address
  %157 = load i64* %stack_var_-8
  store i64 %157, i64* %r15
  %158 = ptrtoint i64* %stack_var_0 to i64
  store i64 %158, i64* %rsp
  store volatile i64 68884, i64* @assembly_address
  %159 = load i64* %rax
  ret i64 %159
}

declare i64 @311(i64, i64, i64)

define i64 @__libc_csu_fini() {
block_10d20:
  %rax = alloca i64
  store volatile i64 68896, i64* @assembly_address
  %0 = load i64* %rax
  %1 = load i64* %rax
  ret i64 %1
}

define i32 @stat(i8* %file, %stat* %buf) {
block_10d30:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %stat* %buf to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %file to i64
  store i64 %1, i64* %rdi
  store volatile i64 68912, i64* @assembly_address
  %2 = load i64* %rsi
  store i64 %2, i64* %rdx
  store volatile i64 68915, i64* @assembly_address
  %3 = load i64* %rdi
  store i64 %3, i64* %rsi
  store volatile i64 68918, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 68923, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = trunc i64 %4 to i32
  %6 = load i64* %rsi
  %7 = inttoptr i64 %6 to i8*
  %8 = load i64* %rdx
  %9 = inttoptr i64 %8 to %stat*
  %10 = call i32 @__xstat(i32 %5, i8* %7, %stat* %9)
  %11 = sext i32 %10 to i64
  store i64 %11, i64* %rax
  %12 = sext i32 %10 to i64
  store i64 %12, i64* %rax
  %13 = load i64* %rax
  %14 = trunc i64 %13 to i32
  ret i32 %14
}

define i32 @fstat(i32 %fd, %stat* %buf) {
block_10d40:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %stat* %buf to i64
  store i64 %0, i64* %rsi
  %1 = sext i32 %fd to i64
  store i64 %1, i64* %rdi
  store volatile i64 68928, i64* @assembly_address
  %2 = load i64* %rsi
  store i64 %2, i64* %rdx
  store volatile i64 68931, i64* @assembly_address
  %3 = load i64* %rdi
  %4 = trunc i64 %3 to i32
  %5 = zext i32 %4 to i64
  store i64 %5, i64* %rsi
  store volatile i64 68933, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 68938, i64* @assembly_address
  %6 = load i64* %rdi
  %7 = trunc i64 %6 to i32
  %8 = load i64* %rsi
  %9 = trunc i64 %8 to i32
  %10 = load i64* %rdx
  %11 = inttoptr i64 %10 to %stat*
  %12 = call i32 @__fxstat(i32 %7, i32 %9, %stat* %11)
  %13 = sext i32 %12 to i64
  store i64 %13, i64* %rax
  %14 = sext i32 %12 to i64
  store i64 %14, i64* %rax
  %15 = load i64* %rax
  %16 = trunc i64 %15 to i32
  ret i32 %16
}

define i32 @lstat(i8* %file, %stat* %buf) {
block_10d50:
  %rdi = alloca i64
  %rsi = alloca i64
  %rdx = alloca i64
  %rax = alloca i64
  %0 = ptrtoint %stat* %buf to i64
  store i64 %0, i64* %rsi
  %1 = ptrtoint i8* %file to i64
  store i64 %1, i64* %rdi
  store volatile i64 68944, i64* @assembly_address
  %2 = load i64* %rsi
  store i64 %2, i64* %rdx
  store volatile i64 68947, i64* @assembly_address
  %3 = load i64* %rdi
  store i64 %3, i64* %rsi
  store volatile i64 68950, i64* @assembly_address
  store i64 1, i64* %rdi
  store volatile i64 68955, i64* @assembly_address
  %4 = load i64* %rdi
  %5 = trunc i64 %4 to i32
  %6 = load i64* %rsi
  %7 = inttoptr i64 %6 to i8*
  %8 = load i64* %rdx
  %9 = inttoptr i64 %8 to %stat*
  %10 = call i32 @__lxstat(i32 %5, i8* %7, %stat* %9)
  %11 = sext i32 %10 to i64
  store i64 %11, i64* %rax
  %12 = sext i32 %10 to i64
  store i64 %12, i64* %rax
  %13 = load i64* %rax
  %14 = trunc i64 %13 to i32
  ret i32 %14
}

define i64 @_fini() {
block_10d60:
  %rsp = alloca i64
  %rax = alloca i64
  %of = alloca i1
  %sf = alloca i1
  %zf = alloca i1
  %az = alloca i1
  %pf = alloca i1
  %cf = alloca i1
  %stack_var_0 = alloca i64
  %stack_var_-8 = alloca i64
  store volatile i64 68960, i64* @assembly_address
  %0 = load i64* %rsp
  %1 = sub i64 %0, 8
  %2 = and i64 %0, 15
  %3 = sub i64 %2, 8
  %4 = icmp ugt i64 %3, 15
  %5 = icmp ult i64 %0, 8
  %6 = xor i64 %0, 8
  %7 = xor i64 %0, %1
  %8 = and i64 %6, %7
  %9 = icmp slt i64 %8, 0
  store i1 %4, i1* %az
  store i1 %5, i1* %cf
  store i1 %9, i1* %of
  %10 = icmp eq i64 %1, 0
  store i1 %10, i1* %zf
  %11 = icmp slt i64 %1, 0
  store i1 %11, i1* %sf
  %12 = trunc i64 %1 to i8
  %13 = call i8 @llvm.ctpop.i8(i8 %12)
  %14 = and i8 %13, 1
  %15 = icmp eq i8 %14, 0
  store i1 %15, i1* %pf
  %16 = ptrtoint i64* %stack_var_-8 to i64
  store i64 %16, i64* %rsp
  store volatile i64 68964, i64* @assembly_address
  %17 = load i64* %rsp
  %18 = add i64 %17, 8
  %19 = and i64 %17, 15
  %20 = add i64 %19, 8
  %21 = icmp ugt i64 %20, 15
  %22 = icmp ult i64 %18, %17
  %23 = xor i64 %17, %18
  %24 = xor i64 8, %18
  %25 = and i64 %23, %24
  %26 = icmp slt i64 %25, 0
  store i1 %21, i1* %az
  store i1 %22, i1* %cf
  store i1 %26, i1* %of
  %27 = icmp eq i64 %18, 0
  store i1 %27, i1* %zf
  %28 = icmp slt i64 %18, 0
  store i1 %28, i1* %sf
  %29 = trunc i64 %18 to i8
  %30 = call i8 @llvm.ctpop.i8(i8 %29)
  %31 = and i8 %30, 1
  %32 = icmp eq i8 %31, 0
  store i1 %32, i1* %pf
  %33 = ptrtoint i64* %stack_var_0 to i64
  store i64 %33, i64* %rsp
  store volatile i64 68968, i64* @assembly_address
  %34 = load i64* %rax
  %35 = load i64* %rax
  ret i64 %35
}

declare i8* @getenv(i8*)

declare i32 @sigprocmask(i32, %_TYPEDEF_sigset_t*, %_TYPEDEF_sigset_t*)

declare i32 @raise(i32)

declare void @free(i64*)

declare i32 @utimensat(i32, i8*, [2 x %timespec], i32)

declare i32 @putchar(i32)

declare %tm* @localtime(i32*)

declare i32* @__errno_location()

declare i32 @fdatasync(i32)

declare i32 @unlink(i8*)

declare void @_exit(i32)

declare i8* @strcpy(i8*, i8*)

declare i32 @unlinkat(i32, i8*, i32)

declare i32 @puts(i8*)

declare void @qsort(i64*, i32, i32, i32 (i64*, i64*)*)

declare i32 @isatty(i32)

declare i32 @sigaction(i32, %sigaction*, %sigaction*)

declare i32 @fcntl(i32, i32, ...)

declare i32 @clock_gettime(i32, %timespec*)

declare i32 @write(i32, i64*, i32)

declare i32 @fclose(%_IO_FILE*)

declare %__dirstream* @opendir(i8*)

declare i8* @stpcpy(i8*, i8*)

declare i32 @strlen(i8*)

declare i32 @__lxstat(i32, i8*, %stat*)

declare i32 @openat(i32, i8*, i32, ...)

declare void @__stack_chk_fail()

declare i32 @getopt_long(i32, i8**, i8*, %option*, i32*)

declare i32 @printf(i8*, ...)

declare i32 @_IO_putc(i32, %_IO_FILE*)

declare i8* @strrchr(i8*, i32)

declare i32 @lseek(i32, i32, i32)

declare i64* @memset(i64*, i32, i32)

declare i32 @close(i32)

declare i32 @strspn(i8*, i8*)

declare i32 @closedir(%__dirstream*)

declare i32 @fputc(i32, %_IO_FILE*)

declare i32 @strcspn(i8*, i8*)

declare i32 @read(i32, i64*, i32)

declare i32 @memcmp(i64*, i64*, i32)

declare i32 @utimes(i8*, [2 x %timeval])

declare i64* @calloc(i32, i32)

declare i32 @strcmp(i8*, i8*)

declare i32 @getchar()

declare void (i32)* @signal(i32, void (i32)*)

declare i32 @dirfd(%__dirstream*)

declare i32 @fprintf(%_IO_FILE*, i8*, ...)

declare i32 @sigemptyset(%_TYPEDEF_sigset_t*)

declare i64* @memcpy(i64*, i64*, i32)

declare i32 @fileno(%_IO_FILE*)

declare i32 @__xstat(i32, i8*, %stat*)

declare %dirent* @readdir(%__dirstream*)

declare i32 @tolower(i32)

declare i64* @malloc(i32)

declare i32 @fflush(%_IO_FILE*)

declare i32 @__fxstat(i32, i32, %stat*)

declare i32 @__freading(%_IO_FILE*)

declare i64* @realloc(i64*, i32)

declare i32 @fchmod(i32, i32)

declare i64* @memmove(i64*, i64*, i32)

declare i32 @fsync(i32)

declare i32 @open(i8*, i32, ...)

declare i32 @fseeko(%_IO_FILE*, i32, i32)

declare i32 @fchown(i32, i32, i32)

declare void @perror(i8*)

declare %__dirstream* @fdopendir(i32)

declare i32 @futimens(i32, [2 x %timespec])

declare i32 @atoi(i8*)

declare i8* @strcat(i8*, i8*)

declare i32 @sigismember(%_TYPEDEF_sigset_t*, i32)

declare void @exit(i32)

declare i32 @fwrite(i64*, i32, i32, %_IO_FILE*)

declare i32 @sigaddset(%_TYPEDEF_sigset_t*, i32)

declare i32 @futimesat(i32, i8*, [2 x %timeval])

declare i16** @__ctype_b_loc()

declare i64 @_ITM_deregisterTMCloneTable(i64*)

declare i32 @__libc_start_main(i64, i32, i8**, void ()*, void ()*, void ()*)

declare void @__gmon_start__()

declare i64 @_ITM_registerTMCloneTable(i64*, i64)

declare void @__cxa_finalize(i64*)

declare i64 @__pseudo_call(i64)

declare void @__pseudo_return(i64)

declare void @__pseudo_branch(i64)

declare void @__pseudo_cond_branch(i1, i64)

declare void @__frontend_reg_store.fpr(i3, x86_fp80)

declare void @__frontend_reg_store.fpu_tag(i3, i2)

declare x86_fp80 @__frontend_reg_load.fpr(i3)

declare i2 @__frontend_reg_load.fpu_tag(i3)

; Function Attrs: nounwind readnone
declare i8 @llvm.ctpop.i8(i8) #0

declare i64 @__asm_hlt()

declare i128 @__asm_cvtsi2sd(i64)

declare i128 @__asm_movsd(i64)

declare i128 @__asm_mulsd(i128, i128)

declare i128 @__asm_divsd(i128, i128)

declare i64 @__asm_movq(i128)

declare i64 @__asm_movaps(i128)

declare void @__ppdasm_undefined_function__store_0(i64)

declare void @__ppdasm_undefined_function__store_1(i32)

attributes #0 = { nounwind readnone }
